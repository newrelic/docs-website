{
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-application-gateway-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95361,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.6607,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66046,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-containers-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95361,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.6607,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66046,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95361,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.6607,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66046,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-cost-management-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95361,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.6607,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66046,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-data-factory-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95361,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.6607,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66046,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95361,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.6607,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure API Management monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "API Management Service data"
      ],
      "title": "Azure API Management monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "c14b2e2440545d9d2c6d4d50df72dd404770d209",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-api-management-monitoring-integration/",
      "published_at": "2021-07-09T17:31:03Z",
      "updated_at": "2021-03-16T06:10:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Microsoft Azure API Management data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure API Management integration: New Relic polling interval: 5 minutes View and use data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event type Provider Service AzureApiManagementServiceSample AzureApiManagementService For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure API Management data for Service. API Management Service data Metric Unit Description totalRequests Count The total number of gateway requests in a given period. successfulRequests Count The total number of successful gateway requests in a given period. unauthorizedRequests Count The total number of unauthorized gateway requests in a given period. failedRequests Count The total number of failed gateway requests in a given period. otherRequests Count The total number of gateway requests in a given period that do not fall into the successful, unauthorized, or failed categories. durationMilliseconds Milliseconds The time between when API Management receives a request from a client and when it returns a response to the client. capacityPercent Percent Indicator of load on an API Management instance. eventHubTotalEvents Count The total number of events sent to EventHub from API Management in a given period. eventHubSuccessfulEvents Count The total number of successful EventHub events in a given period. eventHubTotalFailedEvents Count The total number of failed EventHub events in a given period. eventHubRejectedEvents Count The total number of rejected EventHub events (wrong configuration or unauthorized) in a given period. eventHubThrottledEvents Count The total number of throttled EventHub events in a given period. eventHubTimedoutEvents Count The total number of timed out EventHub events in a given period. eventHubDroppedEvents Count The total number of events skipped because of queue size limit reached in a given period. eventHubTotalBytesSentBytes Bytes The total size of EventHub events in bytes in a given period.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.03783,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> API Management monitoring <em>integration</em>",
        "sections": "<em>Azure</em> API Management monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic offers an integration for reporting your <em>Microsoft</em> <em>Azure</em> API Management data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your <em>Azure</em> service"
      },
      "id": "603ea20164441f8ed44e8872"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95361,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66046,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    },
    {
      "sections": [
        "Azure API Management monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "API Management Service data"
      ],
      "title": "Azure API Management monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "c14b2e2440545d9d2c6d4d50df72dd404770d209",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-api-management-monitoring-integration/",
      "published_at": "2021-07-09T17:31:03Z",
      "updated_at": "2021-03-16T06:10:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Microsoft Azure API Management data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure API Management integration: New Relic polling interval: 5 minutes View and use data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event type Provider Service AzureApiManagementServiceSample AzureApiManagementService For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure API Management data for Service. API Management Service data Metric Unit Description totalRequests Count The total number of gateway requests in a given period. successfulRequests Count The total number of successful gateway requests in a given period. unauthorizedRequests Count The total number of unauthorized gateway requests in a given period. failedRequests Count The total number of failed gateway requests in a given period. otherRequests Count The total number of gateway requests in a given period that do not fall into the successful, unauthorized, or failed categories. durationMilliseconds Milliseconds The time between when API Management receives a request from a client and when it returns a response to the client. capacityPercent Percent Indicator of load on an API Management instance. eventHubTotalEvents Count The total number of events sent to EventHub from API Management in a given period. eventHubSuccessfulEvents Count The total number of successful EventHub events in a given period. eventHubTotalFailedEvents Count The total number of failed EventHub events in a given period. eventHubRejectedEvents Count The total number of rejected EventHub events (wrong configuration or unauthorized) in a given period. eventHubThrottledEvents Count The total number of throttled EventHub events in a given period. eventHubTimedoutEvents Count The total number of timed out EventHub events in a given period. eventHubDroppedEvents Count The total number of events skipped because of queue size limit reached in a given period. eventHubTotalBytesSentBytes Bytes The total size of EventHub events in bytes in a given period.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.03783,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> API Management monitoring <em>integration</em>",
        "sections": "<em>Azure</em> API Management monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic offers an integration for reporting your <em>Microsoft</em> <em>Azure</em> API Management data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your <em>Azure</em> service"
      },
      "id": "603ea20164441f8ed44e8872"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-postgresql-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95361,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66069,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-event-hub-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95361,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66069,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-express-route-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.9536,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66069,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-firewalls-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.9536,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66069,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-front-door-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.9536,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66069,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-functions-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.9536,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66069,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-key-vault-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.9536,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66069,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-load-balancer-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.9536,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66069,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66045,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-logic-apps-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-machine-learning-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-power-bi-dedicated-capacities-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-redis-cache-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-bus-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-managed-instances-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-storage-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machine-scale-sets-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95358,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machines-scale-sets-monitoring-integration": [
    {
      "sections": [
        "Azure virtual machine scale sets monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Virtual machine scale sets ScaleSet data"
      ],
      "title": "Azure virtual machine scale sets monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2b25b6720032817e09a6e844210f020b3a4fc98b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machine-scale-sets-monitoring-integration/",
      "published_at": "2021-07-09T19:33:10Z",
      "updated_at": "2021-03-16T04:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Azure virtual machine scale sets data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure virtual machine scale sets integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. You can query and explore your data using the following event type: Entity Event type Provider ScaleSet AzureVirtualMachineScaleSetSample AzureVirtualMachineScaleSet For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure virtual machine scale sets data for ScaleSet. Virtual machine scale sets ScaleSet data Metric Unit Description cpuPercent Percent The percentage of allocated compute units that are currently in use by the Virtual Machine(s) networkInBytes Bytes The number of billable bytes received on all network interfaces by the Virtual Machine(s) (Incoming Traffic) networkOutBytes Bytes The number of billable bytes out on all network interfaces by the Virtual Machine(s) (Outgoing Traffic) diskReadBytes Bytes Bytes read from disk during monitoring period diskWriteBytes Bytes Bytes written to disk during monitoring period diskReadOperationsCountPerSecond CountPerSecond Disk Read IOPS diskWriteOperationsCountPerSecond CountPerSecond Disk Write IOPS cpuCreditsRemaining Count Total number of credits available to burst cpuCreditsConsumed Count Total number of credits consumed by the Virtual Machine dataDiskReadBytesCountPerSecond CountPerSecond Bytes/Sec read from a single disk during monitoring period dataDiskWriteBytesCountPerSecond CountPerSecond Bytes/Sec written to a single disk during monitoring period dataDiskReadOperationsCountPerSecond CountPerSecond Read IOPS from a single disk during monitoring period dataDiskWriteOperationsCountPerSecond CountPerSecond Write IOPS from a single disk during monitoring period dataDiskQueueDepth Count Data Disk Queue Depth(or Queue Length) osDiskReadBytesCountPerSecond CountPerSecond Bytes/Sec read from a single disk during monitoring period for OS disk osDiskWriteBytesCountPerSecond CountPerSecond Bytes/Sec written to a single disk during monitoring period for OS disk osDiskReadOperationsCountPerSecond CountPerSecond Read IOPS from a single disk during monitoring period for OS disk osDiskWriteOperationsCountPerSecond CountPerSecond Write IOPS from a single disk during monitoring period for OS disk osDiskQueueDepth Count OS Disk Queue Depth(or Queue Length) inboundFlows Count Inbound Flows are number of current flows in the inbound direction (traffic going into the VM) outboundFlows Count Outbound Flows are number of current flows in the outbound direction (traffic going out of the VM) inboundFlowsMaximumCreationRateCountPerSecond CountPerSecond The maximum creation rate of inbound flows (traffic going into the VM) outboundFlowsMaximumCreationRateCountPerSecond CountPerSecond The maximum creation rate of outbound flows (traffic going out of the VM) premiumDataDiskCacheReadHitPercent Percent Premium Data Disk Cache Read Hit premiumDataDiskCacheReadMissPercent Percent Premium Data Disk Cache Read Miss premiumOSDiskCacheReadHitPercent Percent Premium OS Disk Cache Read Hit premiumOSDiskCacheReadMissPercent Percent Premium OS Disk Cache Read Miss networkInTotalBytes Bytes The number of bytes received on all network interfaces by the Virtual Machine(s) (Incoming Traffic) networkOutTotalBytes Bytes The number of bytes out on all network interfaces by the Virtual Machine(s) (Outgoing Traffic)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1209.2839,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>virtual</em> <em>machine</em> <em>scale</em> <em>sets</em> <em>monitoring</em> <em>integration</em>",
        "sections": "<em>Azure</em> <em>virtual</em> <em>machine</em> <em>scale</em> <em>sets</em> <em>monitoring</em> <em>integration</em>",
        "tags": "Microsoft <em>Azure</em> <em>integrations</em>",
        "body": " data. Metric data This <em>integration</em> collects <em>Azure</em> <em>virtual</em> machine <em>scale</em> <em>sets</em> data for <em>ScaleSet</em>. <em>Virtual</em> machine <em>scale</em> <em>sets</em> <em>ScaleSet</em> data Metric Unit Description cpuPercent Percent The percentage of allocated compute units that are currently in use by the <em>Virtual</em> Machine(s) networkInBytes Bytes"
      },
      "id": "603ea1cfe7b9d2a8342a0819"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 148.57777,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs <em>monitoring</em> <em>integration</em>",
        "sections": "<em>Azure</em> VMs <em>monitoring</em> <em>integration</em>",
        "tags": "Microsoft <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>monitoring</em> provides an <em>integration</em> for Microsoft <em>Azure</em> <em>Virtual</em> <em>Machines</em> (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this <em>integration</em> and describes the data that can be captured. Features New Relic&#x27;s <em>integration</em>"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Introduction to the Kubernetes integration",
        "Get started: Install the Kubernetes integration",
        "Tip",
        "Why it matters",
        "Navigate all your Kubernetes events",
        "Bring your cluster logs to New Relic",
        "Check the source code"
      ],
      "title": "Introduction to the Kubernetes integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "c641d1367f1f8fd2b589a2707112759becae609b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/introduction-kubernetes-integration/",
      "published_at": "2021-07-14T01:51:19Z",
      "updated_at": "2021-06-02T01:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration gives you full observability into the health and performance of your environment, no matter whether you run Kubernetes on-premises or in the cloud. With our cluster explorer, you can cut through layers of complexity to see how your cluster is performing, from the heights of the control plane down to applications running on a single pod. one.newrelic.com > Kubernetes cluster explorer: The cluster explorer is our powerful, fully visual answer to the challenges associated with running Kubernetes at a large scale. You can see the power of the Kubernetes integration in the cluster explorer, where the full picture of a cluster is made available on a single screen: nodes and pods are visualized according to their health and performance, with pending and alerting nodes in the innermost circles. Predefined alert conditions help you troubleshoot issues right from the start. Clicking each node reveals its status and how each app is performing. Get started: Install the Kubernetes integration We have an automated installer to help you with many types of installations: servers, virtual machines, and unprivileged environments. It can also help you with installations in managed services or platforms, but you'll need to review a few preliminary notes before getting started. Here's what the automated installer does: Asks for the cluster name and namespace of the integration. Asks for additional setup options, such as Kube state metrics. Asks for the installation method: manifest file or Helm. Generates either the manifest or Helm chart. Read the install docs Start the installer Tip If your New Relic account is in the EU region, access the automated installer from one.eu.newrelic.com. Why it matters Governing the complexity of Kubernetes can be challenging; there's so much going on at any given moment, with containers being created and deleted in a matter of minutes, applications crashing, and resources being consumed unexpectedly. Our integration helps you navigate Kubernetes abstractions across on-premises, cloud, and hybrid deployments. In New Relic, you can build your own charts and query all your Kubernetes data, which our integration collects by instrumenting the container orchestration layer. This gives you additional insight into nodes, namespaces, deployments, replica sets, pods, and containers. one.newrelic.com > Dashboards: Using the query builder you can turn any query on Kubernetes data to clear visuals. With the Kubernetes integration you can also: Link your APM data to Kubernetes to measure the performance of your web and mobile applications, with metrics such as request rate, throughput, error rate, and availability. Monitor services running on Kubernetes, such as Apache, NGINX, Cassandra, and many more (see our tutorial for monitoring Redis on Kubernetes). Create new alert policies and alert conditions based on your Kubernetes data, or extend the predefined alert conditions. These features are in addition to the data New Relic already reports for containerized processes running on instrumented hosts. Navigate all your Kubernetes events The Kubernetes events integration, which is installed separately, watches for events happening in your Kubernetes clusters and sends those events to New Relic. Events data is then visualized in the cluster explorer. To set it up, check the Kubernetes events box in step 3 of our install wizard, or follow the instructions. one.newrelic.com > Kubernetes cluster explorer > Events: Browse and filter all your Kubernetes events, and dig into application logs and infrastructure data. Bring your cluster logs to New Relic Our Kubernetes plugin for log monitoring can collect all your cluster's logs and send them to our platform, so that you can set up new alerts and charts. To set it up, check the Log data box in step 3 of our install wizard, or follow the instructions. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or you can create your own fork and build it. For more information, see the README.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 137.80246,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Kubernetes <em>integration</em>",
        "sections": "Introduction to the Kubernetes <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": " from the start. Clicking each node reveals its status and how each app is performing. Get started: Install the Kubernetes <em>integration</em> We have an automated installer to help you with many types of installations: servers, <em>virtual</em> <em>machines</em>, and unprivileged environments. It can also help you"
      },
      "id": "6043a212196a678d86960f46"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-network-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95357,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66066,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66042,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration": [
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66066,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66042,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    },
    {
      "sections": [
        "Azure API Management monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "API Management Service data"
      ],
      "title": "Azure API Management monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "c14b2e2440545d9d2c6d4d50df72dd404770d209",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-api-management-monitoring-integration/",
      "published_at": "2021-07-09T17:31:03Z",
      "updated_at": "2021-03-16T06:10:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Microsoft Azure API Management data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure API Management integration: New Relic polling interval: 5 minutes View and use data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event type Provider Service AzureApiManagementServiceSample AzureApiManagementService For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure API Management data for Service. API Management Service data Metric Unit Description totalRequests Count The total number of gateway requests in a given period. successfulRequests Count The total number of successful gateway requests in a given period. unauthorizedRequests Count The total number of unauthorized gateway requests in a given period. failedRequests Count The total number of failed gateway requests in a given period. otherRequests Count The total number of gateway requests in a given period that do not fall into the successful, unauthorized, or failed categories. durationMilliseconds Milliseconds The time between when API Management receives a request from a client and when it returns a response to the client. capacityPercent Percent Indicator of load on an API Management instance. eventHubTotalEvents Count The total number of events sent to EventHub from API Management in a given period. eventHubSuccessfulEvents Count The total number of successful EventHub events in a given period. eventHubTotalFailedEvents Count The total number of failed EventHub events in a given period. eventHubRejectedEvents Count The total number of rejected EventHub events (wrong configuration or unauthorized) in a given period. eventHubThrottledEvents Count The total number of throttled EventHub events in a given period. eventHubTimedoutEvents Count The total number of timed out EventHub events in a given period. eventHubDroppedEvents Count The total number of events skipped because of queue size limit reached in a given period. eventHubTotalBytesSentBytes Bytes The total size of EventHub events in bytes in a given period.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.03783,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> API Management monitoring <em>integration</em>",
        "sections": "<em>Azure</em> API Management monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic offers an integration for reporting your <em>Microsoft</em> <em>Azure</em> API Management data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your <em>Azure</em> service"
      },
      "id": "603ea20164441f8ed44e8872"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-07-09T17:40:30Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 176.95357,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-07-09T19:27:49Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66066,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-07-09T19:27:50Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 174.66042,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations": [
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Tip",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-07-09T17:41:21Z",
      "updated_at": "2021-03-13T03:33:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as New Relic APM's .NET support for Azure. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.46478,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as New Relic APM&#x27;s .NET support for <em>Azure</em>. Tip To use <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "82db3eae120c4318365cf0d0e5bfee69930b969f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2021-07-09T08:41:44Z",
      "updated_at": "2021-03-13T03:47:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.07062,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "6044e560196a671d6f960f72"
    },
    {
      "sections": [
        "Azure integration metrics",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "762631e1209bb9abb60f1ea8b185a6def61735b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2021-07-09T21:25:31Z",
      "updated_at": "2021-03-16T15:55:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.8288,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunSuccessLatency runSuccessLatencySeconds <em>Azure</em> Logic"
      },
      "id": "603e8a8928ccbcacc0eba74e"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics": [
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-07-09T08:37:37Z",
      "updated_at": "2021-06-20T12:42:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 165.87292,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    },
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Tip",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-07-09T17:41:21Z",
      "updated_at": "2021-03-13T03:33:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as New Relic APM's .NET support for Azure. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.46478,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as New Relic APM&#x27;s .NET support for <em>Azure</em>. Tip To use <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "82db3eae120c4318365cf0d0e5bfee69930b969f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2021-07-09T08:41:44Z",
      "updated_at": "2021-03-13T03:47:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.07062,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "6044e560196a671d6f960f72"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations": [
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-07-09T08:37:37Z",
      "updated_at": "2021-06-20T12:42:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 165.87292,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "82db3eae120c4318365cf0d0e5bfee69930b969f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2021-07-09T08:41:44Z",
      "updated_at": "2021-03-13T03:47:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 131.07062,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "6044e560196a671d6f960f72"
    },
    {
      "sections": [
        "Azure integration metrics",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "762631e1209bb9abb60f1ea8b185a6def61735b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2021-07-09T21:25:31Z",
      "updated_at": "2021-03-16T15:55:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.8288,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunSuccessLatency runSuccessLatencySeconds <em>Azure</em> Logic"
      },
      "id": "603e8a8928ccbcacc0eba74e"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations": [
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-07-09T08:37:37Z",
      "updated_at": "2021-06-20T12:42:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 165.87291,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    },
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Tip",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-07-09T17:41:21Z",
      "updated_at": "2021-03-13T03:33:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as New Relic APM's .NET support for Azure. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.46478,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as New Relic APM&#x27;s .NET support for <em>Azure</em>. Tip To use <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Azure integration metrics",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "762631e1209bb9abb60f1ea8b185a6def61735b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2021-07-09T21:25:31Z",
      "updated_at": "2021-03-16T15:55:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.8288,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunSuccessLatency runSuccessLatencySeconds <em>Azure</em> Logic"
      },
      "id": "603e8a8928ccbcacc0eba74e"
    }
  ],
  "/docs/integrations/mlops-integrations/algorithmia-mlops-integration": [
    {
      "sections": [
        "StatsD monitoring integration",
        "Requirements",
        "Install",
        "Install for Kubernetes",
        "Kubernetes manifest examples",
        "Configure",
        "Tip",
        "Example of custom configuration",
        "Docker: overwrite default configuration",
        "Kubernetes: overwrite default configuration",
        "Metric format",
        "Metric types",
        "Counter",
        "Gauge",
        "Timer",
        "Add tags (attributes)",
        "Add default tags that apply to all metrics",
        "Add metric-level tags",
        "Create alerts",
        "Alert example",
        "Find and use data",
        "Check the source code"
      ],
      "title": "StatsD monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "85d86411ef83d98a2d2ab602f1259b71864e056d",
      "image": "https://docs.newrelic.com/static/9c86375ad0ec12433df78b2116819aab/c1b63/statsd-nrql-alert-condition-example.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/statsd-monitoring-integration-version-2/",
      "published_at": "2021-07-13T16:53:54Z",
      "updated_at": "2021-07-13T16:53:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our StatsD integration lets you easily get StatsD-format data into New Relic. You can also add any arbitrary tags (key-value pairs) to your data. Once your metrics are in New Relic, you can query your data and create custom charts and dashboards. Requirements This integration uses our Metric API and our Event API to ingest data. To use these APIs, you'll need an Insert API key. The integration adheres to the Metric API requirements and data limits. The default rate limit is 100,000 data points per minute (DPM). If you think you're missing metrics or sending more than 100K DPM, see Request data changes. To see if your account is hitting the rate limit, run the following NRQL query: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature ='Metrics' FACET category, message LIMIT 100 since 1 day ago Copy Install This section will explain how to do a standard install. If you want to run StatsD in Kubernetes, see Kubernetes install. To install the StatsD integration, run the following command and include your New Relic account ID and New Relic Insert API key. This generates a TOML configuration file used by gostatsd. docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy If your account is in the EU data center region, add this to the above command: -e NR_EU_REGION=true \\ Copy After installing, you can: Do optional additional configuration Define your metrics Add custom tags to your data Create alerts Install for Kubernetes Here are examples of Kubernetes manifests for deployment and service objects: Kubernetes manifest examples Below are examples of Kubernetes manifests to deploy StatsD in a Kubernetes environment and create a StatsD service named newrelic-statsd. You need to insert your account ID and your license key. deployment.yml: apiVersion: apps/v1 kind: Deployment metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: selector: matchLabels: app: newrelic-statsd replicas: 2 revisionHistoryLimit: 2 template: metadata: labels: app: newrelic-statsd spec: containers: - name: newrelic-statsd image: newrelic/nri-statsd:2.0.0 env: - name: NR_ACCOUNT_ID value: \"NEW_RELIC_ACCOUNT_ID\" - name: NR_API_KEY value: \"NEW_RELIC_LICENSE_KEY\" Copy service.yml: apiVersion: v1 kind: Service metadata: name: newrelic-statsd namespace: tooling labels: app: newrelic-statsd spec: type: ClusterIP ports: - name: newrelic-statsd port: 80 targetPort: 8125 protocol: UDP selector: app: newrelic-statsd Copy For configuration details, see Kubernetes configuration. Configure In the install procedure, you run nri-statsd with environment variables, and this generates a TOML configuration file. Additionally, you can set these configuration options: Configuration options Description expiry-interval string If a metric is not updated for this amount of time, we stop reporting that metric. Default is 5m. If you want to send the metrics only if the value was updated between the flush intervals, configure this to 1ms. To never expire metrics, set it to 0. percent-threshold list of integers Specifies the percentiles used for metrics aggregation. Default: 90. metrics-addr string Indicates address on which to listen for metrics. Default: :8125. Tip To ensure FedRAMP compliance when using the StatsD integration you must define the following endpoints in the custom configuration: address = 'https://gov-insights-collector.newrelic.com/v1/accounts/ $NR_ACCOUNT_ID/events' Copy address-metrics = 'https://gov-infra-api.newrelic.com/metric/v1' Copy Here are some examples of customizing configuration by overwriting the default configuration: Example of custom configuration # Specify after how long do we expire metrics, default:5m expiry-interval = '1ms' # percent-threshold specify a list of percentiles for metrics aggregation, default:90 percent-threshold = [90, 99] backends='newrelic' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://insights-collector.newrelic.com/v1/accounts/$NR_ACCOUNT_ID/events' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy Disable timer sub-metrics: By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. If you want to disable those metrics you can do it by adding a disabled-sub-metrics configuration section and set true for the ones you want disabled. Here's an example: # disabled-sub-metrics configuration section allows disabling timer sub-metrics [disabled-sub-metrics] # Regular metrics count=false count-per-second=false mean=false median=false lower=false upper=false stddev=false sum=false sum-squares=false # Percentile metrics count-pct=false mean-pct=false sum-pct=false sum-squares-pct=false lower-pct=false upper-pct=false Copy Docker: overwrite default configuration To overwrite the default nri-statsd configuration while running in a container, you can mount a configuration file inside the container. You can adopt the following template as needed for your situation. Example: backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address-metrics = 'https://metric-api.newrelic.com/metric/v1' api-key = 'NEW_RELIC_API_KEY' Copy To run the container with the file mounted in the appropriate path: docker run \\ ... -v ${PWD}/nri-statsd.toml:/etc/opt/newrelic/nri-statsd.toml \\ ... newrelic/nri-statsd:2.0.0 Copy Kubernetes: overwrite default configuration The best approach to configure nri-statsd running in Kubernetes is to use a configMap and mount the configMap into the container. (This is a similar process to mounting the configuration file in Docker.) Example: apiVersion: v1 kind: ConfigMap metadata: name: nri-statsd-config namespace: default data: nri-statsd.toml: | backends='newrelic' flush-interval='10s' [newrelic] # flush types supported: metrics, insights, infra flush-type = 'metrics' transport = 'default' address = 'https://metric-api.newrelic.com/metric/v1' api-key = '$NEW_RELIC_API_KEY' Copy To use the configMap, declare a volume on your deployment spec template and then declare a volumeMount on your container spec. Example: apiVersion: apps/v1 kind: Deployment spec: template: spec: containers: .... volumeMounts: - mountPath: /etc/opt/newrelic/ name: nri-statsd-config volumes: - name: nri-statsd-config configMap: name: nri-statsd-config Copy Metric format The integration receives metrics using the StatsD protocol. Optionally, the sample rate can be configured and tags can be added. Here's the metric data format we use: <metric name>:<value>|<type>|@<sample rate>|#<tags> Copy Here are explanations of these fields: Field name Description < metric name> string Required. Name of the metric. < value> string Required. The metric type: c = counter g = gauge ms = timer @ < sample rate> float Optional for simple counters or timer counters. When many metrics must be sent, you can use sampling to reduce network traffic. The downside is a reduction in the resolution of the data. An example of how this would work for sample rates below 1: If you set this to 0.1, the counter would send a measurement one out of every 10 times. # < tags> string Optional. Tags attached to your metrics are converted into attributes (key-value pairs). For more on tagging options, see Tags. Metric types Here are the types of metrics and how to format them: Counter A counter measures the number of occurrences of an event. Examples include cache hits per reporting interval and the number of threads created per reporting interval. A counter can be incremented or decremented during the same flush interval by adding a sign to the value. In the following example, the counter value will be 2: counter:4|c counter:-2|c Copy At each flush, the current count is sent and reset to 0. If the count is not updated, at the next flush it will send the value 0. You can opt to disable this behavior by setting expiry-interval to 1ms. Heres an example of a counter that is being sampled 1 out of 10 times: counter:4|c@0.1 Copy Gauge A gauge represents a value that can increase or decrease with time. Examples of gauges include temperature, CPU usage, and memory. Here's an example: temperature:40|g Copy If the gauge is not updated, at the next flush it will send the previous value. You can opt to disable this behavior by setting expiry-interval to 1ms. Timer The timer metric type measures timing data. By default, nri_statsd calculates the following for timer metrics: standard deviation, mean, median, sum, lower, and upper bounds for the flush interval. These are sent as sub-metrics in the following format: <metric_base_name>.std_dev <metric_base_name>.median <metric_base_name>.summary <metric_base_name>.sum_squares <metric_base_name>.mean <metric_base_name>.per_second Copy The configured percentiles will generate the following metrics. The percentile threshold value will be attached as a tag. <metric_base_name>.sum_squares.percentiles <metric_base_name>.sum.percentiles <metric_base_name>.count.percentiles <metric_base_name>.upper.percentiles <metric_base_name>.mean.percentiles Copy The percentile threshold can be tweaked with the percent-threshold config option. These can be controlled through the disabled-sub-metrics configuration section. Add tags (attributes) You can add tags to your data, which we save as attributes (key-value pairs). There are two options for adding tags: Add default tags that apply to all metrics: These apply to all metrics. They are fixed and don't change over time. Add metric-level tags: These apply to specific metrics and allow the value to be changed between two submits. Add default tags that apply to all metrics Add tags to metrics and events by defining an environment variable in the startup command. Here's an example that would create two tags: -e TAGS=\"environment:production region:us\" Copy Here's that environment variable used in the startup command: docker run \\ -d --restart unless-stopped \\ --name newrelic-statsd \\ -h $(hostname) \\ -e NR_ACCOUNT_ID=YOUR_ACCOUNT_ID \\ -e NR_API_KEY=YOUR_INSERT_API_KEY \\ -e TAGS=\"environment:production region:us\" \\ -p 8125:8125/udp \\ newrelic/nri-statsd:2.0.0 Copy Add metric-level tags When defining the metric format, you can add tags using this format: <bucket name>:<value>|<type>|#<tags> Copy In this example, <tags> is a comma-separated list of tags. Tags format is: simple or key:value. Here's an example NRQL query that includes a custom tag: SELECT count(*) FROM Metric WHERE environment = 'production' Copy Create alerts You can alert on StatsD data using NRQL alert conditions. Alert example This procedure walks you through sending some sample data and then creating an alert condition using that data. First, send this data to New Relics StatsD container: echo \"prod.test.num:32|g\" | nc -v -w 1 -u localhost 8125 Copy Next, create a NRQL alert condition using this query: SELECT latest(prod.test.num) FROM Metric WHERE metricName = 'prod.test.num' Copy Here's an image showing creating this NRQL alert condition. Notice that the sample data sent in is represented by the blue dot on the upper right of the chart. Now we can create the alert condition with these settings: When you create the NRQL alert condition, be sure to set the Condition name. If a metric with a value above 50 is sent, then an incident is created and notified. The incident is closed automatically after 24 hours. To test that the alert is working, run this command: echo \"prod.test.num:60|g\" | nc -v -w 1 -u localhost 8125 Copy Find and use data To query your data, you'd use any New Relic query option. For example, you might run a NRQL query like: SELECT count(*) FROM Metric WHERE metricName = 'myMetric' and environment = 'production' Copy For more on how to query the Metric data type, see Query metric data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 47.62105,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "StatsD monitoring <em>integration</em>",
        "sections": "StatsD monitoring <em>integration</em>",
        "tags": "<em>Integrations</em>"
      },
      "id": "6043a32364441fa554378eee"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/prometheus-remote-write-integration/",
      "sections": [
        "Prometheus remote write integration",
        "Why it matters",
        "Compatibility",
        "Scale your data and get moving quickly",
        "What's next",
        "For more help"
      ],
      "published_at": "2021-07-14T15:02:55Z",
      "title": "Prometheus remote write integration",
      "updated_at": "2021-07-10T23:03:06Z",
      "type": "docs",
      "external_id": "aaf44eb9ee0ffc4b6f751ca18c5dd5b34cd11649",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use the Prometheus remote write integration to get data flowing into New Relic. Once you integrate, your data will be visible in query-based dashboards (and other query results), often within about five minutes. Why it matters Unlike Kubernetes and Docker OpenMetrics integrations, which scrape data from Prometheus endpoints, the remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. You can leverage the full range of options for setup and management, from raw data to queries and dashboards and beyond. With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional) Compatibility New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Scale your data and get moving quickly Once logged in to New Relic, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. View your data. What's next Ready to get started? Read the setup documentation. Configure a Prometheus data source in Grafana. Set up the integration on New Relic US EU For more help Recommendations for learning more: See the Docs site's landing page for Infrastructure integrations documentation. Browse New Relic's Explorers Hub for community discussions about our Infrastructure integrations. Find additional help or file a support ticket. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 41.626244,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Prometheus remote write <em>integration</em>",
        "sections": "Prometheus remote write <em>integration</em>",
        "body": "You can use the Prometheus remote write integration to get data flowing into New Relic. Once you integrate, your data will be visible in query-based dashboards (and other query results), often within about five minutes. Why it matters Unlike Kubernetes and Docker OpenMetrics <em>integrations</em>, which"
      },
      "id": "60ea272a196a670c6038adbf"
    },
    {
      "sections": [
        "Kafka monitoring integration",
        "Compatibility and requirements",
        "callout.info",
        "Prepare for the installation",
        "Autodiscovery",
        "Bootstrap",
        "Zookeeper",
        "Tip",
        "Topic listing",
        "Broker monitoring (JMX)",
        "Important",
        "Producer/consumer monitoring (JMX)",
        "Connectivity requirements",
        "Install and activate",
        "ECS",
        "Kubernetes",
        "Linux installation",
        "Windows installation",
        "Configure the integration",
        "Commands",
        "Arguments",
        "Labels",
        "Example configuration",
        "Example: Single agent deployment",
        "Example: Multiple agent deployment",
        "Example: Offset collection",
        "Find and use data",
        "Metric data",
        "KafkaBrokerSample event",
        "KafkaConsumerSample event",
        "KafkaProducerSample event",
        "KafkaTopicSample event",
        "KafkaOffsetSample event",
        "Inventory data",
        "Troubleshooting",
        "Duplicate data being reported",
        "Integration is logging errors 'zk: node not found'",
        "JMX connection errors",
        "Kerberos authentication failing",
        "Check the source code"
      ],
      "title": "Kafka monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "cf7f88b89a6a882c1148850b012552990b425330",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/kafka-monitoring-integration/",
      "published_at": "2021-07-14T01:51:20Z",
      "updated_at": "2021-07-10T01:51:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Kafka on-host integration reports metrics and configuration data from your Kafka service. We instrument all the key elements of your cluster, including brokers (both ZooKeeper and Bootstrap), producers, consumers, and topics. Read on to install the Kafka integration, and to see what data it collects. To monitor Kafka with our Java agent, see Instrument Kafka message queues. Compatibility and requirements Our integration is compatible with Kafka versions 0.8 or higher. callout.info Topic data collection is supported for Kafka versions 0.11.0.0 or higher. Before installing the integration, make sure that you meet the following requirements: If Kafka is not running on Kubernetes or Amazon ECS, you must install the infrastructure agent on a host that's running Kafka. Otherwise: If running on Kubernetes, see these requirements. If running on ECS, see these requirements. Java 8 or higher JMX enabled on all brokers Java-based consumers and producers only, and with JMX enabled Total number of monitored topics must be fewer than 10000 For Kafka running on Kubernetes, see the Kubernetes requirements. Prepare for the installation Kafka is a complex piece of software that is built as a distributed system. For this reason, youll need to ensure that the integration can contact all the required hosts and services so the data is collected correctly. Autodiscovery Given the distributed nature of Kafka, the actual number and list of brokers is usually not fixed by the configuration, and it is instead quite dynamic. For this reason, the Kafka integration offers two mechanisms to perform automatic discovery of the list of brokers in the cluster: Bootstrap and Zookeeper. The mechanism you use depends on the setup of the Kafka cluster being monitored. Bootstrap With the bootstrap mechanism, the integration uses a bootstrap broker to perform the autodiscovery. This is a broker whose address is well known and that will be asked for any other brokers it is aware of. The integration needs to be able to contact this broker in the address provided in the bootstrap_broker_host parameter for bootstrap discovery to work. Zookeeper Alternatively, the Kafta integration can also talk to a Zookeeper server in order to obtain the list of brokers. To do this, the integration needs to be provided with the following: The list of Zookeeper hosts to contact (zookeeper_hosts). The proper authentication secrets to connect with the hosts. Together with the list of brokers it knows about, Zookeeper will also advertise which connection mechanisms are supported by each broker. You can configure the Kafka integration to try directly with one of these mechanisms with the preferred_listener parameter. If this parameter is not provided, the integration will try to contact the brokers with all the advertised configurations until one of them succeeds. Tip The integration will use Zookeeper only for discovering brokers and will not retrieve metrics from it. Topic listing To correctly list the topics processed by the brokers, the integration needs to to contact brokers over the Kafka protocol. Depending on how the brokers are configured, this might require setting up SSL and/or SASL to match the broker configuration. Broker monitoring (JMX) The Kafka integration queries JMX, a standard Java extension for exchanging metrics in Java applications. JMX is not enabled by default in Kafka brokers, and you need to enable it for metrics collection to work properly. JMX requires RMI to be enabled, and the RMI port needs to be set to the same port as JMX. You can configure JMX to use username/password authentication, as well as SSL. If such features have been enabled in the broker's JMX settings, you need to configure the integration accordingly. Important We do not recommend enabling anonymous and/or unencrypted JMX/RMI access on public or untrusted network segments because this poses a big security risk. Producer/consumer monitoring (JMX) Producers and consumers written in Java can also be monitored through the same mechanism (JMX). JMX needs to be enabled and configured on those applications where it is not enabled by default. Non-Java producers and consumers do not support JMX and are therefore not supported by the Kafka integration. Connectivity requirements As a summary, the integration needs to be configured and allowed to connect to: Hosts listed in zookeeper_hosts over the Zookeeper protocol, using the Zookeeper authentication mechanism (if autodiscover_strategy is set to zookeeper). Hosts defined in bootstrap_broker_host over the Kafka protocol, using the Kafka brokers authentication/transport mechanisms (if autodiscover_strategy is set to bootstrap). All brokers in the cluster over the Kafka protocol and port, using the Kafka brokers' authentication/transport mechanisms. All brokers in the cluster over the JMX protocol and port, using the authentication/transport mechanisms specified in the JMX configuration of the brokers. All producers/consumers specified in producers and consumers over the JMX protocol and port, if you want producer/consumer monitoring. JMX settings for the consumer must be the same as for the brokers. Important For the cloud: By default, Security Groups (and their equivalents in other cloud providers) in AWS do not have the required ports open by default. JMX requires two ports in order to work: the JMX port and the RMI port. These can be set to the same value when configuring the JVM to enable JMX and must be open for the integration to be able to connect to and collect metrics from brokers. Install and activate To install the Kafka integration, choose your setup: ECS See Monitor service running on ECS. Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Copy of the sample configuration file: sudo cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml file as described in the configuration settings. Restart the Infrastructure agent. Windows installation Download the nri-kafka installer image from: http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-kafka/nri-kafka-amd64-installer.exe To install from the Windows command prompt, run: PATH\\TO\\nri-kafka-amd64-installer.exe Copy In the Integrations directory, C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\, create a copy of the sample configuration file by running: cp kafka-config.yml.sample kafka-config.yml Copy Edit the kafka-config.yml configuration as described in the configuration settings. Restart the infrastructure agent. Additional notes: Advanced: It's also possible to install the integration from a tarball file. This gives you full control over the installation and configuration process. On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. The entire environment can be monitored remotely or on any node in that environment. There are several ways to configure the integration, depending on how it was installed: If enabled via Kubernetes: see Monitor services running on Kubernetes. If enabled via Amazon ECS: see Monitor services running on ECS. If installed on-host: edit the config in the integration's YAML config file, kafka-config.yml. For examples of typical configurations, see the example configurations. Important With secrets management, you can configure on-host integrations with New Relic infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The configuration accepts the following commands: inventory: collects configuration status metrics: collects performance metrics consumer_offset: collects consumer group offset data Arguments The configuration accepts the following arguments: General arguments: cluster_name: user-defined name to uniquely identify the cluster being monitored. Required. kafka_version: the version of the Kafka broker you're connecting to, used for setting optimum API versions. Defaults to 1.0.0. Versions older than 1.0.0 may be missing some features. autodiscover_strategy: the method of discovering brokers. Options are zookeeper or bootstrap. Defaults to zookeeper Zookeeper autodiscovery arguments (only relevant when autodiscover_strategy is zookeeper): zookeeper_hosts: the list of Apache ZooKeeper hosts (in JSON format) that need to be connected. zookeeper_auth_scheme: the ZooKeeper authentication scheme that is used to connect. Currently, the only supported value is digest. If omitted, no authentication is used. zookeeper_auth_secret: the ZooKeeper authentication secret that is used to connect. Should be of the form username:password. Only required if zookeeper_auth_scheme is specified. zookeeper_path: the Zookeeper node under which the Kafka configuration resides. Defaults to /. preferred_listener: use a specific listener to connect to a broker. If unset, the first listener that passes a successful test connection is used. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Bootstrap broker discovery arguments (only relevant when autodiscover_strategy is bootstrap): bootstrap_broker_host: the host for the bootstrap broker. bootstrap_broker_kafka_port: the Kafka port for the bootstrap broker. bootstrap_broker_kafka_protocol: the protocol to use to connect to the bootstrap broker. Supported values are PLAINTEXT, SASL_PLAINTEXT, SSL, and SASL_SSL. Note: The SASL_* protocols only support Kerberos (GSSAPI) authentication. Default: PLAINTEXT. bootstrap_broker_jmx_port: the JMX port to use for collection. bootstrap_broker_jmx_user: the JMX user to use for collection. bootstrap_broker_jmx_password: the JMX password to use for collection. Producer and consumer collection: producers: producers to collect. For each provider a name, hostname, port, username, and password can be provided in JSON form. name is the producers name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. consumers: consumers to collect. For each consumer a name, hostname, port, username, and password can be specified in JSON form. name is the consumers name as it appears in Kafka. hostname, port, username, and password are optional and use the default if unspecified. JMX connection options: default_jmx_host: the default host to collect JMX metrics. If the host field is omitted from a producer or consumer configuration, this value will be used. default_jmx_port: the default port to collect JMX metrics. If the port field is omitted from a producer or consumer configuration, this value will be used. default_jmx_user: the default user that is connecting to the JMX host to collect metrics. This field should only be used if all brokers have a non-default username. If the username field is omitted from a producer or consumer configuration, this value will be used. default_jmx_password: the default password to connect to the JMX host. This field should only be used if all brokers have a non-default password. If the password field is omitted from a producer or consumer configuration, this value will be used. key_store: the filepath of the keystore containing the JMX client's SSL certificate. key_store_password: the password for the JMX SSL key store. trust_store: the filepath of the trust keystore containing the JMX server's SSL certificate. trust_store_password: the password for the JMX trust store. timeout: the timeout for individual JMX queries in milliseconds. Default: 10000. Broker connection options: tls_ca_file: the certificate authority file for SSL and SASL_SSL listeners, in PEM format. tls_cert_file: the client certificate file for SSL and SASL_SSL listeners, in PEM format. tls_key_file: the client key file for SSL and SASL_SSL listeners, in PEM format. tls_insecure_skip_verify: skip verifying the server's certificate chain and host name sasl_mechanism: the type of SASL authentication to use. Supported options are SCRAM-SHA-512, SCRAM-SHA-256, PLAIN, and GSSAPI. sasl_gssapi_realm: kerberos realm. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_service_name: kerberos service name. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_username: kerberos username. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_key_tab_path: path to the kerberos keytab. Required for SASL_SSL or SASL_PLAINTEXT sasl_gssapi_kerberos_config_path: path to the kerberos config file. Default: /etc/krb5.conf Collection filtering: collect_broker_topic_data: signals if broker and topic metrics are collected. Options are true or false, defaults to true. Should only be set to false when monitoring only producers and consumers, and topic_mode is set to all. local_only_collection: collect only the metrics related to the configured bootstrap broker. Only used if autodiscover_strategy is bootstrap. Default: false consumer_group_regex: regex pattern that matches the consumer groups to collect offset statistics for. This is limited to collecting statistics for 300 consumer groups. Note: consumer_groups has been deprecated, use this argument instead. topic_mode: determines how many topics we collect. Options are all, none, list, or regex. collect_topic_size: collect the metric Topic size. Options are true or false, defaults to false. topic_size is a resource-intensive metric to collect. topic_list: array of topic names to monitor. Only in effect if topic_mode is set to list. topic_regex: regex pattern that matches the topic names to monitor. Only in effect if topic_mode is set to regex. topic_bucket: used to split topic collection across multiple instances. Should be of the form <bucket number>/<number of buckets>. Default: 1/1. Labels Labels are optional tags which help to identify collection data. Some examples are included below. env: label to identify the environment. For example: production. role: label to identify which role is accessing the data. Example configuration Tip For more details on configuration parameters, see the kafka-config.yml.sample config file on GitHub. Example: Single agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: Regex topic_regex: 'topic_[0-9]+' labels: env: production role: kafka Copy Example: Multiple agent deployment Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node, the producer node, and the consumer node. Brokers Single ZooKeeper node Single producer: Name: my-producer Host: my-producer.my.localnet JMX Port: 9989 Single consumer: Name: my-consumer Host: my-consumer.my.localnet JMX Port: 9987 Example kafka-config.yml config file for this environment: ZooKeeper node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List collect_topic_size: false topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka - name: kafka-inventory command: inventory arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Producer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: producers: '[{\"name\": \"my-producer\", \"host\": \"my-producer.my.localnet\", \"port\": 9989}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Consumer node configuration: integration_name: com.newrelic.kafka instances: - name: kafka-metrics command: metrics arguments: consumers: '[{\"name\": \"my-consumer\", \"host\": \"my-consumer.my.localnet\", \"port\": 9987}]' topic_mode: List topic_list: '[\"topic_1\", \"topic_2\"]' labels: env: production role: kafka Copy Example: Offset collection Let's consider an environment with the following structure. For this environment, assume the infrastructure agent is installed on the ZooKeeper node. Important Due to the load that collecting offset data can put on the Kafka environment, collecting offsets is done independently of normal metric and inventory data collection. We recommend installing the offset collection only on one node. Brokers Single ZooKeeper node Consumers Consumer Groups consumer_group_a1 consumer_group_a2 consumer_group_b1 For this example environment, if you want to monitor offsets for only consumer_group_a1 and consumer_group_a2, a sample config might look like this: integration_name: com.newrelic.kafka - name: kafka-consumer-offsets command: consumer_offset arguments: zookeeper_hosts: '[{\"host\": \"localhost\", \"port\": 2181}]' consumer_group_regex: 'consumer_group_a.' labels: env: production role: kafka Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data Data from this service is reported to an integration dashboard. Kafka data is attached to the following event types: KafkaBrokerSample KafkaTopicSample KafkaProducerSample KafkaConsumerSample KafkaOffsetSample You can query this data for troubleshooting purposes or to create charts and dashboards. For more on how to find and use your data, see Understand integration data. Metric data The Kafka integration collects the following metric data attributes. Each metric name is prefixed with a category indicator and a period, such as broker. or consumer.. KafkaBrokerSample event Metric Description broker.bytesWrittenToTopicPerSecond Number of bytes written to a topic by the broker per second. broker.IOInPerSecond Network IO into brokers in the cluster in bytes per second. broker.IOOutPerSecond Network IO out of brokers in the cluster in bytes per second. broker.logFlushPerSecond Log flush rate. broker.messagesInPerSecond Incoming messages per second. follower.requestExpirationPerSecond Rate of request expiration on followers in evictions per second. net.bytesRejectedPerSecond Rejected bytes per second. replication.isrExpandsPerSecond Rate of replicas joining the ISR pool. replication.isrShrinksPerSecond Rate of replicas leaving the ISR pool. replication.leaderElectionPerSecond Leader election rate. replication.uncleanLeaderElectionPerSecond Unclean leader election rate. replication.unreplicatedPartitions Number of unreplicated partitions. request.avgTimeFetch Average time per fetch request in milliseconds. request.avgTimeMetadata Average time for metadata request in milliseconds. request.avgTimeMetadata99Percentile Time for metadata requests for 99th percentile in milliseconds. request.avgTimeOffset Average time for an offset request in milliseconds. request.avgTimeOffset99Percentile Time for offset requests for 99th percentile in milliseconds. request.avgTimeProduceRequest Average time for a produce request in milliseconds. request.avgTimeUpdateMetadata Average time for a request to update metadata in milliseconds. request.avgTimeUpdateMetadata99Percentile Time for update metadata requests for 99th percentile in milliseconds. request.clientFetchesFailedPerSecond Client fetch request failures per second. request.fetchTime99Percentile Time for fetch requests for 99th percentile in milliseconds. request.handlerIdle Average fraction of time the request handler threads are idle. request.produceRequestsFailedPerSecond Failed produce requests per second. request.produceTime99Percentile Time for produce requests for 99th percentile. KafkaConsumerSample event Metric Description consumer.avgFetchSizeInBytes Average number of bytes fetched per request for a specific topic. consumer.avgRecordConsumedPerTopic Average number of records in each request for a specific topic. consumer.avgRecordConsumedPerTopicPerSecond Average number of records consumed per second for a specific topic in records per second. consumer.bytesInPerSecond Consumer bytes per second. consumer.fetchPerSecond The minimum rate at which the consumer sends fetch requests to a broke in requests per second. consumer.maxFetchSizeInBytes Maximum number of bytes fetched per request for a specific topic. consumer.maxLag Maximum consumer lag. consumer.messageConsumptionPerSecond Rate of consumer message consumption in messages per second. consumer.offsetKafkaCommitsPerSecond Rate of offset commits to Kafka in commits per second. consumer.offsetZooKeeperCommitsPerSecond Rate of offset commits to ZooKeeper in writes per second. consumer.requestsExpiredPerSecond Rate of delayed consumer request expiration in evictions per second. KafkaProducerSample event Metric Description producer.ageMetadataUsedInMilliseconds Age in seconds of the current producer metadata being used. producer.availableBufferInBytes Total amount of buffer memory that is not being used in bytes. producer.avgBytesSentPerRequestInBytes Average number of bytes sent per partition per-request. producer.avgCompressionRateRecordBatches Average compression rate of record batches. producer.avgRecordAccumulatorsInMilliseconds Average time in ms record batches spent in the record accumulator. producer.avgRecordSizeInBytes Average record size in bytes. producer.avgRecordsSentPerSecond Average number of records sent per second. producer.avgRecordsSentPerTopicPerSecond Average number of records sent per second for a topic. producer.AvgRequestLatencyPerSecond Producer average request latency. producer.avgThrottleTime Average time that a request was throttled by a broker in milliseconds. producer.bufferMemoryAvailableInBytes Maximum amount of buffer memory the client can use in bytes. producer.bufferpoolWaitTime Faction of time an appender waits for space allocation. producer.bytesOutPerSecond Producer bytes per second out. producer.compressionRateRecordBatches Average compression rate of record batches for a topic. producer.iOWaitTime Producer I/O wait time in milliseconds. producer.maxBytesSentPerRequestInBytes Max number of bytes sent per partition per-request. producer.maxRecordSizeInBytes Maximum record size in bytes. producer.maxRequestLatencyInMilliseconds Maximum request latency in milliseconds. producer.maxThrottleTime Maximum time a request was throttled by a broker in milliseconds. producer.messageRatePerSecond Producer messages per second. producer.responsePerSecond Number of producer responses per second. producer.requestPerSecond Number of producer requests per second. producer.requestsWaitingResponse Current number of in-flight requests awaiting a response. producer.threadsWaiting Number of user threads blocked waiting for buffer memory to enqueue their records. KafkaTopicSample event Metric Description topic.diskSize Current topic disk size per broker in bytes. topic.partitionsWithNonPreferredLeader Number of partitions per topic that are not being led by their preferred replica. topic.respondMetaData Number of topics responding to meta data requests. topic.retentionSizeOrTime Whether a partition is retained by size or both size and time. A value of 0 = time and a value of 1 = both size and time. topic.underReplicatedPartitions Number of partitions per topic that are under-replicated. KafkaOffsetSample event Metric Description consumer.offset The last consumed offset on a partition by the consumer group. consumer.lag The difference between a broker's high water mark and the consumer's offset (consumer.hwm - consumer.offset). consumer.hwm The offset of the last message written to a partition (high water mark). consumer.totalLag The sum of lags across partitions consumed by a consumer. consumerGroup.totalLag The sum of lags across all partitions consumed by a consumerGroup. consumerGroup.maxLag The maximum lag across all partitions consumed by a consumerGroup. Inventory data The Kafka integration captures the non-default broker and topic configuration parameters, and collects the topic partition schemes as reported by ZooKeeper. The data is available on the Inventory UI page under the config/kafka source. Troubleshooting Troubleshooting tips: Duplicate data being reported For agents monitoring producers and/or consumers, and that have Topic mode set to All:, there may be a problem of duplicate data being reported. To stop the duplicate data: ensure that the configuration option Collect topic size is set to false. Integration is logging errors 'zk: node not found' Ensure that zookeeper_path is set correctly in the configuration file. JMX connection errors The Kafka integration uses a JMX helper tool called nrjmx to retrieve JMX metrics from brokers, consumers, and producers. JMX needs to be enabled and configured on all brokers in the cluster. Also, firewalls need to be tuned to allow connections from the host running the integration to the brokers over the JMX port. To check whether JMX is correctly configured, run the following command for each broker from the machine running the Kafka integration. Replace the highlighted PORT, USERNAME, and PASSWORD tokens with the corresponding JMX settings for the brokers: $ echo \"*:*\" | nrjmx -hostname HOSTNAME -port PORT -v -username USERNAME -password PASSWORD Copy The command should generate the output showing a long series of metrics without any errors. Kerberos authentication failing The integration might show an error like the following: KRB Error: (6) KDC_ERR_C_PRINCIPAL_UNKNOWN Client not found in Kerberos database Copy Check the keytab with kinit command. Replace the highlighted fields with your values: $ kinit -k -t KEY_TAB_PATH USERNAME Copy If the username/keytab combination is correct, the command above should finish without printing any errors. Check the realm using klist command: $ klist |grep \"Default principal:\" Copy You should see something like this: Default principal: johndoe@a_realm_name Copy Check that the printed user name and realm match the sasl_gssapi_realm and sasl_gssapi_username parameters in the integration configuration. Check the source code This integration is open source software. That means you can browse its source code and send improvements or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 39.725803,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kafka monitoring <em>integration</em>",
        "sections": "Kafka monitoring <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": ". Kubernetes See Monitor service running on Kubernetes. Linux installation Follow the instructions for installing an integration, using the file name nri-kafka. Change the directory to the <em>integrations</em> configuration folder: cd &#x2F;etc&#x2F;newrelic-infra&#x2F;<em>integrations</em>.d Copy Copy of the sample configuration file"
      },
      "id": "6043a32364441f6fef378f1c"
    }
  ],
  "/docs/integrations/new-relic-integrations/getting-started/integration-dashboards-infrastructure-beta-transition-guide": [
    {
      "sections": [
        "Transition to New Relic One from Insights",
        "Important",
        "Features",
        "Improved query abilities",
        "Improved visualizations",
        "Steps for a successful transition"
      ],
      "title": "Transition to New Relic One from Insights",
      "type": "docs",
      "tags": [
        "New Relic One",
        "Use New Relic One",
        "Core concepts"
      ],
      "external_id": "4af99cd8030909a71d21a359a60af5ac93b93a66",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/core-concepts/transition-new-relic-one-insights/",
      "published_at": "2021-07-09T08:58:29Z",
      "updated_at": "2021-07-09T08:58:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important As of April 12, 2021, we're upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. Released in 2014, New Relic Insights was our original way to create custom queries, charts, and dashboards. With New Relic One, we have modernized the experience for you to access, analyze, and visualize your data. New Relic One offers an improved charts and dashboards experience, and it provides a platform where we can more rapidly bring new innovations to you. This transition guide can help you understand: What are some of the new and improved features you get with New Relic One charts, dashboards, and queries Why it's easy to transition to New Relic One What to know and considerations when you make the switch How to get the most out of using New Relic One Features You can scroll down to the transition details, but first here are some features we've added that show how New Relic One dashboards are a clear improvement over Insights dashboards. Improved query abilities With New Relic One, you get: Ability to query many accounts from the same widget: New Relic One lets you query across all your associated accounts in one place. Better querying and charting experiences: Query access is available globally, no matter where you are in New Relic One. This includes a \"basic\" query mode that doesn't require knowledge of NRQL. Improved query experience: You can query both the Metric data type and metric timeslice data. Easy customization: Every visualization now has the query accessible. You can augment any curated chart just by changing the NRQL query. Improved visualizations Not only can you select a wide range of visualization options, you can also add more to your dashboards: Better display options: Make your data easier to understand by using visualizations other than dense, line-heavy charts. New Relic One also offers a better TV mode. Facet linking: You can filter your dashboards by faceted attributes, making your dashboards more interactive and easy to use. There's also support for cases. Learn more. More charts or widgets in an area: Insights restricted you to a 3-across limit. Now you can display up to 12 across your dashboard, providing increased data density along with improved tooltips and tracking across charts. Easier creation of multi-page dashboards: Insights referred to these as data apps. Your Insights data apps are preserved as multi-page dashboards in New Relic One. Chart consistency and flexibility: Dashboards include facet color consistency across widgets and faster loading times for more performant dashboards. Also, you can add any chart type to a dashboard in New Relic One! The New Relic Insights UI has served our users well for many years, but it's time to give you an even better experience. Join us and make the switch to New Relic One! Steps for a successful transition The transition to New Relic One has two parts: the UI and mobile app experience (April 12, 2021) and the Dashboard API (July 2021). Insights functionality Transition to New Relic One UI We have already taken care of your transition from Insights to New Relic One for you! As of April 12, 2021, your old Insights URLs redirect automatically to New Relic One. We recommend that you familiarize yourself with the new UI features available to you, as described in this transition guide. If you need to view any Insights charts embedded in other websites, go to one.newrelic.com > More > Manage Data. (These older embedded charts will continue to function as expected.) Mobile apps The Insights mobile app is deprecated as of April 11, 2021. Go to the Google Play Store 2 or Apple App store. Delete your old Insights mobile app, and download the New Relic One mobile app. tvOS apps and large displays New Relic's tvOS app is still available. No action is needed by you at this time. Some New Relic customers with the original pricing model may have set up dashboards on wall screens for restricted users with kiosk mode. No action is required for you to continue to view these dashboards. APIs In July of 2021, the Insights Dashboard API will be deprecated and replaced with NerdGraph functionality. For more on this change, and tips on how to migrate, see NerdGraph API for dashboards. Partnership accounts This applies only if your account is one of the few using our partnership account structure to deliver New Relic services to your direct customers. In this situation, the Insights EOL will not affect your customers pricing. This is simply an EOL for the UI, not an EOL for the account type. Questions If you have questions about the transition, please comment in our Explorers Hub post. Or, if you work with an account team, they will be happy to help you.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.0126,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Transition</em> to New Relic One from <em>Insights</em>",
        "sections": "<em>Transition</em> to New Relic One from <em>Insights</em>",
        "body": " experience, and it provides a platform where we can more rapidly bring new innovations to you. This <em>transition</em> <em>guide</em> can help you understand: What are some of the new and improved features you get with New Relic One charts, <em>dashboards</em>, and queries Why it&#x27;s easy to <em>transition</em> to New Relic One What to know"
      },
      "id": "6044171164441f454a378ee2"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/release-notes/infrastructure-release-notes/cloud-integration-release-notes/changes-curated-dashboards/",
      "sections": [
        "Changes in curated dashboards",
        "Other changes",
        "New"
      ],
      "published_at": "2021-07-09T18:27:04Z",
      "title": "Changes in curated dashboards",
      "updated_at": "2021-03-16T16:57:40Z",
      "type": "docs",
      "external_id": "e8bcff90d46de24933446830d551bcab22a2da5c",
      "document_type": "release_notes",
      "popularity": 1,
      "body": "Changes in curated dashboards New Relic Infrastructure integrations will not automatically create new dashboards in New Relic Insights when an integration is enabled. Instead, curated dashboards for On-host and Cloud integrations data will be embedded in the New Relic Infrastructure UI, and they can be reached from the following links: On-host integrations: https://infrastructure.newrelic.com/accounts/<your_account_ID>/integrations/onHostIntegrations AWS integrations: https://infrastructure.newrelic.com/accounts/<your_account_ID>/integrations/aws Azure integrations: https://infrastructure.newrelic.com/accounts/<your_account_ID>/integrations/azure GCP integrations: https://infrastructure.newrelic.com/accounts/<your_account_ID>/integrations/gcp Please refer to the transition guide and the Infrastructure integration dashboards and charts documentation for more details about the new pre-built integration dashboards. Note the existing curated dashboards that had been automatically created in New Relic Insights through the InfrastructurePro@newrelic.com user are still available. These dashboards won't be automatically updated anymore, but now you can edit and remove them. By default, they have the same name as the dashboard in New Relic Infrastructure, that takes this format: For on-host integrations: <Integration name> For cloud integrations: <Integration name> - <Linked account name> Other changes The former systemErrors metric in the AWS DynamoDB integration is now reported in several different metrics which represent the total number of requests that generate an HTTP 500 status per operation type. Please refer to AWS DynamoDB monitoring integration for details. New The AWS RDS integration now provides new metrics for RDS database instances and Amazon Aurora clusters. Check AWS RDS monitoring integration for details. The Azure App Service integration now provides new metrics to monitor connections and performance of Web Apps. Check Azure App Service monitoring integration for details.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 135.53925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Changes <em>in</em> curated <em>dashboards</em>",
        "sections": "Changes <em>in</em> curated <em>dashboards</em>",
        "body": ":&#x2F;&#x2F;<em>infrastructure</em>.newrelic.com&#x2F;accounts&#x2F;&lt;your_account_ID&gt;&#x2F;integrations&#x2F;azure GCP integrations: https:&#x2F;&#x2F;<em>infrastructure</em>.newrelic.com&#x2F;accounts&#x2F;&lt;your_account_ID&gt;&#x2F;integrations&#x2F;gcp Please refer to the <em>transition</em> <em>guide</em> and the <em>Infrastructure</em> <em>integration</em> <em>dashboards</em> and charts documentation for more details about the new pre"
      },
      "id": "603e8dcbe7b9d203892a07e2"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/prometheus-remote-write-integration/",
      "sections": [
        "Prometheus remote write integration",
        "Why it matters",
        "Compatibility",
        "Scale your data and get moving quickly",
        "What's next",
        "For more help"
      ],
      "published_at": "2021-07-14T15:02:55Z",
      "title": "Prometheus remote write integration",
      "updated_at": "2021-07-10T23:03:06Z",
      "type": "docs",
      "external_id": "aaf44eb9ee0ffc4b6f751ca18c5dd5b34cd11649",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use the Prometheus remote write integration to get data flowing into New Relic. Once you integrate, your data will be visible in query-based dashboards (and other query results), often within about five minutes. Why it matters Unlike Kubernetes and Docker OpenMetrics integrations, which scrape data from Prometheus endpoints, the remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. You can leverage the full range of options for setup and management, from raw data to queries and dashboards and beyond. With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional) Compatibility New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Scale your data and get moving quickly Once logged in to New Relic, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. View your data. What's next Ready to get started? Read the setup documentation. Configure a Prometheus data source in Grafana. Set up the integration on New Relic US EU For more help Recommendations for learning more: See the Docs site's landing page for Infrastructure integrations documentation. Browse New Relic's Explorers Hub for community discussions about our Infrastructure integrations. Find additional help or file a support ticket. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 121.348625,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Prometheus remote write <em>integration</em>",
        "sections": "Prometheus remote write <em>integration</em>",
        "body": "You can use the Prometheus remote write <em>integration</em> to get data flowing into New Relic. Once you integrate, your data will be visible in query-based <em>dashboards</em> (and other query results), often within about five minutes. Why it matters Unlike Kubernetes and Docker OpenMetrics integrations, which"
      },
      "id": "60ea272a196a670c6038adbf"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/dropwizard/dropwizard-reporter": [
    {
      "sections": [
        "Introduction to New Relic's open source telemetry integrations",
        "Tip",
        "Types of integrations",
        "How they work"
      ],
      "title": "Introduction to New Relic's open source telemetry integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "Get started"
      ],
      "external_id": "239889ec292525fcfd6b417d243943ea7b3e0529",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations/",
      "published_at": "2021-07-09T10:17:37Z",
      "updated_at": "2021-03-16T06:12:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides open source integrations that report telemetry data from telemetry tools to your New Relic account. Tip To use telemetry integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Types of integrations We have open source integrations that report data from OpenCensus, OpenTelemetry, DropWizard, Prometheus, and more. With these solutions, you can aggregate all your telemetry data in one place: the New Relic platform. See our list of open source telemetry integrations (to browse all New Relic solutions, see our integrations page). How they work These integrations were built using our Telemetry SDKs, which are open-source language-specific libraries for reporting metrics, trace data, and other telemetry data to New Relic. If our pre-built integrations don't meet your needs, you can use the Telemetry SDKs to build your own telemetry tools. Under the hood, data reported by these solutions are ingested via our data ingest APIs. For example, metrics reported by the DropWizard exporter are ingested via the Metric API, so to understand how to query and chart that type of data, you could read Query metric data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.65315,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "sections": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to 100GB of data for free each month. Forever. Types of <em>integrations</em> We have <em>open</em> <em>source</em> <em>integrations</em> that report data from <em>Open</em>Census, <em>OpenTelemetry</em>, <em>DropWizard</em>, Prometheus, and more. With these solutions, you can aggregate all your <em>telemetry</em> data in one place: the New Relic platform. See our list"
      },
      "id": "603e95ab28ccbc036aeba789"
    },
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-07-09T17:42:13Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.21146,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-07-09T08:28:54Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.6292,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/elixir/elixir-open-source-agent": [
    {
      "sections": [
        "Roku open-source agent",
        "Tip",
        "Get started",
        "For more help"
      ],
      "title": "Roku open-source agent",
      "type": "docs",
      "tags": [
        "Agents",
        "Open-source licensed agents",
        "Open-source licensed agents"
      ],
      "external_id": "f0982a0ff96c8a85683bf3ef27a2e4cde85ff274",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/roku/roku-open-source-video-agent/",
      "published_at": "2021-07-14T10:53:58Z",
      "updated_at": "2021-04-27T11:09:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Monitor Roku behavior with New Relic using the Roku open-source agent. The agent contains two parts, to capture two separate categories of Roku behavior: App events like app starts and HTTP requests Video playback within the app Tip This agent is released as open source on GitHub. A change log is also available there for the latest updates. Get started For requirements, installation, and configuration information, see the Open Source Roku Agent README on GitHub. Visit New Relics Roku repository on GitHub for questions about installation, usage, or other topics. Report issues or bugs as an issue in the GitHub repository. For more help Recommendations for learning more: Browse New Relic's Explorers Hub for community discussions about the open-source Roku agent. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 368.21417,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Roku <em>open</em>-<em>source</em> <em>agent</em>",
        "sections": "Roku <em>open</em>-<em>source</em> <em>agent</em>",
        "tags": "<em>Open</em>-<em>source</em> <em>licensed</em> <em>agents</em>",
        "body": "Monitor Roku behavior with New Relic using the Roku <em>open</em>-<em>source</em> <em>agent</em>. The <em>agent</em> contains two parts, to capture two separate categories of Roku behavior: App events like app starts and HTTP requests Video playback within the app Tip This <em>agent</em> is released as <em>open</em> <em>source</em> on GitHub. A change log"
      },
      "id": "6087f0ff64441f618a9d8533"
    },
    {
      "image": "https://docs.newrelic.com/static/ba856000a5801ecad9e41358f340f688/10f9a/custom-metric-syntax.png",
      "url": "https://docs.newrelic.com/docs/agents/manage-apm-agents/agent-data/collect-custom-metrics/",
      "sections": [
        "Collect custom metrics (APM, browser, mobile)",
        "Name custom metrics",
        "Implement custom metrics",
        "Tip",
        "Avoid grouping issues",
        "Important"
      ],
      "published_at": "2021-07-09T05:09:18Z",
      "title": "Collect custom metrics (APM, browser, mobile)",
      "updated_at": "2021-07-09T05:09:18Z",
      "type": "docs",
      "external_id": "fc2bd4425b1f6d4c0f50dfbe0879c61b24b52f00",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic APM agents, browser agents, and mobile agents report a type of metric data called metric timeslice data. These agents allow you to report custom metric timeslice data from your application and see it alongside default New Relic data. Create custom metrics to record arbitrary performance data via an API call, such as: Timing data Computer resource data Subscription or purchasing data After reporting this data, you can explore and query it. Name custom metrics Start all custom metric names with Custom/; for example, Custom/MyMetric/My_label. The Custom/ prefix is required for all custom metrics. Any custom metric names that do not start with Custom/ are subject to all other grouping rules. They may not be visible when querying, or they may not appear as expected in the UI. A custom metric name consists of the prefix Custom/, the category or class name, and a method or label, each separated with a slash. Implement custom metrics Implementing custom metrics requires API calls. The exact details of the API call vary by agent. Tip If you are testing your custom metric implementation, run the agent for at least 10 minutes to ensure that the API call is reported to New Relic. New Relic agent Implementation APM agents C SDK: newrelic_record_custom_metric() Go: app.RecordCustomMetric Java: recordMetric .NET: RecordMetric Node.js: recordMetric PHP: newrelic_custom_metric Python: record_custom_metric and register_data_source Ruby: record_metric and increment_metric New Relic mobile agents Android: recordMetric iOS: recordMetricWithName (Objective-C) or NewRelic.recordMetricWithName (Swift) New Relic browser agent Browser does not support custom metrics. For options on adding custom data to browser, see browser instrumentation. Avoid grouping issues Collecting too many metric timeslices can impact the performance of both your application and New Relic. For example, if you have thousands of individual users, avoid creating metrics to track the performance of their unique user IDs. This could result in such a large number of metrics that it becomes nearly impossible to navigate or make sense of the data. Instead, use a placeholder, such as an asterisk ( * ), instead of individual user IDs. Important To avoid potential data problems, try to keep the total number of unique metric timeslices introduced by custom metrics under 2000. When the total number of unique metric names exceeds 2000, limits begin to apply automatically that affect how data appears in the user interface, such as in charts and tables. For more information, see Metric grouping issues.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 27.155437,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": ". New Relic <em>agent</em> Implementation APM <em>agents</em> C SDK: newrelic_record_custom_metric() Go: app.RecordCustomMetric Java: recordMetric .NET: RecordMetric Node.js: recordMetric PHP: newrelic_custom_metric Python: record_custom_metric and register_data_<em>source</em> Ruby: record_metric and increment_metric New Relic"
      },
      "id": "603eb881e7b9d2a9482a0817"
    },
    {
      "sections": [
        "Browser monitoring and the Ruby agent",
        "Requirements",
        "Use auto-instrumentation",
        "Manually instrument via agent API",
        "CSP Nonce support",
        "Troubleshooting",
        "Disable auto-instrumentation"
      ],
      "title": "Browser monitoring and the Ruby agent",
      "type": "docs",
      "tags": [
        "Agents",
        "Ruby agent",
        "Features"
      ],
      "external_id": "c7381e06d48fe877cdbcc935d51c88dd858d3673",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/ruby-agent/features/new-relic-browser-ruby-agent/",
      "published_at": "2021-07-09T22:33:58Z",
      "updated_at": "2021-07-09T22:33:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the Ruby agent, you can add browser monitoring agent instrumentation to your webpages either automatically or manually. To enable browser monitoring in the UI, follow the procedures to install the browser agent. Then follow the procedures in this section to set up the Ruby agent. Requirements Automatic browser instrumentation is supported for most recent versions of Rails: New Relic Ruby agent (version 3.0.1 or higher) For automatic instrumentation: Rails 2.3 or higher For manual instrumentation: Rails 2.0 to 2.2 Use auto-instrumentation Automatic instrumentation works with Rack, and requires Rails 2.3 or higher. Configure the agent to automatically instrument pages with end user monitoring scripts, by adding or editing the following flag in newrelic.yml. browser_monitoring: auto_instrument: true Copy This is the simplest way to monitor end users. The Ruby agent examines each page from your application and automatically injects the browser JavaScript in the header. Auto-instrumentation works for environments that support Rack. For performance reasons the agent scans only the first 50k of your application's response for the header instrumentation point. In cases where a X-UA-Compatible meta tag is present and the <head> tag is longer than 50k, auto-instrumentation will fail gracefully. Auto-instrumentation will look for an X-UA-Compatible meta tag and insert the JavaScript directly after it. If the auto-instrumentation cannot find an X-UA-Compatible meta tag, it will insert after the opening head tag, and failing that it will insert after the opening body tag. If any of these tags are wrapped in conditionals or comments, the automatic instrumentation will likely fail. If auto-instrumentation fails to correctly instrument your applications pages, then you must manually instrument via agent API. Manually instrument via agent API If you are using Rails 2.1 to 2.2 or cannot use auto-instrumentation, you can still set up end user monitoring manually by including appropriate scripts in your pages. Use the New Relic Ruby agent's API to generate the script to be inserted into your pages. For example, to modify your application's template to call the agent, use: <head> <%= ::NewRelic::Agent.browser_timing_header rescue \"\" %> ... existing template code ... </head> Copy CSP Nonce support CSP Nonce usage in our browser instrumentation is supported in version 7.1.0+ of the Ruby agent. To use a nonce with the browser instrumentation, you must disable browser monitoring auto instrumentation and use the manual instrumentation via the agent API. You may now pass a nonce as an argument into the API method to allow the agent to insert the nonce into the browser instrumentation. For example, NewRelic::Agent.browser_timing_header(nonce). Troubleshooting If you do not see any browser data being reported, view the HTML source and confirm two blocks of script similar to this in your HTML head: <script type=\"text/javascript\">window.NREUM||(NREUM={});...</script> Copy If these are not present, check the auto-instrumentation settings or the placement of manual instrumentation in your template files. In versions of the Ruby agent prior to 3.7.0, <%=::NewRelic::Agent.browser_timing_footer rescue \"\" %> also needed to be called as close to the bottom of the document as possible. With 3.7.0, all the required script is included by browser_timing_header. browser_timing_footer returns an empty string for compatibility. Disable auto-instrumentation The default value of auto-instrument is true (even when unspecified). To disable auto-instrumentation, set this flag to false. To disable auto-instrumentation only for certain controllers or controller actions, use newrelic_ignore_enduser. Server-side instrumentation will be unaffected by this call.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 26.72472,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Browser monitoring and the Ruby <em>agent</em>",
        "sections": "Browser monitoring and the Ruby <em>agent</em>",
        "tags": "<em>Agents</em>",
        "body": " instrumentation. For example, NewRelic::<em>Agent</em>.browser_timing_header(nonce). Troubleshooting If you do not see any browser data being reported, view the HTML <em>source</em> and confirm two blocks of script similar to this in your HTML head: &lt;script type=&quot;text&#x2F;javascript&quot;&gt;window.NREUM||(NREUM={});...&lt;&#x2F;script"
      },
      "id": "603eb812e7b9d247a82a07dc"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-07-09T17:42:13Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.43355,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " your applications, as well as detailed information, such as distributed tracing. To <em>get</em> <em>started</em> with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - <em>OpenTelemetry</em>: Click the service you want to know more about. If you need help understanding the data"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-07-09T08:28:54Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.45386,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-07-09T08:48:44Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.43033,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " with other <em>telemetry</em> data produced by the application. The <em>OpenTelemetry</em> semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs <em>get</em>"
      },
      "id": "60ce7bf528ccbc384271b463"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/istio/istio-adapter": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-07-09T17:42:13Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.02162,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-07-09T08:28:54Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.45386,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-07-09T08:48:44Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.43033,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/kamon/kamon-reporter": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-07-09T17:42:13Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.21144,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-07-09T08:28:54Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.62918,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-07-09T08:48:44Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.60504,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/micrometer/micrometer-metrics-registry": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-07-09T17:42:13Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 164.0216,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-07-09T08:28:54Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.45386,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-07-09T08:48:44Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 163.4303,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opencensus/opencensus-exporter": [
    {
      "sections": [
        "Introduction to New Relic's open source telemetry integrations",
        "Tip",
        "Types of integrations",
        "How they work"
      ],
      "title": "Introduction to New Relic's open source telemetry integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "Get started"
      ],
      "external_id": "239889ec292525fcfd6b417d243943ea7b3e0529",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations/",
      "published_at": "2021-07-09T10:17:37Z",
      "updated_at": "2021-03-16T06:12:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides open source integrations that report telemetry data from telemetry tools to your New Relic account. Tip To use telemetry integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Types of integrations We have open source integrations that report data from OpenCensus, OpenTelemetry, DropWizard, Prometheus, and more. With these solutions, you can aggregate all your telemetry data in one place: the New Relic platform. See our list of open source telemetry integrations (to browse all New Relic solutions, see our integrations page). How they work These integrations were built using our Telemetry SDKs, which are open-source language-specific libraries for reporting metrics, trace data, and other telemetry data to New Relic. If our pre-built integrations don't meet your needs, you can use the Telemetry SDKs to build your own telemetry tools. Under the hood, data reported by these solutions are ingested via our data ingest APIs. For example, metrics reported by the DropWizard exporter are ingested via the Metric API, so to understand how to query and chart that type of data, you could read Query metric data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 283.0649,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "sections": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to 100GB of data for free each month. Forever. Types of <em>integrations</em> We have <em>open</em> <em>source</em> <em>integrations</em> that report data from <em>OpenCensus</em>, <em>OpenTelemetry</em>, DropWizard, Prometheus, and more. With these solutions, you can aggregate all your <em>telemetry</em> data in one place: the New Relic platform. See our list"
      },
      "id": "603e95ab28ccbc036aeba789"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Tip",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Collector (recommended)",
        "OTLP (pre-release)",
        "What's next?"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-07-09T08:16:05Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can send OpenTelemetry data to New Relic and analyze the results in our observability platform. This introduction can help you decide if you want to get started with OpenTelemetry. If you're already familiar with OpenTelemetry and want to jump into the setup, go to our OpenTelemetry quick start. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can send data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you wont need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You dont need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they dont need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetrys fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Tip For more, see our blog posts on OpenTelemetry. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start sending data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic APM agents In general, New Relic APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic Here are two basic ways you could set up OpenTelemetry with New Relic: use the OpenTelemetry collector (recommended), or use the native OTLP endpoint (pre-release). Each approach assumes you set up instrumentation in your code, but they differ in how the data is gathered and sent to New Relic. If you are interested in tracing, we recommend that you instrument as many services as you can so that you get the most benefit from distributed tracing. There are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesnt analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Tip We discuss some variations on these basic approaches in our OpenTelemetry architecture recipes. For a detailed discussion about the architecture of OpenTelemetry itself, see our blog. Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. Collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. In this workflow, note the alternate path if you set up New Relic Infinite Tracing: Here are some advantages of this approach: The OpenTelemetry collector can perform processing operations on telemetry data before it is sent to the backend, including filtering attributes, excluding traces, and sampling. The OpenTelemetry collector can receive telemetry data in many formats and forward it on to New Relics backend, including Prometheus, Zipkin, Jaeger, AWS X-Ray, OpenCensus, and much more. The OpenTelemetry collector can collect system-level metrics from the host where it is running. OTLP (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector, or send us data directly from your service. If you are interested, let us know by completing this form. Here are some advantages to operating without an OpenTelemetry collector: There is no need to deploy a collector alongside your service. Telemetry data is sent directly from the instrumented service to New Relic. If something goes wrong, and telemetry data does not arrive, there is one less component to debug. What's next? If you want to set up OpenTelemetry with New Relic, the next step is to check out our OpenTelemetry quick start. If you prefer to use a New Relic APM agent, go to Introduction to APM.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 251.29955,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": ", and as more vendors move to support this <em>open</em> standard, you can be confident that you wont need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/",
      "sections": [
        "Welcome to New Relic",
        "Create a free account",
        "Start collecting data",
        "Set up alerts",
        "Telemetry Data Platform",
        "Introduction to Telemetry Data Platform",
        "Data explorer",
        "Dashboards",
        "Log management",
        "APIs",
        "Manage data",
        "Build on New Relic One",
        "Full-Stack Observability",
        "Introduction to Full-Stack Observability",
        "APM",
        "Browser",
        "Distributed Tracing",
        "Infrastructure",
        "Logs in context",
        "Mobile",
        "Serverless",
        "Synthetics",
        "Workloads",
        "Alerts and Applied Intelligence (AI)",
        "Introduction to Alerts",
        "Introduction to Applied Intelligence",
        "Incident Intelligence",
        "Incident Workflows",
        "Proactive Detection",
        "New Relic integrations",
        "Back-end, front-end, and mobile applications",
        "Infrastructure and cloud platforms",
        "Open-source monitoring systems",
        "Security, privacy, and legal information",
        "Data privacy",
        "Security compliance",
        "Security bulletins",
        "Licenses"
      ],
      "published_at": "2021-07-14T01:38:26Z",
      "title": "New Relic Documentation",
      "updated_at": "2021-07-07T01:38:36Z",
      "type": "docs",
      "external_id": "807276d5fdd805ba0247a1ce1016af986fb31995",
      "popularity": 1,
      "body": "Welcome to New Relic If you're new, follow these three steps to create an account and get going. (It's free!) Scroll on for more about our Telemetry Data Platform, Full-Stack Observability, and Applied Intelligence. Or, to get a wider view of our platform's capabilities, read the Intro to New Relic, and use our solutions and best practices guides. Create a free account No credit card required. Start collecting data Our UI guides you through setup and install. Set up alerts Get notified quickly about changes in your system. Telemetry Data Platform Ingest, visualize, and alert on all your telemetry data in one place. Introduction to Telemetry Data Platform How to manage all your monitoring in one place. Data explorer Query and build charts with NRQL, our PromQL-style syntax, or our visual chart builder. Dashboards Combine data from anywhere in our platform into customized dashboards. Log management Get logs alongside your telemetry data with our fast, scalable log management. APIs Find APIs to send data in, get data out, or manage the New Relic platform. Manage data Monitor and control your data usage. Build on New Relic One Learn how to build custom apps on our platform at developer.newrelic.com Full-Stack Observability Analyze and troubleshoot problems easily across your entire software stack. Introduction to Full-Stack Observability Get deep insight into everything from infrastructure to server code to end-user apps. APM Get real-time and trending data about your app's performance and stability. Browser Measure website performance, track errors, and see how users interact with your site. Distributed Tracing Track requests through your distributed system to find trends and anomalies. Infrastructure Monitor your infrastructurehosts, cloud providers, container services, backend services, orchestrators, and more. Logs in context Link your log data to APM agent data so you can pinpoint where things are going wrong. Mobile Understand user journeys in your Android and iOS apps and troubleshoot crashes. Serverless Monitor AWS Lambda, Azure Functions, and Google Cloud Functions. Synthetics Simulate user activity to detect outages and fix poor website performance. Workloads Group or monitor entities across your entire stack based on a team or a set of responsibilities. Alerts and Applied Intelligence (AI) Automatically detect anomalies, correlate issues, and reduce alert noise. Introduction to Alerts Get notified about important changes in your system based on any data you connect to New Relic. Introduction to Applied Intelligence Recognize issues sooner, resolve problems faster, and reduce noise for better incident management. Incident Intelligence Get an overview of all your incidents. See sources and related events, and find out how they all correlate. Incident Workflows Enrich your incidents with New Relic data before sending them to your notification platform. Proactive Detection Get notified by Slack or webhook of unusual app behavior. New Relic integrations Integrations connect the technologies in your stack to New Relic. Here are a few of our 370+ integrations: Back-end, front-end, and mobile applications Android Browser (JavaScript) C SDK Go iOS Java .NET Node.js PHP Python Ruby Synthetics Infrastructure and cloud platforms Apache AWS Azure Google Cloud Kafka Kubernetes Linux Microsoft SQL MongoDB MySQL NGINX PostgreSQL Redis Windows Open-source monitoring systems Dropwizard Istio JMX Kamon Micrometer OpenCensus OpenTelemetry Prometheus StatsD See all 370+ integrations Security, privacy, and legal information Find out how we ensure security, data privacy, and compliance, and find our terms of service. Data privacy New Relic takes your data privacy seriously. Our principles-based approach aims to go beyond the legal requirements for consent. Security compliance Whether your data is in transit to New Relic or at rest in our storage, strong encryption measures can help protect against unauthorized access or theft of valuable data. Security bulletins Security bulletins are a way for us to let you know about security vulnerabilities, remediation strategies, and applicable updates. Licenses All about New Relic's licenses, Usage Plan, and policies. Find out what open source software we use in our products.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 232.3999,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Open</em>-<em>source</em> monitoring systems",
        "body": " Azure Google Cloud Kafka Kubernetes Linux Microsoft SQL MongoDB MySQL NGINX PostgreSQL Redis Windows <em>Open</em>-<em>source</em> monitoring systems Dropwizard Istio JMX Kamon Micrometer <em>OpenCensus</em> <em>OpenTelemetry</em> Prometheus StatsD See all 370+ <em>integrations</em> Security, privacy, and legal information Find out how we ensure"
      },
      "id": "5be4e5d38e9c0f3585d72edf"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-07-09T17:42:13Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.23904,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-07-09T08:48:44Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.01193,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-07-09T08:28:54Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 235.4107,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-architecture-recipes": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-07-09T17:42:13Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.23904,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-07-09T08:48:44Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.01193,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-07-09T08:28:54Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 235.4107,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-07-09T17:42:13Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.23904,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-07-09T08:48:44Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.01193,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-07-09T09:38:53Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we dont already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesnt analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once youve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 204.62416,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-07-09T17:42:13Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.23903,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-07-09T08:28:54Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 235.41068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-07-09T09:38:53Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we dont already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesnt analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once youve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 204.62416,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-07-09T17:42:13Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.23903,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-07-09T08:48:44Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.0119,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-07-09T08:28:54Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 235.41068,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic": [
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-07-09T08:48:44Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.01189,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-07-09T08:28:54Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 235.41064,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-07-09T09:38:53Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we dont already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesnt analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once youve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 204.62416,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/roku/roku-open-source-video-agent": [
    {
      "sections": [
        "Elixir open-source agent",
        "Tip",
        "Get started",
        "For more help"
      ],
      "title": "Elixir open-source agent",
      "type": "docs",
      "tags": [
        "Agents",
        "Open-source licensed agents",
        "Open-source licensed agents"
      ],
      "external_id": "aa03e1693b6ecdd06fa2940ddb99187247743772",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/elixir/elixir-open-source-agent/",
      "published_at": "2021-07-09T17:33:25Z",
      "updated_at": "2021-04-27T11:09:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Monitor Elixir behavior with New Relic using the Elixir open-source agent. The agent: Helps you track transactions, distributed traces, and other parts of your applications behavior Provides an overview of underlying BEAM activity Tip This agent is released as open source on GitHub. A change log is also available there for the latest updates. Get started For requirements, installation, and configuration information, see the Open Source Elixir Agent README on GitHub. Visit New Relics Elixir repository on GitHub for questions about installation, usage, or other topics. Report issues or bugs as an issue in the GitHub repository. For more help Recommendations for learning more: Browse New Relic's Explorers Hub for community discussions about the open-source Elixir agent. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 368.21417,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elixir <em>open</em>-<em>source</em> <em>agent</em>",
        "sections": "Elixir <em>open</em>-<em>source</em> <em>agent</em>",
        "tags": "<em>Open</em>-<em>source</em> <em>licensed</em> <em>agents</em>",
        "body": "Monitor Elixir behavior with New Relic using the Elixir <em>open</em>-<em>source</em> <em>agent</em>. The <em>agent</em>: Helps you track transactions, distributed traces, and other parts of your applications behavior Provides an overview of underlying BEAM activity Tip This <em>agent</em> is released as <em>open</em> <em>source</em> on GitHub. A change log"
      },
      "id": "6087f0ff28ccbceab351c13f"
    },
    {
      "image": "https://docs.newrelic.com/static/ba856000a5801ecad9e41358f340f688/10f9a/custom-metric-syntax.png",
      "url": "https://docs.newrelic.com/docs/agents/manage-apm-agents/agent-data/collect-custom-metrics/",
      "sections": [
        "Collect custom metrics (APM, browser, mobile)",
        "Name custom metrics",
        "Implement custom metrics",
        "Tip",
        "Avoid grouping issues",
        "Important"
      ],
      "published_at": "2021-07-09T05:09:18Z",
      "title": "Collect custom metrics (APM, browser, mobile)",
      "updated_at": "2021-07-09T05:09:18Z",
      "type": "docs",
      "external_id": "fc2bd4425b1f6d4c0f50dfbe0879c61b24b52f00",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic APM agents, browser agents, and mobile agents report a type of metric data called metric timeslice data. These agents allow you to report custom metric timeslice data from your application and see it alongside default New Relic data. Create custom metrics to record arbitrary performance data via an API call, such as: Timing data Computer resource data Subscription or purchasing data After reporting this data, you can explore and query it. Name custom metrics Start all custom metric names with Custom/; for example, Custom/MyMetric/My_label. The Custom/ prefix is required for all custom metrics. Any custom metric names that do not start with Custom/ are subject to all other grouping rules. They may not be visible when querying, or they may not appear as expected in the UI. A custom metric name consists of the prefix Custom/, the category or class name, and a method or label, each separated with a slash. Implement custom metrics Implementing custom metrics requires API calls. The exact details of the API call vary by agent. Tip If you are testing your custom metric implementation, run the agent for at least 10 minutes to ensure that the API call is reported to New Relic. New Relic agent Implementation APM agents C SDK: newrelic_record_custom_metric() Go: app.RecordCustomMetric Java: recordMetric .NET: RecordMetric Node.js: recordMetric PHP: newrelic_custom_metric Python: record_custom_metric and register_data_source Ruby: record_metric and increment_metric New Relic mobile agents Android: recordMetric iOS: recordMetricWithName (Objective-C) or NewRelic.recordMetricWithName (Swift) New Relic browser agent Browser does not support custom metrics. For options on adding custom data to browser, see browser instrumentation. Avoid grouping issues Collecting too many metric timeslices can impact the performance of both your application and New Relic. For example, if you have thousands of individual users, avoid creating metrics to track the performance of their unique user IDs. This could result in such a large number of metrics that it becomes nearly impossible to navigate or make sense of the data. Instead, use a placeholder, such as an asterisk ( * ), instead of individual user IDs. Important To avoid potential data problems, try to keep the total number of unique metric timeslices introduced by custom metrics under 2000. When the total number of unique metric names exceeds 2000, limits begin to apply automatically that affect how data appears in the user interface, such as in charts and tables. For more information, see Metric grouping issues.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 27.15538,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": ". New Relic <em>agent</em> Implementation APM <em>agents</em> C SDK: newrelic_record_custom_metric() Go: app.RecordCustomMetric Java: recordMetric .NET: RecordMetric Node.js: recordMetric PHP: newrelic_custom_metric Python: record_custom_metric and register_data_<em>source</em> Ruby: record_metric and increment_metric New Relic"
      },
      "id": "603eb881e7b9d2a9482a0817"
    },
    {
      "sections": [
        "Browser monitoring and the Ruby agent",
        "Requirements",
        "Use auto-instrumentation",
        "Manually instrument via agent API",
        "CSP Nonce support",
        "Troubleshooting",
        "Disable auto-instrumentation"
      ],
      "title": "Browser monitoring and the Ruby agent",
      "type": "docs",
      "tags": [
        "Agents",
        "Ruby agent",
        "Features"
      ],
      "external_id": "c7381e06d48fe877cdbcc935d51c88dd858d3673",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/ruby-agent/features/new-relic-browser-ruby-agent/",
      "published_at": "2021-07-09T22:33:58Z",
      "updated_at": "2021-07-09T22:33:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the Ruby agent, you can add browser monitoring agent instrumentation to your webpages either automatically or manually. To enable browser monitoring in the UI, follow the procedures to install the browser agent. Then follow the procedures in this section to set up the Ruby agent. Requirements Automatic browser instrumentation is supported for most recent versions of Rails: New Relic Ruby agent (version 3.0.1 or higher) For automatic instrumentation: Rails 2.3 or higher For manual instrumentation: Rails 2.0 to 2.2 Use auto-instrumentation Automatic instrumentation works with Rack, and requires Rails 2.3 or higher. Configure the agent to automatically instrument pages with end user monitoring scripts, by adding or editing the following flag in newrelic.yml. browser_monitoring: auto_instrument: true Copy This is the simplest way to monitor end users. The Ruby agent examines each page from your application and automatically injects the browser JavaScript in the header. Auto-instrumentation works for environments that support Rack. For performance reasons the agent scans only the first 50k of your application's response for the header instrumentation point. In cases where a X-UA-Compatible meta tag is present and the <head> tag is longer than 50k, auto-instrumentation will fail gracefully. Auto-instrumentation will look for an X-UA-Compatible meta tag and insert the JavaScript directly after it. If the auto-instrumentation cannot find an X-UA-Compatible meta tag, it will insert after the opening head tag, and failing that it will insert after the opening body tag. If any of these tags are wrapped in conditionals or comments, the automatic instrumentation will likely fail. If auto-instrumentation fails to correctly instrument your applications pages, then you must manually instrument via agent API. Manually instrument via agent API If you are using Rails 2.1 to 2.2 or cannot use auto-instrumentation, you can still set up end user monitoring manually by including appropriate scripts in your pages. Use the New Relic Ruby agent's API to generate the script to be inserted into your pages. For example, to modify your application's template to call the agent, use: <head> <%= ::NewRelic::Agent.browser_timing_header rescue \"\" %> ... existing template code ... </head> Copy CSP Nonce support CSP Nonce usage in our browser instrumentation is supported in version 7.1.0+ of the Ruby agent. To use a nonce with the browser instrumentation, you must disable browser monitoring auto instrumentation and use the manual instrumentation via the agent API. You may now pass a nonce as an argument into the API method to allow the agent to insert the nonce into the browser instrumentation. For example, NewRelic::Agent.browser_timing_header(nonce). Troubleshooting If you do not see any browser data being reported, view the HTML source and confirm two blocks of script similar to this in your HTML head: <script type=\"text/javascript\">window.NREUM||(NREUM={});...</script> Copy If these are not present, check the auto-instrumentation settings or the placement of manual instrumentation in your template files. In versions of the Ruby agent prior to 3.7.0, <%=::NewRelic::Agent.browser_timing_footer rescue \"\" %> also needed to be called as close to the bottom of the document as possible. With 3.7.0, all the required script is included by browser_timing_header. browser_timing_footer returns an empty string for compatibility. Disable auto-instrumentation The default value of auto-instrument is true (even when unspecified). To disable auto-instrumentation, set this flag to false. To disable auto-instrumentation only for certain controllers or controller actions, use newrelic_ignore_enduser. Server-side instrumentation will be unaffected by this call.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 26.72466,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Browser monitoring and the Ruby <em>agent</em>",
        "sections": "Browser monitoring and the Ruby <em>agent</em>",
        "tags": "<em>Agents</em>",
        "body": " instrumentation. For example, NewRelic::<em>Agent</em>.browser_timing_header(nonce). Troubleshooting If you do not see any browser data being reported, view the HTML <em>source</em> and confirm two blocks of script similar to this in your HTML head: &lt;script type=&quot;text&#x2F;javascript&quot;&gt;window.NREUM||(NREUM={});...&lt;&#x2F;script"
      },
      "id": "603eb812e7b9d247a82a07dc"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/troubleshooting/troubleshoot-opentelemetry-exporter": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-07-09T17:42:13Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.23901,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-07-09T08:48:44Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 237.01189,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-07-09T08:28:54Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 235.41064,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    }
  ],
  "/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic": [
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-07-14T15:02:55Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 116.15508,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> remote write",
        "sections": "Drop data using <em>Prometheus</em> remote write",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;write?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; write_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "Kubernetes integration: compatibility and requirements",
        "Compatibility",
        "Requirements",
        "Install using Helm"
      ],
      "title": "Kubernetes integration: compatibility and requirements",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "e9bbd729904fa01739eb91e4f3c74561b51c2ba1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/kubernetes-integration-compatibility-requirements/",
      "published_at": "2021-07-09T19:22:43Z",
      "updated_at": "2021-07-09T19:22:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration can be installed directly on a server or VM, or through several cloud platforms, such as GKE, EKS, AKS, or OpenShift. Each has a different compatibility with our integration. Compatibility Our Kubernetes integration is compatible with the following versions, depending on the installation mode: Install mode or feature Kubernetes versions Kubernetes cluster Currently tested with versions 1.10 to 1.21 Kubernetes cluster GKE Currently tested with versions 1.17 to 1.19 Kubernetes cluster EKS (EC2 nodes or Fargate) Compatible with version 1.11 or higher Kubernetes cluster AKS Compatible with version 1.11 or higher Kubernetes cluster OpenShift Currently tested with versions 3.7, 3.9, 4.2, 4.3, 4.4, 4.5 and 4.6 Control plane monitoring Compatible with version 1.11 or higher Service monitoring Compatible with version 1.13 or higher Requirements The New Relic Kubernetes integration has the following requirements: Linux distribution compatible with New Relic infrastructure agent. kube-state-metrics version 1.9.8 running on the cluster. When using CRI-O as the container runtime, the processes inside containers are not reported. Performance data is collected at the container level. Install using Helm For compatibility and requirements when installing the Kubernetes integration using Helm, see Alternative install using Helm.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 111.38011,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Kubernetes <em>integration</em>: compatibility and requirements",
        "sections": "Kubernetes <em>integration</em>: compatibility and requirements",
        "tags": "<em>Get</em> <em>started</em>"
      },
      "id": "603e92dc64441f3a974e8891"
    },
    {
      "sections": [
        "Install the infrastructure agent",
        "Tip",
        "Quick start: Use our guided install",
        "Important",
        "One agent, many capabilities",
        "Install the infrastructure monitoring agent",
        "Linux",
        "Windows Server and 10",
        "Other installation scenarios",
        "Check the source code",
        "What's next"
      ],
      "title": "Install the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "ccb11bfd79824202d189a3e743771cfc81e77710",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent/",
      "published_at": "2021-07-09T15:08:21Z",
      "updated_at": "2021-07-09T15:08:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring agent is a lightweight executable file that collects data about your hosts. It also forwards data from infrastructure integrations to New Relic, as well as log data for log analytics. There are multiple ways to install and deploy the infrastructure monitoring agent, depending on your setup and needs. This document describes how the infrastructure monitoring agent works and how to install it. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. Important If you install the agent using the New Relic One UI, the Infrastructure status API is enabled by default. One agent, many capabilities Our infrastructure monitoring agent collects performance and health data about the system resources and processes of the host where it's enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-host integrations, and logs. If you want to collect data about core services running on your host, you need to install the infrastructure monitoring agent first, and then install or enable on-host integrations. Our infrastructure monitoring agent and its integrations collect data from the system and core services. It can also forward logs to New Relic. Backend application metrics (APM) are collected by separate language agents. Notice how each integration and forwarder feed different data types in the New Relic database (NRDB). Install the infrastructure monitoring agent If our guided install doesn't work for your setup, follow the instructions for your Linux, Windows, or other setup. The infrastructure monitoring agent can currently run on many Linux distributions, as well as Windows Server. For more information on where you can run the agent, check the compatibility and requirements page. Linux The preferred way to install the Linux agent is through the package manager of your distribution. Select your distribution from the list for step-by-step instructions. Amazon Linux CentOS Container (Docker) Debian RHEL SLES Ubuntu Other Linux distros To use these installation links, you must be logged to your New Relic account. If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial. For advanced install needs, you can deploy the agent using our tarball files in assisted or manual mode. Windows Server and 10 To deploy the agent on a Windows Server host, install it using our MSI installer. For a guided procedure, click the button below and follow the step-by-step instructions: Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, see our tutorial. For advanced installation needs, you can deploy the agent using our zip files in assisted or manual mode. Other installation scenarios The infrastructure monitoring agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Check the source code The infrastructure monitoring agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README. What's next After you've installed the infrastructure monitoring agent: Learn how to configure the agent or edit the config template. Install on-host integrations (for example, for Apache or MySQL). Enable log forwarding using the infrastructure agent. Learn how to manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 101.28351,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Quick <em>start</em>: Use our guided install",
        "tags": "<em>Get</em> <em>started</em>",
        "body": ". Then ingest up to 100GB of data for free each month. Forever. Quick <em>start</em>: Use our guided install The quickest way to <em>get</em> <em>started</em> with our infrastructure monitoring agent is through our guided install. Ready to <em>get</em> <em>started</em>? Click one of these button to try it out. Guided install EU Guided install"
      },
      "id": "603e79bd64441f99814e8888"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha": [
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-07-14T15:02:55Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.37793,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "sections": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. <em>remote_write</em>: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em>?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; <em>write</em>_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Tip",
        "Set up the integration",
        "Mapping of Prometheus metric types",
        "Override metric type mappings",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-07-09T17:36:25Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Tip To use Prometheus integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Mapping of Prometheus metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts or sub-accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.05452,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add the <em>remote</em> <em>write</em> URL to your Helm values.yaml file. Replace <em>remoteWrite</em>: [] with two lines similar to the following"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "0d190be5dc4fd91ce6bbcef7343d01f75670ca51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2021-07-09T09:02:17Z",
      "updated_at": "2021-03-13T03:46:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query for NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NrIntegrationError query on the error message using NRQL, then look at the Message field in the UI to see a description of what went wrong. For example: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' Copy Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If youve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.69336,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em>"
      },
      "id": "6044e65d196a67914a960f6b"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Tip",
        "Set up the integration",
        "Mapping of Prometheus metric types",
        "Override metric type mappings",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-07-09T17:36:25Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Tip To use Prometheus integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Mapping of Prometheus metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts or sub-accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.05452,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add the <em>remote</em> <em>write</em> URL to your Helm values.yaml file. Replace <em>remoteWrite</em>: [] with two lines similar to the following"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "0d190be5dc4fd91ce6bbcef7343d01f75670ca51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2021-07-09T09:02:17Z",
      "updated_at": "2021-03-13T03:46:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query for NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NrIntegrationError query on the error message using NRQL, then look at the Message field in the UI to see a description of what went wrong. For example: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' Copy Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If youve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.69336,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em>"
      },
      "id": "6044e65d196a67914a960f6b"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-07-09T08:40:14Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.68828,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "6044e621196a67b846960f6b"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-07-10T06:12:05Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.66824,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-07-09T14:13:02Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.10043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-07-09T08:58:29Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=clusterName SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.09183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-rename-or-copy-prometheus-attributes": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-07-10T06:12:05Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.66824,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-07-09T14:13:02Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.10043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-07-09T08:58:29Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=clusterName SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.09183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-07-10T06:12:05Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.66824,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-07-09T14:13:02Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.10043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-07-09T09:36:50Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.77773,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations": [
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-07-09T14:13:02Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.10043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-07-09T08:58:29Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=clusterName SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.09183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-07-09T09:36:50Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.77773,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/ignore-or-include-prometheus-metrics": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-07-10T06:12:05Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.66824,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-07-09T14:13:02Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.10043,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-07-09T08:58:29Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=clusterName SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.09183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-07-10T06:12:05Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.66823,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-07-09T08:58:29Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=clusterName SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.09183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-07-09T09:36:50Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.77773,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-remote-write/prometheus-remote-write-integration": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 3197.8796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics or <em>remote</em> <em>write</em> <em>integration</em>?",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "This page provides an overview of New Relic&#x27;s <em>Prometheus</em> <em>integration</em> options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. <em>Prometheus</em> OpenMetrics or <em>remote</em> <em>write</em> <em>integration</em>? We currently offer two"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "image": "https://docs.newrelic.com/static/d2a9c929c7541b67b6fe4c87844fc01b/ae694/prometheus_grafana_dashboard.png",
      "url": "https://docs.newrelic.com/whats-new/2020/08/create-grafana-dashboards-prometheus-data-stored-new-relic/",
      "sections": [
        "Create Grafana dashboards with Prometheus data stored in New Relic",
        "Step 1: Get data flowing into New Relic with the Prometheus remote write integration",
        "Step 2: Configure your Grafana dashboards to use Prometheus data stored in New Relic"
      ],
      "published_at": "2021-07-09T21:17:46Z",
      "title": "Create Grafana dashboards with Prometheus data stored in New Relic",
      "updated_at": "2021-03-11T00:16:19Z",
      "type": "docs",
      "external_id": "da09ab47a2ac806ad3ed1fa67e3a02dd54394383",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "Weve teamed up with Grafana Labs so you can use our Telemetry Data Platform as a data source for Prometheus metrics and see them in your existing dashboards, seamlessly tapping into the reliability, scale, and security provided by New Relic. Follow the steps below or use this more detailed walkthrough to send Prometheus data to New Relic, so that Grafana can populate your existing Prometheus-specific dashboards with that data. This process requires Prometheus version 2.15.0 or higher and Grafana version 6.7.0 or higher. Youll also need to sign up for New Relic. Here's an example of how these Grafana dashboards with Prometheus data look in our new dark mode. Step 1: Get data flowing into New Relic with the Prometheus remote write integration Go to Instrument Everything  US or Instrument Everything  EU, then click the Prometheus tile. You can also go to the Prometheus remote write setup page to get your remote_write URL. For more information on how to set up the Prometheus remote write integration, check out our docs. Step 2: Configure your Grafana dashboards to use Prometheus data stored in New Relic For more information on how to configure New Relic as a Prometheus data source for Grafana, check out our docs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1872.2073,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create Grafana dashboards with <em>Prometheus</em> data stored in New Relic",
        "sections": "Step 1: Get data flowing into New Relic with the <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "body": " these Grafana dashboards with <em>Prometheus</em> data look in our new dark mode. Step 1: Get data flowing into New Relic with the <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em> Go to Instrument Everything  US or Instrument Everything  EU, then click the <em>Prometheus</em> tile. You can also go to the <em>Prometheus</em> <em>remote</em> <em>write</em>"
      },
      "id": "60445821e7b9d23b585799e4"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-07-09T08:40:14Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1869.8502,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em> in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can configure your <em>remote</em> <em>write</em> <em>integration</em> so that New"
      },
      "id": "6044e621196a67b846960f6b"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages": [
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-07-14T15:02:55Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.37738,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "sections": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. <em>remote_write</em>: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em>?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; <em>write</em>_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Tip",
        "Set up the integration",
        "Mapping of Prometheus metric types",
        "Override metric type mappings",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-07-09T17:36:25Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Tip To use Prometheus integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Mapping of Prometheus metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts or sub-accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 193.0545,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add the <em>remote</em> <em>write</em> URL to your Helm values.yaml file. Replace <em>remoteWrite</em>: [] with two lines similar to the following"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-07-09T08:40:14Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.68826,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "6044e621196a67b846960f6b"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration": [
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-07-14T15:02:55Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 328.37738,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "sections": "Drop data using <em>Prometheus</em> <em>remote</em> <em>write</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. <em>remote_write</em>: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em>?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; <em>write</em>_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "0d190be5dc4fd91ce6bbcef7343d01f75670ca51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2021-07-09T09:02:17Z",
      "updated_at": "2021-03-13T03:46:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query for NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NrIntegrationError query on the error message using NRQL, then look at the Message field in the UI to see a description of what went wrong. For example: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' Copy Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If youve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.69336,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em>"
      },
      "id": "6044e65d196a67914a960f6b"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-07-09T08:40:14Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 192.68826,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "6044e621196a67b846960f6b"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/debug-issues-data-sent-metric-api-prometheus-integration": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.82458,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-07-14T15:02:55Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 112.882935,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> remote write",
        "sections": "Drop data using <em>Prometheus</em> remote write",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;write?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; write_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-07-09T14:38:28Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.674095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.82452,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-07-14T15:02:55Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 112.88289,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> remote write",
        "sections": "Drop data using <em>Prometheus</em> remote write",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;write?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; write_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-07-09T14:38:28Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.674095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/get-logs-prometheus-integration": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.82452,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-07-14T15:02:55Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 112.88289,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> remote write",
        "sections": "Drop data using <em>Prometheus</em> remote write",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;write?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; write_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-07-09T14:38:28Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.674095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/get-scraper-metrics-prometheus-integration": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.82452,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-07-14T15:02:55Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 112.88289,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> remote write",
        "sections": "Drop data using <em>Prometheus</em> remote write",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;write?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; write_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-07-09T14:38:28Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.674095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.82446,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-07-14T15:02:55Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 112.88284,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> remote write",
        "sections": "Drop data using <em>Prometheus</em> remote write",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;write?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; write_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-07-09T09:35:44Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 103.176186,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.82446,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-07-14T15:02:55Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 112.88284,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> remote write",
        "sections": "Drop data using <em>Prometheus</em> remote write",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;write?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; write_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-07-09T14:38:28Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.674095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/restarts-gaps-data-kubernetes": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.82446,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-07-14T15:02:55Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 112.88284,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> remote write",
        "sections": "Drop data using <em>Prometheus</em> remote write",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;write?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; write_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-07-09T14:38:28Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.674095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.82442,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "Drop data using Prometheus remote write",
        "Tip",
        "Drop entire metric data points from remote write integration",
        "Example",
        "Drop specific labels or attributes from data points",
        "Prometheus or NerdGraph?",
        "Considerations for the Prometheus config file method",
        "Considerations the NerdGraph method",
        "Learn more"
      ],
      "title": "Drop data using Prometheus remote write",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "f3e07dd4f6bbdb65881f13035af5af172c5409e7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/remote-write-drop-data/",
      "published_at": "2021-07-14T15:02:55Z",
      "updated_at": "2021-07-09T08:33:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can drop data you don't want to keep by changing the remote_write section of the YAML config file. Tip You can also drop remote write data using NerdGraph. For more information, see Drop data using NerdGraph. Drop entire metric data points from remote write integration If a target is sending a noisy metric that you don't want sent to New Relic, you can specify that New Relic should drop that data. Example Let's say you don't want to receive data for the metric node_memory_active_bytes from an instance running at localhost:9100. Using the write_relabel_config entry shown below, you can target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - source_labels: ['__name__', 'instance'] regex: 'node_memory_active_bytes;localhost:9100' action: 'drop' Copy This tells Prometheus that you want to do some action against metrics with these labels. To limit which metrics with these labels are affected, you must include some value for regex. By default this value is set to .* and it will include all metrics. In this case, it will drop all metric data points coming out of Prometheus via remote write. Drop specific labels or attributes from data points If a target is sending specific labels or attributes you're not interested in receiving, you can drop these from the metrics you receive. Example Let's say one of your targets is sending a bunch of extra attributes you're not interested in receiving. These might include things like high cardinality attributes such as unique machine identifiers, JVM IDs, or similar. In this case, you need to change both the remote_write and the scrape_configs section of the YAML file. The result will look something like this: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=macbook-server-cluster bearer_token: <redacted> write_relabel_configs: - regex: 'extraLabelToRemove.*' action: 'labeldrop' ... scrape_configs: # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config. - job_name: 'node' # Override the global default and scrape targets from this job every 5 seconds. scrape_interval: 5s static_configs: - targets: ['localhost:9100'] labels: group: 'production' keepLabelName1: 'please-keep-me' extraLabelToRemove: 'please-remove-me' extraLabelToRemove1: 'please-remove-me' extraLabelToRemove2: 'please-remove-me' extraLabelToRemove4: 'please-remove-me' extraLabelToRemove3: 'please-remove-me' extraLabelToRemove5: 'please-remove-me' Copy Prometheus or NerdGraph? There are advantages to both dropping data using the method described on this page and using NerdGraph. This section is intended to help you figure out which method is better for your specific needs and preferences. Considerations for the Prometheus config file method With this method, your dropped data never leaves the associated Prometheus instance. This is a valuable feature if bytes transferred is a cost consideration on the app hosting side. However, this method may be less appealing than the NerdGraph option due to the following considerations: Maintained via config yaml files that need to be loaded onto each Prometheus instance (or via a shared storage mechanism) Requires access to Prometheus server, meaning that either: The server needs to be restarted Served must be be accessed at port with path /-/reload (assuming the server has lifecycle management enabled as described here in the Prometheus configuration docs. Considerations the NerdGraph method NerdGraph is a great option if you want to manage all your data dropping in a single place. It can also be updated easily via the API and requires no restart or interaction with Prometheus. However, this method applies rules to all incoming data points. This means that you should set up your rules with careful consideration using WHERE filtering. For more information, see Drop data using NerdGraph. Learn more Send Prometheus metric data to New Relic Prometheus High Availability (HA)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 112.8828,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Drop data using <em>Prometheus</em> remote write",
        "sections": "Drop data using <em>Prometheus</em> remote write",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": " target the metric name using the __name__ label in combination with the instance name. remote_write: - url: https:&#x2F;&#x2F;metric-api.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;write?<em>prometheus</em>_server=macbook-server-cluster bearer_token: &lt;redacted&gt; write_relabel_configs: - source_labels: [&#x27;__name__&#x27;, &#x27;instance&#x27;] regex"
      },
      "id": "60e809e4e7b9d298bafc1035"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-07-09T14:38:28Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 110.674095,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features": [
    {
      "sections": [
        "Translate PromQL queries to NRQL",
        "Tip",
        "Prometheus and New Relic metric types",
        "Mapping between NRQL and our PromQL-style queries",
        "PromQL-style query example",
        "NRQL query example",
        "Filter examples",
        "PromQL-style to NRQL query examples",
        "For more help"
      ],
      "title": "Translate PromQL queries to NRQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "fcf45fb8fb49f9d22f74574c2e7032533377e584",
      "image": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/images/PROMQL-query-2.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/",
      "published_at": "2021-07-09T08:43:02Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Do you have a PromQL query youd like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style query language to explore your Prometheus OpenMetrics integration data along with other data sent to New Relic. Tip To run PromQL-style queries in New Relic One, go to the query builder advanced PromQL-style mode. Prometheus and New Relic metric types The different metric types supported by Prometheus and New Relic are related to each other: New Relic Prometheus Description Count Counter The Prometheus counter is a cumulative sum while the New Relic count is a delta sum. For example, if you see 2 requests in the first reporting period and 3 requests in the second reporting period. The Prometheus counter will report 2 and then 5, while the New Relic count will report 2 and then 3. Gauge Gauge A Prometheus gauge is similar to a New Relic gauge. Multiple counts Histogram Prometheus automatically maps a histogram to a set of counters. In New Relic, these counters should be changed to deltas and reported as counts. Gauges and counts Summary Prometheus represents a Summary with a given basename as the following time series: a basename_sum a basename_count and 0 or more of basename{quantile=\".xx\"...} metrics New Relic maps the _sum as a Summary, the _count as a Counter, and each quantile metric as a Gauge. Summary (No equivalent in Prometheus) New Relic has a distinct metric type called a summary that is different than the Prometheus summary. It is designed for reporting aggregated discrete events so that you can query the count, sum, min, max, and average values. Mapping between NRQL and our PromQL-style queries Tip To see how New Relic translates PromQL-style queries to NRQL, write a query in the query builder PromQL-style tab, then switch to the NRQL tab. This table shows the mapping between NRQL and our PromQL-style queries when exploring data. For more contextual information, see the examples. Description Mapping between NRQL and PromQL-style queries Search for attributes: Explore the attributes on the container_memory_usage_bytes metric. PromQL: container_memory_usage_bytes Copy NRQL: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy Find attribute's value: Explore the current value of the container_memory_usage_bytes metric for unique id attributes. PromQL: sum(container_memory_usage_bytes) by (id) Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy Visualize the attribute's value: Chart the value of the container_memory_usage_bytes metric with the given id attribute value. PromQL: container_memory_usage_bytes{id=\"/\"} Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = '/' TIMESERIES Copy PromQL-style query example 1. Start your query. When exploring your data for a particular metric in PromQL, such as memory by container usage in bytes, you can start with a query such as: container_memory_usage_bytes Copy This will chart all the unique metric timeseries for the input metric. 2. Filter the query results. Looking at the data, you can add more query parameters to filter down the number of metric timeseries. For example, if you only want timeseries where the id is /, the PromQL-style query will be: container_memory_usage_bytes{id=\"/\"} Copy PromQL-style example: To filter the data, run this PromQL-style query: container_memory_usage_bytes { id=\"/\"}. NRQL query example 1. Query available metrics. To explore your data, start by looking at all the available metrics. Use the following NRQL query: FROM Metric SELECT uniques(metricName) Copy 2. Find unique attributes. Once you have found the metric you want to review, such as container_memory_usage_bytes, you can find the unique attributes with the following query: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy The results will show each available attribute key and the value type (string, boolean, or number). 3. Aggregate and chart the metrics. To chart metrics using NRQL, you first need an aggregation function. For example, you can use latest for gauges, sum for counts, and average for summaries. As the following chart shows, all the unique timeseries are aggregated into one unique timeseries by default: one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes'. 4. View metrics by ID. To view the unique metric timeseries with various id values, run the following query: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT latest(container_memory_usage_bytes) FACET id. 5. Add the selected ID to the query. Next you can select an id value and put it in the NRQL where clause. FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = \"/\" timeseries Copy one.newrelic.com > Query your data: This example shows the data displayed after running From Metric select latest(container_memory_usage_bytes) where id = \"/\" timeseries. Filter examples Both our PromQL-style query language and NRQL provide syntax to filter down the number of unique metric timeseries. PromQL-style uses brackets to filter. NRQL uses a WHERE clause. Here are some example queries: Description PromQL-style and NRQL queries Select data with specific values. PromQL: go_memstats_heap_alloc_bytes{job=\"apiserver\", instance=\"1234\"}) Copy NRQL: To only select data with specific values in NRQL, use the WHERE clause with =. In this example, all data must have the selected value for job and handler. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE job = 'apiserver' AND instance = '1234' TIMESERIES Copy Select data with multiple values. PromQL: go_memstats_heap_alloc_bytes{environment=~\"staging|testing|development\",method!=\"GET\"} Copy NRQL: In NRQL use the in clause to select multiple values for an attribute and the != sign to select all values but the one listed. In this example, the environment can be staging, testing, or development, and the method cannot be GET. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE environment IN ('staging', 'testing', 'development') AND method != 'GET' TIMESERIES Copy Select data using partial string values. PromQL: go_memstats_heap_alloc_bytes{job=~\"api.*\"} Copy NRQL: In NRQL use the LIKE clause to match part of a string value. In this example, all data will be returned where the job attributes start with api. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHEREe job LIKE 'api%' TIMESERIES Copy PromQL-style to NRQL query examples You can simulate the following PromQL-style queries with NRQL queries: Description PromQL-style and NRQL queries Measure the per second rate over the last minute of the http_request_total metric. PromQL: sum(rate(http_requests_total[1m])) Copy NRQL: FROM Metric SELECT rate(sum(http_request_total), 1 second) TIMESERIES 1 minute Copy Chart the difference of the two metrics, then divide by 1024. PromQL: (instance_memory_limit_bytes - instance_memory_usage_bytes) / 1024 Copy NRQL: FROM Metric SELECT (latest(instance_memory_limit_bytes) - latest(instance_memory_usage_bytes)) / 1024 TIMESERIES Copy Provide the summed rate per 30-second interval by each handler. PromQL: sum(rate(http_requests_total[30s])) by (handler) Copy NRQL: FROM Metric SELECT rate(sum(http_requests_total), 30 seconds) FACET handler TIMESERIES Copy Chart the difference in the two metrics where the instance is named foo and the fstype is either ext4 or xfs. PromQL: (node_filesystem_free_bytes{instance='foo',fstype=~\"ext4|xfs\"} / node_filesystem_size_bytes{instance='foo',fstype=~\"ext4|xfs\"}) Copy NRQL: FROM Metric SELECT latest(node_filesystem_free_bytes) / latest(node_filesystem_size_bytes) WHERE instance = 'foo' AND fstype IN ('ext4', 'xfs') Copy For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.8562,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Translate PromQL <em>queries</em> to NRQL",
        "sections": "<em>Prometheus</em> <em>and</em> New Relic metric types",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": "Do you have a PromQL <em>query</em> youd like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style <em>query</em> language to explore your <em>Prometheus</em> OpenMetrics integration <em>data</em> along with other <em>data</em> sent to New"
      },
      "id": "603ead4528ccbcbecfeba77b"
    },
    {
      "sections": [
        "View and query your Prometheus data",
        "Default attributes for the OpenMetrics integration",
        "Default attributes for the remote write integration",
        "NRQL query examples",
        "Get metric names",
        "Get the attributes for a metric",
        "Get the values for an attribute in OpenMetrics",
        "Build the query",
        "Get metric values",
        "Get a chart of the metric",
        "Query counter metrics (deltas)",
        "View connected Redis clients per pod with OpenMetrics",
        "Docker: View average memory free for scraped endpoints",
        "Kubernetes: View average memory usage for pods in a deployment",
        "View data in New Relic",
        "Create histograms and summaries"
      ],
      "title": "View and query your Prometheus data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "91a0388492ae73a9ddb8a1701b639a6b6a71822a",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data/",
      "published_at": "2021-07-09T09:48:08Z",
      "updated_at": "2021-03-16T04:13:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To query and visualize the metrics collected for your Prometheus OpenMetrics or remote write integration with New Relic, you can use NRQL. You can also translate your PromQL-style queries to NRQL using either Grafana or the query builder. All metrics for Docker and Kubernetes are stored in the Metric type. Default attributes for the OpenMetrics integration By default, the following attributes will be added to all metrics for Docker and Kubernetes integrations: Default attributes (all integrations) Description clusterName The name of the cluster provided in the scraper configuration. integrationName The name of this integration (nri-prometheus). integrationVersion The version of the integration; for example, 0.2.0. metricName The name of the metric itself. nrMetricType The type of the New Relic Metric type; for example, Gauges. promMetricType The metric type of the Prometheus metric scrapedEndpoint The URL of the endpoint is being scraped. Kubernetes: If the scraper is running in Kubernetes, New Relic also adds the following attributes to all the metrics: Additional Kubernetes attributes Description deploymentName Name of the deployment, if scraping a pod. label The Kubernetes labels of the object being scraped, prefixed by \"label\". namespaceName Name of the namespace. nodeName Name of the node where the pod being scraped is running, if applicable. podName Name of the pod being scraped, if applicable. serviceName Name of the service being scraped, if applicable Default attributes for the remote write integration By default, the following attributes will be added to Prometheus remote write metrics: Default attributes (all integrations) Description prometheus_server A user supplied label specified as a Prometheus remote write URL parameter. The value supplied should be unique as it is intended to differentiate between source Prometheus servers at query time. Unspecified by default. newrelic.source The name of the New Relic ingest point (prometheusAPI). instrumentation.provider prometheus instrumentation.name remote-write instrumentation.source A user supplied identifier for the source of the Prometheus data that matches the value of prometheus_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL query examples When you build queries, be aware that there is no linking between the metrics, entities, and attributes. Use the following NRQL queries to find out which metrics are available and which attributes are present on these metrics: Get metric names To get all metric names for OpenMetrics: FROM Metric SELECT uniques(metricName) Copy To get metric names for a remote write integration: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' Copy To get metric names for a remote write integration from a single Prometheus source: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' AND instrumentation.source='<ds>' Copy To get metric names for a specific OpenMetrics endpoint: FROM Metric SELECT uniques(metricName) WHERE scrapedEndpoint='<ep>' Copy To get metric names for a specific OpenMetrics cluster, namespace, or pod: FROM Metric SELECT uniques(metricName) WHERE clusterName='<cn>' Copy FROM Metric SELECT uniques(metricName) WHERE namespaceName='<ns>' Copy FROM Metric SELECT uniques(metricName) WHERE podName='<pod>' Copy Get the attributes for a metric To get all attributes for the selected metric: FROM Metric SELECT keyset() WHERE metricName='<mn>' Copy Get the values for an attribute in OpenMetrics The autocomplete will show all values of the attribute, regardless of the pod. To determine the attribute values for a specific pod: FROM Metric SELECT uniques(<attribute>) WHERE metricName='<mn>' AND podName='<pod>' Copy Build the query Using metric name and attributes, you can query your data. For more information about facets, time series, and time selection, see the NRQL documentation. To build PromQL-style queries, see our docs. Get metric values To get raw metric values: FROM Metric SELECT <metricName> WHERE <attribute>='<value>' Copy Get a chart of the metric To get a chart of the metric with an aggregator of average, min, max, or sum: FROM Metric SELECT <aggregator>(<metricname>) WHERE <attribute>='<value>' TIMESERIES Copy Query counter metrics (deltas) Currently the integration calculates the deltas for counter metrics. This is why queries on counter metrics will show the deltas of the counter instead of the absolute value of the counter. View connected Redis clients per pod with OpenMetrics Docker: This example assumes you are scraping Redis exporters. To view the number of connected Redis clients per endpoint in a cluster: FROM Metric SELECT latest(redis_connected_clients) WHERE clusterName='my-cluster' FACET scrapedEndpoint TIMESERIES Copy Kubernetes: This example assumes that you have Redis pods with the Redis exporter installed. To view the number of connected Redis clients per pod in the default namespace: FROM Metric SELECT latest(redis_connected_clients) WHERE namespaceName='default' FACET podName TIMESERIES Copy Docker: View average memory free for scraped endpoints This example assumes you are scraping node exporters for Docker and want to use OpenMetrics. To view average memory free for all scraped endpoints in a cluster: FROM Metric SELECT average(node_memory_MemFree_bytes) WHERE clusterName='my-cluster' Copy Kubernetes: View average memory usage for pods in a deployment To view average memory usage for all pods in a Kubernetes deployment using OpenMetrics: FROM Metric SELECT average(container_memory_usage_bytes) WHERE deploymentName='my-app-deployment' AND namespaceName='default' Copy View data in New Relic When you query the data, you can view the results in the New Relic UI. You can also visualize the data as charts, histograms, etc. To view the NRQL query results for your Prometheus integration's data: Go to one.newrelic.com > Query your data. For more information, see New Relic's query builder documentation. Create histograms and summaries With remote write or version 1.2.0 or higher of the Prometheus OpenMetrics integration, you can create histograms and percentiles (summaries) of your data. The OpenMetrics data is based on New Relic's guidelines in GitHub for higher level metric abstractions, while the remote write data closely matches the schema of the original Prometheus data. Data presentation Comments Histograms A bucket <basename>_bucket{le=\"42\"} will be sent as this: For OpenMetrics: <basename>_buckets For remote write: <basename>_bucket The dimension will be this: For OpenMetrics: {histogram.bucket.upperBound=\"42\"} For remote write: {histogram.bucket.le=\"42\"} Percentiles Quantiles (summaries) are transformed into percentiles. A metric <basename>{quantile=\"0.3\"} will be sent to New Relic as <basename>.percentiles. The dimension will be this: {percentile=\"30\"} NRQL has two functions that work on remote write ingested PromQL: bucketPercentile() and histogram(). The links include query examples.These two functions don't work on OpenMetrics ingested buckets. To better support visualization of histograms, percentiles are calculated based on the histogram metrics and sent to New Relic. To configure the calculated percentiles for OpenMetrics, use the percentiles configuration option.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.85567,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "sections": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " identifier for the source of the <em>Prometheus</em> <em>data</em> that matches the value of <em>prometheus</em>_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL <em>query</em> examples When you build queries, be aware that there is no linking between the metrics, entities"
      },
      "id": "603eb04be7b9d20f9d2a07eb"
    },
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.82442,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric <em>data</em> to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce <em>data</em> volumes by 50%. Set your filters and configure <em>data</em> to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql": [
    {
      "sections": [
        "Supported PromQL Features",
        "Important",
        "Supported features",
        "Aggregation operators and functions",
        "Arithmetic binary operators",
        "Logical operators",
        "Date/time functions",
        "Mathematical functions",
        "Rate-like functions",
        "Predictive functions",
        "Time-series selectors",
        "PromQL troubleshooting",
        "Metric types",
        "Limits",
        "Range vector selectors (sliding windows and smoothing behavior)",
        "Query range and data scraping intervals"
      ],
      "title": "Supported PromQL Features",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "d4a93b9db3bfe5639ed01968b6e55a8e0aaa9389",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features/",
      "published_at": "2021-07-09T10:34:08Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic supports PromQL-style queries, and our query builder offers a PromQL-style query mode that translates PromQL syntax queries into the closest NRQL approximation. Although the method of approximation means that a handful of edge cases are not fully supported, it provides coverage for an overwhelming majority of queries, supporting over 99.5% of queries across the 7.8 million top Grafana dashboard downloads. Read on to learn about how we work with PromQL queries, as well as differences between standard PromQL and our PromQL-like query language we want you to be aware of. Important For general information about Prometheus queries and operators, see the Prometheus.io documentation. Supported features We support the following aggregation, arithmetic, mathematical, and rate-like functions. As we continue to expand support for Prometheus and PromQL, this list will be updated. Aggregation operators and functions Aggregation operators: avg() count() min() max() quantile() stddev() stdvar() sum() topk() Aggregation functions: histogram_quantile() <aggregation>_over_time() functions: avg_over_time count_over_time min_over_time max_over_time quantile_over_time stdev_over_time stvar_over_time sum_over_time Arithmetic binary operators + (addition) - (subtraction) * (multiplication) / (division) % (percent) ^ (power/exponents) Logical operators and or Date/time functions day_of_month() day_of_week() days_in_month() hour() minute() month() time() timestamp() year() Mathematical functions abs() ceil() clamp_max() clamp_min() exp() floor() ln() log10() log2() round() sqrt() Rate-like functions delta() deriv() idelta() increase() irate() rate() Predictive functions predict_linear Time-series selectors We offer support for PromQL time-series selectors including the following: instant vector selectors range series selectors offset modifier Important We only support offset queries if every vector in the query has the same offset value. PromQL troubleshooting This section describes differences in behavior between PromQL and our PromQL-style query behavior and how to work with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the query builder. Metric types Prometheus recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase() on counters, but queries in Prometheus still work most of the time even if they dont follow those instructions. However, because NRDB converts PromQL-style accumulating counters to delta counters, our implementation is unforgiving when using these functions on the wrong data type and will produce different or incorrect answers. For this reason, it's best to follow all Prometheus recommendations when working with our PromQL-style queries, even if you don't follow these recommendations in Prometheus. Limits In order to ensure the stability and performance of our system for all users, we place some limits on what queries can be run. In all cases, we enforce a limit of 366 steps in range queries. We also default to only returning 100 timeseries from queries by default. If you want to see more (or fewer), you need to explicitly add a topk() to your query. (Note that the topk() implementation in our PromQL-style query is different from that of Prometheus.) We limit the total memory a query can use. This means that requests for large numbers of time steps or large numbers of time series may be rejected, particularly if they are combined with an aggregation like unique count or quantile which require significantly more memory to compute than simple arithmetic aggregations. Range vector selectors (sliding windows and smoothing behavior) We provide support for sliding window timeseries aggregations. For more information, see our NRQL syntax, clauses, and functions resource and our sliding windows deep dive. For information on translating between NRQL and our PromQL-style language, see Translate PromQL queries to NRQL. Query range and data scraping intervals The range of your query in PromQL must be larger than the duration of the step size of the query to avoid the error \"TIMESERIES bucket size is larger than the current time window\". We inspect data up to one minute old when servicing instant queries. If your scrape interval is greater than 1 minute, some queries may result in No data found. Avoid this by sending data at least once per minute. If the timeseries unit for your NRQL query is less than the scrape interval for your application, some periods will lack data, and the resulting graph may be jagged or contain peaks and valleys. In general, set the step size to your scrape interval, or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.8579,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Query</em> range <em>and</em> <em>data</em> scraping intervals",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the <em>query</em> builder. Metric types <em>Prometheus</em> recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase"
      },
      "id": "603e9523e7b9d292ec2a07ce"
    },
    {
      "sections": [
        "View and query your Prometheus data",
        "Default attributes for the OpenMetrics integration",
        "Default attributes for the remote write integration",
        "NRQL query examples",
        "Get metric names",
        "Get the attributes for a metric",
        "Get the values for an attribute in OpenMetrics",
        "Build the query",
        "Get metric values",
        "Get a chart of the metric",
        "Query counter metrics (deltas)",
        "View connected Redis clients per pod with OpenMetrics",
        "Docker: View average memory free for scraped endpoints",
        "Kubernetes: View average memory usage for pods in a deployment",
        "View data in New Relic",
        "Create histograms and summaries"
      ],
      "title": "View and query your Prometheus data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "91a0388492ae73a9ddb8a1701b639a6b6a71822a",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data/",
      "published_at": "2021-07-09T09:48:08Z",
      "updated_at": "2021-03-16T04:13:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To query and visualize the metrics collected for your Prometheus OpenMetrics or remote write integration with New Relic, you can use NRQL. You can also translate your PromQL-style queries to NRQL using either Grafana or the query builder. All metrics for Docker and Kubernetes are stored in the Metric type. Default attributes for the OpenMetrics integration By default, the following attributes will be added to all metrics for Docker and Kubernetes integrations: Default attributes (all integrations) Description clusterName The name of the cluster provided in the scraper configuration. integrationName The name of this integration (nri-prometheus). integrationVersion The version of the integration; for example, 0.2.0. metricName The name of the metric itself. nrMetricType The type of the New Relic Metric type; for example, Gauges. promMetricType The metric type of the Prometheus metric scrapedEndpoint The URL of the endpoint is being scraped. Kubernetes: If the scraper is running in Kubernetes, New Relic also adds the following attributes to all the metrics: Additional Kubernetes attributes Description deploymentName Name of the deployment, if scraping a pod. label The Kubernetes labels of the object being scraped, prefixed by \"label\". namespaceName Name of the namespace. nodeName Name of the node where the pod being scraped is running, if applicable. podName Name of the pod being scraped, if applicable. serviceName Name of the service being scraped, if applicable Default attributes for the remote write integration By default, the following attributes will be added to Prometheus remote write metrics: Default attributes (all integrations) Description prometheus_server A user supplied label specified as a Prometheus remote write URL parameter. The value supplied should be unique as it is intended to differentiate between source Prometheus servers at query time. Unspecified by default. newrelic.source The name of the New Relic ingest point (prometheusAPI). instrumentation.provider prometheus instrumentation.name remote-write instrumentation.source A user supplied identifier for the source of the Prometheus data that matches the value of prometheus_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL query examples When you build queries, be aware that there is no linking between the metrics, entities, and attributes. Use the following NRQL queries to find out which metrics are available and which attributes are present on these metrics: Get metric names To get all metric names for OpenMetrics: FROM Metric SELECT uniques(metricName) Copy To get metric names for a remote write integration: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' Copy To get metric names for a remote write integration from a single Prometheus source: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' AND instrumentation.source='<ds>' Copy To get metric names for a specific OpenMetrics endpoint: FROM Metric SELECT uniques(metricName) WHERE scrapedEndpoint='<ep>' Copy To get metric names for a specific OpenMetrics cluster, namespace, or pod: FROM Metric SELECT uniques(metricName) WHERE clusterName='<cn>' Copy FROM Metric SELECT uniques(metricName) WHERE namespaceName='<ns>' Copy FROM Metric SELECT uniques(metricName) WHERE podName='<pod>' Copy Get the attributes for a metric To get all attributes for the selected metric: FROM Metric SELECT keyset() WHERE metricName='<mn>' Copy Get the values for an attribute in OpenMetrics The autocomplete will show all values of the attribute, regardless of the pod. To determine the attribute values for a specific pod: FROM Metric SELECT uniques(<attribute>) WHERE metricName='<mn>' AND podName='<pod>' Copy Build the query Using metric name and attributes, you can query your data. For more information about facets, time series, and time selection, see the NRQL documentation. To build PromQL-style queries, see our docs. Get metric values To get raw metric values: FROM Metric SELECT <metricName> WHERE <attribute>='<value>' Copy Get a chart of the metric To get a chart of the metric with an aggregator of average, min, max, or sum: FROM Metric SELECT <aggregator>(<metricname>) WHERE <attribute>='<value>' TIMESERIES Copy Query counter metrics (deltas) Currently the integration calculates the deltas for counter metrics. This is why queries on counter metrics will show the deltas of the counter instead of the absolute value of the counter. View connected Redis clients per pod with OpenMetrics Docker: This example assumes you are scraping Redis exporters. To view the number of connected Redis clients per endpoint in a cluster: FROM Metric SELECT latest(redis_connected_clients) WHERE clusterName='my-cluster' FACET scrapedEndpoint TIMESERIES Copy Kubernetes: This example assumes that you have Redis pods with the Redis exporter installed. To view the number of connected Redis clients per pod in the default namespace: FROM Metric SELECT latest(redis_connected_clients) WHERE namespaceName='default' FACET podName TIMESERIES Copy Docker: View average memory free for scraped endpoints This example assumes you are scraping node exporters for Docker and want to use OpenMetrics. To view average memory free for all scraped endpoints in a cluster: FROM Metric SELECT average(node_memory_MemFree_bytes) WHERE clusterName='my-cluster' Copy Kubernetes: View average memory usage for pods in a deployment To view average memory usage for all pods in a Kubernetes deployment using OpenMetrics: FROM Metric SELECT average(container_memory_usage_bytes) WHERE deploymentName='my-app-deployment' AND namespaceName='default' Copy View data in New Relic When you query the data, you can view the results in the New Relic UI. You can also visualize the data as charts, histograms, etc. To view the NRQL query results for your Prometheus integration's data: Go to one.newrelic.com > Query your data. For more information, see New Relic's query builder documentation. Create histograms and summaries With remote write or version 1.2.0 or higher of the Prometheus OpenMetrics integration, you can create histograms and percentiles (summaries) of your data. The OpenMetrics data is based on New Relic's guidelines in GitHub for higher level metric abstractions, while the remote write data closely matches the schema of the original Prometheus data. Data presentation Comments Histograms A bucket <basename>_bucket{le=\"42\"} will be sent as this: For OpenMetrics: <basename>_buckets For remote write: <basename>_bucket The dimension will be this: For OpenMetrics: {histogram.bucket.upperBound=\"42\"} For remote write: {histogram.bucket.le=\"42\"} Percentiles Quantiles (summaries) are transformed into percentiles. A metric <basename>{quantile=\"0.3\"} will be sent to New Relic as <basename>.percentiles. The dimension will be this: {percentile=\"30\"} NRQL has two functions that work on remote write ingested PromQL: bucketPercentile() and histogram(). The links include query examples.These two functions don't work on OpenMetrics ingested buckets. To better support visualization of histograms, percentiles are calculated based on the histogram metrics and sent to New Relic. To configure the calculated percentiles for OpenMetrics, use the percentiles configuration option.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.85567,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "sections": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " identifier for the source of the <em>Prometheus</em> <em>data</em> that matches the value of <em>prometheus</em>_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL <em>query</em> examples When you build queries, be aware that there is no linking between the metrics, entities"
      },
      "id": "603eb04be7b9d20f9d2a07eb"
    },
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.82442,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric <em>data</em> to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce <em>data</em> volumes by 50%. Set your filters and configure <em>data</em> to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data": [
    {
      "sections": [
        "Supported PromQL Features",
        "Important",
        "Supported features",
        "Aggregation operators and functions",
        "Arithmetic binary operators",
        "Logical operators",
        "Date/time functions",
        "Mathematical functions",
        "Rate-like functions",
        "Predictive functions",
        "Time-series selectors",
        "PromQL troubleshooting",
        "Metric types",
        "Limits",
        "Range vector selectors (sliding windows and smoothing behavior)",
        "Query range and data scraping intervals"
      ],
      "title": "Supported PromQL Features",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "d4a93b9db3bfe5639ed01968b6e55a8e0aaa9389",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features/",
      "published_at": "2021-07-09T10:34:08Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic supports PromQL-style queries, and our query builder offers a PromQL-style query mode that translates PromQL syntax queries into the closest NRQL approximation. Although the method of approximation means that a handful of edge cases are not fully supported, it provides coverage for an overwhelming majority of queries, supporting over 99.5% of queries across the 7.8 million top Grafana dashboard downloads. Read on to learn about how we work with PromQL queries, as well as differences between standard PromQL and our PromQL-like query language we want you to be aware of. Important For general information about Prometheus queries and operators, see the Prometheus.io documentation. Supported features We support the following aggregation, arithmetic, mathematical, and rate-like functions. As we continue to expand support for Prometheus and PromQL, this list will be updated. Aggregation operators and functions Aggregation operators: avg() count() min() max() quantile() stddev() stdvar() sum() topk() Aggregation functions: histogram_quantile() <aggregation>_over_time() functions: avg_over_time count_over_time min_over_time max_over_time quantile_over_time stdev_over_time stvar_over_time sum_over_time Arithmetic binary operators + (addition) - (subtraction) * (multiplication) / (division) % (percent) ^ (power/exponents) Logical operators and or Date/time functions day_of_month() day_of_week() days_in_month() hour() minute() month() time() timestamp() year() Mathematical functions abs() ceil() clamp_max() clamp_min() exp() floor() ln() log10() log2() round() sqrt() Rate-like functions delta() deriv() idelta() increase() irate() rate() Predictive functions predict_linear Time-series selectors We offer support for PromQL time-series selectors including the following: instant vector selectors range series selectors offset modifier Important We only support offset queries if every vector in the query has the same offset value. PromQL troubleshooting This section describes differences in behavior between PromQL and our PromQL-style query behavior and how to work with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the query builder. Metric types Prometheus recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase() on counters, but queries in Prometheus still work most of the time even if they dont follow those instructions. However, because NRDB converts PromQL-style accumulating counters to delta counters, our implementation is unforgiving when using these functions on the wrong data type and will produce different or incorrect answers. For this reason, it's best to follow all Prometheus recommendations when working with our PromQL-style queries, even if you don't follow these recommendations in Prometheus. Limits In order to ensure the stability and performance of our system for all users, we place some limits on what queries can be run. In all cases, we enforce a limit of 366 steps in range queries. We also default to only returning 100 timeseries from queries by default. If you want to see more (or fewer), you need to explicitly add a topk() to your query. (Note that the topk() implementation in our PromQL-style query is different from that of Prometheus.) We limit the total memory a query can use. This means that requests for large numbers of time steps or large numbers of time series may be rejected, particularly if they are combined with an aggregation like unique count or quantile which require significantly more memory to compute than simple arithmetic aggregations. Range vector selectors (sliding windows and smoothing behavior) We provide support for sliding window timeseries aggregations. For more information, see our NRQL syntax, clauses, and functions resource and our sliding windows deep dive. For information on translating between NRQL and our PromQL-style language, see Translate PromQL queries to NRQL. Query range and data scraping intervals The range of your query in PromQL must be larger than the duration of the step size of the query to avoid the error \"TIMESERIES bucket size is larger than the current time window\". We inspect data up to one minute old when servicing instant queries. If your scrape interval is greater than 1 minute, some queries may result in No data found. Avoid this by sending data at least once per minute. If the timeseries unit for your NRQL query is less than the scrape interval for your application, some periods will lack data, and the resulting graph may be jagged or contain peaks and valleys. In general, set the step size to your scrape interval, or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.85788,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Query</em> range <em>and</em> <em>data</em> scraping intervals",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the <em>query</em> builder. Metric types <em>Prometheus</em> recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase"
      },
      "id": "603e9523e7b9d292ec2a07ce"
    },
    {
      "sections": [
        "Translate PromQL queries to NRQL",
        "Tip",
        "Prometheus and New Relic metric types",
        "Mapping between NRQL and our PromQL-style queries",
        "PromQL-style query example",
        "NRQL query example",
        "Filter examples",
        "PromQL-style to NRQL query examples",
        "For more help"
      ],
      "title": "Translate PromQL queries to NRQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "fcf45fb8fb49f9d22f74574c2e7032533377e584",
      "image": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/images/PROMQL-query-2.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/",
      "published_at": "2021-07-09T08:43:02Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Do you have a PromQL query youd like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style query language to explore your Prometheus OpenMetrics integration data along with other data sent to New Relic. Tip To run PromQL-style queries in New Relic One, go to the query builder advanced PromQL-style mode. Prometheus and New Relic metric types The different metric types supported by Prometheus and New Relic are related to each other: New Relic Prometheus Description Count Counter The Prometheus counter is a cumulative sum while the New Relic count is a delta sum. For example, if you see 2 requests in the first reporting period and 3 requests in the second reporting period. The Prometheus counter will report 2 and then 5, while the New Relic count will report 2 and then 3. Gauge Gauge A Prometheus gauge is similar to a New Relic gauge. Multiple counts Histogram Prometheus automatically maps a histogram to a set of counters. In New Relic, these counters should be changed to deltas and reported as counts. Gauges and counts Summary Prometheus represents a Summary with a given basename as the following time series: a basename_sum a basename_count and 0 or more of basename{quantile=\".xx\"...} metrics New Relic maps the _sum as a Summary, the _count as a Counter, and each quantile metric as a Gauge. Summary (No equivalent in Prometheus) New Relic has a distinct metric type called a summary that is different than the Prometheus summary. It is designed for reporting aggregated discrete events so that you can query the count, sum, min, max, and average values. Mapping between NRQL and our PromQL-style queries Tip To see how New Relic translates PromQL-style queries to NRQL, write a query in the query builder PromQL-style tab, then switch to the NRQL tab. This table shows the mapping between NRQL and our PromQL-style queries when exploring data. For more contextual information, see the examples. Description Mapping between NRQL and PromQL-style queries Search for attributes: Explore the attributes on the container_memory_usage_bytes metric. PromQL: container_memory_usage_bytes Copy NRQL: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy Find attribute's value: Explore the current value of the container_memory_usage_bytes metric for unique id attributes. PromQL: sum(container_memory_usage_bytes) by (id) Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy Visualize the attribute's value: Chart the value of the container_memory_usage_bytes metric with the given id attribute value. PromQL: container_memory_usage_bytes{id=\"/\"} Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = '/' TIMESERIES Copy PromQL-style query example 1. Start your query. When exploring your data for a particular metric in PromQL, such as memory by container usage in bytes, you can start with a query such as: container_memory_usage_bytes Copy This will chart all the unique metric timeseries for the input metric. 2. Filter the query results. Looking at the data, you can add more query parameters to filter down the number of metric timeseries. For example, if you only want timeseries where the id is /, the PromQL-style query will be: container_memory_usage_bytes{id=\"/\"} Copy PromQL-style example: To filter the data, run this PromQL-style query: container_memory_usage_bytes { id=\"/\"}. NRQL query example 1. Query available metrics. To explore your data, start by looking at all the available metrics. Use the following NRQL query: FROM Metric SELECT uniques(metricName) Copy 2. Find unique attributes. Once you have found the metric you want to review, such as container_memory_usage_bytes, you can find the unique attributes with the following query: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy The results will show each available attribute key and the value type (string, boolean, or number). 3. Aggregate and chart the metrics. To chart metrics using NRQL, you first need an aggregation function. For example, you can use latest for gauges, sum for counts, and average for summaries. As the following chart shows, all the unique timeseries are aggregated into one unique timeseries by default: one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes'. 4. View metrics by ID. To view the unique metric timeseries with various id values, run the following query: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT latest(container_memory_usage_bytes) FACET id. 5. Add the selected ID to the query. Next you can select an id value and put it in the NRQL where clause. FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = \"/\" timeseries Copy one.newrelic.com > Query your data: This example shows the data displayed after running From Metric select latest(container_memory_usage_bytes) where id = \"/\" timeseries. Filter examples Both our PromQL-style query language and NRQL provide syntax to filter down the number of unique metric timeseries. PromQL-style uses brackets to filter. NRQL uses a WHERE clause. Here are some example queries: Description PromQL-style and NRQL queries Select data with specific values. PromQL: go_memstats_heap_alloc_bytes{job=\"apiserver\", instance=\"1234\"}) Copy NRQL: To only select data with specific values in NRQL, use the WHERE clause with =. In this example, all data must have the selected value for job and handler. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE job = 'apiserver' AND instance = '1234' TIMESERIES Copy Select data with multiple values. PromQL: go_memstats_heap_alloc_bytes{environment=~\"staging|testing|development\",method!=\"GET\"} Copy NRQL: In NRQL use the in clause to select multiple values for an attribute and the != sign to select all values but the one listed. In this example, the environment can be staging, testing, or development, and the method cannot be GET. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE environment IN ('staging', 'testing', 'development') AND method != 'GET' TIMESERIES Copy Select data using partial string values. PromQL: go_memstats_heap_alloc_bytes{job=~\"api.*\"} Copy NRQL: In NRQL use the LIKE clause to match part of a string value. In this example, all data will be returned where the job attributes start with api. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHEREe job LIKE 'api%' TIMESERIES Copy PromQL-style to NRQL query examples You can simulate the following PromQL-style queries with NRQL queries: Description PromQL-style and NRQL queries Measure the per second rate over the last minute of the http_request_total metric. PromQL: sum(rate(http_requests_total[1m])) Copy NRQL: FROM Metric SELECT rate(sum(http_request_total), 1 second) TIMESERIES 1 minute Copy Chart the difference of the two metrics, then divide by 1024. PromQL: (instance_memory_limit_bytes - instance_memory_usage_bytes) / 1024 Copy NRQL: FROM Metric SELECT (latest(instance_memory_limit_bytes) - latest(instance_memory_usage_bytes)) / 1024 TIMESERIES Copy Provide the summed rate per 30-second interval by each handler. PromQL: sum(rate(http_requests_total[30s])) by (handler) Copy NRQL: FROM Metric SELECT rate(sum(http_requests_total), 30 seconds) FACET handler TIMESERIES Copy Chart the difference in the two metrics where the instance is named foo and the fstype is either ext4 or xfs. PromQL: (node_filesystem_free_bytes{instance='foo',fstype=~\"ext4|xfs\"} / node_filesystem_size_bytes{instance='foo',fstype=~\"ext4|xfs\"}) Copy NRQL: FROM Metric SELECT latest(node_filesystem_free_bytes) / latest(node_filesystem_size_bytes) WHERE instance = 'foo' AND fstype IN ('ext4', 'xfs') Copy For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 177.8562,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Translate PromQL <em>queries</em> to NRQL",
        "sections": "<em>Prometheus</em> <em>and</em> New Relic metric types",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": "Do you have a PromQL <em>query</em> youd like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style <em>query</em> language to explore your <em>Prometheus</em> OpenMetrics integration <em>data</em> along with other <em>data</em> sent to New"
      },
      "id": "603ead4528ccbcbecfeba77b"
    },
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-07-09T09:38:54Z",
      "updated_at": "2021-07-09T09:38:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 128.82437,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric <em>data</em> to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce <em>data</em> volumes by 50%. Set your filters and configure <em>data</em> to target (see metrics or targets). Balance remote write(s) between one or more New Relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    }
  ],
  "/docs/ios-device-id-obfuscation": [
    {
      "sections": [
        "crashNow (iOS SDK API)",
        "Syntax",
        "Requirements",
        "Description",
        "Parameters",
        "Examples",
        "Objective-C",
        "Swift"
      ],
      "title": "crashNow (iOS SDK API)",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "New Relic Mobile iOS",
        "iOS SDK API"
      ],
      "external_id": "7e34108fe8d61b44857d3d34753418dafba3c3f6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-ios/ios-sdk-api/crashnow-ios-sdk-api/",
      "published_at": "2021-07-09T14:20:29Z",
      "updated_at": "2021-07-09T14:20:28Z",
      "document_type": "api_doc",
      "popularity": 1,
      "body": "Syntax crashNow; crashNow:(NSString* __nullable)message; Copy Throws a demo run-time exception named NewRelicDemoException to test New Relic crash reporting. Requirements Requires agent version 4.83 or higher. Description With this method, you can throw a demo run-time exception named NewRelicDemoException with a message. This crash will appear in the Crash analysis page. Parameters Parameter Description $message string Optional. A message attached to the exception. Examples Method: + (void) crashNow:(NSString* __nullable)message; Copy Objective-C Example: [NewRelic crashNow]; Copy Example with message: [NewRelic crashNow:@\"This is a test crash\"]; Copy Swift Example: NewRelic.crashNow() Copy Example with message: NewRelic.crashNow(\"This is a test crash\") Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.12808,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "crashNow (<em>iOS</em> SDK API)",
        "sections": "crashNow (<em>iOS</em> SDK API)",
        "tags": "New Relic Mobile <em>iOS</em>"
      },
      "id": "603e8e46196a67a9b7a83df0"
    },
    {
      "sections": [
        "recordBreadcrumb (iOS SDK API)",
        "Syntax",
        "Requirements",
        "Description",
        "Parameters",
        "Return values",
        "Examples",
        "Objective-C",
        "Swift"
      ],
      "title": "recordBreadcrumb (iOS SDK API)",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "New Relic Mobile iOS",
        "iOS SDK API"
      ],
      "external_id": "38ef7d148d9f6266dbcfe796bef256f15af226be",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-ios/ios-sdk-api/record-breadcrumb/",
      "published_at": "2021-07-09T14:19:31Z",
      "updated_at": "2021-07-09T14:19:31Z",
      "document_type": "api_doc",
      "popularity": 1,
      "body": "Syntax recordBreadcrumb:(NSString* __nonnull)name attributes:(NSDictionary* __nullable)attributes; NewRelic.recordBreadcrumb(string $Name, map $eventAttributes) Copy Records a MobileBreadcrumb event, useful for crash analysis. Requirements Agent version 5.13.0 or higher. Description This call creates and records a MobileBreadcrumb event, which can be found in Insights and in the crash event trail. Mobile breadcrumbs are useful for crash analysis; create them for app activity that you think will help you troubleshoot crashes. In addition to whatever custom attributes you choose, the event will also have associated session attributes. Unlike using setAttribute, adding attributes to a breadcrumb event adds them only to that event; they are not session attributes. Using this call has the same result as using the recordCustomEvent call with MobileBreadcrumb as the event type. For context on how to use this API, see the documentation about sending custom attributes and events to Insights for: Objective-C Swift Parameters Parameter Description $Name string Required. The name you want to give to the breadcrumb event. $eventAttributes map < string, object> Optional. A map that includes a list of attributes of the breadcrumb event. Create attributes for any event descriptors you think will be useful. Return values Returns true if the event is recorded successfully, or false if not. Examples Objective-C Method: + (BOOL) recordBreadcrumb:(NSString* __nonnull)name attributes:(NSDictionary* __nullable)attributes; Copy Example: [NewRelic recordBreadcrumb:@\"user 11-3435 tapped sign-in button on LaunchViewController\" attributes:@{ @\"button\" : @\"sign-in\", @\"action\" : @\"tapped\", @\"userId\" : @\"11-3435\", @\"location\" : @\"LaunchViewController\"}]; Copy Swift Method: NewRelic.recordBreadcrumb(Name: String!, attributes:[NSObject : AnyObject]!) -> Bool Copy Examples: let eventRecorded = NewRelic.recordBreadcrumb(\"Name\", attributes: [\"attributeName1\" : \"value1\", \"attributeName2\": 2]) Copy let eventRecorded = NewRelic.recordBreadcrumb(\"user 11-3435 tapped sign-in from LaunchViewController\" attributes:[\"button\" : \"sign-in\", \"userId\" : \"11-3435\", \"action\" : \"tapped\", \"location\" : \"LaunchViewController\"]) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.1198,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "recordBreadcrumb (<em>iOS</em> SDK API)",
        "sections": "recordBreadcrumb (<em>iOS</em> SDK API)",
        "tags": "New Relic Mobile <em>iOS</em>",
        "body": ":(NSDictionary* __nullable)attributes; Copy Example: [NewRelic recordBreadcrumb:@&quot;user 11-3435 tapped sign-in button on LaunchViewController&quot; attributes:@{ @&quot;button&quot; : @&quot;sign-in&quot;, @&quot;action&quot; : @&quot;tapped&quot;, @&quot;user<em>Id</em>&quot; : @&quot;11-3435&quot;, @&quot;location&quot; : @&quot;LaunchViewController&quot;}]; Copy Swift Method: NewRelic.recordBreadcrumb"
      },
      "id": "60450e5128ccbcd3f42c6079"
    },
    {
      "sections": [
        "setAttribute (iOS SDK API)",
        "Syntax",
        "Requirements",
        "Description",
        "Parameters",
        "Return values",
        "Examples",
        "Objective-C",
        "Swift"
      ],
      "title": "setAttribute (iOS SDK API)",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "New Relic Mobile iOS",
        "iOS SDK API"
      ],
      "external_id": "11e871f1c1fb264d51c91a6b8c83a28ddc817713",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-ios/ios-sdk-api/set-attribute/",
      "published_at": "2021-07-09T14:07:31Z",
      "updated_at": "2021-07-09T14:07:31Z",
      "document_type": "api_doc",
      "popularity": 1,
      "body": "Syntax setAttribute:(NSString*)name value:(id) value; NewRelic.setAttribute(string $name, string or float $value) Copy Creates a session-level attribute shared by multiple mobile monitoring event types. Overwrites its previous value and type each time it is called. Requirements Compatible with all agent versions. Description This static method creates a session-level custom attribute with a specified name and value. When called, it overwrites its previous value and type. The created attribute is shared by multiple Mobile monitoring event types. For more context on how to use this, see the iOS API guide. You can override any of the MobileSession default attributes for New Relic Insights except: appId appName accountId carrier category deviceManufacturer deviceModel eventType install lastInteraction memUsageMb newRelicVersion osMajorVersion osName osVersion platform platformVersion sessionDuration sessionId timestamp type upgradeFrom uuid Anything prefixed with NewRelic For context on how to use this API, see the documentation about sending custom attributes and events to Insights for: Objective-C Swift Parameters Parameter Description $name string Required. Name of the session attribute. $value string or float Required. The second parameter passed can be either a string value or a float value. Return values Returns true if recorded successfully, and false if it doesn't. Examples Objective-C Method: + (BOOL) setAttribute:(NSString*)name value:(id) value; Copy Examples: BOOL attributeSet = [NewRelic setAttribute:@\"buildConfiguration\" value:@\"AppStore-release\"]; BOOL attributeSet = [NewRelic setAttribute:@\"rate\" value:@9999.99]; Copy Swift Method: NewRelic.setAttribute(name: String!, value: AnyObject!>) -> Bool Copy Examples: var attributeSet = NewRelic.setAttribute(\"buildConfiguration\", value: \"AppStore-release\") attributeSet = NewRelic.setAttribute(\"rate\", value: 9999.99) Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 345.01526,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "setAttribute (<em>iOS</em> SDK API)",
        "sections": "setAttribute (<em>iOS</em> SDK API)",
        "tags": "New Relic Mobile <em>iOS</em>",
        "body": " agent versions. Description This static method creates a session-level custom attribute with a specified name and value. When called, it overwrites its previous value and type. The created attribute is shared by multiple Mobile monitoring event types. For more context on how to use this, see the <em>iOS</em>"
      },
      "id": "60450efd28ccbcfc7d2c6084"
    }
  ],
  "/docs/licenses/index": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-07-09T08:11:21Z",
      "updated_at": "2021-07-09T08:11:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 77.26301,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Licenses</em>",
        "body": ", webinars, and other information about using New Relic features, visit New Relic University and newrelic.com&#x2F;resources. For more help Recommendations for learning more: See the Docs site&#x27;s landing page for <em>Licenses</em> documentation. Browse New Relic&#x27;s Explorers Hub for community discussions about New Relic <em>licenses</em>."
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/prometheus-remote-write-integration/",
      "sections": [
        "Prometheus remote write integration",
        "Why it matters",
        "Compatibility",
        "Scale your data and get moving quickly",
        "What's next",
        "For more help"
      ],
      "published_at": "2021-07-14T15:02:55Z",
      "title": "Prometheus remote write integration",
      "updated_at": "2021-07-10T23:03:06Z",
      "type": "docs",
      "external_id": "aaf44eb9ee0ffc4b6f751ca18c5dd5b34cd11649",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use the Prometheus remote write integration to get data flowing into New Relic. Once you integrate, your data will be visible in query-based dashboards (and other query results), often within about five minutes. Why it matters Unlike Kubernetes and Docker OpenMetrics integrations, which scrape data from Prometheus endpoints, the remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. You can leverage the full range of options for setup and management, from raw data to queries and dashboards and beyond. With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional) Compatibility New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Scale your data and get moving quickly Once logged in to New Relic, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. View your data. What's next Ready to get started? Read the setup documentation. Configure a Prometheus data source in Grafana. Set up the integration on New Relic US EU For more help Recommendations for learning more: See the Docs site's landing page for Infrastructure integrations documentation. Browse New Relic's Explorers Hub for community discussions about our Infrastructure integrations. Find additional help or file a support ticket. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 68.22796,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " for learning more: See the Docs site&#x27;s landing page for Infrastructure integrations documentation. Browse New Relic&#x27;s Explorers Hub for community discussions about our Infrastructure integrations. Find additional help or file a support ticket. Review New Relic&#x27;s <em>licenses</em>, attributions, data usage limits, and other notices."
      },
      "id": "60ea272a196a670c6038adbf"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-07-09T10:18:47Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0  2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0  2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0  2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0  2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0  2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright  2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright  1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright  1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright  2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright  1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright  1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 64.07922,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-07-09T21:26:16Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.04666,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "FIT instrumentation end user license agreement"
      ],
      "title": "FIT instrumentation end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "e8e10baf120678407d08c9f78ab708d271cf0223",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement/",
      "published_at": "2021-07-09T09:49:16Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NEW RELIC, INC. FIT INSTRUMENTATION END USER LICENSE AGREEMENT In connection with the work provided by New Relics Field Instrumentation Team, you may be provided with certain custom-created software to enable, optimize, or enhance your use of New Relics Services. By downloading, installing, authorizing installation, or using the FIT Instrumentation with your Underlying Software (defined below), you (Customer) agree to the terms and conditions herein (Agreement) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (New Relic), (collectively the Parties). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the Terms of Service) available here: http://newrelic.com/terms IF YOU DO NOT AGREE TO THIS AGREEMENT, PLEASE DO NOT USE THE FIT INSTRUMENTATION. 1. DEFINITIONS FIT Instrumentation means the New Relic custom-made software, including but not limited to connectors, extensions, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (Underlying Software). For the purposes of the Terms of Service, the FIT Instrumentation shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE FIT INSTRUMENTATION 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the FIT Instrumentation solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the FIT Instrumentation and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the FIT Instrumentation in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (Feedback) regarding the FIT Instrumentation, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the FIT Instrumentation is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that FIT Instrumentation will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an Upgrade Event). Customer shall hold New Relic harmless from any claims or damages arising from Customers Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the FIT Instrumentation except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the FIT Instrumentation; (iii) decompile, disassemble or reverse engineer any software underlying the FIT Instrumentation; (iv) use the FIT Instrumentation to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other partys use and enjoyment of the Services; (v) use the FIT Instrumentation to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the FIT Instrumentation; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend usage of the Services by any user, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time at https://docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential Information means: (a) the FIT Instrumentation, and any features, results or output produced by, and other information relating to the FIT Instrumentation (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. FIT INSTRUMENTATION IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. FIT INSTRUMENTATION IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE. , OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE FIT INSTRUMENTATION THEREFROM. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES OR LIMITATIONS ON APPLICABLE STATUTORY RIGHTS OF A CONSUMER, SO THE ABOVE EXCLUSION AND LIMITATIONS MAY NOT APPLY TO THE CUSTOMER. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE FIT INSTRUMENTATION OR FOR ANY ERROR OR DEFECT IN THE FIT INSTRUMENTATION OR THE SERVICES, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT, STRICT LIABILITY, OR OTHERWISE, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC'S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the FIT Instrumentation is not an official product and has not been commercially released for sale by New Relic; (b) the FIT Instrumentation may not operate properly, being in final form, or fully functional; (c) the FIT Instrumentation may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the FIT Instrumentation fully functional; (e) the information obtained using the FIT Instrumentation may not be accurate; (f) use of the FIT Instrumentation may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the FIT Instrumentation; and (h) New Relic has the right unilaterally to abandon development of the FIT Instrumentation, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the \"Documentation\").This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the FIT Instrumentation. In the absence of a separate agreement between New Relic and Customer with respect to the FIT Instrumentation, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such FIT Instrumentation. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the FIT Instrumentation only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.23683,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "FIT instrumentation end user <em>license</em> agreement",
        "sections": "FIT instrumentation end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " Acceptable Use Policy as may be published and updated from time to time at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential <em>Information</em> means: (a) the FIT Instrumentation, and any features, results"
      },
      "id": "603e9f3ee7b9d206e32a0800"
    },
    {
      "sections": [
        "New Relic Agent Software Notice"
      ],
      "title": "New Relic Agent Software Notice",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "2bf9501c2767105130d3808f1bf3a91a032d903e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice/",
      "published_at": "2021-07-09T09:50:31Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This software is  2008-2021 New Relic, Inc. and its licensors. This software is solely for use with New Relics proprietary SaaS service (New Relic Service), so to use the software you must have a valid account for the New Relic Service under a separate agreement with New Relic (Subscription Agreement). You may only use the software to support your use of the New Relic Service as permitted in the Subscription Agreement. Without a Subscription Agreement, you may not use the software. All other use is prohibited. New Relic and its suppliers retain all right, title and interest (including intellectual property rights) in the software. The Subscription Agreement will control in event of a conflict with this notice. Unless otherwise agreed by New Relic in your Subscription Agreement: You may not use, copy, distribute or sublicense the software, use the software on behalf of third parties, reverse engineer or decompile the software, modify or create derivative works of the software, use the software for competitive analysis or benchmarking, or remove or obscure any proprietary notices in the software. The software is provided AS IS and New Relic disclaims all warranties, whether express, implied, statutory or otherwise, including warranties of merchantability, fitness for a particular purpose, title or noninfringement. To the full extent permitted by law, New Relic will have no liability arising from or related to the software or under this notice for any direct, indirect, special, incidental, or consequential damages of any kind, even if advised of their possibility in advance, and regardless of legal theory (whether contract, tort, negligence, strict liability or otherwise). The software may contain third-party open source software (OSS) as described here and at https://github.com/newrelic. To the extent required by the OSS license, that license will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile and as set forth: https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started and https://docs.newrelic.com/docs/browser/new-relic-browser/installation/install-new-relic-browser-agent Software versions New Relic makes available under an OSS license (such as Apache 2.0) are governed by the terms of the applicable OSS license. For a current list of New Relic software versions released as OSS please visit https://opensource.newrelic.com/.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.23683,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " and at https:&#x2F;&#x2F;github.com&#x2F;newrelic. To the extent required by the OSS <em>license</em>, that <em>license</em> will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile"
      },
      "id": "603eb73828ccbc1f99eba74a"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-07-09T21:26:16Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.04666,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2021-07-09T09:49:16Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (Customer) agree to the terms and conditions herein (Agreement) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (New Relic\"), (collectively the Parties). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the Terms of Service) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS Add-on means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (Underlying Software). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (Feedback) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an Upgrade Event). Customer shall hold New Relic harmless from any claims or damages arising from Customers Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other partys use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential Information means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELICS AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.24617,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential <em>Information</em> means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    },
    {
      "sections": [
        "New Relic Agent Software Notice"
      ],
      "title": "New Relic Agent Software Notice",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "2bf9501c2767105130d3808f1bf3a91a032d903e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice/",
      "published_at": "2021-07-09T09:50:31Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This software is  2008-2021 New Relic, Inc. and its licensors. This software is solely for use with New Relics proprietary SaaS service (New Relic Service), so to use the software you must have a valid account for the New Relic Service under a separate agreement with New Relic (Subscription Agreement). You may only use the software to support your use of the New Relic Service as permitted in the Subscription Agreement. Without a Subscription Agreement, you may not use the software. All other use is prohibited. New Relic and its suppliers retain all right, title and interest (including intellectual property rights) in the software. The Subscription Agreement will control in event of a conflict with this notice. Unless otherwise agreed by New Relic in your Subscription Agreement: You may not use, copy, distribute or sublicense the software, use the software on behalf of third parties, reverse engineer or decompile the software, modify or create derivative works of the software, use the software for competitive analysis or benchmarking, or remove or obscure any proprietary notices in the software. The software is provided AS IS and New Relic disclaims all warranties, whether express, implied, statutory or otherwise, including warranties of merchantability, fitness for a particular purpose, title or noninfringement. To the full extent permitted by law, New Relic will have no liability arising from or related to the software or under this notice for any direct, indirect, special, incidental, or consequential damages of any kind, even if advised of their possibility in advance, and regardless of legal theory (whether contract, tort, negligence, strict liability or otherwise). The software may contain third-party open source software (OSS) as described here and at https://github.com/newrelic. To the extent required by the OSS license, that license will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile and as set forth: https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started and https://docs.newrelic.com/docs/browser/new-relic-browser/installation/install-new-relic-browser-agent Software versions New Relic makes available under an OSS license (such as Apache 2.0) are governed by the terms of the applicable OSS license. For a current list of New Relic software versions released as OSS please visit https://opensource.newrelic.com/.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.23683,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " and at https:&#x2F;&#x2F;github.com&#x2F;newrelic. To the extent required by the OSS <em>license</em>, that <em>license</em> will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile"
      },
      "id": "603eb73828ccbc1f99eba74a"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-07-09T21:26:16Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.04666,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2021-07-09T09:49:16Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (Customer) agree to the terms and conditions herein (Agreement) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (New Relic\"), (collectively the Parties). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the Terms of Service) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS Add-on means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (Underlying Software). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (Feedback) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an Upgrade Event). Customer shall hold New Relic harmless from any claims or damages arising from Customers Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other partys use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential Information means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELICS AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.24617,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential <em>Information</em> means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    },
    {
      "sections": [
        "FIT instrumentation end user license agreement"
      ],
      "title": "FIT instrumentation end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "e8e10baf120678407d08c9f78ab708d271cf0223",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement/",
      "published_at": "2021-07-09T09:49:16Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NEW RELIC, INC. FIT INSTRUMENTATION END USER LICENSE AGREEMENT In connection with the work provided by New Relics Field Instrumentation Team, you may be provided with certain custom-created software to enable, optimize, or enhance your use of New Relics Services. By downloading, installing, authorizing installation, or using the FIT Instrumentation with your Underlying Software (defined below), you (Customer) agree to the terms and conditions herein (Agreement) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (New Relic), (collectively the Parties). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the Terms of Service) available here: http://newrelic.com/terms IF YOU DO NOT AGREE TO THIS AGREEMENT, PLEASE DO NOT USE THE FIT INSTRUMENTATION. 1. DEFINITIONS FIT Instrumentation means the New Relic custom-made software, including but not limited to connectors, extensions, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (Underlying Software). For the purposes of the Terms of Service, the FIT Instrumentation shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE FIT INSTRUMENTATION 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the FIT Instrumentation solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the FIT Instrumentation and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the FIT Instrumentation in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (Feedback) regarding the FIT Instrumentation, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the FIT Instrumentation is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that FIT Instrumentation will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an Upgrade Event). Customer shall hold New Relic harmless from any claims or damages arising from Customers Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the FIT Instrumentation except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the FIT Instrumentation; (iii) decompile, disassemble or reverse engineer any software underlying the FIT Instrumentation; (iv) use the FIT Instrumentation to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other partys use and enjoyment of the Services; (v) use the FIT Instrumentation to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the FIT Instrumentation; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend usage of the Services by any user, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time at https://docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential Information means: (a) the FIT Instrumentation, and any features, results or output produced by, and other information relating to the FIT Instrumentation (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. FIT INSTRUMENTATION IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. FIT INSTRUMENTATION IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE. , OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE FIT INSTRUMENTATION THEREFROM. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES OR LIMITATIONS ON APPLICABLE STATUTORY RIGHTS OF A CONSUMER, SO THE ABOVE EXCLUSION AND LIMITATIONS MAY NOT APPLY TO THE CUSTOMER. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE FIT INSTRUMENTATION OR FOR ANY ERROR OR DEFECT IN THE FIT INSTRUMENTATION OR THE SERVICES, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT, STRICT LIABILITY, OR OTHERWISE, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC'S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the FIT Instrumentation is not an official product and has not been commercially released for sale by New Relic; (b) the FIT Instrumentation may not operate properly, being in final form, or fully functional; (c) the FIT Instrumentation may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the FIT Instrumentation fully functional; (e) the information obtained using the FIT Instrumentation may not be accurate; (f) use of the FIT Instrumentation may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the FIT Instrumentation; and (h) New Relic has the right unilaterally to abandon development of the FIT Instrumentation, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the \"Documentation\").This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the FIT Instrumentation. In the absence of a separate agreement between New Relic and Customer with respect to the FIT Instrumentation, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such FIT Instrumentation. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the FIT Instrumentation only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.23682,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "FIT instrumentation end user <em>license</em> agreement",
        "sections": "FIT instrumentation end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " Acceptable Use Policy as may be published and updated from time to time at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential <em>Information</em> means: (a) the FIT Instrumentation, and any features, results"
      },
      "id": "603e9f3ee7b9d206e32a0800"
    }
  ],
  "/docs/licenses/license-information/faq/new-relic-one-pricing-plan-frequently-asked-questions": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-07-09T08:11:21Z",
      "updated_at": "2021-07-09T08:11:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.40338,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ", webinars, and other <em>information</em> about using New Relic features, visit New Relic University and newrelic.com&#x2F;resources. For more help Recommendations for learning more: See the Docs site&#x27;s landing page for <em>Licenses</em> documentation. Browse New Relic&#x27;s Explorers Hub for community discussions about New Relic <em>licenses</em>."
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-07-09T10:52:38Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customers websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated Documentation page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customers Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relics customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 120.31691,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-07-09T09:23:59Z",
      "updated_at": "2021-05-22T17:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into the Insights event database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 120.316696,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ebacc64441f77774e8872"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-07-09T08:11:21Z",
      "updated_at": "2021-07-09T08:11:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 348.6734,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-07-09T09:23:59Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (PRC Use). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.09105,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-07-09T08:26:18Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.0891,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings": [
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-07-09T09:23:59Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (PRC Use). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.09103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-07-09T08:26:18Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.0891,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    },
    {
      "sections": [
        "Acceptable use policy",
        "You will not, and not to allow third parties, in connection with your use of the New Relic Properties to:",
        "Harm New Relics Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2021-07-09T17:37:25Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relics mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (AUP). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the New Relic Properties) and is part of the New Relic documentation, which can be found at https://docs.newrelic.com (the Documentation). You will not, and not to allow third parties, in connection with your use of the New Relic Properties to: Harm New Relics Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relics or a third partys intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using botnets, robots, spiders and offline readers); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other hidden text; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including scraping or using any data mining methods); Sharing your New Relic Properties account or login credentials with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other users account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by spoofing or phishing); Collecting or harvesting any personally identifiable information, including account names, from any other users account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other peoples private and confidential information without their express permission; Using anyones name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (HIPAA); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any personal information as such term is defined under the Childrens Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, drivers license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; or (vii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R.  120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if youre a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term content means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, you may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and we means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.0891,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Social Security numbers, drivers <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/government-addendum": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-07-09T08:11:21Z",
      "updated_at": "2021-07-09T08:11:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 348.67328,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-07-09T09:23:59Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (PRC Use). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.09103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Acceptable use policy",
        "You will not, and not to allow third parties, in connection with your use of the New Relic Properties to:",
        "Harm New Relics Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2021-07-09T17:37:25Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relics mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (AUP). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the New Relic Properties) and is part of the New Relic documentation, which can be found at https://docs.newrelic.com (the Documentation). You will not, and not to allow third parties, in connection with your use of the New Relic Properties to: Harm New Relics Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relics or a third partys intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using botnets, robots, spiders and offline readers); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other hidden text; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including scraping or using any data mining methods); Sharing your New Relic Properties account or login credentials with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other users account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by spoofing or phishing); Collecting or harvesting any personally identifiable information, including account names, from any other users account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other peoples private and confidential information without their express permission; Using anyones name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (HIPAA); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any personal information as such term is defined under the Childrens Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, drivers license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; or (vii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R.  120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if youre a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term content means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, you may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and we means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.0891,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Social Security numbers, drivers <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-07-09T08:11:21Z",
      "updated_at": "2021-07-09T08:11:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 348.67328,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-07-09T09:23:59Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (PRC Use). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.09103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-07-09T08:26:18Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.0891,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/new-relics-provision-services": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-07-09T08:11:21Z",
      "updated_at": "2021-07-09T08:11:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 348.67313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-07-09T09:23:59Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (PRC Use). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.09103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-07-09T08:26:18Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.0891,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/peoples-republic-china": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-07-09T08:11:21Z",
      "updated_at": "2021-07-09T08:11:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 348.67313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " environments Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "Government addendum",
        "New Relic, Inc.Government Addendum"
      ],
      "title": "Government addendum",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "cfed51b7f4f7583476f56b2b574204d38493e882",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/government-addendum/",
      "published_at": "2021-07-09T08:26:18Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Government Addendum applies only to customers that are United States federal, state, or local government customers with an existing New Relic agreement in place that explicitly references this Government Addendum applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic, Inc.Government Addendum For purposes of this Agreement and to the extent applicable, the Service is \"commercial computer software\" and a \"commercially available off-the-shelf (COTS) item\" as defined at FAR 2.101 developed at the private expense of New Relic. If acquired by or on behalf of a civilian agency, the U.S. Government acquires this commercial computer software and/or commercial computer software documentation and other technical data subject to the terms of the Agreement as specified in 48 C.F.R. 12.212 (Computer Software) and 12.211 (Technical Data) of the Federal Acquisition Regulation (\"FAR\") and its successors. If acquired by or on behalf of any agency within the Department of Defense (\"DOD\"), the U.S. Government acquires this commercial computer software and/or commercial computer software documentation subject to the terms of the Agreement as specified in 48 C.F.R. 227.7202-3 of the DOD FAR Supplement (\"DFARS\") and its successors. This addendum is in lieu of and supersedes any other FAR, DFARS, or other clause or provision that addresses government rights in computer software or technical data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.0891,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>"
      },
      "id": "603ea47ee7b9d2dd0d2a07ef"
    },
    {
      "sections": [
        "Acceptable use policy",
        "You will not, and not to allow third parties, in connection with your use of the New Relic Properties to:",
        "Harm New Relics Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2021-07-09T17:37:25Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relics mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (AUP). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the New Relic Properties) and is part of the New Relic documentation, which can be found at https://docs.newrelic.com (the Documentation). You will not, and not to allow third parties, in connection with your use of the New Relic Properties to: Harm New Relics Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relics or a third partys intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using botnets, robots, spiders and offline readers); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other hidden text; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including scraping or using any data mining methods); Sharing your New Relic Properties account or login credentials with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other users account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by spoofing or phishing); Collecting or harvesting any personally identifiable information, including account names, from any other users account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other peoples private and confidential information without their express permission; Using anyones name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (HIPAA); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any personal information as such term is defined under the Childrens Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, drivers license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; or (vii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R.  120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if youre a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term content means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, you may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and we means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.0891,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Social Security numbers, drivers <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    }
  ],
  "/docs/licenses/license-information/other-licenses/services-licenses": [
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2021-07-09T09:49:16Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (Customer) agree to the terms and conditions herein (Agreement) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (New Relic\"), (collectively the Parties). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the Terms of Service) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS Add-on means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (Underlying Software). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (Feedback) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an Upgrade Event). Customer shall hold New Relic harmless from any claims or damages arising from Customers Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other partys use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential Information means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELICS AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.24617,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential <em>Information</em> means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    },
    {
      "sections": [
        "FIT instrumentation end user license agreement"
      ],
      "title": "FIT instrumentation end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "e8e10baf120678407d08c9f78ab708d271cf0223",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement/",
      "published_at": "2021-07-09T09:49:16Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NEW RELIC, INC. FIT INSTRUMENTATION END USER LICENSE AGREEMENT In connection with the work provided by New Relics Field Instrumentation Team, you may be provided with certain custom-created software to enable, optimize, or enhance your use of New Relics Services. By downloading, installing, authorizing installation, or using the FIT Instrumentation with your Underlying Software (defined below), you (Customer) agree to the terms and conditions herein (Agreement) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (New Relic), (collectively the Parties). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the Terms of Service) available here: http://newrelic.com/terms IF YOU DO NOT AGREE TO THIS AGREEMENT, PLEASE DO NOT USE THE FIT INSTRUMENTATION. 1. DEFINITIONS FIT Instrumentation means the New Relic custom-made software, including but not limited to connectors, extensions, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (Underlying Software). For the purposes of the Terms of Service, the FIT Instrumentation shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE FIT INSTRUMENTATION 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the FIT Instrumentation solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the FIT Instrumentation and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the FIT Instrumentation in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (Feedback) regarding the FIT Instrumentation, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the FIT Instrumentation is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that FIT Instrumentation will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an Upgrade Event). Customer shall hold New Relic harmless from any claims or damages arising from Customers Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the FIT Instrumentation except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the FIT Instrumentation; (iii) decompile, disassemble or reverse engineer any software underlying the FIT Instrumentation; (iv) use the FIT Instrumentation to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other partys use and enjoyment of the Services; (v) use the FIT Instrumentation to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the FIT Instrumentation; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend usage of the Services by any user, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time at https://docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential Information means: (a) the FIT Instrumentation, and any features, results or output produced by, and other information relating to the FIT Instrumentation (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. FIT INSTRUMENTATION IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. FIT INSTRUMENTATION IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE. , OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE FIT INSTRUMENTATION THEREFROM. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES OR LIMITATIONS ON APPLICABLE STATUTORY RIGHTS OF A CONSUMER, SO THE ABOVE EXCLUSION AND LIMITATIONS MAY NOT APPLY TO THE CUSTOMER. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE FIT INSTRUMENTATION OR FOR ANY ERROR OR DEFECT IN THE FIT INSTRUMENTATION OR THE SERVICES, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT, STRICT LIABILITY, OR OTHERWISE, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC'S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the FIT Instrumentation is not an official product and has not been commercially released for sale by New Relic; (b) the FIT Instrumentation may not operate properly, being in final form, or fully functional; (c) the FIT Instrumentation may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the FIT Instrumentation fully functional; (e) the information obtained using the FIT Instrumentation may not be accurate; (f) use of the FIT Instrumentation may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the FIT Instrumentation; and (h) New Relic has the right unilaterally to abandon development of the FIT Instrumentation, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the \"Documentation\").This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the FIT Instrumentation. In the absence of a separate agreement between New Relic and Customer with respect to the FIT Instrumentation, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such FIT Instrumentation. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the FIT Instrumentation only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.23682,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "FIT instrumentation end user <em>license</em> agreement",
        "sections": "FIT instrumentation end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " Acceptable Use Policy as may be published and updated from time to time at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential <em>Information</em> means: (a) the FIT Instrumentation, and any features, results"
      },
      "id": "603e9f3ee7b9d206e32a0800"
    },
    {
      "sections": [
        "New Relic Agent Software Notice"
      ],
      "title": "New Relic Agent Software Notice",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "2bf9501c2767105130d3808f1bf3a91a032d903e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice/",
      "published_at": "2021-07-09T09:50:31Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This software is  2008-2021 New Relic, Inc. and its licensors. This software is solely for use with New Relics proprietary SaaS service (New Relic Service), so to use the software you must have a valid account for the New Relic Service under a separate agreement with New Relic (Subscription Agreement). You may only use the software to support your use of the New Relic Service as permitted in the Subscription Agreement. Without a Subscription Agreement, you may not use the software. All other use is prohibited. New Relic and its suppliers retain all right, title and interest (including intellectual property rights) in the software. The Subscription Agreement will control in event of a conflict with this notice. Unless otherwise agreed by New Relic in your Subscription Agreement: You may not use, copy, distribute or sublicense the software, use the software on behalf of third parties, reverse engineer or decompile the software, modify or create derivative works of the software, use the software for competitive analysis or benchmarking, or remove or obscure any proprietary notices in the software. The software is provided AS IS and New Relic disclaims all warranties, whether express, implied, statutory or otherwise, including warranties of merchantability, fitness for a particular purpose, title or noninfringement. To the full extent permitted by law, New Relic will have no liability arising from or related to the software or under this notice for any direct, indirect, special, incidental, or consequential damages of any kind, even if advised of their possibility in advance, and regardless of legal theory (whether contract, tort, negligence, strict liability or otherwise). The software may contain third-party open source software (OSS) as described here and at https://github.com/newrelic. To the extent required by the OSS license, that license will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile and as set forth: https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started and https://docs.newrelic.com/docs/browser/new-relic-browser/installation/install-new-relic-browser-agent Software versions New Relic makes available under an OSS license (such as Apache 2.0) are governed by the terms of the applicable OSS license. For a current list of New Relic software versions released as OSS please visit https://opensource.newrelic.com/.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.23682,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " and at https:&#x2F;&#x2F;github.com&#x2F;newrelic. To the extent required by the OSS <em>license</em>, that <em>license</em> will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile"
      },
      "id": "603eb73828ccbc1f99eba74a"
    }
  ],
  "/docs/licenses/license-information/product-definitions/legacy-product-definitions": [
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-07-09T10:52:38Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customers websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated Documentation page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customers Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relics customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 215.98843,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One pricing: <em>Definitions</em>",
        "sections": "New Relic One pricing: <em>Definitions</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " pricing, invoicing related <em>information</em>, and <em>product</em>-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing."
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-07-09T08:11:21Z",
      "updated_at": "2021-07-09T08:11:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.40315,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Product</em> training",
        "tags": "<em>License</em> <em>information</em>",
        "body": ", webinars, and other <em>information</em> about using New Relic features, visit New Relic University and newrelic.com&#x2F;resources. For more help Recommendations for learning more: See the Docs site&#x27;s landing page for <em>Licenses</em> documentation. Browse New Relic&#x27;s Explorers Hub for community discussions about New Relic <em>licenses</em>."
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2021-07-09T08:36:28Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing plan. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the Product Usage Ratio) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Products list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 138.99013,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "sections": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "tags": "<em>License</em> <em>information</em>",
        "body": " will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more <em>information</em> about units of measures, see <em>Product</em> <em>definitions</em>. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units"
      },
      "id": "603ea32a28ccbc7e22eba768"
    }
  ],
  "/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions": [
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-07-09T09:23:59Z",
      "updated_at": "2021-05-22T17:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into the Insights event database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 215.98804,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Original <em>product</em>-based pricing <em>definitions</em>",
        "sections": "Original <em>product</em>-based pricing <em>definitions</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": "This is a glossary of terms that appear in contracts for our original <em>product</em>-based pricing. For New Relic One pricing plan terms, see New Relic One pricing <em>definitions</em>. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app"
      },
      "id": "603ebacc64441f77774e8872"
    },
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-07-09T08:11:21Z",
      "updated_at": "2021-07-09T08:11:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.40315,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Product</em> training",
        "tags": "<em>License</em> <em>information</em>",
        "body": ", webinars, and other <em>information</em> about using New Relic features, visit New Relic University and newrelic.com&#x2F;resources. For more help Recommendations for learning more: See the Docs site&#x27;s landing page for <em>Licenses</em> documentation. Browse New Relic&#x27;s Explorers Hub for community discussions about New Relic <em>licenses</em>."
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2021-07-09T08:36:28Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing plan. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the Product Usage Ratio) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Products list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 138.99013,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "sections": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "tags": "<em>License</em> <em>information</em>",
        "body": " will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more <em>information</em> about units of measures, see <em>Product</em> <em>definitions</em>. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units"
      },
      "id": "603ea32a28ccbc7e22eba768"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Data Security",
        "2. Data Access",
        "3. Server Security",
        "4. Network Security",
        "5. Security Audits",
        "6. Security and Incident Response",
        "7. Disaster Recovery",
        "8. Copies and Removal",
        "9. Disclosure by Law"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-07-09T09:38:00Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Data Security 1.1. New Relic shall establish and maintain data security procedures and other safeguards designed to protect against the loss, theft or other unauthorized access or alteration of Customer Data in the possession or under the control of New Relic or to which New Relic has access, which are no less rigorous than accepted security standards in the industry. 1.2. New Relic shall maintain an information security policy that outlines a definition of information security and its overall objectives; a framework for setting control objectives and controls, including the structure of risk assessment and risk management; a brief explanation of the compliance requirements, and procedures for managing information security incidents. 2. Data Access 2.1. Access to Customer Data stored on New Relics systems shall not be granted to members of New Relic unless they have been uniquely identified and have sufficient credentials. 2.2. Access permissions shall be established in a manner that allows for the minimum access level(s) required for each employee. 2.3. Access to Customer Data shall be logged with sufficient information to determine the nature and scope of any inappropriate access. 3. Server Security 3.1. New Relic shall establish and follow reasonable server configuration guidelines and processes to prevent unauthorized access to Customer Data. 3.2. New Relic shall establish and follow reasonable configuration change management procedures for its servers containing Customer Data. 4. Network Security 4.1. New Relic network architecture shall be designed to limit site access and restrict the availability of information services that are considered to be vulnerable to attack. 4.2. New Relic shall utilize SSL certificates for all Internet activity. By default, Customer Data transmitted to and from the New Relic network shall be sent over encrypted medium or an encrypted format. 4.3. New Relic network shall use IDS technologies for network intrusion detection. 4.4. Access to New Relic systems containing Customer Data shall be restricted to authorized personnel. 5. Security Audits 5.1. New Relic shall conduct at least annually a SOC 2 or industry equivalent audit. New Relic shall provide to Customer audit results upon request, and shall explain and provide remediation plans to correct any problems to the extent reasonably possible. 6. Security and Incident Response 6.1. New Relic shall maintain an Information Security Incident Response plan, and make that plan available to Customer if requested. 6.2. In the event of an actual theft, loss, or unauthorized access of Customer Data by New Relics personnel and/or any unauthorized individual or entity, New Relic shall: (a) investigate such breach, (b) attempt to cure such breach, and (c) provide notification to Customer that describes such breach. 7. Disaster Recovery 7.1. New Relic shall have in effect a disaster recovery plan designed to respond to both a component failure of New Relic equipment within its data center and a catastrophic loss of service. This plan shall include documented policies and procedures to restore service in the event of either type of failure. 7.2. New Relic shall establish and follow backup and restore procedures for servers containing Customer Data. 8. Copies and Removal 8.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of this Agreement for any reason: (a) New Relic shall, and shall cause its personnel, to cease and desist all access and use of any Customer Data, (b) New Relic shall delete all copies of Customer Data within ninety (90) days. 9. Disclosure by Law 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 215.13426,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Relic has access, which are no less rigorous than accepted security standards in the industry. 1.2. New Relic shall maintain an <em>information</em> security policy that outlines a definition of <em>information</em> security and its overall objectives; a framework for setting control objectives and controls"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2021-07-09T08:33:38Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to New Relic Full Stack Observability Pro or Enterprise Products, the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to New Relic Full Stack Observability Standard, any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customers access or use of the Service in accordance with this Agreement. This describes Customers sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 215.12341,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    },
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-07-09T08:11:21Z",
      "updated_at": "2021-07-09T08:11:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.61133,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ", webinars, and other <em>information</em> about using New Relic features, visit New Relic University and newrelic.com&#x2F;resources. For more help Recommendations for learning more: See the Docs site&#x27;s landing page for <em>Licenses</em> documentation. Browse New Relic&#x27;s Explorers Hub for community discussions about New Relic <em>licenses</em>."
      },
      "id": "603ea419e7b9d27b942a07b4"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/security-policy": [
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2021-07-09T08:33:38Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to New Relic Full Stack Observability Pro or Enterprise Products, the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to New Relic Full Stack Observability Standard, any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customers access or use of the Service in accordance with this Agreement. This describes Customers sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 215.12341,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2021-07-09T10:33:10Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (New Relic) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the Policy) governing your use and participation in New Relics program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the Pre-Release Service(s)). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as you or your) as of your date of first use of the Pre-Release Service(s) (the Effective Date), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated Documentation page of the New Relic website (the Documentation) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (Feedback). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (Confidential Information). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., personal information of children as defined by the Childrens Online Privacy Protection Act, protected health information as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, special categories of data as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relics request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively Unintended Effects); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED AS IS. TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELICS AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorneys fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.71848,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    },
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-07-09T08:11:21Z",
      "updated_at": "2021-07-09T08:11:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.61133,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ", webinars, and other <em>information</em> about using New Relic features, visit New Relic University and newrelic.com&#x2F;resources. For more help Recommendations for learning more: See the Docs site&#x27;s landing page for <em>Licenses</em> documentation. Browse New Relic&#x27;s Explorers Hub for community discussions about New Relic <em>licenses</em>."
      },
      "id": "603ea419e7b9d27b942a07b4"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/service-level-availability-commitment": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Data Security",
        "2. Data Access",
        "3. Server Security",
        "4. Network Security",
        "5. Security Audits",
        "6. Security and Incident Response",
        "7. Disaster Recovery",
        "8. Copies and Removal",
        "9. Disclosure by Law"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-07-09T09:38:00Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Data Security 1.1. New Relic shall establish and maintain data security procedures and other safeguards designed to protect against the loss, theft or other unauthorized access or alteration of Customer Data in the possession or under the control of New Relic or to which New Relic has access, which are no less rigorous than accepted security standards in the industry. 1.2. New Relic shall maintain an information security policy that outlines a definition of information security and its overall objectives; a framework for setting control objectives and controls, including the structure of risk assessment and risk management; a brief explanation of the compliance requirements, and procedures for managing information security incidents. 2. Data Access 2.1. Access to Customer Data stored on New Relics systems shall not be granted to members of New Relic unless they have been uniquely identified and have sufficient credentials. 2.2. Access permissions shall be established in a manner that allows for the minimum access level(s) required for each employee. 2.3. Access to Customer Data shall be logged with sufficient information to determine the nature and scope of any inappropriate access. 3. Server Security 3.1. New Relic shall establish and follow reasonable server configuration guidelines and processes to prevent unauthorized access to Customer Data. 3.2. New Relic shall establish and follow reasonable configuration change management procedures for its servers containing Customer Data. 4. Network Security 4.1. New Relic network architecture shall be designed to limit site access and restrict the availability of information services that are considered to be vulnerable to attack. 4.2. New Relic shall utilize SSL certificates for all Internet activity. By default, Customer Data transmitted to and from the New Relic network shall be sent over encrypted medium or an encrypted format. 4.3. New Relic network shall use IDS technologies for network intrusion detection. 4.4. Access to New Relic systems containing Customer Data shall be restricted to authorized personnel. 5. Security Audits 5.1. New Relic shall conduct at least annually a SOC 2 or industry equivalent audit. New Relic shall provide to Customer audit results upon request, and shall explain and provide remediation plans to correct any problems to the extent reasonably possible. 6. Security and Incident Response 6.1. New Relic shall maintain an Information Security Incident Response plan, and make that plan available to Customer if requested. 6.2. In the event of an actual theft, loss, or unauthorized access of Customer Data by New Relics personnel and/or any unauthorized individual or entity, New Relic shall: (a) investigate such breach, (b) attempt to cure such breach, and (c) provide notification to Customer that describes such breach. 7. Disaster Recovery 7.1. New Relic shall have in effect a disaster recovery plan designed to respond to both a component failure of New Relic equipment within its data center and a catastrophic loss of service. This plan shall include documented policies and procedures to restore service in the event of either type of failure. 7.2. New Relic shall establish and follow backup and restore procedures for servers containing Customer Data. 8. Copies and Removal 8.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of this Agreement for any reason: (a) New Relic shall, and shall cause its personnel, to cease and desist all access and use of any Customer Data, (b) New Relic shall delete all copies of Customer Data within ninety (90) days. 9. Disclosure by Law 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 215.13426,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Relic has access, which are no less rigorous than accepted security standards in the industry. 1.2. New Relic shall maintain an <em>information</em> security policy that outlines a definition of <em>information</em> security and its overall objectives; a framework for setting control objectives and controls"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2021-07-09T10:33:10Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (New Relic) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the Policy) governing your use and participation in New Relics program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the Pre-Release Service(s)). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as you or your) as of your date of first use of the Pre-Release Service(s) (the Effective Date), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated Documentation page of the New Relic website (the Documentation) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (Feedback). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (Confidential Information). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., personal information of children as defined by the Childrens Online Privacy Protection Act, protected health information as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, special categories of data as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relics request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively Unintended Effects); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED AS IS. TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELICS AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorneys fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 214.71848,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    },
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-07-09T08:11:21Z",
      "updated_at": "2021-07-09T08:11:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.61125,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ", webinars, and other <em>information</em> about using New Relic features, visit New Relic University and newrelic.com&#x2F;resources. For more help Recommendations for learning more: See the Docs site&#x27;s landing page for <em>Licenses</em> documentation. Browse New Relic&#x27;s Explorers Hub for community discussions about New Relic <em>licenses</em>."
      },
      "id": "603ea419e7b9d27b942a07b4"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/data-collector-licenses": [
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-07-09T09:15:32Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.50957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-07-09T09:14:24Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.50696,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-07-09T09:14:25Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (Terms) set forth the terms and conditions under which you (Customer) may use New Relic Diagnostics (the Software), as made available by New Relic, Inc. (New Relic). By clicking accept or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then Customer means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customers use of New Relic products to which Customer has a separate subscription (New Relic Products). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. Usage Data means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered Customer Data, your Data or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customers election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMERS OWN RISK. SOFTWARE IS PROVIDED AS IS, WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELICS ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customers access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customers written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Departments Table of Denial Orders or U.S. Treasury Departments list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relics prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customers consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.12616,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-07-09T08:54:45Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.51822,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-07-09T09:15:32Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.50957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-07-09T09:14:24Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.50696,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-premium-support": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-07-09T08:54:45Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.51822,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-07-09T09:15:32Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.50957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-07-09T09:14:25Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (Terms) set forth the terms and conditions under which you (Customer) may use New Relic Diagnostics (the Software), as made available by New Relic, Inc. (New Relic). By clicking accept or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then Customer means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customers use of New Relic products to which Customer has a separate subscription (New Relic Products). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. Usage Data means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered Customer Data, your Data or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customers election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMERS OWN RISK. SOFTWARE IS PROVIDED AS IS, WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELICS ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customers access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customers written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Departments Table of Denial Orders or U.S. Treasury Departments list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relics prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customers consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.12616,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-priority-support": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-07-09T08:54:45Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.51822,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-07-09T09:14:24Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.50696,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-07-09T09:14:25Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (Terms) set forth the terms and conditions under which you (Customer) may use New Relic Diagnostics (the Software), as made available by New Relic, Inc. (New Relic). By clicking accept or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then Customer means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customers use of New Relic products to which Customer has a separate subscription (New Relic Products). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. Usage Data means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered Customer Data, your Data or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customers election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMERS OWN RISK. SOFTWARE IS PROVIDED AS IS, WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELICS ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customers access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customers written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Departments Table of Denial Orders or U.S. Treasury Departments list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relics prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customers consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 208.12616,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/usage-plans/new-relic-one-usage-plan-descriptions": [
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2021-07-09T08:36:28Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing plan. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the Product Usage Ratio) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Products list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.89175,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Product-based pricing <em>usage</em> and New Relic Platform Pricing <em>Usage</em> <em>Plan</em>",
        "sections": "Product-based pricing <em>usage</em> and New Relic Platform Pricing <em>Usage</em> <em>Plan</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " reference the <em>usage</em> <em>plans</em> set forth below or where a subscription has indeterminate product pricing or <em>usage</em> quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. <em>Usage</em>"
      },
      "id": "603ea32a28ccbc7e22eba768"
    },
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-07-09T08:11:21Z",
      "updated_at": "2021-07-09T08:11:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.40294,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Support <em>plans</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": "The New Relic Support <em>Plan</em> offers a variety of resources based on your service subscription. Check out the Support <em>Plan</em> <em>information</em>, resources, channels, and scope of support below. Support <em>plans</em> These Support <em>Plans</em> apply only to your paid service subscription under an existing New Relic agreement"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "New Relic One pricing plan: Frequently asked questions",
        "Frequently asked questions",
        "Q: Where can I find more information about New Relic One and related products?",
        "Q: Is there a limit on the number of concurrent sessions or IP addresses that may be used with a user account?",
        "Q: What terms govern my use of the Products?",
        "Q: How are the number of full users (also referred to as Monthly Provisioned Users) calculated for each month?",
        "Q: Can I mix and match New Relic One Standard, Pro, and Enterprise users in a billing account?",
        "Q: Can Event extended retention, additional Synthetics Checks, or additional New Relic Edge data be added to my account?",
        "Q: What is included in New Relic's Free Tier of Products?",
        "Q: How does the Free Tier impact my pay-as-you-go (PAYG) or Annual Pool of Funds (APOF) subscription?"
      ],
      "title": "New Relic One pricing plan: Frequently asked questions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "FAQ"
      ],
      "external_id": "cc702038ec7eb4983faa152108a4a233bd285264",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/faq/new-relic-one-pricing-plan-frequently-asked-questions/",
      "published_at": "2021-07-09T08:53:10Z",
      "updated_at": "2021-03-29T20:50:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "These are some frequently asked questions about the New Relic One pricing plan that arise for agreement-level language. This is meant to serve as a supplement to the main New Relic One pricing docs. Frequently asked questions These FAQs are meant to serve as a supplement to the main New Relic One pricing docs. Q: Where can I find more information about New Relic One and related products? A: For a high level description of user entitlements for Telemetry Data Platform, Full Stack Observability, and Applied Intelligence, please visit our pricing page. For more in-depth information, see New Relic One pricing. Q: Is there a limit on the number of concurrent sessions or IP addresses that may be used with a user account? A: Yes. See Manage users. Q: What terms govern my use of the Products? A: If a Customer has paid New Relic during a rolling 12-month period, then Customers usage of the Products is covered by the Paid Terms of Service. If a Customer has not paid New Relic during a 12-month period, then Customers usage of the Products is covered by the Unpaid Terms of Service. Q: How are the number of full users (also referred to as Monthly Provisioned Users) calculated for each month? A: Please see Calculation details. Q: Can I mix and match New Relic One Standard, Pro, and Enterprise users in a billing account? A: No. For the New Relic One pricing plan, you can only have one subscription per organization (group of accounts that share the same billing ID). Q: Can Event extended retention, additional Synthetics Checks, or additional New Relic Edge data be added to my account? A: For additional retention for Events or Logs beyond the Standard Data Retention, contact your New Relic account executive. Note: Minimum requirement for Extended Retention is that the Customer must have a Annual Pool of Funds subscription which includes all of the following: Telemetry Data Platform, Full Stack Observability Pro or above. Telemetry Data Platform Pricing is affected. Q: What is included in New Relic's Free Tier of Products? A: Please see this pricing doc for a description of what's included. Customers use of the Free Tier shall be governed by the terms and conditions described in the Unpaid Terms of Service. If Customers usage exceeds the Free Tier, Customer is fully responsible for fees incurred in excess of the Free Tier as described in the Pay-as-you-go program and as described in the Paid Terms of Service. Q: How does the Free Tier impact my pay-as-you-go (PAYG) or Annual Pool of Funds (APOF) subscription? A: The Free Tier usage will be deducted automatically from the Monthly Product Usage for Pay-as-you-go or Annual pool of funds subscriptions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 120.868416,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One pricing <em>plan</em>: Frequently asked questions",
        "sections": "Q: Where can I find more <em>information</em> about New Relic One and related products?",
        "tags": "<em>License</em> <em>information</em>",
        "body": "These are some frequently asked questions about the New Relic One pricing <em>plan</em> that arise for agreement-level language. This is meant to serve as a supplement to the main New Relic One pricing docs. Frequently asked questions These FAQs are meant to serve as a supplement to the main New Relic One"
      },
      "id": "6044e6e5196a67b568960f3e"
    }
  ],
  "/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan": [
    {
      "sections": [
        "New Relic One Usage plan descriptions",
        "Pay As You Go",
        "Annual Pool of Funds",
        "Applicable Invoicing and Order Terms",
        "User Accounts",
        "New Relic One Pro and Enterprise Service Level Availability Commitment",
        "New Relic One Pro and Enterprise Support Plans",
        "Free tier, lite, no-charge, preview access, New Relic One - Data, New Relic One - Standard User subscriptions"
      ],
      "title": "New Relic One Usage plan descriptions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "c18e1c6c294914c28bba48f9de025333210ed254",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/new-relic-one-usage-plan-descriptions/",
      "published_at": "2021-07-09T08:47:22Z",
      "updated_at": "2021-03-13T04:19:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Usage Plan applies to (i) your Pay As You Go subscription, or (ii) your Commitment Term for the Annual Pool of Funds subscription. New Relic may modify the Usage Plan from time to time. Any changes to the Usage Plan will become effective immediately for changes that provide a benefit or right to the Customer, all other changes will become effective if Customer assents or upon any new or renewal Commitment Term. Usage Plan - Effective as February 19, 2021: The Order and Usage Plan may contain defined terms that are denoted by capitalization. In the event that a capitalized term is not defined in either the Order or the Usage Plan, such terms shall have the meaning set forth in the New Relic One pricing definitions page. Pay As You Go By electing and subscribing to the Pay As You Go subscription model (Pay As You Go or PAYG), Customer commits to paying for the New Relic Products on a month-to-month consumption basis. Monthly Product Usage will be invoiced regardless if a PO is required or not. Any Customer dispute to Monthly Product Usage from the prior month must be in good-faith and received by New Relic in writing within three (3) business days of the start of the next month or such dispute notice will be considered invalid. The dispute notice will set forth in reasonable detail the information concerning the disputed charges. The parties will use good-faith efforts to promptly resolve any disputed charges. Customers usage of the Products in excess of the Free Tier each month shall be billed in arrears on the first business day of the following month based on the Customers Per Unit usage of each Product each month multiplied by the corresponding rates set forth in an Order and summed (Monthly Product Usage). Annual Pool of Funds By electing and subscribing to the Annual Pool of Funds subscription model (Annual Pool of Funds or APoF) for the Commitment Term, Customer commits in an Order to: (i) paying the Commitment Fee amounts described in the Subscription table and any additional commitment fees set forth; and (ii) the Monthly Discounted Fee Rates applying to Customers Monthly Product Usage. New Relic will invoice the Commitment Fee as per the Billing Terms described in an Order. On a monthly cadence during the Commitment Term, Customers Per Unit usage of the Products will be multiplied by the corresponding Monthly Discounted Rate and summed (Monthly Product Usage). Monthly Product Usage will be deducted from the Commitment Fee amounts that are paid in advance. If Monthly Product Usage exceeds any such remaining unconsumed amounts, Customer will be invoiced for the difference (Additional Usage), including for any Monthly Product Usage during the last month of the Commitment Term. Payment of such invoices will be governed as set forth in the Terms. Any Customer dispute to Monthly Product Usage from the prior month must be in good-faith and received by New Relic in writing within three (3) business days of the start of the next month (including for any Monthly Product Usage for the last month of the Commitment Term) or such dispute notice will be considered invalid. The dispute notice will set forth in reasonable detail the information concerning the disputed charges. The parties will use good-faith efforts to promptly resolve any disputed charges. Any unconsumed balances from the Customer payment of each annual Commitment Fee and any additional payments, if applicable, as set out in an Order will expire and lapse at the end of each year of the Commitment Term. Applicable Invoicing and Order Terms All amounts stated in an Order are non-cancelable payment obligations of the Customer for the Commitment Term regardless of usage. Any fees paid are non-refundable and do not represent a deposit for, or a credit towards, the purchase of other products not specified in an Order or for any purchase after the Commitment Term. Customer acknowledges that a final payment for the full outstanding amount of any remaining unpaid Commitment Fee and/or Monthly Product Usage Fee may be invoiced upon each anniversary date of the term start date. Tax will be added where applicable. New Relic may review Customer's use of the Products at any time. If New Relic identifies any Customer usage of the Product(s) that is not in accordance with the Terms or Documentation, New Relic may suspend such unauthorized usage. All existing purchases and related pricing in effect prior to the execution of an Order shall remain in force. Unless otherwise stated in an Order, an Order does not modify or amend any existing purchases. Additional future products or quantities are not subject to promotional pricing unless otherwise stated in an Order. In the event that a Customer indicates in an Order that it requires a purchase order (PO) for its subscription, Customer agrees to provide the required PO prior to the provisioning of the Products. If a Customer does not indicate a PO is required, Customer agrees that New Relic may issue invoice(s) and is entitled to such payment without a PO reference. All Additional Usage fees will be invoiced regardless of a PO requirement. The Product(s) are deemed accepted upon its provisioning. User Accounts Use of the Products require Customer users to create Login Credentials. Customer user Login Credential information must be accurate, current, and complete. New Relics use and collection of Login Credentials (in accordance with its General Data Privacy Notice) is for account and product management and support of its customers. Customer and Customer users must abide by the New Relic Acceptable Use Policy (AUP) and Login Credentials may not be shared. Each Customer user must have their own user account. Entry into an Order indicates your agreement that the amount of provisioned users (at the rate specified in the Order) applies in lieu of and supersedes any other amount of users of the Products that may be specified in the agreement between Customer and New Relic. New Relic One Pro and Enterprise Service Level Availability Commitment With a subscription to New Relic Full Stack Observability Pro or Enterprise Products, you agree that during the Commitment Term the applicable service level availability commitment set forth on the Service level availability commitment page in the Documentation shall apply to the Products. For clarity, if your agreement with New Relic contains a different service level availability commitment or remedies, the above does not apply to your subscription to the New Relic Full Stack Observability Pro or Enterprise Products. If you subscribe to any other New Relic Products with New Relic One pricing, any service level availability commitment or related remedies contained within your agreement with New Relic are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. New Relic One Pro and Enterprise Support Plans With a subscription to New Relic One Users (Full Stack Observability), New Relic provides an updated support plan commitment. By subscribing to New Relic One Users (Full Stack Observability), you agree that during the Commitment Term the applicable Support Plan set forth on the Support plan page in the Documentation shall apply in lieu of, and supersedes and replaces, any other support related commitments that may be contained within your agreement with New Relic. For New Relic K.K. customers (Japan), the above does not currently apply to the support offerings provided by New Relic to you. Free tier, lite, no-charge, preview access, New Relic One - Data, New Relic One - Standard User subscriptions If you are using New Relics Products in the free tier only, or on a no-charge, or a lite or preview access basis you agree that the Unpaid Terms of Service will apply to such Product usage and replace and supersede any other terms. In addition, if your subscription contains the New Relic One - Standard User (Full Stack Observability Standard) Product, you agree that the Paid Terms of Service will apply to your subscription to the Products set forth in an Order and replace and supersede any other terms.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.53761,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One <em>Usage</em> <em>plan</em> descriptions",
        "sections": "New Relic One <em>Usage</em> <em>plan</em> descriptions",
        "tags": "<em>License</em> <em>information</em>",
        "body": "The <em>Usage</em> <em>Plan</em> applies to (i) your Pay As You Go subscription, or (ii) your Commitment Term for the Annual Pool of Funds subscription. New Relic may modify the <em>Usage</em> <em>Plan</em> from time to time. Any changes to the <em>Usage</em> <em>Plan</em> will become effective immediately for changes that provide a benefit or right"
      },
      "id": "6044e74ee7b9d2a4515799c8"
    },
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Important",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-07-09T08:11:21Z",
      "updated_at": "2021-07-09T08:11:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Important NOTE: If you are a New Relic HIPAA customer, please be advised that you must follow the requirements specified in the Global Technical Support Section of HIPAA enablement - what you need to know and do when requesting support and engaging with the New Relic Global Technical Support team for assistance. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 178.40286,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Support <em>plans</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": "The New Relic Support <em>Plan</em> offers a variety of resources based on your service subscription. Check out the Support <em>Plan</em> <em>information</em>, resources, channels, and scope of support below. Support <em>plans</em> These Support <em>Plans</em> apply only to your paid service subscription under an existing New Relic agreement"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "New Relic One pricing plan: Frequently asked questions",
        "Frequently asked questions",
        "Q: Where can I find more information about New Relic One and related products?",
        "Q: Is there a limit on the number of concurrent sessions or IP addresses that may be used with a user account?",
        "Q: What terms govern my use of the Products?",
        "Q: How are the number of full users (also referred to as Monthly Provisioned Users) calculated for each month?",
        "Q: Can I mix and match New Relic One Standard, Pro, and Enterprise users in a billing account?",
        "Q: Can Event extended retention, additional Synthetics Checks, or additional New Relic Edge data be added to my account?",
        "Q: What is included in New Relic's Free Tier of Products?",
        "Q: How does the Free Tier impact my pay-as-you-go (PAYG) or Annual Pool of Funds (APOF) subscription?"
      ],
      "title": "New Relic One pricing plan: Frequently asked questions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "FAQ"
      ],
      "external_id": "cc702038ec7eb4983faa152108a4a233bd285264",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/faq/new-relic-one-pricing-plan-frequently-asked-questions/",
      "published_at": "2021-07-09T08:53:10Z",
      "updated_at": "2021-03-29T20:50:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "These are some frequently asked questions about the New Relic One pricing plan that arise for agreement-level language. This is meant to serve as a supplement to the main New Relic One pricing docs. Frequently asked questions These FAQs are meant to serve as a supplement to the main New Relic One pricing docs. Q: Where can I find more information about New Relic One and related products? A: For a high level description of user entitlements for Telemetry Data Platform, Full Stack Observability, and Applied Intelligence, please visit our pricing page. For more in-depth information, see New Relic One pricing. Q: Is there a limit on the number of concurrent sessions or IP addresses that may be used with a user account? A: Yes. See Manage users. Q: What terms govern my use of the Products? A: If a Customer has paid New Relic during a rolling 12-month period, then Customers usage of the Products is covered by the Paid Terms of Service. If a Customer has not paid New Relic during a 12-month period, then Customers usage of the Products is covered by the Unpaid Terms of Service. Q: How are the number of full users (also referred to as Monthly Provisioned Users) calculated for each month? A: Please see Calculation details. Q: Can I mix and match New Relic One Standard, Pro, and Enterprise users in a billing account? A: No. For the New Relic One pricing plan, you can only have one subscription per organization (group of accounts that share the same billing ID). Q: Can Event extended retention, additional Synthetics Checks, or additional New Relic Edge data be added to my account? A: For additional retention for Events or Logs beyond the Standard Data Retention, contact your New Relic account executive. Note: Minimum requirement for Extended Retention is that the Customer must have a Annual Pool of Funds subscription which includes all of the following: Telemetry Data Platform, Full Stack Observability Pro or above. Telemetry Data Platform Pricing is affected. Q: What is included in New Relic's Free Tier of Products? A: Please see this pricing doc for a description of what's included. Customers use of the Free Tier shall be governed by the terms and conditions described in the Unpaid Terms of Service. If Customers usage exceeds the Free Tier, Customer is fully responsible for fees incurred in excess of the Free Tier as described in the Pay-as-you-go program and as described in the Paid Terms of Service. Q: How does the Free Tier impact my pay-as-you-go (PAYG) or Annual Pool of Funds (APOF) subscription? A: The Free Tier usage will be deducted automatically from the Monthly Product Usage for Pay-as-you-go or Annual pool of funds subscriptions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 120.868416,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One pricing <em>plan</em>: Frequently asked questions",
        "sections": "Q: Where can I find more <em>information</em> about New Relic One and related products?",
        "tags": "<em>License</em> <em>information</em>",
        "body": "These are some frequently asked questions about the New Relic One pricing <em>plan</em> that arise for agreement-level language. This is meant to serve as a supplement to the main New Relic One pricing docs. Frequently asked questions These FAQs are meant to serve as a supplement to the main New Relic One"
      },
      "id": "6044e6e5196a67b568960f3e"
    }
  ],
  "/docs/licenses/product-or-service-licenses/miscellaneous/help-center-documentation-licenses": [
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-07-09T10:18:47Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0  2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0  2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0  2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0  2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0  2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright  2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright  1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright  1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright  2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright  1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright  1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 150.84523,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2021-07-09T10:35:02Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 143.12083,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic APM <em>licenses</em>",
        "sections": "New Relic APM <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view <em>licenses</em> for... See... APM agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;new-relic-apm"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "New Relic Insights licenses"
      ],
      "title": "New Relic Insights licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Insights"
      ],
      "external_id": "3d0f8f4275af8bcf9f6f3ffb02a2e46ff3fc374f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-insights/new-relic-insights-licenses/",
      "published_at": "2021-07-09T10:16:39Z",
      "updated_at": "2021-03-13T04:04:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic Insights. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Product Licenses New Relic Insights See insights.newrelic.com/licenses",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 140.68039,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Insights <em>licenses</em>",
        "sections": "New Relic Insights <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic Insights. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. <em>Product</em> <em>Licenses</em> New Relic Insights See insights.newrelic.com&#x2F;<em>licenses</em>"
      },
      "id": "6044e84428ccbcf6fa2c60c1"
    }
  ],
  "/docs/licenses/product-or-service-licenses/mobile-app-licenses/android-application-licenses": [
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-07-09T08:48:45Z",
      "updated_at": "2021-05-05T16:26:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright  2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright  2010, Ajax.org B.V. ActiveLabel MIT Copyright  2015 Optonaut Alamofire MIT Copyright  2014-2018 Alamofire Software Foundation AlamofireObjectMapper MIT Copyright  2015 Tristan Himmelman Analytics MIT Copyright  2016 Segment.io, Inc. BBlock MIT Copyright  2012 David Keegan BigNumber MIT Copyright  2019 mkrd CDMarkdownKit MIT Copyright  2016-2017 Christopher de Haan contact@christopherdehaan.me ECSlidingViewController MIT Copyright  2013 EdgeCase LoginManagerSDK New Relic License Copyright  2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright  GitHub, Inc. All rights reserved. Copyright  2012, Bitswift, Inc MetricMetadata New Relic License Copyright  2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright  2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright  2016 Vinh Nguyen NewRelicAgent New Relic License Copyright  2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright  2014 Hearst RadarKit New Relic License  2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright  2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright  2011 kishikawa katsumi WidgetLibrary New Relic License  2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright  2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright  2016 JP Simard. jsTimezoneDetect MIT Copyright  2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright  JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 217.21706,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS <em>application</em> <em>licenses</em>",
        "sections": "iOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic iOS <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "tvOS application licenses"
      ],
      "title": "tvOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "0b3ac8ec42cef00f5a4d3ddf354e4be38ad0595f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/tvos-application-licenses/",
      "published_at": "2021-07-09T08:50:28Z",
      "updated_at": "2021-05-05T16:26:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic for TV app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright  2011-2020 Alamofire Software Foundation (http://alamofire.org/) Analytics MIT Copyright  2016 Segment.io, Inc. CDMarkdownKit MIT Copyright  2016-2017 Christopher de Haan contact@christopherdehaan.me LoginManagerSDK New Relic License  2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright  GitHub, Inc. All rights reserved. Copyright  2012, Bitswift, Inc MetricMetadata New Relic License  2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License  2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright  2016 Vinh Nguyen NewRelicAgent New Relic License  2010-2021 New Relic, Inc. All rights reserved. UICKeyChainStore MIT Copyright  2011 kishikawa katsumi WidgetLibrary New Relic License  2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright  2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright  2016 JP Simard. iOS-fontawesome CC BY 3.0 & MIT Copyright  2012 Alex Usbergo. All rights reserved. The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 217.21706,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "tvOS <em>application</em> <em>licenses</em>",
        "sections": "tvOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic for TV <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "6072d619196a6795b664a75c"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-07-09T10:18:47Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0  2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0  2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0  2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0  2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0  2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright  2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright  1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright  1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright  2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright  1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright  1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 152.96005,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ],
  "/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses": [
    {
      "sections": [
        "Android application licenses"
      ],
      "title": "Android application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "68c9bdc9dec6f02240f002494309519e41619f29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/android-application-licenses/",
      "published_at": "2021-07-09T10:25:57Z",
      "updated_at": "2021-05-05T16:29:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Android app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Third Party Dependencies License Copyright Android-FlowLayout Apache 2.0 Copyright  2011, Artem Votincev (apmem.org) AVLoadingIndicatorView Apache 2.0 Copyright  2015, Jack Wang BottomNavigationViewEx MIT Copyright  2017, ittianyu Butterknife Apache 2.0 Copyright  2013, Jake Wharton Crouton Apache 2.0 Copyright  2012 - 2014, Benjamin Weiss CWAC-SafeRoom Apache 2.0 The copyrights are owned by CommonsWare for things unique to this library and a combination of CommonsWare and the Android Open Source Project for code modified from the Architecture Components' Framework* set of classes. Dagger 2 Apache 2.0 Copyright  2012, The Dagger Authors Dragtop Layout Apache 2.0 Copyright  2015, chenupt EventBus Apache 2.0 Copyright  2012-2017 Markus Junginger, greenrobot FlexibleAdapter Apache 2.0 Copyright  2015-2018 Davide Steduto, Davidea Solutions Sprl Gson Apache 2.0 Copyright  2008, Google Inc. markwon Apache 2.0 Copyright  2019 Dimitry Ivanov (legal@noties.io) mockk Apache 2.0 Copyright  [ 2017] [ github.com/mockk] leakcanary Apache 2.0 Copyright  2015 Square, Inc. mockito mockito MIT Copyright  2007 Mockito contributors mosby Apache 2.0 Copyright  2015 Hannes Dorfmann moshi Apache 2.0 Copyright  2015 Square, Inc. MPAndroidChart Apache 2.0 Copyright  2019 Philipp Jahoda New Relic Mobile Agent OKHttp Apache 2.0 Copyright  2019 Square, Inc. okio Apache 2.0 Copyright  2013 Square, Inc. Picasso Apache 2.0 Copyright  2013 Square, Inc. RESTMock Apache 2.0 Copyright  2016 Appflate.io Retrofit Apache 2.0 Copyright  2013 Square, Inc. RxJava Apache 2.0 Copyright  2016-present, RxJava Contributors. Segment IO MIT Copyright  2016 Segment, Inc. Snackyaml Apache 2.0 Copyright  2008, www.snakeyaml.org. StickyHeaders Apache 2.0 Copyright  2014 Emil Sjlander TableView Apache 2.0 Copyright  2017 Evren Cokun Transitions-Everywhere Apache 2.0 The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 217.21759,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Android <em>application</em> <em>licenses</em>",
        "sections": "Android <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Android <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Third Party Dependencies"
      },
      "id": "603e9e30196a67b71fa83d96"
    },
    {
      "sections": [
        "tvOS application licenses"
      ],
      "title": "tvOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "0b3ac8ec42cef00f5a4d3ddf354e4be38ad0595f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/tvos-application-licenses/",
      "published_at": "2021-07-09T08:50:28Z",
      "updated_at": "2021-05-05T16:26:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic for TV app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright  2011-2020 Alamofire Software Foundation (http://alamofire.org/) Analytics MIT Copyright  2016 Segment.io, Inc. CDMarkdownKit MIT Copyright  2016-2017 Christopher de Haan contact@christopherdehaan.me LoginManagerSDK New Relic License  2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright  GitHub, Inc. All rights reserved. Copyright  2012, Bitswift, Inc MetricMetadata New Relic License  2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License  2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright  2016 Vinh Nguyen NewRelicAgent New Relic License  2010-2021 New Relic, Inc. All rights reserved. UICKeyChainStore MIT Copyright  2011 kishikawa katsumi WidgetLibrary New Relic License  2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright  2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright  2016 JP Simard. iOS-fontawesome CC BY 3.0 & MIT Copyright  2012 Alex Usbergo. All rights reserved. The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 217.21706,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "tvOS <em>application</em> <em>licenses</em>",
        "sections": "tvOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic for TV <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "6072d619196a6795b664a75c"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-07-09T10:18:47Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0  2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0  2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0  2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0  2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0  2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright  2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright  1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright  1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright  2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright  1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright  1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 152.96005,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ]
}