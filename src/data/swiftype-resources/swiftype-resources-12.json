{
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-app-service-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14708,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69818,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69781,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-application-gateway-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14708,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69818,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69781,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-containers-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14706,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69817,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69781,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-cosmos-db-document-db-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14706,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69817,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69781,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-cost-management-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14706,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69817,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.6978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-data-factory-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14706,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69817,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.6978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14706,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69817,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure API Management monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "API Management Service data"
      ],
      "title": "Azure API Management monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "c14b2e2440545d9d2c6d4d50df72dd404770d209",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-api-management-monitoring-integration/",
      "published_at": "2021-06-26T14:13:22Z",
      "updated_at": "2021-03-16T06:10:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Microsoft Azure API Management data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure API Management integration: New Relic polling interval: 5 minutes View and use data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event type Provider Service AzureApiManagementServiceSample AzureApiManagementService For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure API Management data for Service. API Management Service data Metric Unit Description totalRequests Count The total number of gateway requests in a given period. successfulRequests Count The total number of successful gateway requests in a given period. unauthorizedRequests Count The total number of unauthorized gateway requests in a given period. failedRequests Count The total number of failed gateway requests in a given period. otherRequests Count The total number of gateway requests in a given period that do not fall into the successful, unauthorized, or failed categories. durationMilliseconds Milliseconds The time between when API Management receives a request from a client and when it returns a response to the client. capacityPercent Percent Indicator of load on an API Management instance. eventHubTotalEvents Count The total number of events sent to EventHub from API Management in a given period. eventHubSuccessfulEvents Count The total number of successful EventHub events in a given period. eventHubTotalFailedEvents Count The total number of failed EventHub events in a given period. eventHubRejectedEvents Count The total number of rejected EventHub events (wrong configuration or unauthorized) in a given period. eventHubThrottledEvents Count The total number of throttled EventHub events in a given period. eventHubTimedoutEvents Count The total number of timed out EventHub events in a given period. eventHubDroppedEvents Count The total number of events skipped because of queue size limit reached in a given period. eventHubTotalBytesSentBytes Bytes The total size of EventHub events in bytes in a given period.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.53308,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> API Management monitoring <em>integration</em>",
        "sections": "<em>Azure</em> API Management monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic offers an integration for reporting your <em>Microsoft</em> <em>Azure</em> API Management data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your <em>Azure</em> service"
      },
      "id": "603ea20164441f8ed44e8872"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14705,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.6978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    },
    {
      "sections": [
        "Azure API Management monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "API Management Service data"
      ],
      "title": "Azure API Management monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "c14b2e2440545d9d2c6d4d50df72dd404770d209",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-api-management-monitoring-integration/",
      "published_at": "2021-06-26T14:13:22Z",
      "updated_at": "2021-03-16T06:10:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Microsoft Azure API Management data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure API Management integration: New Relic polling interval: 5 minutes View and use data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event type Provider Service AzureApiManagementServiceSample AzureApiManagementService For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure API Management data for Service. API Management Service data Metric Unit Description totalRequests Count The total number of gateway requests in a given period. successfulRequests Count The total number of successful gateway requests in a given period. unauthorizedRequests Count The total number of unauthorized gateway requests in a given period. failedRequests Count The total number of failed gateway requests in a given period. otherRequests Count The total number of gateway requests in a given period that do not fall into the successful, unauthorized, or failed categories. durationMilliseconds Milliseconds The time between when API Management receives a request from a client and when it returns a response to the client. capacityPercent Percent Indicator of load on an API Management instance. eventHubTotalEvents Count The total number of events sent to EventHub from API Management in a given period. eventHubSuccessfulEvents Count The total number of successful EventHub events in a given period. eventHubTotalFailedEvents Count The total number of failed EventHub events in a given period. eventHubRejectedEvents Count The total number of rejected EventHub events (wrong configuration or unauthorized) in a given period. eventHubThrottledEvents Count The total number of throttled EventHub events in a given period. eventHubTimedoutEvents Count The total number of timed out EventHub events in a given period. eventHubDroppedEvents Count The total number of events skipped because of queue size limit reached in a given period. eventHubTotalBytesSentBytes Bytes The total size of EventHub events in bytes in a given period.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.53307,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> API Management monitoring <em>integration</em>",
        "sections": "<em>Azure</em> API Management monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic offers an integration for reporting your <em>Microsoft</em> <em>Azure</em> API Management data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your <em>Azure</em> service"
      },
      "id": "603ea20164441f8ed44e8872"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-postgresql-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14705,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69815,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.6978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-event-hub-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14705,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69815,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.6978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-express-route-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14705,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69815,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.6978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-firewalls-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14703,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69815,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69778,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-front-door-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14703,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69815,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69778,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-functions-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14703,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69815,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69778,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-key-vault-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14703,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69814,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69778,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-load-balancer-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14703,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69814,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69778,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-logic-apps-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14702,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69814,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69777,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-machine-learning-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14702,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69814,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69777,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-power-bi-dedicated-capacities-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14702,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69812,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69777,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-redis-cache-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14702,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69812,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69777,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-bus-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14702,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69812,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69777,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-service-fabric-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.147,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69812,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69777,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-database-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.147,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69812,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69777,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-sql-managed-instances-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.147,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69812,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69775,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-storage-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.147,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69812,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69775,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machine-scale-sets-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.147,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69812,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69775,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machines-scale-sets-monitoring-integration": [
    {
      "sections": [
        "Azure virtual machine scale sets monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Virtual machine scale sets ScaleSet data"
      ],
      "title": "Azure virtual machine scale sets monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "2b25b6720032817e09a6e844210f020b3a4fc98b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-machine-scale-sets-monitoring-integration/",
      "published_at": "2021-06-26T00:12:44Z",
      "updated_at": "2021-03-16T04:41:10Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Azure virtual machine scale sets data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure virtual machine scale sets integration: New Relic polling interval: 5 minutes Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. You can query and explore your data using the following event type: Entity Event type Provider ScaleSet AzureVirtualMachineScaleSetSample AzureVirtualMachineScaleSet For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure virtual machine scale sets data for ScaleSet. Virtual machine scale sets ScaleSet data Metric Unit Description cpuPercent Percent The percentage of allocated compute units that are currently in use by the Virtual Machine(s) networkInBytes Bytes The number of billable bytes received on all network interfaces by the Virtual Machine(s) (Incoming Traffic) networkOutBytes Bytes The number of billable bytes out on all network interfaces by the Virtual Machine(s) (Outgoing Traffic) diskReadBytes Bytes Bytes read from disk during monitoring period diskWriteBytes Bytes Bytes written to disk during monitoring period diskReadOperationsCountPerSecond CountPerSecond Disk Read IOPS diskWriteOperationsCountPerSecond CountPerSecond Disk Write IOPS cpuCreditsRemaining Count Total number of credits available to burst cpuCreditsConsumed Count Total number of credits consumed by the Virtual Machine dataDiskReadBytesCountPerSecond CountPerSecond Bytes/Sec read from a single disk during monitoring period dataDiskWriteBytesCountPerSecond CountPerSecond Bytes/Sec written to a single disk during monitoring period dataDiskReadOperationsCountPerSecond CountPerSecond Read IOPS from a single disk during monitoring period dataDiskWriteOperationsCountPerSecond CountPerSecond Write IOPS from a single disk during monitoring period dataDiskQueueDepth Count Data Disk Queue Depth(or Queue Length) osDiskReadBytesCountPerSecond CountPerSecond Bytes/Sec read from a single disk during monitoring period for OS disk osDiskWriteBytesCountPerSecond CountPerSecond Bytes/Sec written to a single disk during monitoring period for OS disk osDiskReadOperationsCountPerSecond CountPerSecond Read IOPS from a single disk during monitoring period for OS disk osDiskWriteOperationsCountPerSecond CountPerSecond Write IOPS from a single disk during monitoring period for OS disk osDiskQueueDepth Count OS Disk Queue Depth(or Queue Length) inboundFlows Count Inbound Flows are number of current flows in the inbound direction (traffic going into the VM) outboundFlows Count Outbound Flows are number of current flows in the outbound direction (traffic going out of the VM) inboundFlowsMaximumCreationRateCountPerSecond CountPerSecond The maximum creation rate of inbound flows (traffic going into the VM) outboundFlowsMaximumCreationRateCountPerSecond CountPerSecond The maximum creation rate of outbound flows (traffic going out of the VM) premiumDataDiskCacheReadHitPercent Percent Premium Data Disk Cache Read Hit premiumDataDiskCacheReadMissPercent Percent Premium Data Disk Cache Read Miss premiumOSDiskCacheReadHitPercent Percent Premium OS Disk Cache Read Hit premiumOSDiskCacheReadMissPercent Percent Premium OS Disk Cache Read Miss networkInTotalBytes Bytes The number of bytes received on all network interfaces by the Virtual Machine(s) (Incoming Traffic) networkOutTotalBytes Bytes The number of bytes out on all network interfaces by the Virtual Machine(s) (Outgoing Traffic)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1211.9263,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>virtual</em> <em>machine</em> <em>scale</em> <em>sets</em> <em>monitoring</em> <em>integration</em>",
        "sections": "<em>Azure</em> <em>virtual</em> <em>machine</em> <em>scale</em> <em>sets</em> <em>monitoring</em> <em>integration</em>",
        "tags": "Microsoft <em>Azure</em> <em>integrations</em>",
        "body": " data. Metric data This <em>integration</em> collects <em>Azure</em> <em>virtual</em> machine <em>scale</em> <em>sets</em> data for <em>ScaleSet</em>. <em>Virtual</em> machine <em>scale</em> <em>sets</em> <em>ScaleSet</em> data Metric Unit Description cpuPercent Percent The percentage of allocated compute units that are currently in use by the <em>Virtual</em> Machine(s) networkInBytes Bytes"
      },
      "id": "603ea1cfe7b9d2a8342a0819"
    },
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 154.6569,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs <em>monitoring</em> <em>integration</em>",
        "sections": "<em>Azure</em> VMs <em>monitoring</em> <em>integration</em>",
        "tags": "Microsoft <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>monitoring</em> provides an <em>integration</em> for Microsoft <em>Azure</em> <em>Virtual</em> <em>Machines</em> (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this <em>integration</em> and describes the data that can be captured. Features New Relic&#x27;s <em>integration</em>"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Introduction to the Kubernetes integration",
        "Get started: Install the Kubernetes integration",
        "Tip",
        "Why it matters",
        "Navigate all your Kubernetes events",
        "Bring your cluster logs to New Relic",
        "Check the source code"
      ],
      "title": "Introduction to the Kubernetes integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Kubernetes integration",
        "Get started"
      ],
      "external_id": "c641d1367f1f8fd2b589a2707112759becae609b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/kubernetes-integration/get-started/introduction-kubernetes-integration/",
      "published_at": "2021-06-30T01:53:03Z",
      "updated_at": "2021-06-02T01:43:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Kubernetes integration gives you full observability into the health and performance of your environment, no matter whether you run Kubernetes on-premises or in the cloud. With our cluster explorer, you can cut through layers of complexity to see how your cluster is performing, from the heights of the control plane down to applications running on a single pod. one.newrelic.com > Kubernetes cluster explorer: The cluster explorer is our powerful, fully visual answer to the challenges associated with running Kubernetes at a large scale. You can see the power of the Kubernetes integration in the cluster explorer, where the full picture of a cluster is made available on a single screen: nodes and pods are visualized according to their health and performance, with pending and alerting nodes in the innermost circles. Predefined alert conditions help you troubleshoot issues right from the start. Clicking each node reveals its status and how each app is performing. Get started: Install the Kubernetes integration We have an automated installer to help you with many types of installations: servers, virtual machines, and unprivileged environments. It can also help you with installations in managed services or platforms, but you'll need to review a few preliminary notes before getting started. Here's what the automated installer does: Asks for the cluster name and namespace of the integration. Asks for additional setup options, such as Kube state metrics. Asks for the installation method: manifest file or Helm. Generates either the manifest or Helm chart. Read the install docs Start the installer Tip If your New Relic account is in the EU region, access the automated installer from one.eu.newrelic.com. Why it matters Governing the complexity of Kubernetes can be challenging; there's so much going on at any given moment, with containers being created and deleted in a matter of minutes, applications crashing, and resources being consumed unexpectedly. Our integration helps you navigate Kubernetes abstractions across on-premises, cloud, and hybrid deployments. In New Relic, you can build your own charts and query all your Kubernetes data, which our integration collects by instrumenting the container orchestration layer. This gives you additional insight into nodes, namespaces, deployments, replica sets, pods, and containers. one.newrelic.com > Dashboards: Using the query builder you can turn any query on Kubernetes data to clear visuals. With the Kubernetes integration you can also: Link your APM data to Kubernetes to measure the performance of your web and mobile applications, with metrics such as request rate, throughput, error rate, and availability. Monitor services running on Kubernetes, such as Apache, NGINX, Cassandra, and many more (see our tutorial for monitoring Redis on Kubernetes). Create new alert policies and alert conditions based on your Kubernetes data, or extend the predefined alert conditions. These features are in addition to the data New Relic already reports for containerized processes running on instrumented hosts. Navigate all your Kubernetes events The Kubernetes events integration, which is installed separately, watches for events happening in your Kubernetes clusters and sends those events to New Relic. Events data is then visualized in the cluster explorer. To set it up, check the Kubernetes events box in step 3 of our install wizard, or follow the instructions. one.newrelic.com > Kubernetes cluster explorer > Events: Browse and filter all your Kubernetes events, and dig into application logs and infrastructure data. Bring your cluster logs to New Relic Our Kubernetes plugin for log monitoring can collect all your cluster's logs and send them to our platform, so that you can set up new alerts and charts. To set it up, check the Log data box in step 3 of our install wizard, or follow the instructions. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or you can create your own fork and build it. For more information, see the README.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 146.80986,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the Kubernetes <em>integration</em>",
        "sections": "Introduction to the Kubernetes <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": " from the start. Clicking each node reveals its status and how each app is performing. Get started: Install the Kubernetes <em>integration</em> We have an automated installer to help you with many types of installations: servers, <em>virtual</em> <em>machines</em>, and unprivileged environments. It can also help you"
      },
      "id": "6043a212196a678d86960f46"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-virtual-network-monitoring-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14699,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.6981,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69775,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration": [
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.6981,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69774,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    },
    {
      "sections": [
        "Azure API Management monitoring integration",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "API Management Service data"
      ],
      "title": "Azure API Management monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "c14b2e2440545d9d2c6d4d50df72dd404770d209",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-api-management-monitoring-integration/",
      "published_at": "2021-06-26T14:13:22Z",
      "updated_at": "2021-03-16T06:10:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers an integration for reporting your Microsoft Azure API Management data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your Azure service in New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Azure API Management integration: New Relic polling interval: 5 minutes View and use data To view your integration data, go to one.newrelic.com > Infrastructure > Azure and select an integration. Data is attached to the following event type: Entity Event type Provider Service AzureApiManagementServiceSample AzureApiManagementService For more on how to use your data, see Understand and use integration data. Metric data This integration collects Azure API Management data for Service. API Management Service data Metric Unit Description totalRequests Count The total number of gateway requests in a given period. successfulRequests Count The total number of successful gateway requests in a given period. unauthorizedRequests Count The total number of unauthorized gateway requests in a given period. failedRequests Count The total number of failed gateway requests in a given period. otherRequests Count The total number of gateway requests in a given period that do not fall into the successful, unauthorized, or failed categories. durationMilliseconds Milliseconds The time between when API Management receives a request from a client and when it returns a response to the client. capacityPercent Percent Indicator of load on an API Management instance. eventHubTotalEvents Count The total number of events sent to EventHub from API Management in a given period. eventHubSuccessfulEvents Count The total number of successful EventHub events in a given period. eventHubTotalFailedEvents Count The total number of failed EventHub events in a given period. eventHubRejectedEvents Count The total number of rejected EventHub events (wrong configuration or unauthorized) in a given period. eventHubThrottledEvents Count The total number of throttled EventHub events in a given period. eventHubTimedoutEvents Count The total number of timed out EventHub events in a given period. eventHubDroppedEvents Count The total number of events skipped because of queue size limit reached in a given period. eventHubTotalBytesSentBytes Bytes The total size of EventHub events in bytes in a given period.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.53307,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> API Management monitoring <em>integration</em>",
        "sections": "<em>Azure</em> API Management monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic offers an integration for reporting your <em>Microsoft</em> <em>Azure</em> API Management data. This document explains how to activate this integration and describes the data that can be reported. Activate integration To enable the integration follow standard procedures to activate your <em>Azure</em> service"
      },
      "id": "603ea20164441f8ed44e8872"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vpn-gateway-integration": [
    {
      "sections": [
        "Azure VMs monitoring integration",
        "Features",
        "Requirements",
        "Activate integration",
        "Important",
        "Configuration and polling",
        "Find and use data",
        "Inventory data",
        "Other system data",
        "Troubleshooting"
      ],
      "title": "Azure VMs monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "b77f3bb6f9dd73582e5789d2c2553a946de28e2b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-vms-monitoring-integration/",
      "published_at": "2021-06-26T04:29:02Z",
      "updated_at": "2021-05-21T18:18:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure monitoring provides an integration for Microsoft Azure Virtual Machines (VMs) that reports data from your Azure VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic's integration for Azure Virtual Machines reports data about your VMs service, like the VM ID, the VM size, the availability set, and the region name. You can monitor and alert on your Azure VMs data from New Relic, and you can create custom queries and chart dashboards. Requirements Requirements include: New Relic infrastructure agent installed with Infrastructure agent version 1.0.775 or higher. (Update the infrastructure agent.) New Relic Azure integrations activated Activate integration To enable this integration follow standard procedures to activate your Azure service in New Relic. Important You must install the infrastructure agent on each VM to see metrics from that host. Connecting your Azure subscription allows New Relic to access VM metadata. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the Virtual Machines integration: Polling interval: 5 minutes Resolution: 1 data point per minute Find and use data To find your integration data, go to one.newrelic.com > Infrastructure > Azure and look for the integration. You can query and explore your data using the AzureVirtualMachineSample event type. The provider value is AzureVirtualMachine. For more on how to find and use integration data, see Understand and use data. Inventory data Inventory data is information about your system's state and configuration. For details on how to find and use inventory data, see Understand and use data. The Azure Virtual Machines integration reports this inventory data: availabilitySet bootDiagnosticsEnabled image linuxConfiguration name networkInterfaces osDisk provisioningState regionName resourceGroupName (deprecates resourceGroup) vmId vmSize windowsConfiguration Other system data The Azure Virtual Machines integration also collects the following attributes about the service and its configuration: Region Availability zone Instance type Instance ID Troubleshooting If you use Host not reporting alert conditions, the importing of Azure metadata for your VMs will change the hosts registry and result in false alert notifications. To prevent false positives: For each Azure instance, disable its Host not reporting alert condition. Update the infrastructure agent for all instances that used this alert condition. Re-enable each Host not reporting alert condition.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.14699,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "sections": "<em>Azure</em> VMs monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure monitoring provides an integration for <em>Microsoft</em> <em>Azure</em> Virtual Machines (VMs) that reports data from your <em>Azure</em> VMs service to New Relic. This document explains how to activate this integration and describes the data that can be captured. Features New Relic&#x27;s integration"
      },
      "id": "6044e56164441faf31378f07"
    },
    {
      "sections": [
        "Azure Database for MySQL monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "Find and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mysql/server/"
      ],
      "title": "Azure Database for MySQL monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "8155643271b086f6fee3b52ca040ff863fab6ed9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mysql-monitoring-integration/",
      "published_at": "2021-06-26T14:16:04Z",
      "updated_at": "2021-05-16T00:03:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the Azure Database for MySQL service, which provides fully managed, enterprise-ready MySQL Community database as a service. The service provides high availability, elastic scaling, automatic backups, and data protection at-rest and in-motion. Using New Relic, you can: View Azure Database for MySQL data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure. Configuration and polling You can change the polling frequency and filter data using configuration options. New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for MySQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute Find and use data To explore your integration data, go to one.newrelic.com > Infrastructure > Azure > (select an integration). Data about a single database is attached to the AzureMySqlServerSample event type, with a provider value of AzureMySqlServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. azure/mysql/server/ administratorLogin configuration databaseNames databases domainName earliestRestoreDate firewalls geoRedundantBackup isDataWarehouse isReplica MasterServerid maxConnections name regionName replicaCapacity resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.6981,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MySQL monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MySQL metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers database data from the <em>Azure</em> Database"
      },
      "id": "603ec29a196a677188a83de6"
    },
    {
      "sections": [
        "Azure Database for MariaDB monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Database sample metrics",
        "Inventory data",
        "azure/mariadb/server"
      ],
      "title": "Azure Database for MariaDB monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Azure integrations list"
      ],
      "external_id": "048605d109fc89086a983491bfbb1280923ea186",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/azure-integrations-list/azure-database-mariadb-monitoring-integration/",
      "published_at": "2021-06-26T14:15:12Z",
      "updated_at": "2021-05-16T00:02:29Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's integrations include an integration for reporting your Microsoft Azure Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the Azure Database for MariaDB service, which provides a fully managed, enterprise-ready database as a service. MariaDB provides predictable performance and scalability for applications using open-source tools and platforms. Using New Relic, you can: View Azure Database for MariaDB data in pre-built dashboards. Run custom queries and visualize the data in New Relic One. Create alert conditions to notify you of changes in data. Activate integration Follow standard procedures to activate your Azure service in New Relic Infrastructure, using the generic Reader role. Configuration and polling New Relic queries your Azure Database services according to a default polling interval, which varies depending on the integration. For Azure Database for PostgreSQL integrations: Polling interval: 5 minutes (maximum recommended polling frequency: 1 hour) Resolution: 1 data point per minute You can change the polling frequency and filter data using configuration options. View and use data To explore your integration data: Go to one.newrelic.com > Infrastructure > Azure > (select an integration). In New Relic, data about a single database is attached to the AzureMariaDbServerSample event type, with a provider value of AzureMariaDbServer. Metric data This integration collects the following metric data. Database sample metrics Metric Description activeConnections Count of active connections. backupStorageUsedBytes Backup storage used, in bytes. connectionsFailed Count of failed connections. cpuPercent Percentage of CPU used. ioConsumptionPercent Percentage of I/O consumption used. memoryPercent Percentage of memory used. networkEgressBytes Network Out across active connections, in bytes. networkIngressBytes Network In across active connections, in bytes. secondsBehindMaster Replication lag, in seconds. serverlogStorageLimitBytes Server log storage limit, in bytes. serverlogStoragePercent Percentage of server log storage used. serverlogStorageUsageBytes Server log storage used, in bytes. storageLimitBytes Amount of storage available, in bytes. storagePercent Percentage of available storage used. storageUsedBytes Amount of storage used, in bytes. Inventory data This integration collects the following inventory data about your system's state and configuration. For more information, see the Microsoft Azure documentation for MariaDB. azure/mariadb/server administratorLogin configuration databaseNames databases, including id, name, type, charset, and collation domainName earliestRestoreDate firewalls, including id, name, type, startIpAddress, and endIpAddress geoRedundantBackup id isReplica masterServerId (only for a replica server) maxConnections name regionName replicaCapacity (only for a replica server) replicationRole (only for a replica server) resourceGroupName skuCapacity skuFamily skuName skuTier sslEnforcement storageAutoGrow tags type userVisibleState version",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 166.69774,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "sections": "<em>Azure</em> Database for MariaDB monitoring <em>integration</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>integrations</em> include an integration for reporting your <em>Microsoft</em> <em>Azure</em> Database for MariaDB metrics and inventory data to New Relic. This document explains how to activate the integration and describes the data reported. Features New Relic gathers data from the <em>Azure</em> Database"
      },
      "id": "603ea5b6196a674f45a83df2"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations": [
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Tip",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-06-26T14:22:36Z",
      "updated_at": "2021-03-13T03:33:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as New Relic APM's .NET support for Azure. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 130.88544,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as New Relic APM&#x27;s .NET support for <em>Azure</em>. Tip To use <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "82db3eae120c4318365cf0d0e5bfee69930b969f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2021-06-26T14:22:36Z",
      "updated_at": "2021-03-13T03:47:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 126.46966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "6044e560196a671d6f960f72"
    },
    {
      "sections": [
        "Azure integration metrics",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "762631e1209bb9abb60f1ea8b185a6def61735b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2021-06-26T08:50:43Z",
      "updated_at": "2021-03-16T15:55:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 122.444885,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunSuccessLatency runSuccessLatencySeconds <em>Azure</em> Logic"
      },
      "id": "603e8a8928ccbcacc0eba74e"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics": [
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-06-26T00:13:37Z",
      "updated_at": "2021-06-20T12:42:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.78677,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    },
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Tip",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-06-26T14:22:36Z",
      "updated_at": "2021-03-13T03:33:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as New Relic APM's .NET support for Azure. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 130.88544,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as New Relic APM&#x27;s .NET support for <em>Azure</em>. Tip To use <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "82db3eae120c4318365cf0d0e5bfee69930b969f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2021-06-26T14:22:36Z",
      "updated_at": "2021-03-13T03:47:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 126.46966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "6044e560196a671d6f960f72"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations": [
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-06-26T00:13:37Z",
      "updated_at": "2021-06-20T12:42:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.78671,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    },
    {
      "sections": [
        "Polling intervals for Azure integrations",
        "View polling data",
        "New Relic polling intervals"
      ],
      "title": "Polling intervals for Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "82db3eae120c4318365cf0d0e5bfee69930b969f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations/",
      "published_at": "2021-06-26T14:22:36Z",
      "updated_at": "2021-03-13T03:47:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's Azure integrations query your Azure services according to a polling interval specific to the integration. The polling interval applies for every Azure entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances will be polled every five minutes. View polling data After you activate an Azure integration, New Relic starts polling data from Azure and makes the data accessible through infrastructure Inventory and New Relic dashboards. You can query the Azure data along with additional data imported from any other New Relic features. You can also view dashboard data for a specific integration or across your account. For visualizations of polling intervals, API calls, and other data for your Azure integrations: Go to one.newrelic.com > Infrastructure > Azure. To view data for a specific integration: Select the Dashboards link for the integration's row. New Relic polling intervals For polling and resolution details, see the documentation for a specific integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 126.46966,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "sections": "Polling intervals for <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic&#x27;s <em>Azure</em> <em>integrations</em> query your <em>Azure</em> services according to a polling interval specific to the integration. The polling interval applies for every <em>Azure</em> entity related to the integrated service. For example, if you have thirteen CosmosDB instances, each of the thirteen instances"
      },
      "id": "6044e560196a671d6f960f72"
    },
    {
      "sections": [
        "Azure integration metrics",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "762631e1209bb9abb60f1ea8b185a6def61735b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2021-06-26T08:50:43Z",
      "updated_at": "2021-03-16T15:55:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 122.444885,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunSuccessLatency runSuccessLatencySeconds <em>Azure</em> Logic"
      },
      "id": "603e8a8928ccbcacc0eba74e"
    }
  ],
  "/docs/integrations/microsoft-azure-integrations/getting-started/polling-intervals-azure-integrations": [
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-06-26T00:13:37Z",
      "updated_at": "2021-06-20T12:42:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.78671,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate <em>Azure</em> <em>integrations</em>",
        "sections": "Activate <em>Azure</em> <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The <em>Microsoft</em> <em>Azure</em> <em>integrations</em> report data from various <em>Azure</em> platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    },
    {
      "sections": [
        "Introduction to Azure monitoring integrations",
        "Tip",
        "Requirements",
        "Features"
      ],
      "title": "Introduction to Azure monitoring integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f99e6127548c87b6d54587ee8fba6f03ef3fdf2e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/introduction-azure-monitoring-integrations/",
      "published_at": "2021-06-26T14:22:36Z",
      "updated_at": "2021-03-13T03:33:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft Azure integrations allow you to monitor and report data about your Azure services to New Relic, providing a comprehensive view of your entire architecture in one place. The Azure integrations are not the same as New Relic APM's .NET support for Azure. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements Check the Azure integrations documentation for requirements on individual integrations. New Relic cannot obtain monitoring data from resources that are located in Azure Government or that were created through the classic deployment model. Features After you activate your Azure integration, New Relic begins to query your Azure platform services according to a regular polling interval. You can use our integrations UI to: View performance data from Integrations dashboards that automatically scale as you make changes to your ecosystem. Manage alert conditions with alerts. Query your data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 130.88544,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "sections": "Introduction to <em>Azure</em> monitoring <em>integrations</em>",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": "Our <em>Microsoft</em> <em>Azure</em> <em>integrations</em> allow you to monitor and report data about your <em>Azure</em> services to New Relic, providing a comprehensive view of your entire architecture in one place. The <em>Azure</em> <em>integrations</em> are not the same as New Relic APM&#x27;s .NET support for <em>Azure</em>. Tip To use <em>Azure</em> <em>integrations</em>"
      },
      "id": "6044e562e7b9d2e5c15799f8"
    },
    {
      "sections": [
        "Azure integration metrics",
        "Azure Metrics"
      ],
      "title": "Azure integration metrics",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "762631e1209bb9abb60f1ea8b185a6def61735b9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/azure-integration-metrics/",
      "published_at": "2021-06-26T08:50:43Z",
      "updated_at": "2021-03-16T15:55:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Azure Metrics The following table contains the metrics we collect for Azure. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Azure API Management azure.apimanagement.service.Capacity capacityPercent Azure API Management azure.apimanagement.service.Duration durationMilliseconds Azure API Management azure.apimanagement.service.EventHubDroppedEvents eventHubDroppedEvents Azure API Management azure.apimanagement.service.EventHubRejectedEvents eventHubRejectedEvents Azure API Management azure.apimanagement.service.EventHubSuccessfulEvents eventHubSuccessfulEvents Azure API Management azure.apimanagement.service.EventHubThrottledEvents eventHubThrottledEvents Azure API Management azure.apimanagement.service.EventHubTimedoutEvents eventHubTimedoutEvents Azure API Management azure.apimanagement.service.EventHubTotalBytesSent eventHubTotalBytesSentBytes Azure API Management azure.apimanagement.service.EventHubTotalEvents eventHubTotalEvents Azure API Management azure.apimanagement.service.EventHubTotalFailedEvents eventHubTotalFailedEvents Azure API Management azure.apimanagement.service.FailedRequests failedRequests Azure API Management azure.apimanagement.service.OtherRequests otherRequests Azure API Management azure.apimanagement.service.SuccessfulRequests successfulRequests Azure API Management azure.apimanagement.service.TotalRequests totalRequests Azure API Management azure.apimanagement.service.UnauthorizedRequests unauthorizedRequests Azure App Gateway azure.network.applicationgateways.ApplicationGatewayTotalTime applicationGatewayTotalTimeMilliseconds Azure App Gateway azure.network.applicationgateways.AvgRequestCountPerHealthyHost avgRequestCountPerHealthyHost Azure App Gateway azure.network.applicationgateways.BackendConnectTime backendConnectTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendFirstByteResponseTime backendFirstByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendLastByteResponseTime backendLastByteResponseTimeMilliseconds Azure App Gateway azure.network.applicationgateways.BackendResponseStatus backendResponseStatus Azure App Gateway azure.network.applicationgateways.BlockedCount blockedCount Azure App Gateway azure.network.applicationgateways.BlockedReqCount blockedReqCount Azure App Gateway azure.network.applicationgateways.BytesReceived bytesReceivedBytes Azure App Gateway azure.network.applicationgateways.BytesSent bytesSentBytes Azure App Gateway azure.network.applicationgateways.CapacityUnits capacityUnits Azure App Gateway azure.network.applicationgateways.ClientRtt clientRttMilliseconds Azure App Gateway azure.network.applicationgateways.ComputeUnits computeUnits Azure App Gateway azure.network.applicationgateways.CpuUtilization cpuUtilizationPercent Azure App Gateway azure.network.applicationgateways.CurrentConnections currentConnections Azure App Gateway azure.network.applicationgateways.EstimatedBilledCapacityUnits estimatedBilledCapacityUnits Azure App Gateway azure.network.applicationgateways.FailedRequests failedRequests Azure App Gateway azure.network.applicationgateways.FixedBillableCapacityUnits fixedBillableCapacityUnits Azure App Gateway azure.network.applicationgateways.HealthyHostCount healthyHostCount Azure App Gateway azure.network.applicationgateways.MatchedCount matchedCount Azure App Gateway azure.network.applicationgateways.NewConnectionsPerSecond newConnectionsPerSecondCountPerSecond Azure App Gateway azure.network.applicationgateways.ResponseStatus responseStatus Azure App Gateway azure.network.applicationgateways.Throughput throughputBytesPerSecond Azure App Gateway azure.network.applicationgateways.TlsProtocol tlsProtocol Azure App Gateway azure.network.applicationgateways.TotalRequests totalRequests Azure App Gateway azure.network.applicationgateways.UnhealthyHostCount unhealthyHostCount Azure App Service azure.web.serverfarms.BytesReceived bytesReceivedBytes Azure App Service azure.web.serverfarms.BytesSent bytesSentBytes Azure App Service azure.web.serverfarms.CpuPercentage cpuPercent Azure App Service azure.web.serverfarms.DiskQueueLength diskQueueLength Azure App Service azure.web.serverfarms.HttpQueueLength httpQueueLength Azure App Service azure.web.serverfarms.MemoryPercentage memoryPercent Azure App Service azure.web.sites.AppConnections appConnections Azure App Service azure.web.sites.AverageMemoryWorkingSet.byWebApp averageMemoryWorkingSetBytes Azure App Service azure.web.sites.AverageResponseTime averageResponseTimeSeconds Azure App Service azure.web.sites.BytesReceived.byWebApp receivedBytes Azure App Service azure.web.sites.BytesSent.byWebApp sentBytes Azure App Service azure.web.sites.CpuTime cpuTimeSeconds Azure App Service azure.web.sites.CurrentAssemblies currentAssemblies Azure App Service azure.web.sites.Gen0Collections gen0Collections Azure App Service azure.web.sites.Gen1Collections gen1Collections Azure App Service azure.web.sites.Gen2Collections gen2Collections Azure App Service azure.web.sites.Handles handles Azure App Service azure.web.sites.Http101 http101 Azure App Service azure.web.sites.Http2xx http2xx Azure App Service azure.web.sites.Http3xx http3xx Azure App Service azure.web.sites.Http401 http401 Azure App Service azure.web.sites.Http403 http403 Azure App Service azure.web.sites.Http404 http404 Azure App Service azure.web.sites.Http406 http406 Azure App Service azure.web.sites.Http4xx http4xx Azure App Service azure.web.sites.Http5xx.byWebApp http5xx Azure App Service azure.web.sites.MemoryWorkingSet.byWebApp memoryWorkingSetBytes Azure App Service azure.web.sites.Requests requests Azure App Service azure.web.sites.Threads threads Azure App Service azure.web.sites.TotalAppDomains totalAppDomains Azure App Service azure.web.sites.TotalAppDomainsUnloaded totalAppDomainsUnloaded Azure Containers azure.containerinstance.containergroups.CpuUsage cpuUsage Azure Containers azure.containerinstance.containergroups.MemoryUsage memoryUsageBytes Azure Containers azure.containerinstance.containergroups.NetworkBytesReceivedPerSecond networkReceivedBytesPerSecond Azure Containers azure.containerinstance.containergroups.NetworkBytesTransmittedPerSecond networkTransmittedBytesPerSecond Azure Containers azure.containerregistry.registries.RunDuration runDurationMilliseconds Azure Containers azure.containerregistry.registries.SuccessfulPullCount successfulPullCount Azure Containers azure.containerregistry.registries.SuccessfulPushCount successfulPushCount Azure Containers azure.containerregistry.registries.TotalPullCount totalPullCount Azure Containers azure.containerregistry.registries.TotalPushCount totalPushCount Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_cpu_cores kubeNodeStatusAllocatableCpuCores Azure Containers azure.containerservice.managedclusters.kube_node_status_allocatable_memory_bytes kubeNodeStatusAllocatableMemoryBytes Azure Containers azure.containerservice.managedclusters.kube_node_status_condition kubeNodeStatusCondition Azure Containers azure.containerservice.managedclusters.kube_pod_status_phase kubePodStatusPhase Azure Containers azure.containerservice.managedclusters.kube_pod_status_ready kubePodStatusReady Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byAccount availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byAccount cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byAccount cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byAccount cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byAccount dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byAccount documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byAccount documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byAccount indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byAccount metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byAccount mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byAccount mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byAccount provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byAccount replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byAccount serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byAccount totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byAccount totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byCollection availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byCollection cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byCollection cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byCollection cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byCollection dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byCollection documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byCollection documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byCollection indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byCollection metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byCollection mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byCollection mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byCollection provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byCollection replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byCollection serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byCollection totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byCollection totalRequestUnits Azure Cosmos DB azure.documentdb.databaseaccounts.AvailableStorage.byDatabase availableStorageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraConnectionClosures.byDatabase cassandraConnectionClosures Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequestCharges.byDatabase cassandraRequestCharges Azure Cosmos DB azure.documentdb.databaseaccounts.CassandraRequests.byDatabase cassandraRequests Azure Cosmos DB azure.documentdb.databaseaccounts.DataUsage.byDatabase dataUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentCount.byDatabase documentCount Azure Cosmos DB azure.documentdb.databaseaccounts.DocumentQuota.byDatabase documentQuotaBytes Azure Cosmos DB azure.documentdb.databaseaccounts.IndexUsage.byDatabase indexUsageBytes Azure Cosmos DB azure.documentdb.databaseaccounts.MetadataRequests.byDatabase metadataRequests Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequestCharge.byDatabase mongoRequestCharge Azure Cosmos DB azure.documentdb.databaseaccounts.MongoRequests.byDatabase mongoRequests Azure Cosmos DB azure.documentdb.databaseaccounts.ProvisionedThroughput.byDatabase provisionedThroughput Azure Cosmos DB azure.documentdb.databaseaccounts.ReplicationLatency.byDatabase replicationLatencyMilliseconds Azure Cosmos DB azure.documentdb.databaseaccounts.ServiceAvailability.byDatabase serviceAvailabilityPercent Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequests.byDatabase totalRequests Azure Cosmos DB azure.documentdb.databaseaccounts.TotalRequestUnits.byDatabase totalRequestUnits Azure Cost Management azure.costmanagement.cost.byLocation cost Azure Cost Management azure.costmanagement.cost.byResourceGroup cost Azure Cost Management azure.costmanagement.cost.byService cost Azure Cost Management azure.costmanagement.cost.byTag cost Azure Data Factory azure.datafactory.datafactories.FailedRuns failedRuns Azure Data Factory azure.datafactory.datafactories.SuccessfulRuns successfulRuns Azure Data Factory azure.datafactory.factories.ActivityCancelledRuns activityCancelledRuns Azure Data Factory azure.datafactory.factories.ActivityFailedRuns activityFailedRuns Azure Data Factory azure.datafactory.factories.ActivitySucceededRuns activitySucceededRuns Azure Data Factory azure.datafactory.factories.FactorySizeInGbUnits factorySizeInGbUnits Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableMemory integrationRuntimeAvailableMemoryBytes Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAvailableNodeNumber integrationRuntimeAvailableNodeNumber Azure Data Factory azure.datafactory.factories.IntegrationRuntimeAverageTaskPickupDelay integrationRuntimeAverageTaskPickupDelaySeconds Azure Data Factory azure.datafactory.factories.IntegrationRuntimeCpuPercentage integrationRuntimeCpuPercentagePercent Azure Data Factory azure.datafactory.factories.IntegrationRuntimeQueueLength integrationRuntimeQueueLength Azure Data Factory azure.datafactory.factories.MaxAllowedFactorySizeInGbUnits maxAllowedFactorySizeInGbUnits Azure Data Factory azure.datafactory.factories.MaxAllowedResourceCount maxAllowedResourceCount Azure Data Factory azure.datafactory.factories.PipelineCancelledRuns pipelineCancelledRuns Azure Data Factory azure.datafactory.factories.PipelineFailedRuns pipelineFailedRuns Azure Data Factory azure.datafactory.factories.PipelineSucceededRuns pipelineSucceededRuns Azure Data Factory azure.datafactory.factories.ResourceCount resourceCount Azure Data Factory azure.datafactory.factories.TriggerCancelledRuns triggerCancelledRuns Azure Data Factory azure.datafactory.factories.TriggerFailedRuns triggerFailedRuns Azure Data Factory azure.datafactory.factories.TriggerSucceededRuns triggerSucceededRuns Azure Database for MariaDB azure.dbformariadb.servers.active_connections activeConnections Azure Database for MariaDB azure.dbformariadb.servers.backup_storage_used backupStorageUsedBytes Azure Database for MariaDB azure.dbformariadb.servers.connections_failed connectionsFailed Azure Database for MariaDB azure.dbformariadb.servers.cpu_percent cpuPercent Azure Database for MariaDB azure.dbformariadb.servers.io_consumption_percent ioConsumptionPercent Azure Database for MariaDB azure.dbformariadb.servers.memory_percent memoryPercent Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_egress networkEgressBytes Azure Database for MariaDB azure.dbformariadb.servers.network_bytes_ingress networkIngressBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MariaDB azure.dbformariadb.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_limit storageLimitBytes Azure Database for MariaDB azure.dbformariadb.servers.storage_percent storagePercent Azure Database for MariaDB azure.dbformariadb.servers.storage_used storageUsedBytes Azure Database for MySQL azure.dbformysql.servers.active_connections activeConnections Azure Database for MySQL azure.dbformysql.servers.backup_storage_used backupStorageUsedBytes Azure Database for MySQL azure.dbformysql.servers.connections_failed connectionsFailed Azure Database for MySQL azure.dbformysql.servers.cpu_percent cpuPercent Azure Database for MySQL azure.dbformysql.servers.io_consumption_percent ioConsumptionPercent Azure Database for MySQL azure.dbformysql.servers.memory_percent memoryPercent Azure Database for MySQL azure.dbformysql.servers.network_bytes_egress networkEgressBytes Azure Database for MySQL azure.dbformysql.servers.network_bytes_ingress networkIngressBytes Azure Database for MySQL azure.dbformysql.servers.seconds_behind_master secondsBehindMaster Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for MySQL azure.dbformysql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for MySQL azure.dbformysql.servers.storage_limit storageLimitBytes Azure Database for MySQL azure.dbformysql.servers.storage_percent storagePercent Azure Database for MySQL azure.dbformysql.servers.storage_used storageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.active_connections activeConnections Azure Database for PostgreSQL azure.dbforpostgresql.servers.backup_storage_used backupStorageUsedBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.connections_failed connectionsFailed Azure Database for PostgreSQL azure.dbforpostgresql.servers.cpu_percent cpuPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.io_consumption_percent ioConsumptionPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.memory_percent memoryPercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_egress networkEgressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.network_bytes_ingress networkIngressBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_bytes pgReplicaLogDelayBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.pg_replica_log_delay_in_seconds pgReplicaLogDelaySeconds Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_limit serverlogStorageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_percent serverlogStoragePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.serverlog_storage_usage serverlogStorageUsageBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_limit storageLimitBytes Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_percent storagePercent Azure Database for PostgreSQL azure.dbforpostgresql.servers.storage_used storageUsedBytes Azure Event Hub azure.eventhub.namespaces.ActiveConnections activeConnections Azure Event Hub azure.eventhub.namespaces.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.namespaces.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.namespaces.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.namespaces.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.namespaces.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.namespaces.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.namespaces.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.namespaces.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.namespaces.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.namespaces.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.namespaces.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.namespaces.ServerErrors serverErrors Azure Event Hub azure.eventhub.namespaces.Size sizeBytes Azure Event Hub azure.eventhub.namespaces.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.namespaces.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.namespaces.UserErrors userErrors Azure Event Hub azure.eventhub.clusters.ActiveConnections activeConnections Azure Event Hub azure.eventhub.clusters.AvailableMemory availableMemoryPercent Azure Event Hub azure.eventhub.clusters.CaptureBacklog captureBacklog Azure Event Hub azure.eventhub.clusters.CapturedBytes capturedBytes Azure Event Hub azure.eventhub.clusters.CapturedMessages capturedMessages Azure Event Hub azure.eventhub.clusters.ConnectionsClosed connectionsClosed Azure Event Hub azure.eventhub.clusters.ConnectionsOpened connectionsOpened Azure Event Hub azure.eventhub.clusters.CPU cpuPercent Azure Event Hub azure.eventhub.clusters.IncomingBytes incomingBytes Azure Event Hub azure.eventhub.clusters.IncomingMessages incomingMessages Azure Event Hub azure.eventhub.clusters.IncomingRequests incomingRequests Azure Event Hub azure.eventhub.clusters.OutgoingBytes outgoingBytes Azure Event Hub azure.eventhub.clusters.OutgoingMessages outgoingMessages Azure Event Hub azure.eventhub.clusters.QuotaExceededErrors quotaExceededErrors Azure Event Hub azure.eventhub.clusters.ServerErrors serverErrors Azure Event Hub azure.eventhub.clusters.Size sizeBytes Azure Event Hub azure.eventhub.clusters.SuccessfulRequests successfulRequests Azure Event Hub azure.eventhub.clusters.ThrottledRequests throttledRequests Azure Event Hub azure.eventhub.clusters.UserErrors userErrors Azure Express Route azure.network.expressrouteports.AdminState adminState Azure Express Route azure.network.expressrouteports.LineProtocol lineProtocol Azure Express Route azure.network.expressrouteports.PortBitsInPerSecond portBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.PortBitsOutPerSecond portBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressrouteports.RxLightLevel rxLightLevel Azure Express Route azure.network.expressrouteports.TxLightLevel txLightLevel Azure Express Route azure.network.expressroutecircuits.ArpAvailability arpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BgpAvailability bgpAvailabilityPercent Azure Express Route azure.network.expressroutecircuits.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsInPerSecond globalReachBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.GlobalReachBitsOutPerSecond globalReachBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsInPerSecond qosDropBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.QosDropBitsOutPerSecond qosDropBitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutecircuits.peerings.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsInPerSecond bitsInPerSecondCountPerSecond Azure Express Route azure.network.connections.BitsOutPerSecond bitsOutPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsInPerSecond erGatewayConnectionBitsInPerSecondCountPerSecond Azure Express Route azure.network.expressroutegateways.ErGatewayConnectionBitsOutPerSecond erGatewayConnectionBitsOutPerSecondCountPerSecond Azure Firewalls azure.network.azurefirewalls.ApplicationRuleHit applicationRuleHit Azure Firewalls azure.network.azurefirewalls.DataProcessed dataProcessedBytes Azure Firewalls azure.network.azurefirewalls.FirewallHealth firewallHealthPercent Azure Firewalls azure.network.azurefirewalls.NetworkRuleHit networkRuleHit Azure Firewalls azure.network.azurefirewalls.SNATPortUtilization sNATPortUtilizationPercent Azure Firewalls azure.network.azurefirewalls.Throughput throughputBitsPerSecond Azure Front Door azure.network.frontdoors.BackendHealthPercentage backendHealthPercent Azure Front Door azure.network.frontdoors.BackendRequestCount backendRequestCount Azure Front Door azure.network.frontdoors.BackendRequestLatency backendRequestLatencyMilliseconds Azure Front Door azure.network.frontdoors.BillableResponseSize billableResponseSizeBytes Azure Front Door azure.network.frontdoors.RequestCount requestCount Azure Front Door azure.network.frontdoors.RequestSize requestSizeBytes Azure Front Door azure.network.frontdoors.ResponseSize responseSizeBytes Azure Front Door azure.network.frontdoors.TotalLatency totalLatencyMilliseconds Azure Front Door azure.network.frontdoors.WebApplicationFirewallRequestCount webApplicationFirewallRequestCount Azure Functions azure.web.sites.AverageMemoryWorkingSet.byFunctionsApp averageMemoryWorkingSetBytes Azure Functions azure.web.sites.BytesReceived.byFunctionsApp receivedBytes Azure Functions azure.web.sites.BytesSent.byFunctionsApp sentBytes Azure Functions azure.web.sites.FunctionExecutionCount functionExecutionCount Azure Functions azure.web.sites.FunctionExecutionUnits functionExecutionUnits Azure Functions azure.web.sites.Http5xx.byFunctionsApp http5xx Azure Functions azure.web.sites.MemoryWorkingSet.byFunctionsApp memoryWorkingSetBytes Azure Key Vault azure.keyvault.vaults.Availability availabilityPercent Azure Key Vault azure.keyvault.vaults.SaturationShoebox saturationShoeboxPercent Azure Key Vault azure.keyvault.vaults.ServiceApiHit serviceApiHit Azure Key Vault azure.keyvault.vaults.ServiceApiLatency serviceApiLatencyMilliseconds Azure Key Vault azure.keyvault.vaults.ServiceApiResult serviceApiResult Azure Load Balancer azure.network.loadbalancers.AllocatedSnatPorts allocatedSnatPorts Azure Load Balancer azure.network.loadbalancers.ByteCount byteCountBytes Azure Load Balancer azure.network.loadbalancers.DipAvailability dipAvailability Azure Load Balancer azure.network.loadbalancers.PacketCount packetCount Azure Load Balancer azure.network.loadbalancers.SnatConnectionCount snatConnectionCount Azure Load Balancer azure.network.loadbalancers.SYNCount synCount Azure Load Balancer azure.network.loadbalancers.UsedSnatPorts usedSnatPorts Azure Load Balancer azure.network.loadbalancers.VipAvailability vipAvailability Azure Logic Apps azure.logic.workflows.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.workflows.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.workflows.ActionsFailed actionsFailed Azure Logic Apps azure.logic.workflows.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.workflows.ActionsStarted actionsStarted Azure Logic Apps azure.logic.workflows.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.workflows.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.workflows.BillableActionExecutions billableActionExecutions Azure Logic Apps azure.logic.workflows.BillableTriggerExecutions billableTriggerExecutions Azure Logic Apps azure.logic.workflows.BillingUsageNativeOperation billingUsageNativeOperation Azure Logic Apps azure.logic.workflows.BillingUsageStandardConnector billingUsageStandardConnector Azure Logic Apps azure.logic.workflows.BillingUsageStorageConsumption billingUsageStorageConsumption Azure Logic Apps azure.logic.workflows.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.workflows.RunLatency runLatencySeconds Azure Logic Apps azure.logic.workflows.RunsCancelled runsCancelled Azure Logic Apps azure.logic.workflows.RunsCompleted runsCompleted Azure Logic Apps azure.logic.workflows.RunsFailed runsFailed Azure Logic Apps azure.logic.workflows.RunsStarted runsStarted Azure Logic Apps azure.logic.workflows.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.workflows.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.workflows.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.workflows.TotalBillableExecutions totalBillableExecutions Azure Logic Apps azure.logic.workflows.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.workflows.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.workflows.TriggersFailed triggersFailed Azure Logic Apps azure.logic.workflows.TriggersFired triggersFired Azure Logic Apps azure.logic.workflows.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.workflows.TriggersStarted triggersStarted Azure Logic Apps azure.logic.workflows.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.workflows.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.workflows.TriggerThrottledEvents triggerThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.ActionLatency actionLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsCompleted actionsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsFailed actionsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSkipped actionsSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsStarted actionsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.ActionsSucceeded actionsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.ActionSuccessLatency actionSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.ActionThrottledEvents actionThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorMemoryUsage integrationServiceEnvironmentConnectorMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentConnectorProcessorUsage integrationServiceEnvironmentConnectorProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowMemoryUsage integrationServiceEnvironmentWorkflowMemoryUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.IntegrationServiceEnvironmentWorkflowProcessorUsage integrationServiceEnvironmentWorkflowProcessorUsagePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunFailurePercentage runFailurePercent Azure Logic Apps azure.logic.integrationserviceenvironments.RunLatency runLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCancelled runsCancelled Azure Logic Apps azure.logic.integrationserviceenvironments.RunsCompleted runsCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsFailed runsFailed Azure Logic Apps azure.logic.integrationserviceenvironments.RunsStarted runsStarted Azure Logic Apps azure.logic.integrationserviceenvironments.RunsSucceeded runsSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.RunStartThrottledEvents runStartThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.RunSuccessLatency runSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.RunThrottledEvents runThrottledEvents Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerFireLatency triggerFireLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerLatency triggerLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersCompleted triggersCompleted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFailed triggersFailed Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersFired triggersFired Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSkipped triggersSkipped Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersStarted triggersStarted Azure Logic Apps azure.logic.integrationserviceenvironments.TriggersSucceeded triggersSucceeded Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerSuccessLatency triggerSuccessLatencySeconds Azure Logic Apps azure.logic.integrationserviceenvironments.TriggerThrottledEvents triggerThrottledEvents Azure Machine Learning azure.machinelearningservices.workspaces.ActiveCores activeCores Azure Machine Learning azure.machinelearningservices.workspaces.ActiveNodes activeNodes Azure Machine Learning azure.machinelearningservices.workspaces.CompletedRuns completedRuns Azure Machine Learning azure.machinelearningservices.workspaces.CpuUtilization cpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.FailedRuns failedRuns Azure Machine Learning azure.machinelearningservices.workspaces.GpuUtilization gpuUtilization Azure Machine Learning azure.machinelearningservices.workspaces.IdleCores idleCores Azure Machine Learning azure.machinelearningservices.workspaces.IdleNodes idleNodes Azure Machine Learning azure.machinelearningservices.workspaces.LeavingCores leavingCores Azure Machine Learning azure.machinelearningservices.workspaces.LeavingNodes leavingNodes Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployFailed modelDeployFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeployStarted modelDeployStarted Azure Machine Learning azure.machinelearningservices.workspaces.ModelDeploySucceeded modelDeploySucceeded Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterFailed modelRegisterFailed Azure Machine Learning azure.machinelearningservices.workspaces.ModelRegisterSucceeded modelRegisterSucceeded Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedCores preemptedCores Azure Machine Learning azure.machinelearningservices.workspaces.PreemptedNodes preemptedNodes Azure Machine Learning azure.machinelearningservices.workspaces.QuotaUtilizationPercentage quotaUtilizationPercentage Azure Machine Learning azure.machinelearningservices.workspaces.StartedRuns startedRuns Azure Machine Learning azure.machinelearningservices.workspaces.TotalCores totalCores Azure Machine Learning azure.machinelearningservices.workspaces.TotalNodes totalNodes Azure Machine Learning azure.machinelearningservices.workspaces.UnusableCores unusableCores Azure Machine Learning azure.machinelearningservices.workspaces.UnusableNodes unusableNodes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_metric memoryMetricBytes Azure Power BI Dedicated azure.powerbidedicated.capacities.memory_thrashing_metric memoryThrashingMetricPercent Azure Power BI Dedicated azure.powerbidedicated.capacities.qpu_high_utilization_metric qpuHighUtilizationMetric Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryDuration queryDurationMilliseconds Azure Power BI Dedicated azure.powerbidedicated.capacities.QueryPoolJobQueueLength queryPoolJobQueueLength Azure Redis azure.cache.redis.cachehits cacheHits Azure Redis azure.cache.redis.cachemisses cacheMisses Azure Redis azure.cache.redis.cacheRead cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients connectedClients Azure Redis azure.cache.redis.evictedkeys evictedKeys Azure Redis azure.cache.redis.expiredkeys expiredKeys Azure Redis azure.cache.redis.getcommands getCommands Azure Redis azure.cache.redis.operationsPerSecond operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime processorTimePercent Azure Redis azure.cache.redis.serverLoad serverLoadPercent Azure Redis azure.cache.redis.setcommands setCommands Azure Redis azure.cache.redis.totalcommandsprocessed totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys totalKeys Azure Redis azure.cache.redis.usedmemory usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss usedMemoryRssBytes Azure Redis azure.cache.redis.cachehits0 cacheHits Azure Redis azure.cache.redis.cachemisses0 cacheMisses Azure Redis azure.cache.redis.cacheRead0 cacheReadBytesPerSecond Azure Redis azure.cache.redis.cacheWrite0 cacheWriteBytesPerSecond Azure Redis azure.cache.redis.connectedclients0 connectedClients Azure Redis azure.cache.redis.evictedkeys0 evictedKeys Azure Redis azure.cache.redis.expiredkeys0 expiredKeys Azure Redis azure.cache.redis.getcommands0 getCommands Azure Redis azure.cache.redis.operationsPerSecond0 operationsPerSecond Azure Redis azure.cache.redis.percentProcessorTime0 processorTimePercent Azure Redis azure.cache.redis.serverLoad0 serverLoadPercent Azure Redis azure.cache.redis.setcommands0 setCommands Azure Redis azure.cache.redis.totalcommandsprocessed0 totalCommandsProcessed Azure Redis azure.cache.redis.totalkeys0 totalKeys Azure Redis azure.cache.redis.usedmemory0 usedMemoryBytes Azure Redis azure.cache.redis.usedmemoryRss0 usedMemoryRssBytes Azure Service Bus azure.servicebus.namespaces.ActiveConnections activeConnections Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byNamespace activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byNamespace connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byNamespace connectionsOpened Azure Service Bus azure.servicebus.namespaces.CPUXNS cpuUsagePercent Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byNamespace deadletteredMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byNamespace incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byNamespace incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byNamespace messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byNamespace outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byNamespace scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byNamespace serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byNamespace sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byNamespace successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byNamespace throttledRequests Azure Service Bus azure.servicebus.namespaces.UserErrors.byNamespace userErrors Azure Service Bus azure.servicebus.namespaces.WSXNS memoryUsagePercent Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byQueue activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byQueue connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byQueue connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byQueue currentSizeBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byQueue deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byQueue deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byQueue incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byQueue incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byQueue messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byQueue outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byQueue scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byQueue serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byQueue sizeBytes Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byQueue successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byQueue throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byQueue transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byQueue transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byQueue userErrors Azure Service Bus azure.servicebus.namespaces.activeMessageCount activeMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.bySubscription deadLetterMessages Azure Service Bus azure.servicebus.namespaces.messageCount messages Azure Service Bus azure.servicebus.namespaces.scheduledMessageCount scheduledMessages Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.bySubscription transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.bySubscription transferMessages Azure Service Bus azure.servicebus.namespaces.ActiveMessages.byTopic activeMessages Azure Service Bus azure.servicebus.namespaces.ConnectionsClosed.byTopic connectionsClosed Azure Service Bus azure.servicebus.namespaces.ConnectionsOpened.byTopic connectionsOpened Azure Service Bus azure.servicebus.namespaces.currentSizeInBytes.byTopic currentSizeInBytes Azure Service Bus azure.servicebus.namespaces.DeadletteredMessages.byTopic deadletteredMessages Azure Service Bus azure.servicebus.namespaces.deadLetterMessageCount.byTopic deadLetterMessages Azure Service Bus azure.servicebus.namespaces.IncomingMessages.byTopic incomingMessages Azure Service Bus azure.servicebus.namespaces.IncomingRequests.byTopic incomingRequests Azure Service Bus azure.servicebus.namespaces.Messages.byTopic messages Azure Service Bus azure.servicebus.namespaces.OutgoingMessages.byTopic outgoingMessages Azure Service Bus azure.servicebus.namespaces.ScheduledMessages.byTopic scheduledMessages Azure Service Bus azure.servicebus.namespaces.ServerErrors.byTopic serverErrors Azure Service Bus azure.servicebus.namespaces.Size.byTopic sizeBytes Azure Service Bus azure.servicebus.namespaces.subscriptionCount subscriptions Azure Service Bus azure.servicebus.namespaces.SuccessfulRequests.byTopic successfulRequests Azure Service Bus azure.servicebus.namespaces.ThrottledRequests.byTopic throttledRequests Azure Service Bus azure.servicebus.namespaces.transferDeadLetterMessageCount.byTopic transferDeadLetterMessages Azure Service Bus azure.servicebus.namespaces.transferMessageCount.byTopic transferMessages Azure Service Bus azure.servicebus.namespaces.UserErrors.byTopic userErrors Azure Service Fabric azure.servicefabricmesh.applications.ActualCpu actualCpu Azure Service Fabric azure.servicefabricmesh.applications.ActualMemory actualMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.AllocatedCpu allocatedCpu Azure Service Fabric azure.servicefabricmesh.applications.AllocatedMemory allocatedMemoryBytes Azure Service Fabric azure.servicefabricmesh.applications.ApplicationStatus applicationStatus Azure Service Fabric azure.servicefabricmesh.applications.ContainerStatus containerStatus Azure Service Fabric azure.servicefabricmesh.applications.CpuUtilization cpuUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.MemoryUtilization memoryUtilizationPercent Azure Service Fabric azure.servicefabricmesh.applications.RestartCount restartCount Azure Service Fabric azure.servicefabricmesh.applications.ServiceReplicaStatus serviceReplicaStatus Azure Service Fabric azure.servicefabricmesh.applications.ServiceStatus serviceStatus Azure SQL azure.sql.servers.database.currentSize databaseSizeCurrentBytes Azure SQL azure.sql.servers.database.limitSize databaseSizeLimitBytes Azure SQL azure.sql.servers.databases.blocked_by_firewall blockedByFirewall Azure SQL azure.sql.servers.databases.connection_failed connectionFailed Azure SQL azure.sql.servers.databases.connection_successful connectionSuccessful Azure SQL azure.sql.servers.databases.cpu_percent cpuPercent Azure SQL azure.sql.servers.databases.deadlock deadlock Azure SQL azure.sql.servers.databases.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.databases.dtu_limit dtuLimit Azure SQL azure.sql.servers.databases.dtu_used dtuUsed Azure SQL azure.sql.servers.databases.dw_cpu_percent dwCpuPercent Azure SQL azure.sql.servers.databases.dw_physical_data_read_percent dwPhysicalDataReadPercent Azure SQL azure.sql.servers.databases.dwu_consumption_percent dwuConsumptionPercent Azure SQL azure.sql.servers.databases.dwu_limit dwuLimit Azure SQL azure.sql.servers.databases.dwu_used dwuUsed Azure SQL azure.sql.servers.databases.log_write_percent logWritePercent Azure SQL azure.sql.servers.databases.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.databases.sessions_percent sessionsPercent Azure SQL azure.sql.servers.databases.storage storageBytes Azure SQL azure.sql.servers.databases.storage_percent storagePercent Azure SQL azure.sql.servers.databases.workers_percent workersPercent Azure SQL azure.sql.servers.databases.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.elasticPool.database_physical_data_read_percent databasePhysicalDataRead Azure SQL azure.sql.elasticPool.database_storage_used databaseStorageUsed Azure SQL azure.sql.servers.elasticpools.cpu_percent cpuPercent Azure SQL azure.sql.servers.elasticpools.database_cpu_percent databaseCpuPercent Azure SQL azure.sql.servers.elasticpools.database_dtu_consumption_percent databaseDtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.database_log_write_percent databaseLogWritePercent Azure SQL azure.sql.servers.elasticpools.database_sessions_percent databaseSessionsPercent Azure SQL azure.sql.servers.elasticpools.database_workers_percent databaseWorkersPercent Azure SQL azure.sql.servers.elasticpools.dtu_consumption_percent dtuConsumptionPercent Azure SQL azure.sql.servers.elasticpools.eDTU_limit eDTULimit Azure SQL azure.sql.servers.elasticpools.eDTU_used eDTUUsed Azure SQL azure.sql.servers.elasticpools.log_write_percent logWritePercent Azure SQL azure.sql.servers.elasticpools.physical_data_read_percent physicalDataReadPercent Azure SQL azure.sql.servers.elasticpools.sessions_percent sessionsPercent Azure SQL azure.sql.servers.elasticpools.storage_limit storageLimitBytes Azure SQL azure.sql.servers.elasticpools.storage_percent storagePercent Azure SQL azure.sql.servers.elasticpools.storage_used storageUsedBytes Azure SQL azure.sql.servers.elasticpools.workers_percent workersPercent Azure SQL azure.sql.servers.elasticpools.xtp_storage_percent xtpStoragePercent Azure SQL azure.sql.server.dtuLimit dtuLimit Azure SQL azure.sql.servers.dtuCurrent dtuCurrent Azure SQL Managed Instance azure.sql.managedinstances.avg_cpu_percent avgCpuPercent Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_read ioReadBytes Azure SQL Managed Instance azure.sql.managedinstances.io_bytes_written ioWrittenBytes Azure SQL Managed Instance azure.sql.managedinstances.io_requests ioRequests Azure SQL Managed Instance azure.sql.managedinstances.reserved_storage_mb reservedStorage Azure SQL Managed Instance azure.sql.managedinstances.storage_space_used_mb storageSpaceUsed Azure SQL Managed Instance azure.sql.managedinstances.virtual_core_count virtualCore Azure Storage Account azure.storage.storageaccounts.Availability availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.Availability blobs.availabilityPercent Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCapacity blobs.blobCapacityBytes Azure Storage Account azure.storage.storageaccounts.blobservices.BlobCount blobs.blobCount Azure Storage Account azure.storage.storageaccounts.blobservices.ContainerCount blobs.containerCount Azure Storage Account azure.storage.storageaccounts.blobservices.Egress blobs.egressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.Ingress blobs.ingressBytes Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessE2ELatency blobs.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.SuccessServerLatency blobs.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.blobservices.Transactions blobs.transactions Azure Storage Account azure.storage.storageaccounts.Egress egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.Availability files.availabilityPercent Azure Storage Account azure.storage.storageaccounts.fileservices.Egress files.egressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCapacity files.fileCapacityBytes Azure Storage Account azure.storage.storageaccounts.fileservices.FileCount files.fileCount Azure Storage Account azure.storage.storageaccounts.fileservices.FileShareCount files.fileShareCount Azure Storage Account azure.storage.storageaccounts.fileservices.Ingress files.ingressBytes Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessE2ELatency files.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.SuccessServerLatency files.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.fileservices.Transactions files.transactions Azure Storage Account azure.storage.storageaccounts.Ingress ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Availability queues.availabilityPercent Azure Storage Account azure.storage.storageaccounts.queueservices.Egress queues.egressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.Ingress queues.ingressBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCapacity queues.queueCapacityBytes Azure Storage Account azure.storage.storageaccounts.queueservices.QueueCount queues.queueCount Azure Storage Account azure.storage.storageaccounts.queueservices.QueueMessageCount queues.queueMessagesCount Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessE2ELatency queues.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.SuccessServerLatency queues.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.queueservices.Transactions queues.transactions Azure Storage Account azure.storage.storageaccounts.SuccessE2ELatency successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.SuccessServerLatency successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.Availability tables.availabilityPercent Azure Storage Account azure.storage.storageaccounts.tableservices.Egress tables.egressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.Ingress tables.ingressBytes Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessE2ELatency tables.successE2ELatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.SuccessServerLatency tables.successServerLatencyMilliseconds Azure Storage Account azure.storage.storageaccounts.tableservices.TableCapacity tables.tableCapacityBytes Azure Storage Account azure.storage.storageaccounts.tableservices.TableCount tables.tableCount Azure Storage Account azure.storage.storageaccounts.tableservices.TableEntityCount tables.tableEntityCount Azure Storage Account azure.storage.storageaccounts.tableservices.Transactions tables.transactions Azure Storage Account azure.storage.storageaccounts.Transactions transactions Azure Storage Account azure.storage.storageaccounts.UsedCapacity usedCapacityBytes Azure Virtual Network azure.network.virtualnetworks.PingMeshAverageRoundtripMs pingMeshAverageRoundtripMs Azure Virtual Network azure.network.virtualnetworks.PingMeshProbesFailedPercent pingMeshProbesFailedPercent Azure Virtual Network azure.network.publicipaddresses.BytesDroppedDDoS droppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesForwardedDDoS forwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.BytesInDDoS inDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerTCPPackets ddosTriggerTcpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.DDoSTriggerUDPPackets ddosTriggerUdpPacketsPerSecond Azure Virtual Network azure.network.publicipaddresses.IfUnderDDoSAttack ifUnderDdosAttack Azure Virtual Network azure.network.publicipaddresses.PacketsDroppedDDoS packetsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsForwardedDDoS packetsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.PacketsInDDoS packetsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesDroppedDDoS tcpDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesForwardedDDoS tcpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPBytesInDDoS tcpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsDroppedDDoS tcpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsForwardedDDoS tcpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.TCPPacketsInDDoS tcpPacketsInDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesDroppedDDoS udpDroppedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesForwardedDDoS udpForwardedDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPBytesInDDoS udpInDdosBytesPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsDroppedDDoS udpPacketsDroppedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsForwardedDDoS udpPacketsForwardedDdosPerSecond Azure Virtual Network azure.network.publicipaddresses.UDPPacketsInDDoS udpPacketsInDdosPerSecond Azure Virtual Network azure.network.virtualnetworks.availableAddresses availableAddresses Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsConsumed cpuCreditsConsumed Azure VMs Scale Sets azure.compute.virtualmachinescalesets.CPUCreditsRemaining cpuCreditsRemaining Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskQueueDepth dataDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadBytessec dataDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskReadOperationsSec dataDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteBytessec dataDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DataDiskWriteOperationsSec dataDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadBytes diskReadBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskReadOperationsSec diskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteBytes diskWriteBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.DiskWriteOperationsSec diskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlows inboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.InboundFlowsMaximumCreationRate inboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkInTotal networkInTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.NetworkOutTotal networkOutTotalBytes Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskQueueDepth osDiskQueueDepth Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadBytessec osDiskReadBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskReadOperationsSec osDiskReadOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteBytessec osDiskWriteBytesCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OSDiskWriteOperationsSec osDiskWriteOperationsCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlows outboundFlows Azure VMs Scale Sets azure.compute.virtualmachinescalesets.OutboundFlowsMaximumCreationRate outboundFlowsMaximumCreationRateCountPerSecond Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PercentageCPU cpuPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadHit premiumDataDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumDataDiskCacheReadMiss premiumDataDiskCacheReadMissPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadHit premiumOsDiskCacheReadHitPercent Azure VMs Scale Sets azure.compute.virtualmachinescalesets.PremiumOSDiskCacheReadMiss premiumOsDiskCacheReadMissPercent Azure VMs azure.compute.virtualmachines.DiskReadBytes diskReadBytes Azure VMs azure.compute.virtualmachines.DiskReadOperations.Sec diskReadOpsPerSecond Azure VMs azure.compute.virtualmachines.DiskWriteBytes diskWriteBytes Azure VMs azure.compute.virtualmachines.DiskWriteOperations.Sec diskWriteOpsPerSecond Azure VMs azure.compute.virtualmachines.NetworkIn networkInBytes Azure VMs azure.compute.virtualmachines.NetworkOut networkOutBytes Azure VMs azure.compute.virtualmachines.PercentageCPU cpuUsagePercent Azure VPN Gateways azure.network.virtualnetworkgateways.AverageBandwidth averageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SBandwidth p2SBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.P2SConnectionCount p2SConnectionCount Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelAverageBandwidth tunnelAverageBandwidthBytesPerSecond Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressBytes tunnelEgressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPacketDropTSMismatch tunnelEgressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelEgressPackets tunnelEgressPackets Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressBytes tunnelIngressBytes Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPacketDropTSMismatch tunnelIngressPacketDropTSMismatch Azure VPN Gateways azure.network.virtualnetworkgateways.TunnelIngressPackets tunnelIngressPackets",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 122.444885,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Azure</em> <em>integration</em> metrics",
        "sections": "<em>Azure</em> <em>integration</em> metrics",
        "tags": "<em>Microsoft</em> <em>Azure</em> <em>integrations</em>",
        "body": " Apps <em>azure.logic.workflows.RunsStarted</em> runs<em>Started</em> <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunsSucceeded runsSucceeded <em>Azure</em> Logic Apps <em>azure.logic.workflows.RunStart</em>ThrottledEvents run<em>Start</em>ThrottledEvents <em>Azure</em> Logic Apps <em>azure</em>.logic.workflows.RunSuccessLatency runSuccessLatencySeconds <em>Azure</em> Logic"
      },
      "id": "603e8a8928ccbcacc0eba74e"
    }
  ],
  "/docs/integrations/mlops-integrations/algorithmia-mlops-integration": [
    {
      "sections": [
        "Microsoft SQL Server monitoring integration",
        "Compatibility and requirements",
        "Quick start",
        "Microsoft SQL users and privileges",
        "Install and activate",
        "32-bit Windows",
        "64-bit Windows",
        "Configure the integration",
        "Important",
        "Commands",
        "Arguments",
        "Tip",
        "Example configurations",
        "Configuration for one query",
        "Configuration for multiple queries",
        "Find and use data",
        "Metric data",
        "Database metrics",
        "Instance metrics",
        "Wait metrics",
        "Inventory data",
        "Check the source code"
      ],
      "title": "Microsoft SQL Server monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "67275d2c4aca22f026b50f733e6a804a3cb5a111",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/microsoft-sql-server-monitoring-integration/",
      "published_at": "2021-06-30T01:53:06Z",
      "updated_at": "2021-06-30T01:53:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Microsoft SQL Server integration collects and sends inventory and metrics from your MS SQL Server environment to our platform, where you can see the health of your MS SQL Server environment. We collect both database and instance-level metrics so that you can pinpoint the source of any problems. Read on to install the integration, and to see what data we collect. Compatibility and requirements Our integration is compatible with Microsoft SQL Server 2008 R2 SP3 or higher. Before installing the integration, make sure that you meet the following requirements: Install the infrastructure agent. Windows distribution compatible with the infrastructure agent. Microsoft SQL Server user or Domain user with user privileges for both CONNECT and VIEW SERVER STATE, and READ access permissions. Quick start Instrument your MS SQL Server environment quickly and send your telemetry data with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install uses the infrastructure agent to set up the Microsoft SQL Server integration. Not only that, it discovers other applications and log sources running in your environment and then recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your MS SQL Server environment. Microsoft SQL users and privileges In the Microsoft SQL Server that is to be monitored, execute the following script to create a new user and grant CONNECT, VIEW SERVER STATE, and read access permissions to that user. See the Microsoft documentation for details on creating logins and users in Microsoft SQL Server. Use the following statements to create a new login and to grant CONNECT and VIEW SERVER STATE permissions to the login. USE master; CREATE LOGIN newrelic WITH PASSWORD = 'tmp_password'; --insert new password here GRANT CONNECT SQL TO newrelic; GRANT VIEW SERVER STATE TO newrelic; GRANT VIEW ANY DEFINITION TO newrelic; Copy Use the following statements to grant read access privileges to the user. DECLARE @name SYSNAME DECLARE db_cursor CURSOR READ_ONLY FORWARD_ONLY FOR SELECT NAME FROM master.sys.databases WHERE NAME NOT IN ('master','msdb','tempdb','model','rdsadmin','distribution') OPEN db_cursor FETCH NEXT FROM db_cursor INTO @name WHILE @@FETCH_STATUS = 0 BEGIN EXECUTE('USE \"' + @name + '\"; CREATE USER newrelic FOR LOGIN newrelic;' ); FETCH next FROM db_cursor INTO @name END CLOSE db_cursor DEALLOCATE db_cursor Copy Run the following command to verify that the user was successfully created. sqlcmd -U user_name -S host_name Copy Install and activate To install the Microsoft SQL Server integration: Download the latest .MSI installer image from: 32-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/386/nri-mssql-386.msi Copy 64-bit Windows http://download.newrelic.com/infrastructure_agent/windows/integrations/nri-mssql/nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd386.msi Copy 64-bit Windows msiexec.exe /qn /i PATH\\TO\\nri-mssql-amd64.msi Copy Rename C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d\\mssql-config.yml.sample to mssql-config.yml, and edit according to your instance. Restart the infrastructure agent. Additional notes: On-host integrations do not automatically update. For best results, regularly update the integration package and the infrastructure agent. Configure the integration An integration's YAML-format configuration is where you can place required login credentials and configure how data is collected. Which options you change depend on your setup and preference. For an example configuration, see the example config file. Important With secrets management, you can configure on-host integrations with New Relic Infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. Commands The mssql-config.yml file accepts the following commands: all_data: collects both inventory and metric data from the Microsoft SQL Server environment. Arguments The all_data command accepts the following arguments: username: username used to authenticate the MS SQL Server. To use Windows Authentication, specify this argument in the DOMAIN\\username form. This field is required. password: password used to authenticate the MS SQL Server. This field is required. hostname: hostname or IP of the MS SQL Server installation. Default: 127.0.0.1. port: port number on which the MS SQL Server is listening. This is only required when instance is not specified. instance: instance the Microsoft SQL Server is connected to. instance can be used in place of port by enabling SQL Browser; if enabled, do not include port in the argument. enable_ssl: indicates whether SSL is used to connect to the MS SQL Server. Default: false. trust_server_certificate: if set to true, server certificate is not verified for SSL. If set to false, certificate will be verified against supplied certificate. certificate_location: certificate file to verify SSL encryption against. timeout: timeout for queries, in seconds. Default: 30. enable_buffer_metrics: enables the collection of buffer pool metrics. These can be resource intensive for large systems. Default: true. enable_database_reserve_metrics: enables the collection of database partition reserve space. These can be resource intensive for large systems. Default: true. custom_metrics_query: an SQL query with the required columns metric_name, metric_type, and metric_value. The metric_type can be gauge, rate, delta, or attribute. Additional columns collected with the query are added to the metric set as attributes. Custom queries metrics will be attached to the MssqlCustomQuerySample event type. custom_metrics_config: full path and file name for an external queries YAML file. Use this instead of custom_metrics_query if you need execute multiple custom queries. For an example configuration, see the example config file. extra_connection_url_args: appends additional parameters to connection URL. Ex. 'applicationintent=readonly&foo=bar' Tip If both port and instance are omitted, the default port of 1433 is used. Example configurations Example mssql-config.yml file configuration: Configuration for one query integration_name: com.newrelic.mssql instances: - name: mssql-server command: all_data arguments: hostname: mssql.localnet username: mssql_user password: mssql_password port: 1433 custom_metrics_query: >- SELECT 'instance_buffer_pool_size' AS metric_name, Count_big(*) * (8*1024) AS metric_value, 'gauge' as metric_type, database_id FROM sys.dm_os_buffer_descriptors WITH (nolock) GROUP BY database_id labels: env: production role: mssql Copy Configuration for multiple queries If you need multiple SQL queries, add them to mssql-custom-query.yml, and then reference that file. integration_name: com.newrelic.mssql instances: - name: mssql-server command: all_data arguments: hostname: mssql.localnet username: mssql_user password: mssql_password port: 1433 custom_metrics_config: full_path_to_file labels: env: production role: mssql Copy Here's an example mssql-custom-query.yml. queries: # Example for metric_name / metric_type specified in this config - query: SELECT count(*) AS 'metric_value' FROM sys.databases metric_name: dbCount metric_type: gauge # Example for metric_name from query, metric_type auto-detected, additional attribute 'category_type' - query: SELECT CONCAT('category_', category_id) AS metric_name, name AS metric_value, category_type FROM syscategories database: msdb # Example for stored procedure 'exec dbo.sp_server_info @attribute_id = 2' - query: dbo.sp_server_info @attribute_id = 2 Copy For more about the general structure of on-host integration configuration, see Configuration. Find and use data To find your integration data go to one.newrelic.com > Infrastructure > Third-party services and select one of the Microsoft SQL Server integration links. Microsoft SQL Server data is attached to the following event types: MssqlDatabaseSample MssqlInstanceSample MssqlWaitSample MssqlCustomQuerySample (if you running custom queries) For more on how to find and use your data, see Understand integration data. Metric data The Microsoft SQL Server integration collects the following metric data attributes. Some metric name are prefixed with a category indicator and a period, such as asserts. or flush.. Database metrics These attributes can be found by querying the MssqlDatabaseSample event. Metric Description bufferpool.sizePerDatabaseInBytes The size of the buffer pool per database. io.stallInMilliseconds Wait time of stall since last restart, in milliseconds. log.transactionGrowth Total number of times the transaction log for the database has been expanded since the last restart. pageFileAvailable Available page file size, in bytes. pageFileTotal Total page file size, in bytes. Instance metrics The Microsoft SQL Server integration collects the following instance metrics. These attributes can be found by querying the MssqlInstanceSample event. Metric Description access.pageSplitsPerSecond The number of page splits per second. activeConnections The number of active connections. buffer.checkpointPagesPerSecond The number of pages flushed to disk per second by a checkpoint or other operation that require all dirty pages to be flushed. bufferpool.batchRequestsPerSecond The number of batch requests per second on the buffer pool. bufferpool.pageLifeExptancyInMilliseconds The life expectancy of a page in the buffer pool, in milliseconds. bufferpool.sizeInBytes The size of the buffer pool, in bytes. instance.backgroundProcessesCount The number of background processes on the instance. instance.blockedProcessesCount The number of blocked processes on the instance. instance.diskInBytes The amount of disk space on the instance, in bytes. instance.dormantProcessesCount The number of dormant processes on the instance. instance.forcedParameterizationsPerSecond The number of forced parameterizations per second on the instance. instance.preconnectProcessesCount The number of preconnect processes on the instance. instance.runnableProcessesCount The number of runnable processes on the instance. instance.runnableTasks The number of runnable tasks on the instance. instance.runningProcessesCount The number of running processes on the instance. instance.sleepingProcessesCount The number of sleeping processes on the instance. instance.suspendedProcessesCount The number of suspended processes on the instance. instance.transactionsPerSecond The number of transactions per second on the instance. memoryAvailable The available physical memory, in bytes. memoryTotal The total physical memory, in bytes. memoryUtilization The percentage of memory utilization. stats.connections The number of user connections. stats.deadlocksPerSecond The number of lock requests per second that resulted in a deadlock since the last restart. stats.killConnectionErrorsPerSecond The number of kill connection errors per second since the last restart. stats.lockWaitsPerSecond The number of times per second that MS SQL Server is unable to retain a lock right away for a resource. stats.sqlCompilations The number of MS SQL compilations per second. stats.sqlRecompilationsPerSecond The number of MS SQL re-compilations per second. stats.userErrorsPerSecond The number of user errors per second since the last restart. system.bufferPoolHitPercent The percentage of buffer pools hits on the instance. system.waitTimeInMillisecondsPerSecond The number of milliseconds per second spent waiting across the instance. Wait metrics These attributes can be found by querying the MssqlWaitSample event. Metric Description system.waitTimeCount Total wait time for this wait type, in milliseconds. This time is inclusive of signal_wait_time_ms. system.waitTimeInMillisecondsPerSecond The number of waits on this wait type, in milliseconds. This counter is incremented at the start of each wait. Inventory data The Microsoft SQL Server integration captures the configuration parameters and current settings of the Microsoft SQL Server environment. It collects the results of the sp_configure stored procedure, as well as current running configuration settings from the sys.configurations table. The data is available on the Inventory page, under the config/mssql source. For more about inventory data, see Understand integration data. Check the source code This integration is open source software. That means you can browse its source code and send improvements, or create your own fork and build it.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 44.564293,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Microsoft SQL Server monitoring <em>integration</em>",
        "sections": "Microsoft SQL Server monitoring <em>integration</em>",
        "tags": "<em>Integrations</em>",
        "body": "&#x2F;infrastructure_agent&#x2F;windows&#x2F;<em>integrations</em>&#x2F;nri-mssql&#x2F;386&#x2F;nri-mssql-386.msi Copy 64-bit Windows http:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;windows&#x2F;<em>integrations</em>&#x2F;nri-mssql&#x2F;nri-mssql-amd64.msi Copy In an admin account, run the install script using an absolute path. 32-bit Windows msiexec.exe &#x2F;qn &#x2F;i PATH\\TO\\nri"
      },
      "id": "6043a676196a67eb76960f7b"
    },
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 40.651505,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Prometheus OpenMetrics <em>integrations</em>",
        "tags": "<em>Integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-06-26T00:13:37Z",
      "updated_at": "2021-06-20T12:42:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 36.987526,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate Azure <em>integrations</em>",
        "sections": "Activate Azure <em>integrations</em>",
        "tags": "<em>Integrations</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The Microsoft Azure <em>integrations</em> report data from various Azure platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    }
  ],
  "/docs/integrations/new-relic-integrations/getting-started/integration-dashboards-infrastructure-beta-transition-guide": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/release-notes/infrastructure-release-notes/cloud-integration-release-notes/changes-curated-dashboards/",
      "sections": [
        "Changes in curated dashboards",
        "Other changes",
        "New"
      ],
      "published_at": "2021-06-26T15:40:18Z",
      "title": "Changes in curated dashboards",
      "updated_at": "2021-03-16T16:57:40Z",
      "type": "docs",
      "external_id": "e8bcff90d46de24933446830d551bcab22a2da5c",
      "document_type": "release_notes",
      "popularity": 1,
      "body": "Changes in curated dashboards New Relic Infrastructure integrations will not automatically create new dashboards in New Relic Insights when an integration is enabled. Instead, curated dashboards for On-host and Cloud integrations data will be embedded in the New Relic Infrastructure UI, and they can be reached from the following links: On-host integrations: https://infrastructure.newrelic.com/accounts/<your_account_ID>/integrations/onHostIntegrations AWS integrations: https://infrastructure.newrelic.com/accounts/<your_account_ID>/integrations/aws Azure integrations: https://infrastructure.newrelic.com/accounts/<your_account_ID>/integrations/azure GCP integrations: https://infrastructure.newrelic.com/accounts/<your_account_ID>/integrations/gcp Please refer to the transition guide and the Infrastructure integration dashboards and charts documentation for more details about the new pre-built integration dashboards. Note the existing curated dashboards that had been automatically created in New Relic Insights through the InfrastructurePro@newrelic.com user are still available. These dashboards won't be automatically updated anymore, but now you can edit and remove them. By default, they have the same name as the dashboard in New Relic Infrastructure, that takes this format: For on-host integrations: <Integration name> For cloud integrations: <Integration name> - <Linked account name> Other changes The former systemErrors metric in the AWS DynamoDB integration is now reported in several different metrics which represent the total number of requests that generate an HTTP 500 status per operation type. Please refer to AWS DynamoDB monitoring integration for details. New The AWS RDS integration now provides new metrics for RDS database instances and Amazon Aurora clusters. Check AWS RDS monitoring integration for details. The Azure App Service integration now provides new metrics to monitor connections and performance of Web Apps. Check Azure App Service monitoring integration for details.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 138.00345,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Changes <em>in</em> curated <em>dashboards</em>",
        "sections": "Changes <em>in</em> curated <em>dashboards</em>",
        "body": ":&#x2F;&#x2F;<em>infrastructure</em>.newrelic.com&#x2F;accounts&#x2F;&lt;your_account_ID&gt;&#x2F;integrations&#x2F;azure GCP integrations: https:&#x2F;&#x2F;<em>infrastructure</em>.newrelic.com&#x2F;accounts&#x2F;&lt;your_account_ID&gt;&#x2F;integrations&#x2F;gcp Please refer to the <em>transition</em> <em>guide</em> and the <em>Infrastructure</em> <em>integration</em> <em>dashboards</em> and charts documentation for more details about the new pre"
      },
      "id": "603e8dcbe7b9d203892a07e2"
    },
    {
      "sections": [
        "Transition to New Relic One from Insights",
        "Important",
        "Features",
        "Improved query abilities",
        "Improved visualizations",
        "Steps for a successful transition"
      ],
      "title": "Transition to New Relic One from Insights",
      "type": "docs",
      "tags": [
        "New Relic One",
        "Use New Relic One",
        "Core concepts"
      ],
      "external_id": "4af99cd8030909a71d21a359a60af5ac93b93a66",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-one/use-new-relic-one/core-concepts/transition-new-relic-one-insights/",
      "published_at": "2021-06-26T14:45:52Z",
      "updated_at": "2021-05-22T00:14:35Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important As of April 12, 2021, we're upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. Released in 2014, New Relic Insights was our original way to create custom queries, charts, and dashboards. With New Relic One, we have modernized the experience for you to access, analyze, and visualize your data. New Relic One offers an improved charts and dashboards experience, and it provides a platform where we can more rapidly bring new innovations to you. This transition guide can help you understand: What are some of the new and improved features you get with New Relic One charts, dashboards, and queries Why it's easy to transition to New Relic One What to know and considerations when you make the switch How to get the most out of using New Relic One Features You can scroll down to the transition details, but first here are some features we've added that show how New Relic One dashboards are a clear improvement over Insights dashboards. Improved query abilities With New Relic One, you get: Ability to query many accounts from the same widget: New Relic One lets you query across all your associated accounts in one place. Better querying and charting experiences: Query access is available globally, no matter where you are in New Relic One. This includes a \"basic\" query mode that doesn't require knowledge of NRQL. Improved query experience: You can query both the Metric data type and metric timeslice data. Easy customization: Every visualization now has the query accessible. You can augment any curated chart just by changing the NRQL query. Improved visualizations Not only can you select a wide range of visualization options, you can also add more to your dashboards: Better display options: Make your data easier to understand by using visualizations other than dense, line-heavy charts. New Relic One also offers a better TV mode. Facet linking: You can filter your dashboards by faceted attributes, making your dashboards more interactive and easy to use. There's also support for cases. Learn more. More charts or widgets in an area: Insights restricted you to a 3-across limit. Now you can display up to 12 across your dashboard, providing increased data density along with improved tooltips and tracking across charts. Easier creation of multi-page dashboards: Insights referred to these as data apps. Your Insights data apps are preserved as multi-page dashboards in New Relic One. Chart consistency and flexibility: Dashboards include facet color consistency across widgets and faster loading times for more performant dashboards. Also, you can add any chart type to a dashboard in New Relic One! The New Relic Insights UI has served our users well for many years, but it's time to give you an even better experience. Join us and make the switch to New Relic One! Steps for a successful transition The transition to New Relic One has two parts: the UI and mobile app experience (April 12, 2021) and the Dashboard API (July 2021). Insights functionality Transition to New Relic One UI We have already taken care of your transition from Insights to New Relic One for you! As of April 12, 2021, your old Insights web URLs redirect automatically to New Relic One. We recommend that you familiarize yourself with the new UI features available to you, as described in this transition guide. If you need to view any Insights charts embedded in other websites, go to one.newrelic.com > More > Manage Data. (These older embedded charts will continue to function as expected.) Mobile apps Your Insights mobile app is deprecated as of April 11, 2021. Go to the Google Play Store 2 or Apple App store. Delete your old Insights mobile app, and download the New Relic One mobile app. tvOS apps and large displays New Relic's tvOS app is still available. No action is needed by you at this time. Some New Relic customers with the original pricing model may have set up dashboards on wall screens for restricted users with kiosk mode. No action is required for you to continue to view these dashboards. APIs In July of 2021, the Insights Dashboard API will be deprecated and replaced with NerdGraph functionality. For more on this change, and tips on how to migrate, see NerdGraph API for dashboards. Partnership accounts This applies only if your account is one of the few using our partnership account structure to deliver New Relic services to your direct customers. In this situation, the Insights EOL will not affect your customers pricing. This is simply an EOL for the UI, not an EOL for the account type. Questions If you have questions about the transition, please comment in our Explorers Hub post. Or, if you work with an account team, they will be happy to help you.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 117.517395,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Transition</em> to New Relic One from <em>Insights</em>",
        "sections": "<em>Transition</em> to New Relic One from <em>Insights</em>",
        "body": " experience, and it provides a platform where we can more rapidly bring new innovations to you. This <em>transition</em> <em>guide</em> can help you understand: What are some of the new and improved features you get with New Relic One charts, <em>dashboards</em>, and queries Why it&#x27;s easy to <em>transition</em> to New Relic One What to know"
      },
      "id": "6044171164441f454a378ee2"
    },
    {
      "sections": [
        "Introduction to Infrastructure Integrations SDK",
        "Important",
        "What is the Integrations SDK?",
        "What data can you report with an on-host integration?",
        "Create a custom integration"
      ],
      "title": "Introduction to Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "0662f6c3fbc151a0c836b54c104e81756b34827f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/introduction-infrastructure-integrations-sdk/",
      "published_at": "2021-06-25T18:26:21Z",
      "updated_at": "2021-03-16T07:28:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "There are several ways New Relic lets you create your own integration: General telemetry (metrics, traces) solutions: Use our Telemetry SDKs. Build a custom New Relic One application that uses your own JavaScript UI functionality. If you have New Relic Infrastructure, you can use our lightweight Flex integration tool (recommended) or use our Integrations SDK to build a complete Infrastructure on-host integration. Important New Relic is transitioning to rely on open source standards like Prometheus for future on-host integrations. Though the infrastructure SDK is the foundation of that transition, some of the tutorials and tools around this SDK might not be up-to-date with the latest developments. What is the Integrations SDK? Our Infrastructure Integrations SDK lets you build an on-host integration that reports custom data from your hosts or services. That data can then be found in New Relic Infrastructure and can be used to create custom queries and charts. What data can you report with an on-host integration? When you build an integration using the Integrations SDK, you assign the entities that report data. In New Relic terms, entity is a purposefully ambiguous term meaning something that reports data to New Relic (for example: a local host, a load balancer, or a database). A single integration can report data from multiple entities, which gives you the ability to report data from more than one service or host instance. There are three types of data an entity can generate: Metrics: Metric data is used for numerical measurement data. Examples: how many requests are in a queue, or the number of hits on a database per minute. Metric data from a custom integration can be queried and used to create dashboards. Inventory: Live system state and configuration information. This data will show up on the Infrastructure Inventory UI page. Events: Events are used to record important activities on a system. Examples: a service starting or a new table being created. Event data will be shown in the Infrastructure Events UI page. Create a custom integration To create an integration using the Integrations SDK, use these resources: See the Go language build tools and tutorial. The tutorial walks you through creating a Redis integration in Go. (Note: Go is not required; it's just the language for which we provide additional build tools. For more information, see Integrations SDK requirements.) See the integration file structure documentation, which describes the files required to create an integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 99.13922,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Introduction</em> to <em>Infrastructure</em> <em>Integrations</em> SDK",
        "sections": "<em>Introduction</em> to <em>Infrastructure</em> <em>Integrations</em> SDK",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> SDK",
        "body": " of that <em>transition</em>, some of the tutorials and tools around this SDK might not be up-to-date with the latest developments. What is the Integrations SDK? Our <em>Infrastructure</em> Integrations SDK lets you build an on-host <em>integration</em> that reports custom data from your hosts or services. That data can then be found in New"
      },
      "id": "603eb55be7b9d2dcb72a07e8"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/dropwizard/dropwizard-reporter": [
    {
      "sections": [
        "Introduction to New Relic's open source telemetry integrations",
        "Tip",
        "Types of integrations",
        "How they work"
      ],
      "title": "Introduction to New Relic's open source telemetry integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "Get started"
      ],
      "external_id": "239889ec292525fcfd6b417d243943ea7b3e0529",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations/",
      "published_at": "2021-06-26T14:24:15Z",
      "updated_at": "2021-03-16T06:12:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides open source integrations that report telemetry data from telemetry tools to your New Relic account. Tip To use telemetry integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Types of integrations We have open source integrations that report data from OpenCensus, OpenTelemetry, DropWizard, Prometheus, and more. With these solutions, you can aggregate all your telemetry data in one place: the New Relic platform. See our list of open source telemetry integrations (to browse all New Relic solutions, see our integrations page). How they work These integrations were built using our Telemetry SDKs, which are open-source language-specific libraries for reporting metrics, trace data, and other telemetry data to New Relic. If our pre-built integrations don't meet your needs, you can use the Telemetry SDKs to build your own telemetry tools. Under the hood, data reported by these solutions are ingested via our data ingest APIs. For example, metrics reported by the DropWizard exporter are ingested via the Metric API, so to understand how to query and chart that type of data, you could read Query metric data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.77863,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "sections": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to 100GB of data for free each month. Forever. Types of <em>integrations</em> We have <em>open</em> <em>source</em> <em>integrations</em> that report data from <em>Open</em>Census, <em>OpenTelemetry</em>, <em>DropWizard</em>, Prometheus, and more. With these solutions, you can aggregate all your <em>telemetry</em> data in one place: the New Relic platform. See our list"
      },
      "id": "603e95ab28ccbc036aeba789"
    },
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-06-26T04:31:11Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.66617,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-06-26T14:25:58Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.9748,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/elixir/elixir-open-source-agent": [
    {
      "sections": [
        "Roku open-source agent",
        "Tip",
        "Get started",
        "For more help"
      ],
      "title": "Roku open-source agent",
      "type": "docs",
      "tags": [
        "Agents",
        "Open-source licensed agents",
        "Open-source licensed agents"
      ],
      "external_id": "f0982a0ff96c8a85683bf3ef27a2e4cde85ff274",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/roku/roku-open-source-video-agent/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-04-27T11:09:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Monitor Roku behavior with New Relic using the Roku open-source agent. The agent contains two parts, to capture two separate categories of Roku behavior: App events like app starts and HTTP requests Video playback within the app Tip This agent is released as open source on GitHub. A change log is also available there for the latest updates. Get started For requirements, installation, and configuration information, see the Open Source Roku Agent README on GitHub. Visit New Relics Roku repository on GitHub for questions about installation, usage, or other topics. Report issues or bugs as an issue in the GitHub repository. For more help Recommendations for learning more: Browse New Relic's Explorers Hub for community discussions about the open-source Roku agent. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 356.4996,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Roku <em>open</em>-<em>source</em> <em>agent</em>",
        "sections": "Roku <em>open</em>-<em>source</em> <em>agent</em>",
        "tags": "<em>Open</em>-<em>source</em> <em>licensed</em> <em>agents</em>",
        "body": "Monitor Roku behavior with New Relic using the Roku <em>open</em>-<em>source</em> <em>agent</em>. The <em>agent</em> contains two parts, to capture two separate categories of Roku behavior: App events like app starts and HTTP requests Video playback within the app Tip This <em>agent</em> is released as <em>open</em> <em>source</em> on GitHub. A change log"
      },
      "id": "6087f0ff64441f618a9d8533"
    },
    {
      "sections": [
        "Real-time profiling for Java using JFR metrics",
        "Find performance bottlenecks",
        "JFR daemon",
        "Supported Java versions",
        "Requirements and Usage Instructions",
        "Important",
        "View your data",
        "Understand JVM cluster behavior over time",
        "JVM details",
        "Identify resource intensive code paths using flamegraphs"
      ],
      "title": "Real-time profiling for Java using JFR metrics",
      "type": "docs",
      "tags": [
        "Agents",
        "Java agent",
        "Features"
      ],
      "external_id": "e37ad20639884cbd94e3032b5b51788eb6d79618",
      "image": "https://docs.newrelic.com/static/d995a7d337779024851a421fcc95ad79/e5166/new-relic-one-java-flight-record-ui.jpg",
      "url": "https://docs.newrelic.com/docs/agents/java-agent/features/real-time-profiling-java-using-jfr-metrics/",
      "published_at": "2021-06-26T13:15:32Z",
      "updated_at": "2021-06-26T13:15:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's real-time profiling for Java using Java Flight Recorder (JFR) metrics, you can run continuous, always-on profiling of your Java code in production environments. The accompanying JVM cluster timeline view provides a fast and intuitive way to diagnose cluster-wide performance problems. For example, you can quickly see how an applications deployment affects the overall health of the cluster. Find performance bottlenecks Troubleshooting performance bottlenecks in your Java application or service can help you better understand the following: Where youre wasting resources When an incident occurs What happened during an incident What performance issues led up to an incident To make troubleshooting faster and easier, you need to see the high fidelity runtime characteristics of your code running on the JVM, and you need that data in real time. JFR daemon The JFR daemon is an exporter for JFR events which allows you to harness the power of the New Relic One platform for visualizing your JVM's behavior. Using the New Relic Java telemetry SDK as the underlying implementation, the JFR daemon converts JFR events into New Relic telemetry types and reports them to New Relic's metric and event ingest APIs. There are three different usage scenarios for the JFR daemon: New Relic Java agent JFR service (RECOMMENDED) - JFR monitoring built into the flagship New Relic Java agent. No additional setup is needed, simply install the Java agent, make sure that the JFR service is enabled, and JFR data will flow into the same APM application as the Java agent. Requires New Relic Java agent version 7.0.0+. Standalone process - Run the jfr-daemon as a standalone process and configure it to monitor an existing Java process using remote JMX. Standalone Java agent - Attach the jfr-daemon to your Java process as a Java agent. A lightweight alternative to the New Relic Java agent. Supported Java versions While the JFR daemon supports any version of Java 11 and above as well as specific versions of Java 8 (specifically version 8u262+), we don't recommend using any non-LTS version of Java in production environments. See supported Java versions for full details. Requirements and Usage Instructions Requirements and instructions vary for each usage scenario. For full details please see the links for your specific usage scenario: New Relic Java agent JFR service - Requirements, configuration, and usage instructions Standalone process - Requirements, configuration, and usage instructions Standalone Java agent - Requirements, configuration, and usage instructions Important Apps running with the JFR daemon should expect the JFR subsystem to use additional memory. View your data To view your data, go to one.newrelic.com > Explorer > (select service) > More Views > Realtime Profiling Java. Understand JVM cluster behavior over time The JVM cluster timeline view shows the JVM behavior across your entire cluster. This timeline enables quicker troubleshooting and issue detection; for example, at a glance you can see: How a recent deployment affected the rest of the JVM cluster When a JVM restarted How an individual instance was affected by its noisy neighbor To make troubleshooting easier, you need to see the high fidelity runtime characteristics of your code running on the JVM, and you need that data in real time. one.newrelic.com > Explorer > (select service) > Realtime Profiling Java: The JVM cluster timeline view shows the JVM behavior across the cluster. Each row of the timeline represents a specific JVM over time. Inside each row, a box represents a 5-minute period of that JVMs life. From least severe to most severe, yellow, orange, and red traffic lights indicate anomalous behavior for a JVM, so you can drill down into that instance and the right time period when investigating errors or other performance issues. Select How is JVM health determined? for a detailed breakdown of how JVM health is calculated. JVM details The details panel for each JVM provides several critical views: How resources are allocated within a process How garbage collection affects performance How to track garbage collection with logs How CPU is used one.newrelic.com > Explorer > (select service) > Realtime Profiling Java > (select JVM): You can see details for each JVM. Identify resource intensive code paths using flamegraphs Important The flamegraph feature is only compatible with the New Relic Java agent JFR service usage scenario. Use flamegraphs to identify the Java classes and methods that are most frequently executed in your application code. By using flamegraphs to optimize the hot spots in your code you can reduce resource consumption and increase your applications overall performance. one.newrelic.com > Explorer > (select service) > Realtime Profiling Java > (select JVM): You can see details for each JVM, including flamegraphs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 27.624928,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Agents</em>",
        "body": "&#x27;s metric and event ingest APIs. There are three different usage scenarios for the JFR daemon: New Relic Java <em>agent</em> JFR service (RECOMMENDED) - JFR monitoring built into the flagship New Relic Java <em>agent</em>. No additional setup is needed, simply install the Java <em>agent</em>, make sure that the JFR service"
      },
      "id": "603e9db2e7b9d2a5432a07f5"
    },
    {
      "sections": [
        "How New Relic distributed tracing works",
        "Tip",
        "Trace sampling",
        "Head-based sampling (standard distributed tracing)",
        "Language agents: adaptive sampling",
        "Language agents: limits and sampling",
        "Trace rate limiting",
        "Lambda trace sampling",
        "Tail-based sampling (Infinite Tracing)",
        "Architecture",
        "Tail-based sampling algorithms",
        "No sampling",
        "Browser and Mobile trace reporting",
        "Trace API",
        "How trace data is structured",
        "How trace data is stored",
        "How trace context is passed between applications",
        "Important",
        "Scenario 1: Trace touching three agent types",
        "Scenario 2: Trace with W3C New Relic and middleware",
        "Scenario 3: Trace with any W3C-compliant agent and a New Relic agent."
      ],
      "title": "How New Relic distributed tracing works",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "4dbe0119017f78ad4db2a2b8a9ca2d287222753a",
      "image": "https://docs.newrelic.com/static/406c9f3af4012dab16df681c8feab256/c1b63/new-relic-distributed-tracing-trace-structure.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/how-new-relic-distributed-tracing-works/",
      "published_at": "2021-06-25T20:20:18Z",
      "updated_at": "2021-06-25T20:20:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Here are some technical details about how New Relic distributed tracing works: How trace sampling works How we structure trace data How we store trace data How trace context is passed between applications Tip For instructions about setting up distributed tracing, see Overview: Enable distributed tracing. Trace sampling How your traces are sampled will depend on your setup and the New Relic tracing tool you're using. For example, you may be using a third-party telemetry service (like OpenTelemetry) to implement sampling of traces before your data gets to us or, if you're using Infinite Tracing, you'd probably send us all your trace data and use our sampling. We have a few sampling strategies available: Head-based sampling (standard distributed tracing) Tail-based sampling (Infinite Tracing) No sampling Head-based sampling (standard distributed tracing) With the exception of our Infinite Tracing feature, most of our tracing tools use a head-based sampling approach. This applies filters to individual spans before all spans in a trace arrive, which means decisions about whether to accept spans is made at the beginning (the \"head\") of the filtering process. We use this sampling strategy to capture a representative sample of activity while avoiding storage and performance issues. Here are some details about how head-based sampling is implemented in our standard distributed tracing tools: Language agents: adaptive sampling Our APM language agents use adaptive sampling to capture a representative sample of system activity. The following is an explanation of how adaptive sampling works. For the first service in a distributed trace, 10 requests are chosen to be sampled. The throughput to that service is used to adjust how frequently requests are sampled. This is explained in more detail below. The first service we monitor in a distributed trace is called the trace origin. The trace origin chooses requests at random to be traced. That decision propagates to the downstream services touched by that request. When the request has completed, all of the spans touched by that request that we've detected are made available in the UI as a complete trace (though agent limits may result in fragmented traces). APM agents have a limit on the number of transactions collected per minute (this can vary, depending on agent) and a limit on the number of spans collected per minute (1000 per agent instance). To adhere to these limits, the default number of traces at the trace origin is 10 traces per minute. An APM agent spreads out the collection of these 10 traces over a minute in order to get a representative sample over time. The exact sampling rate depends on the number of transactions in the previous minute. The rate responds to changes in transaction throughput, going up or down. For example, if the previous minute had 100 transactions, the agent would anticipate a similar number of transactions and select 1 out of every 10 transactions to be traced. Language agents: limits and sampling An APM language agent instance using head-based sampling has a limit of 1000 spans per minute. The agent attempts to keep all spans that are marked to be sampled as part of a distributed trace. In many distributed systems, the average microservice may generate 10 to 20 spans per request. In those cases, the agent span limit can accommodate all spans chosen, and that service will have full detail in a trace. However, some requests to services will generate many spans, and the agent span limit will be reached. As a result, some traces will not have full detail for that service. One solution to this would be to custom instrument an agent to report less activity and therefore report fewer spans. To read about how browser monitoring of trace data may vary from our language agents, see Browser traces. Trace rate limiting If the above sampling methods still result in too much trace data, we may limit incoming data by sampling traces after they're received. By making this decision at the trace level, it avoids fragmenting traces (accepting only part of a trace). This process works similarly to adaptive sampling. The total spans received in a minute are totaled. If too many spans are received, fewer spans may be accepted in the following minute, in order to achieve a floating-average throughput rate. Lambda trace sampling Our AWS Lambda monitoring uses its own sampling process. Tail-based sampling (Infinite Tracing) Our Infinite Tracing feature uses a tail-based sampling approach. \"Tail-based sampling\" means that trace-retention decisions are done at the tail end of processing after all the spans in a trace have arrived. With Infinite Tracing, you can send us 100% of your trace data from your application or third-party telemetry service, and Infinite Tracing will figure out which trace data is most important. And you can configure the sampling to ensure the traces important to you are retained. Architecture For Infinite Tracing, agents or integrations send 100% of all instrumented spans to a trace observer. The trace observer is a distributed tracing service residing in a cluster of services on AWS called New Relic Edge. Tip Only your spans go to the trace observerall other data such as metrics, custom events, and transaction traces are sent the normal route to New Relic and are subject to local sampling. You configure a unique trace observer endpoint for the AWS region you want to send data to. You can request multiple endpoints, one per AWS region. The endpoint represents a trace observer for a particular workload. For example, all spans from a single trace (request) must go to that endpoint. Here are two architectural diagrams: one showing how data flows if you use APM agents and another if you use New Relic integrations like OpenTelemetry exporters: The trace observer holds traces open while spans for that trace arrive. Once the first span in a trace arrives, a session is kept open for 10 seconds. Each time a new span for that trace arrives, the expiration time is reset to 10 seconds. Traces that haven't seen a span arrive within the last 10 seconds will automatically expire. Tail-based sampling algorithms By default, each trace observer offers traces to three samplers: one looking for duration outliers, one looking for traces with errors, and one trying to randomly sample across all trace types. Each sampler keeps a target percentage of traces that match their criteria. Here are details about each sampler: Sampler Matching criteria Target percent Duration Traces with an outlier duration, using two algorithms: Gaussian (Assumes a normal distribution and a threshold at the 99th percentile) Eccentricity (Assumes no distribution and a threshold based on cluster) 100% Error Traces having at least one span with an error 100% Random All traces 1% (This is configurable. See Infinite Tracing: Random trace filter) If the matching criteria matches the trace, each sampler looks at the traces shape. A traces shape is the unique combination of the root spans entity name and span name. This is a simple way to separate traces using the entry point of the request. Once the shape is determined, the sampler makes a decision to keep or reject the trace based on its target sampling percent. If its 100%, the trace is automatically kept. If its anything less, the probability the sampler keeps a given trace is determined by the target percent. For example, the default target percent is 1 for random traces, so 1% of those traces are kept. If you prefer, you can change the random filter percentage. Because the trace observer uses percentages of throughput, the number of traces selected will vary with that throughput. No sampling Some of our tools don't use sampling. Sampling details for these tools: Browser and Mobile trace reporting Browser monitoring distributed tracing and Mobile monitoring report all spans. Our APM language agents are often used in conjunction with browser and mobile monitoring, and our language agents use sampling. This means that there will likely be many more browser and mobile spans than back-end spans, which can result in browser and mobile app spans disconnected from back-end spans. For tips on querying for traces that contain front and back-end spans, see Find browser span data. Trace API If you don't have Infinite Tracing enabled, our Trace API does no sampling (unless the default data limits are exceeded). It's expected that you set up the Trace API to send us the traces you think are important. How trace data is structured Understanding the structure of a distributed trace can help you: Understand how traces are displayed in our UI Help you query trace data A distributed trace has a tree-like structure, with \"child\" spans that refer to one \"parent\" span. This diagram shows some important span relationships in a trace: This diagram shows how spans in a distributed trace relate to each other. This diagram shows several important concepts: Trace root. The first service or process in a trace is referred to as the root service or process. Process boundaries. A process represents the execution of a logical piece of code. Examples of a process include a backend service or Lambda function. Spans within a process are categorized as one of the following: Entry span: the first span in a process. Exit span: a span is a considered an exit span if it a) is the parent of an entry span, or b) has http. or db. attributes and therefore represents an external call. In-process span: a span that represents an internal method call or function and that is not an exit or entry span. Client spans. A client span represents a call to another entity or external dependency. Currently, there are two client span types: Datastore. If a client span has any attributes prefixed with db. (like db.statement), it's categorized as a datastore span. External. If a client span has any attributes prefixed with http. (like http.url) or has a child span in another process, it's categorized as an external span. This is a general category for any external calls that are not datastore queries. Trace duration. A trace's total duration is determined by the length of time from the start of the earliest span to the completion of the last span. You can query span relationship data with the NerdGraph GraphiQL explorer at api.newrelic.com/graphiql. How trace data is stored Understanding how we store trace data can help you query your trace data. We save trace data as: Span: A span represents operations that are part of a distributed trace. The operations that a span can represent include browser-side interactions, datastore queries, calls to other services, method-level timing, and Lambda functions. One example: in an HTTP service, a span is created at the start of an HTTP request and completed when the HTTP server returns a response. Span attributes contain important information about that operation (such as duration, host data, etc.), including trace-relationship details (such as traceId, guid). For span-related data, see span attributes. Transaction: If an entity in a trace is monitored by an agent, a request to that entity generates a single Transaction event. Transactions allow trace data to be tied to other New Relic features. For transaction-related data, see transaction attributes. Contextual metadata. We store metadata that shows calculations about a trace and the relationships between its spans. To query this data, use the NerdGraph GraphiQL explorer. How trace context is passed between applications We support the W3C Trace Context standard, which makes it easier to trace transactions across networks and services. When you enable distributed tracing, New Relic agents add HTTP headers to a service's outbound requests. HTTP headers act like passports on an international trip: They identify your software traces and carry important information as they travel through various networks, processes, and security systems. The headers also contain information that helps us link the spans together later: metadata like the trace ID, span ID, the New Relic account ID, and sampling information. See the table below for more details on the header: Item Description accountId This is your New Relic account ID. However, only those on your account and New Relic Admins can associate this Id with your account information in any way. appId This is the application ID of the application generating the trace header. Much like accountId, this identifier is not going to provide any information unless you're a user on the account. guid With Distributed Tracing, each segment of work in a trace is represented by a span, and each span has a guid attribute. The guid of the last span within the process is sent with the outgoing request so that the first segment of work in the receiving service can add this guid as the parentId attribute which connects data within the trace. Parent type The source of the trace header, as in Mobile, Browser, Ruby app, etc. This becomes the parent.type attribute on the Transaction triggered by the request this header is attached to. Priority A randomly generated priority ranking value that helps determine which data is sampled when sampling limits are reached. This is a float value set by the first New Relic agent thats part of the request so all data in the trace will have the same priority value. Sampled A boolean value that tells the agent if traced data should be collected for the request. This is also added as an attribute on any span and transaction data collected. If you want to read more about this sampling process, this guide goes into more detail. Timestamp Unix timestamp in milliseconds when the payload was created. traceId The unique ID (a randomly generated string) used to identify a single request as it crosses inter- and intra- process boundaries. This ID allows the linking of spans in a distributed trace. This also is added as an attribute on the span and transaction data. transactionId The unique identifier for the transaction event. Trusted acount key This is a key that helps identify any other accounts associated with your account. So if you have multiple sub-accounts that the trace crosses, we can confirm that any data included in the trace is coming from a trusted source, and tells us what users should have access to the data. Version and data key This identifies major/minor versions, so if an agent receives a trace header from a version with breaking changes from the one it is on, it can reject that header and report the rejection and reason. This header information is passed along each span of a trace, unless the progress is stopped by something like middleware or agents that don't recognize the header format (see Figure 1). Figure 1 To address the problem of header propagation, we support the W3C Trace Context specification that requires two standardized headers. Our latest W3C New Relic agents send and receive these two required headers, and by default, they also send and receive the header of the prior New Relic agent: W3C (traceparent): The primary header that identifies the entire trace (trace ID) and the calling service (span id). W3C (tracestate): A required header that carries vendor-specific information and tracks where a trace has been. New Relic (newrelic): The original, proprietary header that is still sent to maintain backward compatibility with prior New Relic agents. This combination of three headers allows traces to be propagated across services instrumented with these types of agents: W3C New Relic agents Non-W3C New Relic agents W3C Trace Context-compatible agents Important If your requests only touch W3C Trace Context-compatible agents, you can opt to turn off the New Relic header. See the agent configuration documentation for details about turning off the newrelic header. The scenarios below show various types of successful header propagation. Scenario 1: Trace touching three agent types This shows the flow of headers when a request touches three different agent types: Scenario 2: Trace with W3C New Relic and middleware This shows the combination of headers sent by a W3C New Relic agent to some middleware. Scenario 3: Trace with any W3C-compliant agent and a New Relic agent. This shows the two required W3C headers from another vendor accepted by a W3C New Relic agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 25.9426,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Language <em>agents</em>: adaptive sampling",
        "body": " by that request that we&#x27;ve detected are made available in the UI as a complete trace (though <em>agent</em> limits may result in fragmented traces). APM <em>agents</em> have a limit on the number of transactions collected per minute (this can vary, depending on <em>agent</em>) and a limit on the number of spans collected per minute (1000"
      },
      "id": "6072a66664441f14089d856c"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-06-26T04:31:11Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 210.57462,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " your applications, as well as detailed information, such as distributed tracing. To <em>get</em> <em>started</em> with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - <em>OpenTelemetry</em>: Click the service you want to know more about. If you need help understanding the data"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-06-26T14:25:58Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.97977,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-06-26T14:26:00Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.69405,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " with other <em>telemetry</em> data produced by the application. The <em>OpenTelemetry</em> semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs <em>get</em>"
      },
      "id": "60ce7bf528ccbc384271b463"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/istio/istio-adapter": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-06-26T04:31:11Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.4718,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-06-26T14:25:58Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.9797,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-06-26T14:26:00Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.694,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/kamon/kamon-reporter": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-06-26T04:31:11Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.66608,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-06-26T14:25:58Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.9747,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-06-26T14:26:00Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.9053,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/micrometer/micrometer-metrics-registry": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-06-26T04:31:11Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.47176,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-06-26T14:25:58Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.97968,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-06-26T14:26:00Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.69395,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opencensus/opencensus-exporter": [
    {
      "sections": [
        "Introduction to New Relic's open source telemetry integrations",
        "Tip",
        "Types of integrations",
        "How they work"
      ],
      "title": "Introduction to New Relic's open source telemetry integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "Get started"
      ],
      "external_id": "239889ec292525fcfd6b417d243943ea7b3e0529",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/get-started/introduction-new-relics-open-source-telemetry-integrations/",
      "published_at": "2021-06-26T14:24:15Z",
      "updated_at": "2021-03-16T06:12:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides open source integrations that report telemetry data from telemetry tools to your New Relic account. Tip To use telemetry integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Types of integrations We have open source integrations that report data from OpenCensus, OpenTelemetry, DropWizard, Prometheus, and more. With these solutions, you can aggregate all your telemetry data in one place: the New Relic platform. See our list of open source telemetry integrations (to browse all New Relic solutions, see our integrations page). How they work These integrations were built using our Telemetry SDKs, which are open-source language-specific libraries for reporting metrics, trace data, and other telemetry data to New Relic. If our pre-built integrations don't meet your needs, you can use the Telemetry SDKs to build your own telemetry tools. Under the hood, data reported by these solutions are ingested via our data ingest APIs. For example, metrics reported by the DropWizard exporter are ingested via the Metric API, so to understand how to query and chart that type of data, you could read Query metric data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 277.25543,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "sections": "Introduction to New Relic&#x27;s <em>open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to 100GB of data for free each month. Forever. Types of <em>integrations</em> We have <em>open</em> <em>source</em> <em>integrations</em> that report data from <em>OpenCensus</em>, <em>OpenTelemetry</em>, DropWizard, Prometheus, and more. With these solutions, you can aggregate all your <em>telemetry</em> data in one place: the New Relic platform. See our list"
      },
      "id": "603e95ab28ccbc036aeba789"
    },
    {
      "sections": [
        "Introduction to OpenTelemetry with New Relic",
        "Benefits of OpenTelemetry",
        "Tip",
        "Should I use OpenTelemetry instrumentation or New Relic agents?",
        "OpenTelemetry: A work in progress",
        "New Relic APM agents",
        "How OpenTelemetry works with New Relic",
        "Important",
        "Collector (recommended)",
        "OTLP (pre-release)",
        "What's next?"
      ],
      "title": "Introduction to OpenTelemetry with New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "c87898d2d5835c00930c173eabd1bf93040badea",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic/",
      "published_at": "2021-06-26T04:30:04Z",
      "updated_at": "2021-04-28T06:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can send OpenTelemetry data to New Relic and analyze the results in our observability platform. This introduction can help you decide if you want to get started with OpenTelemetry. If you're already familiar with OpenTelemetry and want to jump into the setup, go to our OpenTelemetry quick start. If you're just getting acquainted with OpenTelemetry, this is what we'll explore here: Benefits of OpenTelemetry Should I use OpenTelemetry or New Relic agents? How OpenTelemetry works with New Relic Benefits of OpenTelemetry OpenTelemetry provides a secure, vendor-neutral specification for service instrumentation so that you can send data to distinct backends of your choice, such as New Relic. OpenTelemetry offers a single set of APIs and libraries that standardize how you collect and transfer telemetry data for your services. The following components make up the OpenTelemetry project: Specifications for the core pillars of observability to drive consistency across all projects (initially, traces and metrics are supported, followed by logs) APIs that contain interfaces and implementations based on the specifications SDKs (reference implementations of the APIs) created specifically for languages like Java, Python, Go, Erlang, and more Collectors that offer a vendor-agnostic implementation for processing and exporting Exporters that enable you to send data to a backend of your choice The components of OpenTelemetry work together to create some distinct advantages for capturing telemetry data: Feature Description Ubiquitous instrumentation A single, open standard of instrumentation provides better coverage and flexibility as engineers from all over the world contribute to the instrumentation. Future proof As the instrumentation gets built into libraries and frameworks, and as more vendors move to support this open standard, you can be confident that you wont need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build integrations into OpenTelemetry or add instrumentation directly to source code, ensuring end users can easily monitor these new technologies. Simplified choice You dont need to decide which instrumentation option to use (a proprietary option or one of the other open standards). Cross-platform compatibility OpenTelemetry supports a variety of languages and backends. It represents a vendor-neutral path for capturing and transmitting telemetry to backends without altering existing instrumentation. Streamlined observability It is easier for vendors to support and test against a single standard as they dont need to develop their own agents or collectors. High dimensionality OpenTelemetry uses dimensional metrics, so you can filter and facet on more aspects of the data, such as AWS regions, Kubernetes clusters, or service versions. Dimensional metrics also lead to less time between occurrence and reporting. Efficiency OpenTelemetrys fire-and-forget trace-centric approach to instrumentation often has lower overhead than New Relic agents, especially for asynchronous workloads. It will also result in better handling of trace data for asynchronous requests. Tip For more, see our blog posts on OpenTelemetry. Should I use OpenTelemetry instrumentation or New Relic agents? As you consider OpenTelemetry, you may also be looking at New Relic APM agents that also capture telemetry data. As you'd expect, there is a lot of overlap between features available from OpenTelemetry agents and SDKs versus those available from New Relic APM agents. This is especially true if you're interested in distributed tracing telemetry data. The choice you make depends on what you need. We recommend that you explore both New Relic and OpenTelemetry instrumentation or discuss it directly with us at New Relic to decide what works best for you. OpenTelemetry: A work in progress OpenTelemetry is still an emerging standard, so your choices may be affected by what's available. You can check on the current state of the specification at the OpenTelemetry site. The current state of language-specific OpenTelemetry APIs and SDKs varies: some languages are still pre-alpha and may be missing instructions on how to instrument your service. Most languages have some implementation of traces that is sufficient to start sending data to New Relic. Check out this table in GitHub that provides an overview of the state of OpenTelemetry specification compliance for each language. For languages that New Relic does not currently provide an agent or SDK, OpenTelemetry may offer you a good alternative. Also, in cases where you want explicit control over sampling of your telemetry data, OpenTelemetry provides a lot of flexibility. As OpenTelemetry matures, New Relic will continue to support new OpenTelemetry data models and to provide a curated UI experience for our Full Stack Observability customers. New Relic APM agents In general, New Relic APM agents will collect more telemetry data for your services, and they offer a wide range configuration options and an extensive set of auto-instrumentation capabilities. APM agents offer detailed transaction trace visibility for individual services. They also offer predefined sampling to balance the performance impact of your instrumentation against the need to capture enough data to gain helpful insights. How OpenTelemetry works with New Relic Here are two basic ways you could set up OpenTelemetry with New Relic: use the OpenTelemetry collector (recommended), or use the native OTLP endpoint (pre-release). Each approach assumes you set up instrumentation in your code, but they differ in how the data is gathered and sent to New Relic. If you are interested in tracing, we recommend that you instrument as many services as you can so that you get the most benefit from distributed tracing. There are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesnt analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Tip We discuss some variations on these basic approaches in our OpenTelemetry architecture recipes. For a detailed discussion about the architecture of OpenTelemetry itself, see our blog. Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. Collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. In this workflow, note the alternate path if you set up New Relic Infinite Tracing: Here are some advantages of this approach: The OpenTelemetry collector can perform processing operations on telemetry data before it is sent to the backend, including filtering attributes, excluding traces, and sampling. The OpenTelemetry collector can receive telemetry data in many formats and forward it on to New Relics backend, including Prometheus, Zipkin, Jaeger, AWS X-Ray, OpenCensus, and much more. The OpenTelemetry collector can collect system-level metrics from the host where it is running. OTLP (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector, or send us data directly from your service. If you are interested, let us know by completing this form. Here are some advantages to operating without an OpenTelemetry collector: There is no need to deploy a collector alongside your service. Telemetry data is sent directly from the instrumented service to New Relic. If something goes wrong, and telemetry data does not arrive, there is one less component to debug. What's next? If you want to set up OpenTelemetry with New Relic, the next step is to check out our OpenTelemetry quick start. If you prefer to use a New Relic APM agent, go to Introduction to APM.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 244.68166,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "sections": "Introduction to <em>OpenTelemetry</em> with New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": ", and as more vendors move to support this <em>open</em> standard, you can be confident that you wont need to change your instrumentation. Support for newer technologies When new technologies emerge, contributors can build <em>integrations</em> into <em>OpenTelemetry</em> or add instrumentation directly to <em>source</em> code, ensuring end"
      },
      "id": "603e81ba196a67304da83dab"
    },
    {
      "sections": [
        "Collect data from any source",
        "Agent APIs",
        "Telemetry SDK",
        "Trace API",
        "Metric API",
        "Event API",
        "Log API"
      ],
      "title": "Collect data from any source",
      "type": "developer",
      "tags": [
        "Agent API",
        "Telemetry SDK",
        "Trace API",
        "Metric API",
        "Event API"
      ],
      "external_id": "5bfb043fffe42ea4a78d5a90bf8e92aa8b8f8c33",
      "image": "",
      "url": "https://developer.newrelic.com/collect-data/collect-data-from-any-source/",
      "published_at": "2021-06-30T01:41:09Z",
      "updated_at": "2021-06-25T01:40:38Z",
      "document_type": "page",
      "popularity": 1,
      "info": "Open source emitters. APIs. New Relic agents. Get data from anywhere. ",
      "body": "New Relic products report a lot of data out of the box. When you use products like APM, Browser, Mobile, Infrastructure monitoring, or an integration, by default you receive performance data. But you may want to bring data into New Relic that isn't collected by default. Maybe you want an API-based solution that doesn't require install of an agent. Maybe you want to bring telemetry data from another analysis service into New Relic. This page describes several ways to get data into New Relic. Step 1 of 6 Agent APIs If you use our APM, Browser, or Mobile agents to report data, you can use their associated APIs to report custom data. For example, if you monitor your application with our APM Python agent, you can use the Python agent API to set up custom instrumentation. See the agent APIs. Step 2 of 6 Telemetry SDK Our Telemetry SDKs are language wrappers for our Trace API and Metric API (and eventually our Log API and Event API). These SDKs let you easily send metrics and trace data to New Relic without needing to install an agent. For customers, we offer open-source exporters and integrations that use the Telemetry SDKs to send metrics and trace data: Istio adaptor Prometheus OpenMetrics (for Docker | for Kubernetes) OpenCensus exporter (for Go | for Python) DropWizard exporter Micrometer exporter Want to build your own solution? See our Telemetry SDK docs. Step 3 of 6 Trace API Our Trace API lets you send distributed tracing data to New Relic and consolidate tracing data from multiple sources in one place. We accept trace data in two formats: Zipkin format New Relic format (if you dont have Zipkin-format data, youd use this) bash Copy $ curl -i -X POST https://trace-api.newrelic.com/trace/v1 \\ > -H \"Content-Type: application/json\" \\ > -H \"Api-Key: $INSIGHTS_INSERT_API_KEY\" \\ > -H 'Data-Format: newrelic' \\ > -H 'Data-Format-Version: 1' \\ > -d '[ $ { $ \"common\": { $ \"attributes\": { $ \"service.name\": \"Test Service A\", $ \"host\": \"host123.test.com\" $ } $ }, $ \"spans\": [ $ { $ \"trace.id\": \"123456\", $ \"id\": \"ABC\", $ \"attributes\": { $ \"duration.ms\": 12.53, $ \"name\": \"/home\" $ } $ }, $ { $ \"trace.id\": \"123456\", $ \"id\": \"DEF\", $ \"attributes\": { $ \"service.name\": \"Test Service A\", $ \"host\": \"host456.test.com\", $ \"duration.ms\": 2.97, $ \"name\": \"/auth\", $ \"parent.id\": \"ABC\" $ } $ } $ ] $ } $ ]' Step 4 of 6 Metric API You can use our Metric API to send metric data to New Relic from any source. bash Copy $ curl -i -X POST https://metric-api.newrelic.com/metric/v1 \\ > -H \"Content-Type: application/json\" \\ > -H \"Api-Key: $INSIGHTS_INSERT_API_KEY\" \\ > -d '[ $ { $ \"metrics\": [ $ { $ \"name\": \"memory.heap\", $ \"type\": \"gauge\", $ \"value\": 2.3, $ \"timestamp\": 1531414060739, $ \"attributes\": { $ \"host.name\": \"dev.server.com\" $ } $ } $ ] $ } $ ]' Step 5 of 6 Event API For sending arbitrary events to New Relic, you can use our Event API. We save these events as a new event type, which can then be queried via NRQL. (Eventually, the Telemetry SDKs will support the Event API.) bash Copy $ curl -i -X POST https://insights-collector.newrelic.com/v1/accounts/$ACCOUNT_ID/events \\ > -H \"Content-Type: application/json\" \\ > -H \"x-insert-key: $INSIGHTS_INSERT_API_KEY\" \\ > -d '[ $ { $ \"eventType\": \"LoginEvent\", $ \"service\": \"login-service\", $ \"customerId\": \"xyz\" $ } $ ]' Step 6 of 6 Log API If our existing logging integrations dont meet your needs, you can use our Log API to send any arbitrary log data to New Relic. (Eventually, the Telemetry SDKs will support the Log API.) bash Copy $ curl -i -X POST https://log-api.newrelic.com/log/v1 \\ > -H \"Content-Type: application/json\" \\ > -H \"Api-Key: $INSIGHTS_INSERT_API_KEY\" \\ > -d '[ $ \"logs\": [ $ { $ \"timestamp\": 1593538496000, $ \"message\": \"User xyz logged in\", $ \"service\": \"login-service\", $ \"hostname\": \"login.example.com\" $ } $ ] $ ]'",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 238.0651,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Collect data from any <em>source</em>",
        "sections": "Collect data from any <em>source</em>",
        "info": "<em>Open</em> <em>source</em> emitters. APIs. New Relic agents. Get data from anywhere. ",
        "tags": "<em>Telemetry</em> SDK",
        "body": " (and eventually our Log API and Event API). These SDKs let you easily send metrics and trace data to New Relic without needing to install an agent. For customers, we offer <em>open</em>-<em>source</em> exporters and <em>integrations</em> that use the <em>Telemetry</em> SDKs to send metrics and trace data: Istio adaptor Prometheus"
      },
      "id": "6091fa39e7b9d2cedb506906"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/introduction-opentelemetry-new-relic": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-06-26T04:31:11Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 283.1142,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-06-26T14:26:00Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 281.2381,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-06-26T14:25:58Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 279.4646,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-architecture-recipes": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-06-26T04:31:11Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 283.11414,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-06-26T14:26:00Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 281.23804,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-06-26T14:25:58Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 279.46454,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-06-26T04:31:11Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 283.11414,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-06-26T14:26:00Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 281.23804,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-06-26T04:30:05Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we dont already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesnt analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once youve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 209.78357,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-06-26T04:31:11Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 283.11407,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-06-26T14:25:58Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 279.46448,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-06-26T04:30:05Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we dont already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesnt analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once youve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 209.78357,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-06-26T04:31:11Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 283.11407,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-06-26T14:26:00Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 281.23798,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-06-26T14:25:58Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 279.46448,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic": [
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-06-26T14:26:00Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 281.2379,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-06-26T14:25:58Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 279.46442,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    },
    {
      "sections": [
        "OpenTelemetry quick start",
        "Step 1. Prerequisites",
        "Step 2. Instrument your service with OpenTelemetry",
        "Tip",
        "Step 3. Send your telemetry data to New Relic",
        "Important",
        "Use the OpenTelemetry collector (recommended)",
        "Use the native OTLP endpoint (pre-release)",
        "Step 4. View your data in the New Relic UI"
      ],
      "title": "OpenTelemetry quick start",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "1b846417a2958b61b047c838db49aea06f09a2a8",
      "image": "https://docs.newrelic.com/static/820ec30261e57dd485d471987fae4a28/0f2bc/collector_introduction_1.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-quick-start/",
      "published_at": "2021-06-26T04:30:05Z",
      "updated_at": "2021-05-09T18:01:24Z",
      "document_type": "page",
      "popularity": 1,
      "body": "OpenTelemetry is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up OpenTelemetry with New Relic. Here's an overview of the process, followed by details for each step. Prerequisites Instrument your service with OpenTelemetry Send your telemetry data to New Relic View your data in the New Relic UI In the following sections, we explain some basic architectural approaches, but if you want to explore other implementation options, check out OpenTelemetry architecture recipes. Step 1. Prerequisites First things first: If we dont already know you, sign up for a free New Relic account. Make sure you have an Insights insert key to send spans and metrics to New Relic. Step 2. Instrument your service with OpenTelemetry To get started, you instrument your service with OpenTelemetry. OpenTelemetry has language-specific products and SDKs to help you. Many languages offer out-the-box instrumentation for common libraries and frameworks. Each language also provides an API for further instrumenting your service manually. Tip We recommend that you instrument as many services as possible to get the most benefit from distributed tracing. Go to the repository for your language and follow the instructions to instrument your service. When you're done, return here to complete the next step of sending your telemetry data to New Relic. C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...See a complete list of languages in GitHub Step 3. Send your telemetry data to New Relic Choose how you want to export your telemetry data to New Relic: Use the OpenTelemetry collector (recommended) Use the native OTLP endpoint (pre-release) Both approaches involve configuring your service with an OTLP exporter to send data over the OpenTelemetry Protocol (OTLP). To do so, follow the documentation of the OTLP exporter for your service's language: C++ Erlang Go Java Javascript/Node.js .NET PHP Python Ruby Rust Swift ...Find additional OTLP language support in GitHub Important New Relic's language-specific exporters for OpenTelemetry are now deprecated in favor of the OpenTelemetry collector and native OTLP endpoint options described here. If you were previously using a New Relic language-specific exporter consider signing up for the pre-release of New Relic's native OTLP endpoint. If you are interested in tracing, there are two main options for trace sampling: Configure the head-based, native sampling in OpenTelemetry, which means OpenTelemetry samples traces before they are sent to New Relic. Head-based sampling doesnt analyze all traces, but instead randomly samples traces up front before details about the completed traces are known. Both the OpenTelemetry collector and the native OTLP endpoint support this option. If you want New Relic to analyze all your traces, configure tail-based sampling with New Relic Infinite Tracing, which reroutes traces to our cloud-based trace observer. The trace observer accepts all your traces and sorts through them to find useful ones. If you want to know more about this option, especially if you want to use it in the EU, see Introduction to Infinite Tracing. While Infinite Tracing is not yet compatible with the native OTLP endpoint, it is still possible to configure tail-based sampling via the collector, for more information see Tail Sampling Processor. Use the OpenTelemetry collector (recommended) The OpenTelemetry project provides a tool called the OpenTelemetry Collector that you can deploy and use as an intermediate data aggregator. In your service, you use the OpenTelemetry exporter to send telemetry data first to the OpenTelemetry collector. Then, in the OpenTelemetry collector, you enable the New Relic exporter to send data to New Relic. The diagram below shows the flow of data with the collector. To use the collector: Configure your OpenTelemetry collector to export data to New Relic, using our example as a guide. Important The New Relic exporter for the collector will be deprecated in the future. If you are running your own collector, you will instead be able to use the OpenTelemetry Collector's OTLP exporter When we release our native OTLP endpoint (pre-release), th If you were previously using a New Relic language-specific exporter consider using the OTLP exporter for your language and send data directly to New Relic's native OTLP endpoint (pre-release). Use the native OTLP endpoint (pre-release) The example above uses a New Relic exporter, but we have a pre-release program if you want to try out the native OTLP endpoint for sending your data to New Relic. You can either use the OTLP exporter in the OpenTelemetry collector or send us data directly from your service. If you are interested, let us know by completing this form. You can configure your service's OTLP exporter to send data directly to New Relic. If you're running your own OpenTelemetry collector, you can use the OTLP exporter for the collector to send data to New Relic. Step 4. View your data in the New Relic UI Once youve instrumented your service and configured it to export its data to New Relic, you can go to New Relic and view your data. The UI for OpenTelemetry has some similarities to the APM agent UI, so if you are familiar with that, you can go right to the UI. If you need help understanding your OpenTelemetry UI options, see View your OpenTelemetry data in New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 209.78355,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> quick start",
        "sections": "<em>OpenTelemetry</em> quick start",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "<em>OpenTelemetry</em> is a flexible toolkit that you can implement in a variety of ways. We recommend a basic four-step approach for setting up <em>OpenTelemetry</em> with New Relic. Here&#x27;s an overview of the process, followed by details for each step. Prerequisites Instrument your service with <em>OpenTelemetry</em> Send"
      },
      "id": "6044e5dfe7b9d2aadc5799d4"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/roku/roku-open-source-video-agent": [
    {
      "sections": [
        "Elixir open-source agent",
        "Tip",
        "Get started",
        "For more help"
      ],
      "title": "Elixir open-source agent",
      "type": "docs",
      "tags": [
        "Agents",
        "Open-source licensed agents",
        "Open-source licensed agents"
      ],
      "external_id": "aa03e1693b6ecdd06fa2940ddb99187247743772",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/elixir/elixir-open-source-agent/",
      "published_at": "2021-06-26T14:23:28Z",
      "updated_at": "2021-04-27T11:09:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Monitor Elixir behavior with New Relic using the Elixir open-source agent. The agent: Helps you track transactions, distributed traces, and other parts of your applications behavior Provides an overview of underlying BEAM activity Tip This agent is released as open source on GitHub. A change log is also available there for the latest updates. Get started For requirements, installation, and configuration information, see the Open Source Elixir Agent README on GitHub. Visit New Relics Elixir repository on GitHub for questions about installation, usage, or other topics. Report issues or bugs as an issue in the GitHub repository. For more help Recommendations for learning more: Browse New Relic's Explorers Hub for community discussions about the open-source Elixir agent. Review New Relic's licenses, attributions, data usage limits, and other notices.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 356.49957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Elixir <em>open</em>-<em>source</em> <em>agent</em>",
        "sections": "Elixir <em>open</em>-<em>source</em> <em>agent</em>",
        "tags": "<em>Open</em>-<em>source</em> <em>licensed</em> <em>agents</em>",
        "body": "Monitor Elixir behavior with New Relic using the Elixir <em>open</em>-<em>source</em> <em>agent</em>. The <em>agent</em>: Helps you track transactions, distributed traces, and other parts of your applications behavior Provides an overview of underlying BEAM activity Tip This <em>agent</em> is released as <em>open</em> <em>source</em> on GitHub. A change log"
      },
      "id": "6087f0ff28ccbceab351c13f"
    },
    {
      "sections": [
        "Real-time profiling for Java using JFR metrics",
        "Find performance bottlenecks",
        "JFR daemon",
        "Supported Java versions",
        "Requirements and Usage Instructions",
        "Important",
        "View your data",
        "Understand JVM cluster behavior over time",
        "JVM details",
        "Identify resource intensive code paths using flamegraphs"
      ],
      "title": "Real-time profiling for Java using JFR metrics",
      "type": "docs",
      "tags": [
        "Agents",
        "Java agent",
        "Features"
      ],
      "external_id": "e37ad20639884cbd94e3032b5b51788eb6d79618",
      "image": "https://docs.newrelic.com/static/d995a7d337779024851a421fcc95ad79/e5166/new-relic-one-java-flight-record-ui.jpg",
      "url": "https://docs.newrelic.com/docs/agents/java-agent/features/real-time-profiling-java-using-jfr-metrics/",
      "published_at": "2021-06-26T13:15:32Z",
      "updated_at": "2021-06-26T13:15:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's real-time profiling for Java using Java Flight Recorder (JFR) metrics, you can run continuous, always-on profiling of your Java code in production environments. The accompanying JVM cluster timeline view provides a fast and intuitive way to diagnose cluster-wide performance problems. For example, you can quickly see how an applications deployment affects the overall health of the cluster. Find performance bottlenecks Troubleshooting performance bottlenecks in your Java application or service can help you better understand the following: Where youre wasting resources When an incident occurs What happened during an incident What performance issues led up to an incident To make troubleshooting faster and easier, you need to see the high fidelity runtime characteristics of your code running on the JVM, and you need that data in real time. JFR daemon The JFR daemon is an exporter for JFR events which allows you to harness the power of the New Relic One platform for visualizing your JVM's behavior. Using the New Relic Java telemetry SDK as the underlying implementation, the JFR daemon converts JFR events into New Relic telemetry types and reports them to New Relic's metric and event ingest APIs. There are three different usage scenarios for the JFR daemon: New Relic Java agent JFR service (RECOMMENDED) - JFR monitoring built into the flagship New Relic Java agent. No additional setup is needed, simply install the Java agent, make sure that the JFR service is enabled, and JFR data will flow into the same APM application as the Java agent. Requires New Relic Java agent version 7.0.0+. Standalone process - Run the jfr-daemon as a standalone process and configure it to monitor an existing Java process using remote JMX. Standalone Java agent - Attach the jfr-daemon to your Java process as a Java agent. A lightweight alternative to the New Relic Java agent. Supported Java versions While the JFR daemon supports any version of Java 11 and above as well as specific versions of Java 8 (specifically version 8u262+), we don't recommend using any non-LTS version of Java in production environments. See supported Java versions for full details. Requirements and Usage Instructions Requirements and instructions vary for each usage scenario. For full details please see the links for your specific usage scenario: New Relic Java agent JFR service - Requirements, configuration, and usage instructions Standalone process - Requirements, configuration, and usage instructions Standalone Java agent - Requirements, configuration, and usage instructions Important Apps running with the JFR daemon should expect the JFR subsystem to use additional memory. View your data To view your data, go to one.newrelic.com > Explorer > (select service) > More Views > Realtime Profiling Java. Understand JVM cluster behavior over time The JVM cluster timeline view shows the JVM behavior across your entire cluster. This timeline enables quicker troubleshooting and issue detection; for example, at a glance you can see: How a recent deployment affected the rest of the JVM cluster When a JVM restarted How an individual instance was affected by its noisy neighbor To make troubleshooting easier, you need to see the high fidelity runtime characteristics of your code running on the JVM, and you need that data in real time. one.newrelic.com > Explorer > (select service) > Realtime Profiling Java: The JVM cluster timeline view shows the JVM behavior across the cluster. Each row of the timeline represents a specific JVM over time. Inside each row, a box represents a 5-minute period of that JVMs life. From least severe to most severe, yellow, orange, and red traffic lights indicate anomalous behavior for a JVM, so you can drill down into that instance and the right time period when investigating errors or other performance issues. Select How is JVM health determined? for a detailed breakdown of how JVM health is calculated. JVM details The details panel for each JVM provides several critical views: How resources are allocated within a process How garbage collection affects performance How to track garbage collection with logs How CPU is used one.newrelic.com > Explorer > (select service) > Realtime Profiling Java > (select JVM): You can see details for each JVM. Identify resource intensive code paths using flamegraphs Important The flamegraph feature is only compatible with the New Relic Java agent JFR service usage scenario. Use flamegraphs to identify the Java classes and methods that are most frequently executed in your application code. By using flamegraphs to optimize the hot spots in your code you can reduce resource consumption and increase your applications overall performance. one.newrelic.com > Explorer > (select service) > Realtime Profiling Java > (select JVM): You can see details for each JVM, including flamegraphs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 27.624863,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Agents</em>",
        "body": "&#x27;s metric and event ingest APIs. There are three different usage scenarios for the JFR daemon: New Relic Java <em>agent</em> JFR service (RECOMMENDED) - JFR monitoring built into the flagship New Relic Java <em>agent</em>. No additional setup is needed, simply install the Java <em>agent</em>, make sure that the JFR service"
      },
      "id": "603e9db2e7b9d2a5432a07f5"
    },
    {
      "sections": [
        "How New Relic distributed tracing works",
        "Tip",
        "Trace sampling",
        "Head-based sampling (standard distributed tracing)",
        "Language agents: adaptive sampling",
        "Language agents: limits and sampling",
        "Trace rate limiting",
        "Lambda trace sampling",
        "Tail-based sampling (Infinite Tracing)",
        "Architecture",
        "Tail-based sampling algorithms",
        "No sampling",
        "Browser and Mobile trace reporting",
        "Trace API",
        "How trace data is structured",
        "How trace data is stored",
        "How trace context is passed between applications",
        "Important",
        "Scenario 1: Trace touching three agent types",
        "Scenario 2: Trace with W3C New Relic and middleware",
        "Scenario 3: Trace with any W3C-compliant agent and a New Relic agent."
      ],
      "title": "How New Relic distributed tracing works",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "4dbe0119017f78ad4db2a2b8a9ca2d287222753a",
      "image": "https://docs.newrelic.com/static/406c9f3af4012dab16df681c8feab256/c1b63/new-relic-distributed-tracing-trace-structure.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/how-new-relic-distributed-tracing-works/",
      "published_at": "2021-06-25T20:20:18Z",
      "updated_at": "2021-06-25T20:20:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Here are some technical details about how New Relic distributed tracing works: How trace sampling works How we structure trace data How we store trace data How trace context is passed between applications Tip For instructions about setting up distributed tracing, see Overview: Enable distributed tracing. Trace sampling How your traces are sampled will depend on your setup and the New Relic tracing tool you're using. For example, you may be using a third-party telemetry service (like OpenTelemetry) to implement sampling of traces before your data gets to us or, if you're using Infinite Tracing, you'd probably send us all your trace data and use our sampling. We have a few sampling strategies available: Head-based sampling (standard distributed tracing) Tail-based sampling (Infinite Tracing) No sampling Head-based sampling (standard distributed tracing) With the exception of our Infinite Tracing feature, most of our tracing tools use a head-based sampling approach. This applies filters to individual spans before all spans in a trace arrive, which means decisions about whether to accept spans is made at the beginning (the \"head\") of the filtering process. We use this sampling strategy to capture a representative sample of activity while avoiding storage and performance issues. Here are some details about how head-based sampling is implemented in our standard distributed tracing tools: Language agents: adaptive sampling Our APM language agents use adaptive sampling to capture a representative sample of system activity. The following is an explanation of how adaptive sampling works. For the first service in a distributed trace, 10 requests are chosen to be sampled. The throughput to that service is used to adjust how frequently requests are sampled. This is explained in more detail below. The first service we monitor in a distributed trace is called the trace origin. The trace origin chooses requests at random to be traced. That decision propagates to the downstream services touched by that request. When the request has completed, all of the spans touched by that request that we've detected are made available in the UI as a complete trace (though agent limits may result in fragmented traces). APM agents have a limit on the number of transactions collected per minute (this can vary, depending on agent) and a limit on the number of spans collected per minute (1000 per agent instance). To adhere to these limits, the default number of traces at the trace origin is 10 traces per minute. An APM agent spreads out the collection of these 10 traces over a minute in order to get a representative sample over time. The exact sampling rate depends on the number of transactions in the previous minute. The rate responds to changes in transaction throughput, going up or down. For example, if the previous minute had 100 transactions, the agent would anticipate a similar number of transactions and select 1 out of every 10 transactions to be traced. Language agents: limits and sampling An APM language agent instance using head-based sampling has a limit of 1000 spans per minute. The agent attempts to keep all spans that are marked to be sampled as part of a distributed trace. In many distributed systems, the average microservice may generate 10 to 20 spans per request. In those cases, the agent span limit can accommodate all spans chosen, and that service will have full detail in a trace. However, some requests to services will generate many spans, and the agent span limit will be reached. As a result, some traces will not have full detail for that service. One solution to this would be to custom instrument an agent to report less activity and therefore report fewer spans. To read about how browser monitoring of trace data may vary from our language agents, see Browser traces. Trace rate limiting If the above sampling methods still result in too much trace data, we may limit incoming data by sampling traces after they're received. By making this decision at the trace level, it avoids fragmenting traces (accepting only part of a trace). This process works similarly to adaptive sampling. The total spans received in a minute are totaled. If too many spans are received, fewer spans may be accepted in the following minute, in order to achieve a floating-average throughput rate. Lambda trace sampling Our AWS Lambda monitoring uses its own sampling process. Tail-based sampling (Infinite Tracing) Our Infinite Tracing feature uses a tail-based sampling approach. \"Tail-based sampling\" means that trace-retention decisions are done at the tail end of processing after all the spans in a trace have arrived. With Infinite Tracing, you can send us 100% of your trace data from your application or third-party telemetry service, and Infinite Tracing will figure out which trace data is most important. And you can configure the sampling to ensure the traces important to you are retained. Architecture For Infinite Tracing, agents or integrations send 100% of all instrumented spans to a trace observer. The trace observer is a distributed tracing service residing in a cluster of services on AWS called New Relic Edge. Tip Only your spans go to the trace observerall other data such as metrics, custom events, and transaction traces are sent the normal route to New Relic and are subject to local sampling. You configure a unique trace observer endpoint for the AWS region you want to send data to. You can request multiple endpoints, one per AWS region. The endpoint represents a trace observer for a particular workload. For example, all spans from a single trace (request) must go to that endpoint. Here are two architectural diagrams: one showing how data flows if you use APM agents and another if you use New Relic integrations like OpenTelemetry exporters: The trace observer holds traces open while spans for that trace arrive. Once the first span in a trace arrives, a session is kept open for 10 seconds. Each time a new span for that trace arrives, the expiration time is reset to 10 seconds. Traces that haven't seen a span arrive within the last 10 seconds will automatically expire. Tail-based sampling algorithms By default, each trace observer offers traces to three samplers: one looking for duration outliers, one looking for traces with errors, and one trying to randomly sample across all trace types. Each sampler keeps a target percentage of traces that match their criteria. Here are details about each sampler: Sampler Matching criteria Target percent Duration Traces with an outlier duration, using two algorithms: Gaussian (Assumes a normal distribution and a threshold at the 99th percentile) Eccentricity (Assumes no distribution and a threshold based on cluster) 100% Error Traces having at least one span with an error 100% Random All traces 1% (This is configurable. See Infinite Tracing: Random trace filter) If the matching criteria matches the trace, each sampler looks at the traces shape. A traces shape is the unique combination of the root spans entity name and span name. This is a simple way to separate traces using the entry point of the request. Once the shape is determined, the sampler makes a decision to keep or reject the trace based on its target sampling percent. If its 100%, the trace is automatically kept. If its anything less, the probability the sampler keeps a given trace is determined by the target percent. For example, the default target percent is 1 for random traces, so 1% of those traces are kept. If you prefer, you can change the random filter percentage. Because the trace observer uses percentages of throughput, the number of traces selected will vary with that throughput. No sampling Some of our tools don't use sampling. Sampling details for these tools: Browser and Mobile trace reporting Browser monitoring distributed tracing and Mobile monitoring report all spans. Our APM language agents are often used in conjunction with browser and mobile monitoring, and our language agents use sampling. This means that there will likely be many more browser and mobile spans than back-end spans, which can result in browser and mobile app spans disconnected from back-end spans. For tips on querying for traces that contain front and back-end spans, see Find browser span data. Trace API If you don't have Infinite Tracing enabled, our Trace API does no sampling (unless the default data limits are exceeded). It's expected that you set up the Trace API to send us the traces you think are important. How trace data is structured Understanding the structure of a distributed trace can help you: Understand how traces are displayed in our UI Help you query trace data A distributed trace has a tree-like structure, with \"child\" spans that refer to one \"parent\" span. This diagram shows some important span relationships in a trace: This diagram shows how spans in a distributed trace relate to each other. This diagram shows several important concepts: Trace root. The first service or process in a trace is referred to as the root service or process. Process boundaries. A process represents the execution of a logical piece of code. Examples of a process include a backend service or Lambda function. Spans within a process are categorized as one of the following: Entry span: the first span in a process. Exit span: a span is a considered an exit span if it a) is the parent of an entry span, or b) has http. or db. attributes and therefore represents an external call. In-process span: a span that represents an internal method call or function and that is not an exit or entry span. Client spans. A client span represents a call to another entity or external dependency. Currently, there are two client span types: Datastore. If a client span has any attributes prefixed with db. (like db.statement), it's categorized as a datastore span. External. If a client span has any attributes prefixed with http. (like http.url) or has a child span in another process, it's categorized as an external span. This is a general category for any external calls that are not datastore queries. Trace duration. A trace's total duration is determined by the length of time from the start of the earliest span to the completion of the last span. You can query span relationship data with the NerdGraph GraphiQL explorer at api.newrelic.com/graphiql. How trace data is stored Understanding how we store trace data can help you query your trace data. We save trace data as: Span: A span represents operations that are part of a distributed trace. The operations that a span can represent include browser-side interactions, datastore queries, calls to other services, method-level timing, and Lambda functions. One example: in an HTTP service, a span is created at the start of an HTTP request and completed when the HTTP server returns a response. Span attributes contain important information about that operation (such as duration, host data, etc.), including trace-relationship details (such as traceId, guid). For span-related data, see span attributes. Transaction: If an entity in a trace is monitored by an agent, a request to that entity generates a single Transaction event. Transactions allow trace data to be tied to other New Relic features. For transaction-related data, see transaction attributes. Contextual metadata. We store metadata that shows calculations about a trace and the relationships between its spans. To query this data, use the NerdGraph GraphiQL explorer. How trace context is passed between applications We support the W3C Trace Context standard, which makes it easier to trace transactions across networks and services. When you enable distributed tracing, New Relic agents add HTTP headers to a service's outbound requests. HTTP headers act like passports on an international trip: They identify your software traces and carry important information as they travel through various networks, processes, and security systems. The headers also contain information that helps us link the spans together later: metadata like the trace ID, span ID, the New Relic account ID, and sampling information. See the table below for more details on the header: Item Description accountId This is your New Relic account ID. However, only those on your account and New Relic Admins can associate this Id with your account information in any way. appId This is the application ID of the application generating the trace header. Much like accountId, this identifier is not going to provide any information unless you're a user on the account. guid With Distributed Tracing, each segment of work in a trace is represented by a span, and each span has a guid attribute. The guid of the last span within the process is sent with the outgoing request so that the first segment of work in the receiving service can add this guid as the parentId attribute which connects data within the trace. Parent type The source of the trace header, as in Mobile, Browser, Ruby app, etc. This becomes the parent.type attribute on the Transaction triggered by the request this header is attached to. Priority A randomly generated priority ranking value that helps determine which data is sampled when sampling limits are reached. This is a float value set by the first New Relic agent thats part of the request so all data in the trace will have the same priority value. Sampled A boolean value that tells the agent if traced data should be collected for the request. This is also added as an attribute on any span and transaction data collected. If you want to read more about this sampling process, this guide goes into more detail. Timestamp Unix timestamp in milliseconds when the payload was created. traceId The unique ID (a randomly generated string) used to identify a single request as it crosses inter- and intra- process boundaries. This ID allows the linking of spans in a distributed trace. This also is added as an attribute on the span and transaction data. transactionId The unique identifier for the transaction event. Trusted acount key This is a key that helps identify any other accounts associated with your account. So if you have multiple sub-accounts that the trace crosses, we can confirm that any data included in the trace is coming from a trusted source, and tells us what users should have access to the data. Version and data key This identifies major/minor versions, so if an agent receives a trace header from a version with breaking changes from the one it is on, it can reject that header and report the rejection and reason. This header information is passed along each span of a trace, unless the progress is stopped by something like middleware or agents that don't recognize the header format (see Figure 1). Figure 1 To address the problem of header propagation, we support the W3C Trace Context specification that requires two standardized headers. Our latest W3C New Relic agents send and receive these two required headers, and by default, they also send and receive the header of the prior New Relic agent: W3C (traceparent): The primary header that identifies the entire trace (trace ID) and the calling service (span id). W3C (tracestate): A required header that carries vendor-specific information and tracks where a trace has been. New Relic (newrelic): The original, proprietary header that is still sent to maintain backward compatibility with prior New Relic agents. This combination of three headers allows traces to be propagated across services instrumented with these types of agents: W3C New Relic agents Non-W3C New Relic agents W3C Trace Context-compatible agents Important If your requests only touch W3C Trace Context-compatible agents, you can opt to turn off the New Relic header. See the agent configuration documentation for details about turning off the newrelic header. The scenarios below show various types of successful header propagation. Scenario 1: Trace touching three agent types This shows the flow of headers when a request touches three different agent types: Scenario 2: Trace with W3C New Relic and middleware This shows the combination of headers sent by a W3C New Relic agent to some middleware. Scenario 3: Trace with any W3C-compliant agent and a New Relic agent. This shows the two required W3C headers from another vendor accepted by a W3C New Relic agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 25.942543,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Language <em>agents</em>: adaptive sampling",
        "body": " by that request that we&#x27;ve detected are made available in the UI as a complete trace (though <em>agent</em> limits may result in fragmented traces). APM <em>agents</em> have a limit on the number of transactions collected per minute (this can vary, depending on <em>agent</em>) and a limit on the number of spans collected per minute (1000"
      },
      "id": "6072a66664441f14089d856c"
    }
  ],
  "/docs/integrations/open-source-telemetry-integrations/troubleshooting/troubleshoot-opentelemetry-exporter": [
    {
      "sections": [
        "View your OpenTelemetry data in New Relic",
        "Explorer: Get the big picture along with the details",
        "Summary page",
        "Distributed tracing",
        "Tip",
        "Transactions",
        "Databases",
        "Externals",
        "Errors",
        "Logs",
        "Metrics explorer",
        "Data explorer and query builder"
      ],
      "title": "View your OpenTelemetry data in New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "a5213cb2206f4c161dd97c015a7c6679b08e867b",
      "image": "https://docs.newrelic.com/static/2f8a3baa6793edf958ecb9db5346efa7/c1b63/explorer_otel_services.png",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/view-your-opentelemetry-data-new-relic/",
      "published_at": "2021-06-26T04:31:11Z",
      "updated_at": "2021-06-20T10:17:06Z",
      "document_type": "page",
      "popularity": 1,
      "body": "After you import OpenTelemetry data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about your applications, as well as detailed information, such as distributed tracing. To get started with the explorer: Go to one.newrelic.com and click Explorer. In the left sidebar, click Services - OpenTelemetry: Click the service you want to know more about. If you need help understanding the data, see the explanations which follow. Summary page The opening page of the explorer is the Summary page listing various golden signals about your entity. Golden signals are key monitoring details such as response time, throughput, and error rate. By using this information, you can quickly decide if you need to dig deeper. Distributed tracing When you access distributed tracing through the explorer, you are looking at traces that include that service. Once youre in that service, you can filter spans to find the ones you want. For example, to query service.name or trace.id, you can use the following: service.name = YOUR_SERVICE_NAME trace.id = YOUR_TRACE_ID For more ways to filter and analyze your spans, see our distributed tracing UI page. Tip If you prefer to search traces across all New Relic accounts in your organization, you can go outside explorer: one.newrelic.com > Apps > Favorites > Distributed tracing. Transactions Use Transactions to identify slow or error transactions that might be causing a spike in your application's response time. To get a list of transactions: From the Transaction Summary page, select the transactions table. Databases The Databases page shows an application's database and cache data. The page shows individual database transactions as a sortable table, and shows operations, throughput, and response time as charts. Externals Externals capture calls to out-of-process services such as web services, resources in the cloud, and other network entities. Errors On the Errors page, you can see total errors as well as charts showing error count and error rate. Logs The Logs page displays logs from your application. For more information about how to associate log data to your application in New Relic, see our OpenTelemetry and logging documentation. Metrics explorer For selected OpenTelemetry languages, you can see information about your metrics in this section. Also, if you are using the Prometheus exporter with OpenTelemetry, you can view your metric data here. Data explorer and query builder Explore your metrics and traces using the data explorer, or write your own queries in query builder using NRQL. For more on how to query your data once it's in New Relic, see Query your data and Introduction to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 283.11392,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "View your <em>OpenTelemetry</em> data in New Relic",
        "sections": "View your <em>OpenTelemetry</em> data in New Relic",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "After you import <em>OpenTelemetry</em> data into New Relic, you can use a variety of tools to analyze it. Take a look at these UI options: Explorer Data explorer and query builder Explorer: Get the big picture along with the details The New Relic explorer is a good place to get overview information about"
      },
      "id": "6044e5dfe7b9d283d3579a04"
    },
    {
      "sections": [
        "OpenTelemetry and logging",
        "Send logs to New Relic",
        "Application log correlation",
        "View OpenTelemetry logs"
      ],
      "title": "OpenTelemetry and logging",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "60c75872ae9add85b19e377a75d539a46b04e372",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-logs/",
      "published_at": "2021-06-26T14:26:00Z",
      "updated_at": "2021-06-19T23:21:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Logs are one of the core data types in OpenTelemetry. They may represent application logs, machine generated events, or system logs. Our OpenTelemetry log data model in GitHub describes them in detail. Let's look at how to send logs, correlate them with applications, and view them in New Relic. Send logs to New Relic The New Relic Exporter for the OpenTelemetry Collector supports exporting logs to New Relic. The general pattern is to configure the collector to: Receive logs from any of the log receivers. Some of the receiver options include Filelog Receiver, Fluent Forward Receiver, and Syslog Receiver. Process logs, potentially annotating them with resource information. Some of the processor options include Resource Detection Processor and Resource Processor. Export logs via the New Relic Exporter. Application log correlation Application logs are more useful if they're correlated with other telemetry data produced by the application. The OpenTelemetry semantic convention for services specifies service.name as a required field. All application metric, trace, and log data sent to New Relic with the same service.name are associated with the same entity. The specifics of how logs get annotated with the service.name resource attribute depends on the application's environment: Applications may produce structured JSON logs, which you can configure to include service.name as another field. You can deploy applications alongside a dedicated Collector Agent instance, which you can configure with a Resource Processor to annotate logs with the service.name attribute. Optionally, additional application trace context (sometimes called execution context) can be propagated to log messages. The setup and availability of this depends on the language and logging framework used by the application. The general strategy is to set up the application to write structured JSON logs and to configure it to extract trace context into specified trace context fields on available log messages. The Logs in Context with Log4j2 example in GitHub demonstrates an end-to-end working example for a simple Java application using Log4j2. View OpenTelemetry logs Here are two ways you can view logs: Look in the New Relic Logs UI. If your logs are correlated with an application, view them in the context of the application.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 281.23782,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em> and logging",
        "sections": "<em>OpenTelemetry</em> and logging",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": "Logs are one of the core data types in <em>OpenTelemetry</em>. They may represent application logs, machine generated events, or system logs. Our <em>OpenTelemetry</em> log data model in GitHub describes them in detail. Let&#x27;s look at how to send logs, correlate them with applications, and view them in New Relic"
      },
      "id": "60ce7bf528ccbc384271b463"
    },
    {
      "sections": [
        "OpenTelemetry: Endpoint configuration",
        "EU region",
        "Tip",
        "Infinite Tracing"
      ],
      "title": "OpenTelemetry: Endpoint configuration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Open source telemetry integrations",
        "OpenTelemetry"
      ],
      "external_id": "770eb7e4d89b3e05ef34c3f8a4b7d731cba14b33",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/open-source-telemetry-integrations/opentelemetry/opentelemetry-endpoints/",
      "published_at": "2021-06-26T14:25:58Z",
      "updated_at": "2021-06-19T23:47:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can change the New Relic endpoints where you send your data. By default, New Relic OpenTelemetry exporters send data to these US data centers: Spans: https://trace-api.newrelic.com/trace/v1 Metrics: https://metric-api.newrelic.com/metric/v1 You may need to override these default endpoints to send data to the EU region or to use Infinite Tracing. EU region To send telemetry data to New Relics endpoints in the EU region, use the following: Tip These URLs don't apply to Infinite Tracing Spans: https://trace-api.eu.newrelic.com/trace/v1 Metrics: https://metric-api.eu.newrelic.com/trace/v1 Infinite Tracing If you are setting up Infinite Tracing, you need to override the default span endpoint and send telemetry data to the New Relic trace observer: Follow the steps in Set up the trace observer to get the value for YOUR_TRACE_OBSERVER_URL. Use the value of YOUR_TRACE_OBSERVER_URL to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your OpenTelemetry integrations use the AlwaysOn sampler.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 279.46436,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>OpenTelemetry</em>: Endpoint configuration",
        "sections": "<em>OpenTelemetry</em>: Endpoint configuration",
        "tags": "<em>Open</em> <em>source</em> <em>telemetry</em> <em>integrations</em>",
        "body": " to configure your integration. Since you want New Relic to analyze all your traces, make sure to verify that your <em>OpenTelemetry</em> <em>integrations</em> use the AlwaysOn sampler."
      },
      "id": "60ce822964441f4ff391f8dd"
    }
  ],
  "/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic": [
    {
      "sections": [
        "Use integration data in New Relic dashboards",
        "Get started with integration data",
        "Example NRQL queries",
        "AWS EBS query example",
        "Azure Service Bus query example",
        "Azure Functions query example",
        "Azure VMs query example",
        "NGINX query example",
        "MySQL query example",
        "Inventory change query example",
        "Tip",
        "Tips for using different data types",
        "Metric data tips",
        "Event data tips",
        "Inventory data tips"
      ],
      "title": "Use integration data in New Relic dashboards",
      "type": "docs",
      "tags": [
        "Integrations",
        "Infrastructure integrations",
        "Get started"
      ],
      "external_id": "b236b0fae29853de085d0430fdec27fba74c15d4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/infrastructure-integrations/get-started/use-integration-data-new-relic-dashboards/",
      "published_at": "2021-06-26T04:26:52Z",
      "updated_at": "2021-06-20T19:34:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Most data generated by integrations is available in New Relic One dashboards, where you can query your data using NRQL and build custom dashboards. The following tips and sample queries were created for New Relic-built integrations, but most will also apply to integrations built with the Integrations SDK. For a general look at how to find and use integration data, see New Relic data types. Get started with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com > Infrastructure > Third-party services page, select an integration dashboard. There, you can view the NRQL queries that generated a chart. For examples of NRQL queries for integration data, see the example queries. Use the data explorer or the dashboards to explore and understand the available data. Read the documentation for a specific integration to learn about the reported data. When you create a useful query you'd like to add to your Insights dashboard, select Add to dashboard. Example NRQL queries Here are some examples of NRQL queries that use integration data: AWS EBS query example Here's a NRQL query for the AWS EBS service, showing the total write time metric, faceted by entityName: SELECT sum('provider.volumeTotalWriteTime.Sum') FROM BlockDeviceSample WHERE provider = 'EbsVolume' FACET entityName Copy Azure Service Bus query example Here's an Insights NRQL query for the maximum number of messages in an Azure Service Bus topic queue, faceted by resource group: SELECT max(activeMessages.Maximum) FROM AzureServiceBusTopicSample FACET resourceGroupName Copy Azure Functions query example Here's an Insights NRQL query for Azure Functions, showing the count of executed functions over the past six hours by region over time: SELECT sum(functionExecutionCount.Total) FROM AzureFunctionsAppSample FACET regionName TIMESERIES SINCE 6 hours ago Copy Azure VMs query example Here's an Insights NRQL query for Azure VMs that compares the count of VM events over the past thirty minutes with the same time a week ago: SELECT uniqueCount(vMName) FROM AzureVirtualMachineScaleSetSample FACET name SINCE 30 minutes ago COMPARE WITH 1 week ago Copy NGINX query example Here's an example of a query that you might run on your NGINX integration data and place in an Insights dashboard. This query creates a chart showing the average value of NGINX requests per second over time: SELECT average(net.requestsPerSecond) FROM NginxSample TIMESERIES Copy For more on how to create queries, see NRQL syntax. MySQL query example Here's an example of a query that you might run on your MySQL integration data. This query generates a chart showing the maximum number of used MySQL connections: SELECT max(net.maxUsedConnections) FROM MysqlSample Copy For more on how to create queries, see NRQL syntax. Inventory change query example Here's an example of a query that groups inventory change events from the last day by the type of change: SELECT count(*) FROM InfrastructureEvent WHERE format='inventoryChange' FACET changeType SINCE 1 DAY AGO Copy Tip You can also perform these queries using dimensional metrics. Tips for using different data types Integrations can generate metric, event, and inventory data, all of which are available for querying in New Relic Insights. Here are some tips for using the different types of integration data in Insights: Metric data tips Tips for finding and using integration metric data in Insights: All integration data is attached to a data type known as an event (not to be confused with events reported by integrations, which represent important activity in your host/service). This means that all integration data can be found via the data explorer. For more about these two basic New Relic data types, see New Relic data collection. Metric values are treated as attributes: key-value pairs attached to an event. For example, the MySQL integration has an 'active connections' metric; this would be found by querying the connectionsActive attribute of the MysqlSample event. For general information about metrics, see Integration metric data. Event data tips Here are some tips for finding and using integration event data in Insights: Most integration events are inventory change events. When inventory is changed, it generates an InfrastructureEvent with a format value of inventoryChange. Integration data can be found via the data explorer. For general information about events, see Event data. Inventory data tips Here are some tips for finding and using integration inventory data in Insights: For general information about inventory data, see Integration inventory data. Some inventory data is added as attributes (key-value pairs) to Insights data. For example, the AWS EC2 integration collects awsRegion as inventory data; this would be found in Insights by querying the awsRegion attribute of the ComputeSample event type and provider Ec2Instance. When inventory data changes, an Insights InfrastructureEvent event type is generated with a format value of inventoryChange. See the query examples for an example of querying this data. For more on using NRQL queries, see Intro to NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 92.429276,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Use <em>integration</em> data in New Relic dashboards",
        "sections": "<em>Get</em> <em>started</em> with <em>integration</em> data",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " with the <em>Integrations</em> SDK. For a general look at how to find and use integration data, see New Relic data types. <em>Get</em> <em>started</em> with integration data Here are some tips for finding and exploring your integration data in New Relic: From the one.newrelic.com &gt; Infrastructure &gt; Third-party services page, select"
      },
      "id": "60450a39196a67d7dc960f7c"
    },
    {
      "sections": [
        "Activate Azure integrations",
        "Tip",
        "Requirements",
        "Step 1: Get Azure subscription and tenant IDs",
        "Step 2: Register your app and get ID",
        "Step 3: Create a client secret in Azure",
        "Step 4: Provide permissions to services",
        "Step 5: Add app to New Relic",
        "Update application details and rotate client secrets",
        "Explore app data in New Relic Infrastructure's UI"
      ],
      "title": "Activate Azure integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Microsoft Azure integrations",
        "Get started"
      ],
      "external_id": "f65679179e13aa1b503b4b95010e296cbe269c29",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/microsoft-azure-integrations/get-started/activate-azure-integrations/",
      "published_at": "2021-06-26T00:13:37Z",
      "updated_at": "2021-06-20T12:42:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations allow you to report data from specific systems and supplement infrastructure's default, automatic monitoring. The Microsoft Azure integrations report data from various Azure platform services to your New Relic account. This document explains how to activate Azure integrations. Tip To use Azure integrations and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirements The Azure integration activation process requires you to: Create a New Relic application and key in Azure. Grant this application access to the Azure services you want to monitor. Place required information in the New Relic's Integrations UI. To use these integration activation instructions directly from the Infrastructure UI, go to one.newrelic.com > Infrastructure > Azure > Add an Azure account. Step 1: Get Azure subscription and tenant IDs To get your Azure account's subscription id and tenantId, use your local terminal if you have Azure's tools installed, or use Azure's Cloud Shell terminal in the Azure portal. Open a terminal with access to your Azure account. Type the following: az account show Copy Copy and save the subscription id and tenantID from the output response for later use. The response should look similar to the response below. The subscription id and tenantID are highlighted. @Azure:~$ az account show { \"environmentName\": \"AzureCloud\", \"id\": \"9ffe9512-f4a2-42dd-1230-518aec34be21\" , \"isDefault\": true, \"name\": \"Beyond Team Sandbox\", \"state\": \"Enabled\", \"tenantId\": \"ac6692da-1231-422f-22a8-9eed6dbe83f1\" , \"user\": { \"name\": \"youremail@domain\", \"type\": \"user\" } Copy Step 2: Register your app and get ID You must have Azure permissions to register your application and copy its Application ID. To register your app in Azure: Sign in to the Azure portal and go to the Azure Active Directory. From Manage, select App registrations > New registration. Enter a name for the application. We recommend that you name your app NewRelic-Integrations. In Redirect URI select Weband add https://www.newrelic.com as the sign-on URI. Create the application by clicking Register. From the Overview of your app, copy the Application (client) ID, and save it for later use. Step 3: Create a client secret in Azure To create a client secret associated with your application: In Azure, under the application you've just created, select Certificates & secrets. Under Client secrets, click on New client secret and then on Add. Copy the value of Client Secret and save it for later use. Step 4: Provide permissions to services Your app must provide Reader permissions for each Azure service you want New Relic to monitor: In the Azure Subscriptions section, select the subscriptions that you want New Relic to monitor. Select Access control (IAM) > Add > Add role assignment. From the Role dropdown, select Reader. From the Select dropdown, select the app's name; for example, NewRelic-Integrations. From Selected members, verify your app name appears, then select Save. Some Azure services, including Azure CosmosDB and Azure VMs, require additional steps. See the Azure integration documentation for the services you want to enable. Step 5: Add app to New Relic Now you can activate the Azure integration in the Infrastructure UI. The UI will require the information you have saved in the previous steps, including: Your Azure account's subscription id and tenantId The application's application ID The application's client secret To add your Azure app to New Relic: Go to one.newrelic.com > Infrastructure > Azure and select the Azure Service you wish to add. Follow the steps in the UI to activate the integration in New Relic. If you have already completed the Azure account steps, skip to the end of the steps to fill out the form. (For Azure account name, enter the name you want to use to identify the account in your Integrations dashboard.) Update application details and rotate client secrets It's possible to update the application's name and authentication credentials using the Infrastructure UI or the Cloud Integrations API at any time. Follow these steps to rotate the Azure client secret in the Infratructure UI: Go to one.newrelic.com > Infrastructure > Azure and click on Manage Services on the Azure account you wish to edit. Select the edit action next to Account Name to see and edit any application value. Edit the Client Secret field with the new value and confirm with Save Changes. Explore app data in New Relic Infrastructure's UI After you activate an Azure integration, New Relic will start monitoring your Azure data at regular polling intervals. To find and use your data, use the data explorer or go to one.newrelic.com > Infrastructure > Azure",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 90.10809,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Activate Azure <em>integrations</em>",
        "sections": "Activate Azure <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": "New Relic infrastructure <em>integrations</em> allow you to report data from specific systems and supplement infrastructure&#x27;s default, automatic monitoring. The Microsoft Azure <em>integrations</em> report data from various Azure platform services to your New Relic account. This document explains how to activate"
      },
      "id": "6044e5a9196a671bfa960f79"
    },
    {
      "sections": [
        "Introduction to New Relic integrations",
        "Tip",
        "Choose what's right for you",
        "Create your own solutions"
      ],
      "title": "Introduction to New Relic integrations",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Instrument everything",
        "Get started"
      ],
      "external_id": "03217983a29af22737c1163da9ef0811b29c2bcd",
      "image": "",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/instrument-everything/get-started-new-relic-instrumentation/introduction-new-relic-integrations/",
      "published_at": "2021-06-25T20:09:45Z",
      "updated_at": "2021-03-16T07:30:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We provide hundreds of solutions to get your data into New Relic so you can analyze the data in one place. They give you a steady flow of useful data to fix problems quickly, maintain complex systems, improve your code, and accelerate your digital transformation. You can bring in data from hundreds of applications, frameworks, services, operating systems, and other technologies. Our integrations gather the data, and the agents send it to New Relic. The solution you need may require you to install both an integration and an agent. In some cases, you can just install our agents that contain integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to get you started. Tip To use integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Choose what's right for you We offer a wide range of solutions so you can easily collect data across your environment. You may only need one of our solutions to get the data you need, or you can choose a variety of options to capture a broader range of data types. Go to New Relic Integrations to find solutions that fit your environment. Here is a sample of what youll find there: Application performance monitoring (APM): C, Go, Java, Node, .NET, PHP, Python, and Ruby Mobile apps: Android and iOS Browser monitoring: Google Chrome, Mozilla Firefox, Microsoft Internet Explorer, and Apple Safari Host monitoring: Linux and Microsoft Windows Cloud platform monitoring: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry integrations: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you create your own: Use New Relic Flex to create lightweight monitoring solutions using infrastructure monitoring. Use New Relic Telemetry SDKs to build custom solutions for sending metrics, traces, and more. Build your own New Relic One applications that you can share with your colleagues, or edit open source applications in our catalog.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 84.09971,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to New Relic <em>integrations</em>",
        "sections": "Introduction to New Relic <em>integrations</em>",
        "tags": "<em>Get</em> <em>started</em>",
        "body": " <em>integrations</em>, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to <em>get</em> you <em>started</em>. Tip To use <em>integrations</em> and infrastructure monitoring, as well as the rest of our"
      },
      "id": "603e817f28ccbc4857eba798"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Tip",
        "Set up the integration",
        "Mapping of Prometheus metric types",
        "Override metric type mappings",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-06-26T04:33:14Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Tip To use Prometheus integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Mapping of Prometheus metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts or sub-accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 191.07094,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add the <em>remote</em> <em>write</em> URL to your Helm values.yaml file. Replace <em>remoteWrite</em>: [] with two lines similar to the following"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "0d190be5dc4fd91ce6bbcef7343d01f75670ca51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2021-06-26T04:32:06Z",
      "updated_at": "2021-03-13T03:46:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query for NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NrIntegrationError query on the error message using NRQL, then look at the Message field in the UI to see a description of what went wrong. For example: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' Copy Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If youve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.626,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em>"
      },
      "id": "6044e65d196a67914a960f6b"
    },
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.38742,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics or <em>remote</em> <em>write</em> <em>integration</em>?",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and <em>configure</em> data to target (see metrics or targets). Balance <em>remote</em> <em>write</em>(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-06-26T14:27:48Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.63503,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-06-25T22:13:55Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.36763,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-06-25T23:17:10Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=clusterName SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.35713,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-rename-or-copy-prometheus-attributes": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-06-26T14:27:48Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.63503,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-06-25T22:13:55Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.36763,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-06-25T23:17:10Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=clusterName SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.35713,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-06-26T14:27:48Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.63501,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-06-25T22:13:55Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.36763,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-06-26T14:27:47Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.97466,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations": [
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-06-25T22:13:55Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.36763,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-06-25T23:17:10Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=clusterName SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.35713,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-06-26T14:27:47Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.97466,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/ignore-or-include-prometheus-metrics": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-06-26T14:27:48Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.63501,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Install, update, or uninstall your Prometheus OpenMetrics integration",
        "Install the integration",
        "Docker installation",
        "Kubernetes installation",
        "Important",
        "Update the integration",
        "Docker update procedures",
        "Kubernetes update procedures",
        "Uninstall"
      ],
      "title": "Install, update, or uninstall your Prometheus OpenMetrics integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "89b53bf5ac9ce6c14663eca2ab44d96cfe897bc8",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration/",
      "published_at": "2021-06-25T22:13:55Z",
      "updated_at": "2021-03-16T06:17:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Prometheus OpenMetrics integration, review the requirements for your environment: Docker requirements Kubernetes requirements Install the integration To install the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker installation To install the New Relic Prometheus OpenMetrics integration in a Docker environment: Create a configuration file config.yaml. Use the example configuration file, or look at the nri-prometheus-latest.yaml manifest file, which includes the nri-prometheus-cfg config map and an example configuration. Required: Add your New Relic license key and a cluster name to identify your Docker container. Add the endpoints to scrape; for example, add the http://localhost:8080/metrics endpoint to collect metrics about the integration itself. Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. For more information, see the metrics filtering documentation. Start the integration in the background: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:1.5 Copy Confirm the container is running properly: docker ps -f \"name=nri-prometheus\" Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Kubernetes installation Important To prevent your data from being duplicated, configure your New Relic Prometheus OpenMetrics integration only with one replica. Running two or more replicas will result in duplicated data. For more information, see the troubleshooting procedures for restarts and gaps in data. To install the New Relic Prometheus OpenMetrics integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https://download.newrelic.com/infrastructure_agent/integrations/kubernetes/nri-prometheus-latest.yaml Copy Edit the nri-prometheus-latest.yaml manifest file: Required: Add your New Relic license key and a cluster name to identify your Kubernetes cluster. env: - name: LICENSE_KEY value: \"<YOUR_LICENSE_KEY>\" [...] config.yaml: | cluster_name: \"<YOUR_CLUSTER_NAME>\" Copy Specify which metrics you want to ignore or include according to the prefixes for the metrics and labels. By default, the New Relic Prometheus OpenMetrics integration uses the same labels as Prometheus to discover targets. For more information, see the metrics filtering documentation. Deploy the integration in your Kubernetes cluster: kubectl apply -f nri-prometheus-latest.yaml Copy To confirm that the deployment has been created successfully, look at the CURRENT replicas in the results generated by this command: kubectl get deployments nri-prometheus Copy Confirm that the integration has been configured correctly: Wait a few minutes, then go to the New Relic UI, and run this NRQL query to see if data has been reported: FROM Metric SELECT count(*) WHERE clusterName = 'YOUR_CLUSTER_NAME' since 1 hour ago Copy Update the integration To update the Prometheus OpenMetrics integration, follow the procedures for Docker or Kubernetes as applicable: Docker update procedures Remove the Docker container. Follow standard installation procedures to start a new Docker container. The integration logs its current version when it starts up. To determine the running version: docker logs nri-prometheus 2>&1 | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Kubernetes update procedures Follow standard installation procedures. Reapply the nri-prometheus-latest.yaml manifest file. The integration logs its version when it starts up. To determine the running version: kubectl logs deploy/nri-prometheus | grep \"Integration version\" Copy Example output: time=\"2019-02-26T09:21:21Z\" level=info msg=\"Starting New Relic's Prometheus OpenMetrics Integration version 1.0.0 \" Copy Uninstall To uninstall the Prometheus OpenMetrics integration for Docker or Kubernetes, execute the following command: Docker: docker rm -f nri-prometheus Copy Kubernetes: kubectl delete -f nri-prometheus-latest.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.36763,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "sections": "<em>Install</em>, update, or uninstall your <em>Prometheus</em> <em>OpenMetrics</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": " information, see the troubleshooting procedures for restarts and gaps in data. To <em>install</em> the New Relic <em>Prometheus</em> <em>OpenMetrics</em> integration in a Kubernetes environment: Download the integration manifest .yaml file: curl -O https:&#x2F;&#x2F;download.newrelic.com&#x2F;infrastructure_agent&#x2F;<em>integrations</em>&#x2F;kubernetes&#x2F;nri"
      },
      "id": "603e8309e7b9d2d69d2a07cc"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-06-25T23:17:10Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=clusterName SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.35713,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-openmetrics/install-update-or-uninstall-your-prometheus-openmetrics-integration": [
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations",
        "Configure nri-prometheus-latest.yaml",
        "Example configuration file",
        "Key names and definitions",
        "Configure objects in target key",
        "Kubernetes port and endpoint path",
        "Example: Labels for Kubernetes port and path",
        "Services and Endpoints scrape behaviour",
        "Reload the configuration",
        "Docker: Run previous config file"
      ],
      "title": "Configure Prometheus OpenMetrics integrations",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "12be9e8bb8c03ca3f0eed948d0bc6e863b60efef",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations/",
      "published_at": "2021-06-26T14:27:48Z",
      "updated_at": "2021-05-11T06:21:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Unless otherwise noted, configuration options for your Prometheus OpenMetrics integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: Configure your New Relic license key as an environment variable named LICENSE_KEY. This provides a more secure environment, as New Relic can load your environment variable from a mutual TLS authentication secret. Configure nri-prometheus-latest.yaml The nri-prometheus-latest.yaml manifest file includes the nri-prometheus-cfg map showing an example configuration. Use the manifest file to configure the following parameters. Example configuration file The following is an example configuration file that you can save and modify to fit your needs. For more information, see the documentation about mutual TLS authentication and translating PromQL to NRQL. # The name of your cluster. It's important to match other New Relic products to relate the data. cluster_name: \"<YOUR_CLUSTER_NAME>\" # When standalone is set to false nri-prometheus requires an infrastructure agent to work and send data. Defaults to true # standalone: true # How often the integration should run. Defaults to 30s. # scrape_duration: \"30s\" # The HTTP client timeout when fetching data from targets. Defaults to 5s. # scrape_timeout: \"5s\" # How old must the entries used for calculating the counters delta be # before the telemetry emitter expires them. Defaults to 5m. # telemetry_emitter_delta_expiration_age: \"5m\" # How often must the telemetry emitter check for expired delta entries. # Defaults to 5m. # telemetry_emitter_delta_expiration_check_interval: \"5m\" # Wether the integration should run in verbose mode or not. Defaults to false. verbose: false # Whether the integration should run in audit mode or not. Defaults to false. # Audit mode logs the uncompressed data sent to New Relic. Use this to log all data sent. # It does not include verbose mode. This can lead to a high log volume, use with care. audit: false # Wether the integration should skip TLS verification or not. Defaults to false. insecure_skip_verify: false # The label used to identify scrapable targets. Defaults to \"prometheus.io/scrape\". scrape_enabled_label: \"prometheus.io/scrape\" # scrape_services Allows to enable scraping the service and not the endpoints behind. # When endpoints are scraped this is no longer needed scrape_services: true # scrape_endpoints Allows to enable scraping directly endpoints instead of services as prometheus service natively does. # Please notice that depending on the number of endpoints behind a service the load can increase considerably scrape_endpoints: false # Whether k8s nodes need to be labelled to be scraped or not. Defaults to true. require_scrape_enabled_label_for_nodes: true # Number of worker threads used for scraping targets. # For large clusters with many (>400) targets, slowly increase until scrape # time falls between the desired `scrape_duration`. # Increasing this value too much will result in huge memory consumption if too # many metrics are being scraped. # Default: 4 # worker_threads: 4 # Maximum number of metrics to keep in memory until a report is triggered. # Changing this value is not recommended unless instructed by the New Relic support team. # max_stored_metrics: 10000 # Minimum amount of time to wait between reports. Cannot be lowered than the default, 200ms. # Changing this value is not recommended unless instructed by the New Relic support team. # min_emitter_harvest_period: 200ms # targets: # - description: Secure etcd example # urls: [\"https://192.168.3.1:2379\", \"https://192.168.3.2:2379\", \"https://192.168.3.3:2379\"] # tls_config: # ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" # cert_file_path: \"/etc/etcd/etcd-client.crt\" # key_file_path: \"/etc/etcd/etcd-client.key\" # Proxy to be used by the emitters when submitting metrics. It should be # in the format [scheme]://[domain]:[port]. # The emitter is the component in charge of sending the scraped metrics. # This proxy won't be used when scraping metrics from the targets. # By default it's empty, meaning that no proxy will be used. # emitter_proxy: \"http://localhost:8888\" # Certificate to add to the root CA that the emitter will use when # verifying server certificates. # If left empty, TLS uses the host's root CA set. # emitter_ca_file: \"/path/to/cert/server.pem\" # Set to true in order to stop autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account # having limited privileges. Defaults to false. # disable_autodiscovery: false # Whether the emitter should skip TLS verification when submitting data. # Defaults to false. # emitter_insecure_skip_verify: false # Histogram support is based on New Relic's guidelines for higher # level metrics abstractions https://github.com/newrelic/newrelic-exporter-specs/blob/master/Guidelines.md. # To better support visualization of this data, percentiles are calculated # based on the histogram metrics and sent to New Relic. # By default, the following percentiles are calculated: 50, 95 and 99. # # percentiles: # - 50 # - 95 # - 99 # transformations: # - description: \"General processing rules\" # rename_attributes: # - metric_prefix: \"\" # attributes: # container_name: \"containerName\" # pod_name: \"podName\" # namespace: \"namespaceName\" # node: \"nodeName\" # container: \"containerName\" # pod: \"podName\" # deployment: \"deploymentName\" # ignore_metrics: # # Ignore all the metrics except the ones listed below. # # This is a list that complements the data retrieved by the New # # Relic Kubernetes Integration, that's why Pods and containers are # # not included, because they are already collected by the # # Kubernetes Integration. # - except: # - kube_hpa_ # - kube_daemonset_ # - kube_statefulset_ # - kube_endpoint_ # - kube_service_ # - kube_limitrange # - kube_node_ # - kube_poddisruptionbudget_ # - kube_resourcequota # - nr_stats # copy_attributes: # # Copy all the labels from the timeseries with metric name # # `kube_hpa_labels` into every timeseries with a metric name that # # starts with `kube_hpa_` only if they share the same `namespace` # # and `hpa` labels. # - from_metric: \"kube_hpa_labels\" # to_metrics: \"kube_hpa_\" # match_by: # - namespace # - hpa # - from_metric: \"kube_daemonset_labels\" # to_metrics: \"kube_daemonset_\" # match_by: # - namespace # - daemonset # - from_metric: \"kube_statefulset_labels\" # to_metrics: \"kube_statefulset_\" # match_by: # - namespace # - statefulset # - from_metric: \"kube_endpoint_labels\" # to_metrics: \"kube_endpoint_\" # match_by: # - namespace # - endpoint # - from_metric: \"kube_service_labels\" # to_metrics: \"kube_service_\" # match_by: # - namespace # - service # - from_metric: \"kube_node_labels\" # to_metrics: \"kube_node_\" # match_by: # - namespace # - node # integration definition files required to map metrics to entities # definition_files_path: /etc/newrelic-infra/definition-files Copy Key names and definitions Here are some key names and definitions for your Prometheus OpenMetrics config file. Key name Description cluster_name Required. The name of the cluster. This value will be included as the clusterName attribute for all metrics. verbose Stringified boolean. true (default): Logs debugging information. false: Only logs error messages. targets Configuration of static endpoints to be scraped by the integration. It contains a list of objects. For more information about this structure, see the documentation about target configuration. scrape_enabled_label Kubernetes String. The integration will check if the Kubernetes pod and service are annotated or have a label with this value to decide if it has to be scraped. This is particularly useful when you want to limit the amount of data by ignoring metrics or including specific metrics that are sent to New Relic. Since by default we use the same label Prometheus uses to discover targets that can be scraped, most exporters that you install automatically set this label. To keep a fine-grained control on the targets you want the integration to scrape, you can set this option to some other value (such as newrelic/scrape) and then add the annotation or label newrelic/scrape: \"true\" to your Kubernetes objects. If both are set, annotations take precedence over labels. Default: \"prometheus.io/scrape\" scrape_duration How often should the scraper run. To lower memory usage, increase this value. To raise memory usage, decrease this value. The impact on memory usage is due to distributing target fetching over the scrape interval to avoid querying (and buffering) all the data at once. Default is 30s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. scrape_timeout The HTTP client timeout when fetching data from endpoints. Default: 5s. Valid values include 1s, 15s, 30s, 1m, 5m, etc. worker_threads Number of worker threads used for scraping targets. Can be increased on environments with a high number of targets or targets with high latency, but might increase memory consumption. Default: 4. It is not recommended to use more than 10. require_scrape_enabled_label_for_nodes Kubernetes Whether or not Kubernetes nodes need labels to be scraped. Default: true. percentiles Histogram support is based on New Relic's guidelines for higher level metrics abstractions. To better support visualization of this data, percentiles are calculated based on the histogram metrics and sent to New Relic. Valid values include 50, 95, and 99. emitter_proxy Proxy used by the integration when submitting metrics: [scheme]://[domain]:[port] This proxy won't be used when fetching metrics from the targets. By default this is empty, and no proxy will be used. emitter_ca_file Certificate to add to the root CA that the emitter will use when verifying server certificates. If left empty, TLS uses the host's root CA set. emitter_insecure_skip_verify Whether the emitter should skip TLS verification when submitting data. Default: false. disable_autodiscovery Set to true in order to disable autodiscovery in the k8s cluster. It can be useful when running the Pod with a service account having limited privileges. Default: false. Configure objects in target key If you want the target key in the configuration file to contain one or more objects, use the following structure in the YAML list: Key name Description description A description for the URLs in this target. urls A list of strings with the URLs to be scraped. tls_config Authentication configuration used to send requests. It supports TLS and Mutual TLS. For more information, see the documentation about mutual TLS authentication. Kubernetes port and endpoint path New Relic's Prometheus OpenMetrics integration automatically discovers which targets to scrape. To specify the port and endpoint path to be used when constructing the target, you can use the prometheus.io/port and prometheus.io/path annotations or label in your Kubernetes pods and services. Annotations take precedence over labels. If prometheus.io/port is not present, the integration will try to scrape each port or ContainerPort defined for the service. If prometheus.io/path is not present, the integration will default to /metrics. If a service is not running on the default /my-metrics-path path, add a label to the pod prometheus.io/path=my-metrics-path. If the path to the metrics endpoint is more complex and cannot be a valid label value (for example, foo/bar), use annotations instead. Example: Labels for Kubernetes port and path In this example, you have a deployment in your cluster, and the pods expose Prometheus metrics on port 8080 and in the path my-metrics. In the PodSpec metadata of the deployment manifest, set the labels prometheus.io/port: \"8080\" and prometheus.io/path: \"my-metrics\". When the integration tries to retrieve the metrics from your pods, it will send a request to http://<pod-ip>:8080/my-metrics. apiVersion: apps/v1 kind: Deployment metadata: name: my-deployment spec: replicas: 2 selector: matchLabels: app: my-app template: metadata: labels: app: my-app prometheus.io/scrape: \"true\" prometheus.io/port: \"8080\" prometheus.io/path: \"my-metrics\" Copy Services and Endpoints scrape behaviour By default, services are scraped directly instead of the underlying endpoints since scrape_services is set to true and scrape_endpoints to false. In order to change this behaviour set scrape_endpoints to true configuring Prometheus OpenMetrics integrations to scrape the underlying endpoints, as Prometheus server natively does, instead of directly the services. Please notice that depending on the number of endpoints behind the services in the cluster the load and the data injested can increase considerably, monitor and, if needed, increase resource requirements. Moreover, even if it is possible to set both scrape_services and scrape_endpoints to true to assure retrocompatibility, it would lead to duplicate data. Reload the configuration The Prometheus OpenMetrics integration does not automatically reload the configuration when you make changes to the configuration file. Docker: To reload the configuration, restart the container running the integration: docker restart nri-prometheus Copy Kubernetes: To reload the configuration, restart the integration. Recommendation: Scale the deployment down to zero replicas, and then scale it back to one replica: kubectl scale deployment nri-prometheus --replicas=0 kubectl scale deployment nri-prometheus --replicas=1 Copy Docker: Run previous config file Docker: To run the integration with the previous configuration file: Copy the content and save it to a config.yaml file. From within the same directory, run the command: docker run -d --restart unless-stopped \\ --name nri-prometheus \\ -e CLUSTER_NAME=\"YOUR_CLUSTER_NAME\" \\ -e LICENSE_KEY=\"YOUR_LICENSE_KEY\" \\ -v \"$(pwd)/config.yaml:/config.yaml\" \\ newrelic/nri-prometheus:latest --configfile=/config.yaml Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.63501,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "Unless otherwise noted, configuration options for your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic apply to both Docker and Kubernetes environments. At a minimum, the following configuration values are required: License key Cluster name Recommendation: <em>Configure</em> your New Relic license key"
      },
      "id": "603e830964441f85a04e8877"
    },
    {
      "sections": [
        "Configure Prometheus OpenMetrics integrations in large Kubernetes environments",
        "Configure the integration for large environments"
      ],
      "title": "Configure Prometheus OpenMetrics integrations in large  Kubernetes environments",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "84e7d3b803e614a6362e0246a58b48e3209094ad",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/configure-prometheus-openmetrics-integrations-large-kubernetes-environments/",
      "published_at": "2021-06-25T23:17:10Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of metrics exposed by each target. For example, a Prometheus OpenMetrics integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30 seconds, consumes 2.5CPU and 700MB of RAM. Configure the integration for large environments To estimate the size of the environment you are monitoring, run the following query to see how many targets are being scraped: SELECT latest(nr_stats_targets) FROM Metric where clusterName=clusterName SINCE 30 MINUTES AGO TIMESERIES Copy In huge environments with hundreds of targets to be scraped, the latency on the /metrics endpoints must be below 1 second. Run this query to check the latency of the different targets. This query retrieves the data exposed by the Prometheus OpenMetrics integration, and shows the time required to fetch each endpoint. SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy In order to keep the time needed to scrape all the targets below 30 seconds, use the following configurations: Targets Configuration Targets < 400, with 1000 metrics each No modification is required. CPU ranges roughly between 0.1 and 1.5 cores, and the memory required should be no more than 256MB. 400 < targets < 1000, with 1000 metrics each The number of workers should be increased to 6-8. CPU ranges roughly between 1.5 and 3.5 cores, and the memory required is around 100MB. Targets > 1000, with 1000 metrics each The number of workers should be increased to 10 or more. CPU is over 3.5 cores, and the memory required is around 1GB or more.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 168.35713,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large  Kubernetes environments",
        "sections": "<em>Configure</em> <em>Prometheus</em> <em>OpenMetrics</em> <em>integrations</em> in large Kubernetes environments",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "CPU and memory limits and requests can vary according to the number of targets monitored, and the number of <em>metrics</em> exposed by each target. For example, a <em>Prometheus</em> <em>OpenMetrics</em> integration which scrapes 800 targets, exposing 1000 timeseries each, with a latency of 150ms and a scrape_duration of 30"
      },
      "id": "603e9b3b196a676cd0a83d81"
    },
    {
      "sections": [
        "Add mutual TLS to Prometheus endpoints",
        "Add secret to config file"
      ],
      "title": "Add mutual TLS to Prometheus endpoints",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure OpenMetrics"
      ],
      "external_id": "707c96de26f106ddeaea4e18d5b71290170fea90",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-openmetrics/add-mutual-tls-prometheus-endpoints/",
      "published_at": "2021-06-26T14:27:47Z",
      "updated_at": "2021-03-13T03:34:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can configure mutual TLS authentication when needed for the endpoints in your Prometheus OpenMetrics integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key, and cert files in a secret, and include them in the Prometheus OpenMetrics integration's container. Mutual TLS authentication is limited to a static list of URLs. To configure endpoints that require MTLS authentication, follow this example: targets: - description: \"Secure etcd example\" urls: [\"https://123.456.7.1:2379\", \"https://123.456.7.2:2379\"] tls_config: ca_file_path: \"/etc/etcd/etcd-client-ca.crt\" cert_file_path: \"/etc/etcd/etcd-client.crt\" key_file_path: \"/etc/etcd/etcd-client.key\" transformations: ... Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 167.97464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "sections": "Add mutual TLS to <em>Prometheus</em> endpoints",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>OpenMetrics</em>",
        "body": "You can <em>configure</em> mutual TLS authentication when needed for the endpoints in your <em>Prometheus</em> <em>OpenMetrics</em> integration with New Relic. Add tls_config to your configuration file for Docker or Kubernetes, as explained in this example. Add secret to config file Recommendation: Put the CA bundle, key"
      },
      "id": "6044e621196a67efb9960f37"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages": [
    {
      "sections": [
        "Set up your Prometheus remote write integration",
        "Tip",
        "Set up the integration",
        "Mapping of Prometheus metric types",
        "Override metric type mappings",
        "Customize remote write behavior",
        "X-License Key",
        "prometheus_server URL parameter",
        "Optimize throughput and memory consumption",
        "Troubleshoot error messages",
        "Remove the integration"
      ],
      "title": "Set up your Prometheus remote write integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "e2a503880e8e1c38284434d5829fad3f48dc7abf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration/",
      "published_at": "2021-06-26T04:33:14Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can get Prometheus data flowing in New Relic with just a few simple steps. This page covers basic setup for the remote write integration, as well as a few common troubleshooting topics. For information on integrating Prometheus servers in a high availability (HA) configuration, see our Prometheus high availability documentation. Tip To use Prometheus integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Set up the integration Go to the Prometheus remote write setup launcher in New Relic One, then complete these steps. Add Prometheus data Enter a name for the Prometheus server to be connected and your remote_write URL. Important: The name you enter for the server will create an attribute on your data. It will also be the name that identifies which Prometheus server is sending data to New Relic. Add a new remote_write URL to your Prometheus YML file. Add this information under global_config in the file, at the same indentation level as the global section. Use the following syntax: remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy OR remote_write: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=YOUR_LICENSE_KEY&prometheus_server=YOUR_DATA_SOURCE_NAME Copy European Union accounts: If you're connecting from the EU, use the following URL: https://metric-api.eu.newrelic.com/prometheus/v1/write Copy Kubernetes and Helm remote write integrations: Add the remote write URL to your Helm values.yaml file. Replace remoteWrite: [] with two lines similar to the following example. Be sure to use your remote write URL and use indentation that matches the rest of the file: remoteWrite: - url: https://metric-api.newrelic.com/prometheus/v1/write?prometheus_server=YOUR_DATA_SOURCE_NAME bearer_token:YOUR_LICENSE_KEY Copy Restart your Prometheus server. View your data in the New Relic UI. For example, use the remote write dashboard we automatically create when you set up your integration. Mapping of Prometheus metric types The Prometheus remote write protocol does not include metric type information or other helpful metric metadata when sending metrics to New Relic. Because the remote write protocol doesn't include this information, New Relic infers the metric type based on Prometheus naming conventions. Metrics not following these naming conventions may not be mapped correctly. New Relic maps Prometheus metrics types into New Relic metric types based on Prometheus metric naming conventions as follows: metricName_bucket is stored as a New Relic count metric type. metricName_count is stored as a New Relic count metric type. metricName_total is stored as a New Relic count metric type. metricName_sum is stored as a New Relic summary metric type. Everything else is stored as a New Relic gauge metric type. Override metric type mappings If you have metrics that don't follow Prometheus naming conventions, you can configure remote-write to tag the metric with a newrelic_metric_type label that indicates the metric type. This label is stripped when received by New Relic. Example: You have a counter metric named my_counter, which does not have our naming convention suffix of _bucket, _count or _total. In this situation, your metric would be identified as a gauge rather than a counter. To correct this, add the following relabel configuration to your prometheus.yml: - url: https://metric-api.newrelic.com/prometheus/v1/write?X-License-Key=... write_relabel_configs: - source_labels: [__name__] regex: ^my_counter$ target_label: newrelic_metric_type replacement: \"counter\" action: replace Copy This rule matches any metric with the name my_counter and adds a newrelic_metric_type label that identifies it as a counter. You can use the following (case sensitive) values as the replacement value: counter gauge summary When a newrelic_metric_type label is present on a metric received and set to one of the valid values, New Relic will assign the indicated type to the metric (and strip the label) before downstream consumption in the data pipeline. If you have multiple metrics that don't follow the above naming conventions, you can add multiple rules with each rule matching different source labels. Customize remote write behavior You can customize the following parameters if you are writing to more than one account in New Relic or are connecting more than one Prometheus data source to the same account in New Relic. For more information, see the docs on remote write tuning. X-License Key Your account's license key is not an API key. The license key is used for authentication and to identify which account to write data into. If you are configuring Prometheus to write into different New Relic accounts or sub-accounts, use a different key on each remote write URL. prometheus_server URL parameter The prometheus_server parameter is a label or attribute used to add to stats that are written to NRDB. Use this same label when configuring your Grafana data source to limit results to just those from a particular prometheus_server. Optimize throughput and memory consumption Remote write increases the total memory consumption of your Prometheus servers. If you're experiencing issues we recommend the following: Increase max_samples_per_send for higher throughput workloads, along a proportional increase in capacity. If memory consumption is still a problem, try limiting the number of max_shards per server. Troubleshoot error messages If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, review our remote write troubleshooting documentation. This includes fixing common errors, such as missing or incorrect characters, bad requests, request entity too large, and rate limit errors. Remove the integration When you remove the Prometheus remote write integration, this stops new data from flowing, but it will not purge or remove any historical data. To remove the integration, remove the configuration code snippet from your Prometheus YML file, then restart the server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 191.07094,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "sections": "Set up your <em>Prometheus</em> <em>remote</em> <em>write</em> <em>integration</em>",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " Union accounts: If you&#x27;re connecting from the EU, use the following URL: https:&#x2F;&#x2F;metric-api.eu.newrelic.com&#x2F;<em>prometheus</em>&#x2F;v1&#x2F;<em>write</em> Copy Kubernetes and Helm <em>remote</em> <em>write</em> <em>integrations</em>: Add the <em>remote</em> <em>write</em> URL to your Helm values.yaml file. Replace <em>remoteWrite</em>: [] with two lines similar to the following"
      },
      "id": "603e94de196a674e6ca83def"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-06-25T23:57:13Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.61975,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "6044e621196a67b846960f6b"
    },
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.38718,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics or <em>remote</em> <em>write</em> <em>integration</em>?",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and <em>configure</em> data to target (see metrics or targets). Balance <em>remote</em> <em>write</em>(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    }
  ],
  "/docs/integrations/prometheus-integrations/install-configure-remote-write/set-your-prometheus-remote-write-integration": [
    {
      "sections": [
        "Remote write errors and error messages",
        "Common errors and issues",
        "Configuration errors",
        "400: bad request error",
        "413: request entity too large error",
        "429: rate limit error",
        "Investigate error messages"
      ],
      "title": "Remote write errors and error messages",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "0d190be5dc4fd91ce6bbcef7343d01f75670ca51",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure-remote-write/remote-write-errors-error-messages/",
      "published_at": "2021-06-26T04:32:06Z",
      "updated_at": "2021-03-13T03:46:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This resource contains information about common errors and error messages that may alert you to issues with data visibility and availability, as well as information about how to respond. Common errors and issues If you receive an integration error message from New Relic or error messages in your Prometheus server logs after restarting your Prometheus server, there are several actions you can take to troubleshoot and get data flowing properly. Below are a few tips regarding common issues and error messages. For specific information on how to query for NrIntegrationError events, see Investigate error messages below. Configuration errors Missing or incorrect characters in the remote write URL in the config file (for example the endpoint, license key, or prometheus_server name) or incorrect placement of the information in the file will result in the Prometheus server not starting, remote write not working properly, or errors appearing in Prometheus server logs. 400: bad request error If no data appears with a bad request error, check your configuration file to confirm that the placement of the remote write information is correct, and that there are no missing or incorrect characters. 413: request entity too large error This means you have sent a request in which one or more fields, or the entire payload, has exceeded our limits. 429: rate limit error This means you have hit a rate limit on the amount of data being sent at one time (for example cardinality or data points per minute). You can troubleshoot by reducing the amount of Prometheus or general metric data you are sending, or by requesting a rate-limit increase. Investigate error messages You can investigate error messages in New Relic by doing either or both of the following. Run a NrIntegrationError query on the error message using NRQL, then look at the Message field in the UI to see a description of what went wrong. For example: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' Copy Investigate individual errors in time to see when and where they occur and any simultaneously occurring issues, and perform targeted troubleshooting based on what you find out. For example: SELECT count(*) FROM NrIntegrationError WHERE newRelicFeature = 'Metrics' TIMESERIES Copy If youve validated that you can send data successfully but are unable to query it, you may be running into other kinds of limits, like the inspected count limit. This may manifest itself as an error message during the integration process that says: Unable to retrieve data for Prometheus data source <name>.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.626,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "sections": "<em>Remote</em> <em>write</em> errors <em>and</em> error messages",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": " error messages below. Configuration errors Missing or incorrect characters in the <em>remote</em> <em>write</em> URL in the config file (for example the endpoint, license key, or <em>prometheus</em>_server name) or incorrect placement of the information in the file will result in the <em>Prometheus</em> server not starting, <em>remote</em>"
      },
      "id": "6044e65d196a67914a960f6b"
    },
    {
      "sections": [
        "Prometheus High Availability (HA)",
        "Tip",
        "External labels",
        "Prometheus Operator",
        "Standalone Prometheus"
      ],
      "title": "Prometheus High Availability (HA)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Install and configure remote write"
      ],
      "external_id": "3c0fddd6e878f30f8ba4c132f537b88cd47f2eba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/install-configure/prometheus-high-availability-ha/",
      "published_at": "2021-06-25T23:57:13Z",
      "updated_at": "2021-03-13T02:41:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are using our Prometheus remote write integration in a high-availability (HA) configuration, you need to make sure your Prometheus servers aren't sending multiple copies of the same metrics to New Relic. This document describes how you can configure your remote write integration so that New Relic does not keep duplicated metrics. Tip For information on standard Prometheus remote write integration without using a high-availability configuration, see Set up your Prometheus remote write integration. External labels New Relic requires two external labels to deduplicate data from replicas in a high-availability configuration: Label name Description Example value prometheus A label whose value identifies the name of a high-availability cluster or group of Prometheus servers. monitoring-cluster prometheus_replica A label whose value identifies the unique replica sending this data. replica-1 The remaining sections explain how labels work with Prometheus Operator and standalone Prometheus. Prometheus Operator These external labels are added by default if you use Prometheus Operator version 0.19.0 (or higher). This applies whether you use Prometheus Operator directly or via the helm chart. The operator sets the value of the prometheus label (the one identifying a cluster) as <prometheus deployment namespace>/<prometheus deployment name>. For example, if your namespace for the Prometheus deployment is monitoring and the name of the deployment is prometheus-cluster1, the value is monitoring/prometheus-cluster1. The operator sets the value of the prometheus_replica label as the name of the pod for each replica. This follows the format replica-<replica number>, where the number is the ordinal of that replica (for example, the first replica is named replica-1). Tip If you still see duplicate copies of replica data, make sure you do not have replicaExternalLabelName or prometheusExternalLabelName in your Prometheus spec or chart configuration because these overrides change the label name. Standalone Prometheus When deploying a Prometheus server directly, you need to add the external labels to the configuration file. Here are two different example configurations for replicas within the same high-availability cluster: Replica 1 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-1 Copy Replica 2 (prometheus.yml) global: external_labels: prometheus: monitoring-cluster prometheus_replica: replica-2 Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.61975,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Prometheus</em> High Availability (HA)",
        "sections": "<em>Prometheus</em> High Availability (HA)",
        "tags": "<em>Install</em> <em>and</em> <em>configure</em> <em>remote</em> <em>write</em>",
        "body": "If you are using our <em>Prometheus</em> <em>remote</em> <em>write</em> integration in a high-availability (HA) configuration, you need to make sure your <em>Prometheus</em> servers aren&#x27;t sending multiple copies of the same metrics to New Relic. This document describes how you can <em>configure</em> your <em>remote</em> <em>write</em> integration so that New"
      },
      "id": "6044e621196a67b846960f6b"
    },
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.38718,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics or <em>remote</em> <em>write</em> <em>integration</em>?",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and <em>configure</em> data to target (see metrics or targets). Balance <em>remote</em> <em>write</em>(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/debug-issues-data-sent-metric-api-prometheus-integration": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.38718,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-06-25T22:05:07Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.18861,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-06-26T14:28:52Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.70051,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.3871,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-06-25T22:05:07Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.18861,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Sparse data, missing metrics, and data gaps",
        "Problem",
        "Solution"
      ],
      "title": "Sparse data, missing metrics, and data gaps",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "0389b736613331d774daf3c3643ca8362fd4babb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps/",
      "published_at": "2021-06-25T22:51:30Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic's UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled, and if the memory allocated to the container is enough. In case the container doesn't have enough resources available, it may send data with a longer interval between samples. Low memory limit may cause the integration to be killed and restarted. The correct limits and requests of resources can vary according to the number of targets monitored, and the number of metrics exposed by each target.For more information, see the guidelines in the large environment configuration documentation. Check for the following error message in the logs of the integration: {\"err\":\"unexpected post response code: 413: Request Entity Too Large\"} Copy This issue causes some payloads to be dropped, and it is currently fixed in new releases. If present, update the image to the most recent version. If some /metrics endpoints that are monitored by the integration time out or take several seconds to answer, they may slow down the scraping of data. The integration's performance can degrade if multiple endpoints take a huge amount of time to answer. This leads to intermittent and missing data. To detect those endpoints, run: SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy This query retrieves the data exposed by the Prometheus OpenMetrics integration, and it shows the time required to fetch each endpoint. Fix the endpoints with a latency above 1s, or exclude them from the monitoring removing the Prometheus scraping label. If it's not feasible to remove these endpoints, and the latency in the answer can't be avoided, configure the integration to run more workers in parallel. This allows the integration to fetch more endpoints at the same time. To do so, update the integration to the most recent version, and apply the new configuration option worker_threads. We advise to do it gradually, from 4, 6, 8 and up to 16. This workaround only minimizes the issue, and if multiple endpoints are misbehaving, performances will still be degraded. Moreover, memory and CPU consumption will increase with the number of workers, so memory and CPU must be increased accordingly. In case all the endpoints monitored have a low latency, and the container is not being throttled, run the following query. This helps you verify how much time the integration is taking to scrape all the targets, and to send the data if it's exceeding the configured scrape_duration. SELECT latest(nr_stats_integration_process_duration_seconds) FROM Metric where clusterName=clustername' SINCE 30 MINUTES AGO TIMESERIES Copy First, update the integration to the latest image published. Then, to reduce the time needed to scrape all the targets, increase the number of workers with the configuration option worker_threads. We advise is to do it gradually, from 4 to 6, 8, and up to 16 until r_stats_integration_process_duration_seconds approaches the defined scrape_duration. Note that memory consumption and CPU usage will increase.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.695366,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic&#x27;s UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled"
      },
      "id": "603e9b8228ccbc95f4eba75b"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/get-logs-prometheus-integration": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.3871,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-06-25T22:05:07Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.18861,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-06-26T14:28:52Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.70051,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/get-scraper-metrics-prometheus-integration": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.38704,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-06-25T22:05:07Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.18861,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-06-26T14:28:52Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.70051,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.38704,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-06-26T14:28:52Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.70051,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    },
    {
      "sections": [
        "Sparse data, missing metrics, and data gaps",
        "Problem",
        "Solution"
      ],
      "title": "Sparse data, missing metrics, and data gaps",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "0389b736613331d774daf3c3643ca8362fd4babb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps/",
      "published_at": "2021-06-25T22:51:30Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic's UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled, and if the memory allocated to the container is enough. In case the container doesn't have enough resources available, it may send data with a longer interval between samples. Low memory limit may cause the integration to be killed and restarted. The correct limits and requests of resources can vary according to the number of targets monitored, and the number of metrics exposed by each target.For more information, see the guidelines in the large environment configuration documentation. Check for the following error message in the logs of the integration: {\"err\":\"unexpected post response code: 413: Request Entity Too Large\"} Copy This issue causes some payloads to be dropped, and it is currently fixed in new releases. If present, update the image to the most recent version. If some /metrics endpoints that are monitored by the integration time out or take several seconds to answer, they may slow down the scraping of data. The integration's performance can degrade if multiple endpoints take a huge amount of time to answer. This leads to intermittent and missing data. To detect those endpoints, run: SELECT average(nr_stats_integration_fetch_target_duration_seconds) FROM Metric where clusterName=clustername' SINCE 30 MINUTES AGO FACET target LIMIT 30 Copy This query retrieves the data exposed by the Prometheus OpenMetrics integration, and it shows the time required to fetch each endpoint. Fix the endpoints with a latency above 1s, or exclude them from the monitoring removing the Prometheus scraping label. If it's not feasible to remove these endpoints, and the latency in the answer can't be avoided, configure the integration to run more workers in parallel. This allows the integration to fetch more endpoints at the same time. To do so, update the integration to the most recent version, and apply the new configuration option worker_threads. We advise to do it gradually, from 4, 6, 8 and up to 16. This workaround only minimizes the issue, and if multiple endpoints are misbehaving, performances will still be degraded. Moreover, memory and CPU consumption will increase with the number of workers, so memory and CPU must be increased accordingly. In case all the endpoints monitored have a low latency, and the container is not being throttled, run the following query. This helps you verify how much time the integration is taking to scrape all the targets, and to send the data if it's exceeding the configured scrape_duration. SELECT latest(nr_stats_integration_process_duration_seconds) FROM Metric where clusterName=clustername' SINCE 30 MINUTES AGO TIMESERIES Copy First, update the integration to the latest image published. Then, to reduce the time needed to scrape all the targets, increase the number of workers with the configuration option worker_threads. We advise is to do it gradually, from 4 to 6, 8, and up to 16 until r_stats_integration_process_duration_seconds approaches the defined scrape_duration. Note that memory consumption and CPU usage will increase.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.695366,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but you notice sparse data, missing metrics, and data gaps in New Relic&#x27;s UI. Solution If some metrics are not being collected regularly, do the following: Check if the CPU is being throttled"
      },
      "id": "603e9b8228ccbc95f4eba75b"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/rate-limit-errors-prometheus-integration": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.38704,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-06-25T22:05:07Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.18861,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-06-26T14:28:52Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.70051,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/restarts-gaps-data-kubernetes": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.38698,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-06-25T22:05:07Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.18861,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-06-26T14:28:52Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.70051,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    }
  ],
  "/docs/integrations/prometheus-integrations/troubleshooting/sparse-data-missing-metrics-data-gaps": [
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.38698,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric data to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    },
    {
      "sections": [
        "No data appears (Prometheus integration)",
        "Problem",
        "Solution",
        "Docker troubleshooting",
        "Kubernetes troubleshooting"
      ],
      "title": "No data appears (Prometheus integration)",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "1e21826044fef6dc088721c30a0fb6d61636919a",
      "image": "https://docs.newrelic.com/static/img-integration-k8-f16fcb798b1f0f56aa1be798a28c2b0b.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/no-data-appears-prometheus-integration/",
      "published_at": "2021-06-25T22:05:07Z",
      "updated_at": "2021-03-13T02:21:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic's UI. Solution Follow these troubleshooting tips for Docker or Kubernetes as applicable: Docker troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: docker ps -f \"name=nri-prometheus\" Copy Check the Status field for the container: docker inspect nri-prometheus Copy For more detailed information, use Docker inspect. If no data appears in New Relic's UI: Run this NRQL query: docker logs nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your Docker config file. Kubernetes troubleshooting If you are having problems with the integration: Check if the Prometheus OpenMetrics integration is running: kubectl describe pod -l \"app=nri-prometheus\" Copy Check the Ready field for the pod. If the pod is not ready, check the Events. If no data appears in New Relic's UI: Run this NRQL query: kubectl logs deploy/nri-prometheus | grep \"error emitting metrics\" Copy Check whether the log contains this message: metrics api responded with status code 403 Copy If yes, check the LICENSE_KEY in your nri-prometheus-latest.yaml manifest file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.18861,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "sections": "No data appears (<em>Prometheus</em> <em>integration</em>)",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, but no data appears in New Relic&#x27;s UI. Solution Follow these <em>troubleshooting</em> tips for Docker or Kubernetes as applicable: Docker <em>troubleshooting</em> If you are having problems with the integration: Check"
      },
      "id": "6044e6a128ccbc64f22c6068"
    },
    {
      "sections": [
        "Excessive CPU or memory consumption",
        "Problem",
        "Solution"
      ],
      "title": "Excessive CPU or memory consumption",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Troubleshooting"
      ],
      "external_id": "130c15368dcecaeb128789171b818014d112919d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/troubleshooting/excessive-cpu-or-memory-consumption/",
      "published_at": "2021-06-26T14:28:52Z",
      "updated_at": "2021-03-16T06:18:08Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have installed the Prometheus OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect scrape_duration. For example, a Prometheus OpenMetrics integration consumes 2.5 CPU and 700Mb of RAM because: It scrapes 800 targets, exposing 1000 timeseries each. Each one has a latency of 150ms with a scrape_duration of 30 seconds. To reduce resource consumption: Update the integration to the latest available image. Reduce harvest time by lowering emitter_harvest_period. (The default value is 1s, and the interval cannot be smaller than 200ms.) Since metrics are sent more often, memory consumption is reduced. Collect metrics less frequently by increasing scrape_duration to reduce both memory consumption and CPU usage. Reduce the number of workers to reduce both memory consumption and CPU usage. Scraping will slow down and could exceed scrape_duration. To do so: Update the integration to the latest version available of the image. Decrease worker_threads from the default value of 4 to your preferred value.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.70051,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": "Problem You have installed the <em>Prometheus</em> OpenMetrics integration for Docker or Kubernetes, and it consumes too much memory or CPU. Solution When running the integration in a huge cluster scraping hundreds of targets, CPU and memory consumption will increase, and the number of workers could affect"
      },
      "id": "603e9b3b28ccbcaae3eba790"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features": [
    {
      "sections": [
        "Translate PromQL queries to NRQL",
        "Tip",
        "Prometheus and New Relic metric types",
        "Mapping between NRQL and our PromQL-style queries",
        "PromQL-style query example",
        "NRQL query example",
        "Filter examples",
        "PromQL-style to NRQL query examples",
        "For more help"
      ],
      "title": "Translate PromQL queries to NRQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "fcf45fb8fb49f9d22f74574c2e7032533377e584",
      "image": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/images/PROMQL-query-2.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/",
      "published_at": "2021-06-26T14:28:52Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Do you have a PromQL query youd like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style query language to explore your Prometheus OpenMetrics integration data along with other data sent to New Relic. Tip To run PromQL-style queries in New Relic One, go to the query builder advanced PromQL-style mode. Prometheus and New Relic metric types The different metric types supported by Prometheus and New Relic are related to each other: New Relic Prometheus Description Count Counter The Prometheus counter is a cumulative sum while the New Relic count is a delta sum. For example, if you see 2 requests in the first reporting period and 3 requests in the second reporting period. The Prometheus counter will report 2 and then 5, while the New Relic count will report 2 and then 3. Gauge Gauge A Prometheus gauge is similar to a New Relic gauge. Multiple counts Histogram Prometheus automatically maps a histogram to a set of counters. In New Relic, these counters should be changed to deltas and reported as counts. Gauges and counts Summary Prometheus represents a Summary with a given basename as the following time series: a basename_sum a basename_count and 0 or more of basename{quantile=\".xx\"...} metrics New Relic maps the _sum as a Summary, the _count as a Counter, and each quantile metric as a Gauge. Summary (No equivalent in Prometheus) New Relic has a distinct metric type called a summary that is different than the Prometheus summary. It is designed for reporting aggregated discrete events so that you can query the count, sum, min, max, and average values. Mapping between NRQL and our PromQL-style queries Tip To see how New Relic translates PromQL-style queries to NRQL, write a query in the query builder PromQL-style tab, then switch to the NRQL tab. This table shows the mapping between NRQL and our PromQL-style queries when exploring data. For more contextual information, see the examples. Description Mapping between NRQL and PromQL-style queries Search for attributes: Explore the attributes on the container_memory_usage_bytes metric. PromQL: container_memory_usage_bytes Copy NRQL: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy Find attribute's value: Explore the current value of the container_memory_usage_bytes metric for unique id attributes. PromQL: sum(container_memory_usage_bytes) by (id) Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy Visualize the attribute's value: Chart the value of the container_memory_usage_bytes metric with the given id attribute value. PromQL: container_memory_usage_bytes{id=\"/\"} Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = '/' TIMESERIES Copy PromQL-style query example 1. Start your query. When exploring your data for a particular metric in PromQL, such as memory by container usage in bytes, you can start with a query such as: container_memory_usage_bytes Copy This will chart all the unique metric timeseries for the input metric. 2. Filter the query results. Looking at the data, you can add more query parameters to filter down the number of metric timeseries. For example, if you only want timeseries where the id is /, the PromQL-style query will be: container_memory_usage_bytes{id=\"/\"} Copy PromQL-style example: To filter the data, run this PromQL-style query: container_memory_usage_bytes { id=\"/\"}. NRQL query example 1. Query available metrics. To explore your data, start by looking at all the available metrics. Use the following NRQL query: FROM Metric SELECT uniques(metricName) Copy 2. Find unique attributes. Once you have found the metric you want to review, such as container_memory_usage_bytes, you can find the unique attributes with the following query: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy The results will show each available attribute key and the value type (string, boolean, or number). 3. Aggregate and chart the metrics. To chart metrics using NRQL, you first need an aggregation function. For example, you can use latest for gauges, sum for counts, and average for summaries. As the following chart shows, all the unique timeseries are aggregated into one unique timeseries by default: one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes'. 4. View metrics by ID. To view the unique metric timeseries with various id values, run the following query: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT latest(container_memory_usage_bytes) FACET id. 5. Add the selected ID to the query. Next you can select an id value and put it in the NRQL where clause. FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = \"/\" timeseries Copy one.newrelic.com > Query your data: This example shows the data displayed after running From Metric select latest(container_memory_usage_bytes) where id = \"/\" timeseries. Filter examples Both our PromQL-style query language and NRQL provide syntax to filter down the number of unique metric timeseries. PromQL-style uses brackets to filter. NRQL uses a WHERE clause. Here are some example queries: Description PromQL-style and NRQL queries Select data with specific values. PromQL: go_memstats_heap_alloc_bytes{job=\"apiserver\", instance=\"1234\"}) Copy NRQL: To only select data with specific values in NRQL, use the WHERE clause with =. In this example, all data must have the selected value for job and handler. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE job = 'apiserver' AND instance = '1234' TIMESERIES Copy Select data with multiple values. PromQL: go_memstats_heap_alloc_bytes{environment=~\"staging|testing|development\",method!=\"GET\"} Copy NRQL: In NRQL use the in clause to select multiple values for an attribute and the != sign to select all values but the one listed. In this example, the environment can be staging, testing, or development, and the method cannot be GET. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE environment IN ('staging', 'testing', 'development') AND method != 'GET' TIMESERIES Copy Select data using partial string values. PromQL: go_memstats_heap_alloc_bytes{job=~\"api.*\"} Copy NRQL: In NRQL use the LIKE clause to match part of a string value. In this example, all data will be returned where the job attributes start with api. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHEREe job LIKE 'api%' TIMESERIES Copy PromQL-style to NRQL query examples You can simulate the following PromQL-style queries with NRQL queries: Description PromQL-style and NRQL queries Measure the per second rate over the last minute of the http_request_total metric. PromQL: sum(rate(http_requests_total[1m])) Copy NRQL: FROM Metric SELECT rate(sum(http_request_total), 1 second) TIMESERIES 1 minute Copy Chart the difference of the two metrics, then divide by 1024. PromQL: (instance_memory_limit_bytes - instance_memory_usage_bytes) / 1024 Copy NRQL: FROM Metric SELECT (latest(instance_memory_limit_bytes) - latest(instance_memory_usage_bytes)) / 1024 TIMESERIES Copy Provide the summed rate per 30-second interval by each handler. PromQL: sum(rate(http_requests_total[30s])) by (handler) Copy NRQL: FROM Metric SELECT rate(sum(http_requests_total), 30 seconds) FACET handler TIMESERIES Copy Chart the difference in the two metrics where the instance is named foo and the fstype is either ext4 or xfs. PromQL: (node_filesystem_free_bytes{instance='foo',fstype=~\"ext4|xfs\"} / node_filesystem_size_bytes{instance='foo',fstype=~\"ext4|xfs\"}) Copy NRQL: FROM Metric SELECT latest(node_filesystem_free_bytes) / latest(node_filesystem_size_bytes) WHERE instance = 'foo' AND fstype IN ('ext4', 'xfs') Copy For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.60962,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Translate PromQL <em>queries</em> to NRQL",
        "sections": "<em>Prometheus</em> <em>and</em> New Relic metric types",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": "Do you have a PromQL <em>query</em> youd like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style <em>query</em> language to explore your <em>Prometheus</em> OpenMetrics integration <em>data</em> along with other <em>data</em> sent to New"
      },
      "id": "603ead4528ccbcbecfeba77b"
    },
    {
      "sections": [
        "View and query your Prometheus data",
        "Default attributes for the OpenMetrics integration",
        "Default attributes for the remote write integration",
        "NRQL query examples",
        "Get metric names",
        "Get the attributes for a metric",
        "Get the values for an attribute in OpenMetrics",
        "Build the query",
        "Get metric values",
        "Get a chart of the metric",
        "Query counter metrics (deltas)",
        "View connected Redis clients per pod with OpenMetrics",
        "Docker: View average memory free for scraped endpoints",
        "Kubernetes: View average memory usage for pods in a deployment",
        "View data in New Relic",
        "Create histograms and summaries"
      ],
      "title": "View and query your Prometheus data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "91a0388492ae73a9ddb8a1701b639a6b6a71822a",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data/",
      "published_at": "2021-06-25T22:51:30Z",
      "updated_at": "2021-03-16T04:13:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To query and visualize the metrics collected for your Prometheus OpenMetrics or remote write integration with New Relic, you can use NRQL. You can also translate your PromQL-style queries to NRQL using either Grafana or the query builder. All metrics for Docker and Kubernetes are stored in the Metric type. Default attributes for the OpenMetrics integration By default, the following attributes will be added to all metrics for Docker and Kubernetes integrations: Default attributes (all integrations) Description clusterName The name of the cluster provided in the scraper configuration. integrationName The name of this integration (nri-prometheus). integrationVersion The version of the integration; for example, 0.2.0. metricName The name of the metric itself. nrMetricType The type of the New Relic Metric type; for example, Gauges. promMetricType The metric type of the Prometheus metric scrapedEndpoint The URL of the endpoint is being scraped. Kubernetes: If the scraper is running in Kubernetes, New Relic also adds the following attributes to all the metrics: Additional Kubernetes attributes Description deploymentName Name of the deployment, if scraping a pod. label The Kubernetes labels of the object being scraped, prefixed by \"label\". namespaceName Name of the namespace. nodeName Name of the node where the pod being scraped is running, if applicable. podName Name of the pod being scraped, if applicable. serviceName Name of the service being scraped, if applicable Default attributes for the remote write integration By default, the following attributes will be added to Prometheus remote write metrics: Default attributes (all integrations) Description prometheus_server A user supplied label specified as a Prometheus remote write URL parameter. The value supplied should be unique as it is intended to differentiate between source Prometheus servers at query time. Unspecified by default. newrelic.source The name of the New Relic ingest point (prometheusAPI). instrumentation.provider prometheus instrumentation.name remote-write instrumentation.source A user supplied identifier for the source of the Prometheus data that matches the value of prometheus_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL query examples When you build queries, be aware that there is no linking between the metrics, entities, and attributes. Use the following NRQL queries to find out which metrics are available and which attributes are present on these metrics: Get metric names To get all metric names for OpenMetrics: FROM Metric SELECT uniques(metricName) Copy To get metric names for a remote write integration: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' Copy To get metric names for a remote write integration from a single Prometheus source: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' AND instrumentation.source='<ds>' Copy To get metric names for a specific OpenMetrics endpoint: FROM Metric SELECT uniques(metricName) WHERE scrapedEndpoint='<ep>' Copy To get metric names for a specific OpenMetrics cluster, namespace, or pod: FROM Metric SELECT uniques(metricName) WHERE clusterName='<cn>' Copy FROM Metric SELECT uniques(metricName) WHERE namespaceName='<ns>' Copy FROM Metric SELECT uniques(metricName) WHERE podName='<pod>' Copy Get the attributes for a metric To get all attributes for the selected metric: FROM Metric SELECT keyset() WHERE metricName='<mn>' Copy Get the values for an attribute in OpenMetrics The autocomplete will show all values of the attribute, regardless of the pod. To determine the attribute values for a specific pod: FROM Metric SELECT uniques(<attribute>) WHERE metricName='<mn>' AND podName='<pod>' Copy Build the query Using metric name and attributes, you can query your data. For more information about facets, time series, and time selection, see the NRQL documentation. To build PromQL-style queries, see our docs. Get metric values To get raw metric values: FROM Metric SELECT <metricName> WHERE <attribute>='<value>' Copy Get a chart of the metric To get a chart of the metric with an aggregator of average, min, max, or sum: FROM Metric SELECT <aggregator>(<metricname>) WHERE <attribute>='<value>' TIMESERIES Copy Query counter metrics (deltas) Currently the integration calculates the deltas for counter metrics. This is why queries on counter metrics will show the deltas of the counter instead of the absolute value of the counter. View connected Redis clients per pod with OpenMetrics Docker: This example assumes you are scraping Redis exporters. To view the number of connected Redis clients per endpoint in a cluster: FROM Metric SELECT latest(redis_connected_clients) WHERE clusterName='my-cluster' FACET scrapedEndpoint TIMESERIES Copy Kubernetes: This example assumes that you have Redis pods with the Redis exporter installed. To view the number of connected Redis clients per pod in the default namespace: FROM Metric SELECT latest(redis_connected_clients) WHERE namespaceName='default' FACET podName TIMESERIES Copy Docker: View average memory free for scraped endpoints This example assumes you are scraping node exporters for Docker and want to use OpenMetrics. To view average memory free for all scraped endpoints in a cluster: FROM Metric SELECT average(node_memory_MemFree_bytes) WHERE clusterName='my-cluster' Copy Kubernetes: View average memory usage for pods in a deployment To view average memory usage for all pods in a Kubernetes deployment using OpenMetrics: FROM Metric SELECT average(container_memory_usage_bytes) WHERE deploymentName='my-app-deployment' AND namespaceName='default' Copy View data in New Relic When you query the data, you can view the results in the New Relic UI. You can also visualize the data as charts, histograms, etc. To view the NRQL query results for your Prometheus integration's data: Go to one.newrelic.com > Query your data. For more information, see New Relic's query builder documentation. Create histograms and summaries With remote write or version 1.2.0 or higher of the Prometheus OpenMetrics integration, you can create histograms and percentiles (summaries) of your data. The OpenMetrics data is based on New Relic's guidelines in GitHub for higher level metric abstractions, while the remote write data closely matches the schema of the original Prometheus data. Data presentation Comments Histograms A bucket <basename>_bucket{le=\"42\"} will be sent as this: For OpenMetrics: <basename>_buckets For remote write: <basename>_bucket The dimension will be this: For OpenMetrics: {histogram.bucket.upperBound=\"42\"} For remote write: {histogram.bucket.le=\"42\"} Percentiles Quantiles (summaries) are transformed into percentiles. A metric <basename>{quantile=\"0.3\"} will be sent to New Relic as <basename>.percentiles. The dimension will be this: {percentile=\"30\"} NRQL has two functions that work on remote write ingested PromQL: bucketPercentile() and histogram(). The links include query examples.These two functions don't work on OpenMetrics ingested buckets. To better support visualization of histograms, percentiles are calculated based on the histogram metrics and sent to New Relic. To configure the calculated percentiles for OpenMetrics, use the percentiles configuration option.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.60893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "sections": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " identifier for the source of the <em>Prometheus</em> <em>data</em> that matches the value of <em>prometheus</em>_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL <em>query</em> examples When you build queries, be aware that there is no linking between the metrics, entities"
      },
      "id": "603eb04be7b9d20f9d2a07eb"
    },
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.38692,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric <em>data</em> to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce <em>data</em> volumes by 50%. Set your filters and configure <em>data</em> to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql": [
    {
      "sections": [
        "Supported PromQL Features",
        "Important",
        "Supported features",
        "Aggregation operators and functions",
        "Arithmetic binary operators",
        "Logical operators",
        "Date/time functions",
        "Mathematical functions",
        "Rate-like functions",
        "Predictive functions",
        "Time-series selectors",
        "PromQL troubleshooting",
        "Metric types",
        "Limits",
        "Range vector selectors (sliding windows and smoothing behavior)",
        "Query range and data scraping intervals"
      ],
      "title": "Supported PromQL Features",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "d4a93b9db3bfe5639ed01968b6e55a8e0aaa9389",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features/",
      "published_at": "2021-06-25T22:05:07Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic supports PromQL-style queries, and our query builder offers a PromQL-style query mode that translates PromQL syntax queries into the closest NRQL approximation. Although the method of approximation means that a handful of edge cases are not fully supported, it provides coverage for an overwhelming majority of queries, supporting over 99.5% of queries across the 7.8 million top Grafana dashboard downloads. Read on to learn about how we work with PromQL queries, as well as differences between standard PromQL and our PromQL-like query language we want you to be aware of. Important For general information about Prometheus queries and operators, see the Prometheus.io documentation. Supported features We support the following aggregation, arithmetic, mathematical, and rate-like functions. As we continue to expand support for Prometheus and PromQL, this list will be updated. Aggregation operators and functions Aggregation operators: avg() count() min() max() quantile() stddev() stdvar() sum() topk() Aggregation functions: histogram_quantile() <aggregation>_over_time() functions: avg_over_time count_over_time min_over_time max_over_time quantile_over_time stdev_over_time stvar_over_time sum_over_time Arithmetic binary operators + (addition) - (subtraction) * (multiplication) / (division) % (percent) ^ (power/exponents) Logical operators and or Date/time functions day_of_month() day_of_week() days_in_month() hour() minute() month() time() timestamp() year() Mathematical functions abs() ceil() clamp_max() clamp_min() exp() floor() ln() log10() log2() round() sqrt() Rate-like functions delta() deriv() idelta() increase() irate() rate() Predictive functions predict_linear Time-series selectors We offer support for PromQL time-series selectors including the following: instant vector selectors range series selectors offset modifier Important We only support offset queries if every vector in the query has the same offset value. PromQL troubleshooting This section describes differences in behavior between PromQL and our PromQL-style query behavior and how to work with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the query builder. Metric types Prometheus recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase() on counters, but queries in Prometheus still work most of the time even if they dont follow those instructions. However, because NRDB converts PromQL-style accumulating counters to delta counters, our implementation is unforgiving when using these functions on the wrong data type and will produce different or incorrect answers. For this reason, it's best to follow all Prometheus recommendations when working with our PromQL-style queries, even if you don't follow these recommendations in Prometheus. Limits In order to ensure the stability and performance of our system for all users, we place some limits on what queries can be run. In all cases, we enforce a limit of 366 steps in range queries. We also default to only returning 100 timeseries from queries by default. If you want to see more (or fewer), you need to explicitly add a topk() to your query. (Note that the topk() implementation in our PromQL-style query is different from that of Prometheus.) We limit the total memory a query can use. This means that requests for large numbers of time steps or large numbers of time series may be rejected, particularly if they are combined with an aggregation like unique count or quantile which require significantly more memory to compute than simple arithmetic aggregations. Range vector selectors (sliding windows and smoothing behavior) We provide support for sliding window timeseries aggregations. For more information, see our NRQL syntax, clauses, and functions resource and our sliding windows deep dive. For information on translating between NRQL and our PromQL-style language, see Translate PromQL queries to NRQL. Query range and data scraping intervals The range of your query in PromQL must be larger than the duration of the step size of the query to avoid the error \"TIMESERIES bucket size is larger than the current time window\". We inspect data up to one minute old when servicing instant queries. If your scrape interval is greater than 1 minute, some queries may result in No data found. Avoid this by sending data at least once per minute. If the timeseries unit for your NRQL query is less than the scrape interval for your application, some periods will lack data, and the resulting graph may be jagged or contain peaks and valleys. In general, set the step size to your scrape interval, or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.61177,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Query</em> range <em>and</em> <em>data</em> scraping intervals",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the <em>query</em> builder. Metric types <em>Prometheus</em> recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase"
      },
      "id": "603e9523e7b9d292ec2a07ce"
    },
    {
      "sections": [
        "View and query your Prometheus data",
        "Default attributes for the OpenMetrics integration",
        "Default attributes for the remote write integration",
        "NRQL query examples",
        "Get metric names",
        "Get the attributes for a metric",
        "Get the values for an attribute in OpenMetrics",
        "Build the query",
        "Get metric values",
        "Get a chart of the metric",
        "Query counter metrics (deltas)",
        "View connected Redis clients per pod with OpenMetrics",
        "Docker: View average memory free for scraped endpoints",
        "Kubernetes: View average memory usage for pods in a deployment",
        "View data in New Relic",
        "Create histograms and summaries"
      ],
      "title": "View and query your Prometheus data",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "91a0388492ae73a9ddb8a1701b639a6b6a71822a",
      "image": "https://docs.newrelic.com/static/ed6795cfdb010c5eabb1cfe9c83a82a9/69538/img-integration-k8.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data/",
      "published_at": "2021-06-25T22:51:30Z",
      "updated_at": "2021-03-16T04:13:27Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To query and visualize the metrics collected for your Prometheus OpenMetrics or remote write integration with New Relic, you can use NRQL. You can also translate your PromQL-style queries to NRQL using either Grafana or the query builder. All metrics for Docker and Kubernetes are stored in the Metric type. Default attributes for the OpenMetrics integration By default, the following attributes will be added to all metrics for Docker and Kubernetes integrations: Default attributes (all integrations) Description clusterName The name of the cluster provided in the scraper configuration. integrationName The name of this integration (nri-prometheus). integrationVersion The version of the integration; for example, 0.2.0. metricName The name of the metric itself. nrMetricType The type of the New Relic Metric type; for example, Gauges. promMetricType The metric type of the Prometheus metric scrapedEndpoint The URL of the endpoint is being scraped. Kubernetes: If the scraper is running in Kubernetes, New Relic also adds the following attributes to all the metrics: Additional Kubernetes attributes Description deploymentName Name of the deployment, if scraping a pod. label The Kubernetes labels of the object being scraped, prefixed by \"label\". namespaceName Name of the namespace. nodeName Name of the node where the pod being scraped is running, if applicable. podName Name of the pod being scraped, if applicable. serviceName Name of the service being scraped, if applicable Default attributes for the remote write integration By default, the following attributes will be added to Prometheus remote write metrics: Default attributes (all integrations) Description prometheus_server A user supplied label specified as a Prometheus remote write URL parameter. The value supplied should be unique as it is intended to differentiate between source Prometheus servers at query time. Unspecified by default. newrelic.source The name of the New Relic ingest point (prometheusAPI). instrumentation.provider prometheus instrumentation.name remote-write instrumentation.source A user supplied identifier for the source of the Prometheus data that matches the value of prometheus_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL query examples When you build queries, be aware that there is no linking between the metrics, entities, and attributes. Use the following NRQL queries to find out which metrics are available and which attributes are present on these metrics: Get metric names To get all metric names for OpenMetrics: FROM Metric SELECT uniques(metricName) Copy To get metric names for a remote write integration: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' Copy To get metric names for a remote write integration from a single Prometheus source: FROM Metric SELECT uniques(metricName) WHERE instrumentation.provider='prometheus' AND instrumentation.name='remote-write' AND instrumentation.source='<ds>' Copy To get metric names for a specific OpenMetrics endpoint: FROM Metric SELECT uniques(metricName) WHERE scrapedEndpoint='<ep>' Copy To get metric names for a specific OpenMetrics cluster, namespace, or pod: FROM Metric SELECT uniques(metricName) WHERE clusterName='<cn>' Copy FROM Metric SELECT uniques(metricName) WHERE namespaceName='<ns>' Copy FROM Metric SELECT uniques(metricName) WHERE podName='<pod>' Copy Get the attributes for a metric To get all attributes for the selected metric: FROM Metric SELECT keyset() WHERE metricName='<mn>' Copy Get the values for an attribute in OpenMetrics The autocomplete will show all values of the attribute, regardless of the pod. To determine the attribute values for a specific pod: FROM Metric SELECT uniques(<attribute>) WHERE metricName='<mn>' AND podName='<pod>' Copy Build the query Using metric name and attributes, you can query your data. For more information about facets, time series, and time selection, see the NRQL documentation. To build PromQL-style queries, see our docs. Get metric values To get raw metric values: FROM Metric SELECT <metricName> WHERE <attribute>='<value>' Copy Get a chart of the metric To get a chart of the metric with an aggregator of average, min, max, or sum: FROM Metric SELECT <aggregator>(<metricname>) WHERE <attribute>='<value>' TIMESERIES Copy Query counter metrics (deltas) Currently the integration calculates the deltas for counter metrics. This is why queries on counter metrics will show the deltas of the counter instead of the absolute value of the counter. View connected Redis clients per pod with OpenMetrics Docker: This example assumes you are scraping Redis exporters. To view the number of connected Redis clients per endpoint in a cluster: FROM Metric SELECT latest(redis_connected_clients) WHERE clusterName='my-cluster' FACET scrapedEndpoint TIMESERIES Copy Kubernetes: This example assumes that you have Redis pods with the Redis exporter installed. To view the number of connected Redis clients per pod in the default namespace: FROM Metric SELECT latest(redis_connected_clients) WHERE namespaceName='default' FACET podName TIMESERIES Copy Docker: View average memory free for scraped endpoints This example assumes you are scraping node exporters for Docker and want to use OpenMetrics. To view average memory free for all scraped endpoints in a cluster: FROM Metric SELECT average(node_memory_MemFree_bytes) WHERE clusterName='my-cluster' Copy Kubernetes: View average memory usage for pods in a deployment To view average memory usage for all pods in a Kubernetes deployment using OpenMetrics: FROM Metric SELECT average(container_memory_usage_bytes) WHERE deploymentName='my-app-deployment' AND namespaceName='default' Copy View data in New Relic When you query the data, you can view the results in the New Relic UI. You can also visualize the data as charts, histograms, etc. To view the NRQL query results for your Prometheus integration's data: Go to one.newrelic.com > Query your data. For more information, see New Relic's query builder documentation. Create histograms and summaries With remote write or version 1.2.0 or higher of the Prometheus OpenMetrics integration, you can create histograms and percentiles (summaries) of your data. The OpenMetrics data is based on New Relic's guidelines in GitHub for higher level metric abstractions, while the remote write data closely matches the schema of the original Prometheus data. Data presentation Comments Histograms A bucket <basename>_bucket{le=\"42\"} will be sent as this: For OpenMetrics: <basename>_buckets For remote write: <basename>_bucket The dimension will be this: For OpenMetrics: {histogram.bucket.upperBound=\"42\"} For remote write: {histogram.bucket.le=\"42\"} Percentiles Quantiles (summaries) are transformed into percentiles. A metric <basename>{quantile=\"0.3\"} will be sent to New Relic as <basename>.percentiles. The dimension will be this: {percentile=\"30\"} NRQL has two functions that work on remote write ingested PromQL: bucketPercentile() and histogram(). The links include query examples.These two functions don't work on OpenMetrics ingested buckets. To better support visualization of histograms, percentiles are calculated based on the histogram metrics and sent to New Relic. To configure the calculated percentiles for OpenMetrics, use the percentiles configuration option.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.60893,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "sections": "<em>View</em> <em>and</em> <em>query</em> your <em>Prometheus</em> <em>data</em>",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " identifier for the source of the <em>Prometheus</em> <em>data</em> that matches the value of <em>prometheus</em>_server. instrumentation.version Used to identify the version of the remote write API; for example, 0.0.1. NRQL <em>query</em> examples When you build queries, be aware that there is no linking between the metrics, entities"
      },
      "id": "603eb04be7b9d20f9d2a07eb"
    },
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.38692,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric <em>data</em> to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce <em>data</em> volumes by 50%. Set your filters and configure <em>data</em> to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    }
  ],
  "/docs/integrations/prometheus-integrations/view-query-data/view-query-your-prometheus-data": [
    {
      "sections": [
        "Supported PromQL Features",
        "Important",
        "Supported features",
        "Aggregation operators and functions",
        "Arithmetic binary operators",
        "Logical operators",
        "Date/time functions",
        "Mathematical functions",
        "Rate-like functions",
        "Predictive functions",
        "Time-series selectors",
        "PromQL troubleshooting",
        "Metric types",
        "Limits",
        "Range vector selectors (sliding windows and smoothing behavior)",
        "Query range and data scraping intervals"
      ],
      "title": "Supported PromQL Features",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "d4a93b9db3bfe5639ed01968b6e55a8e0aaa9389",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/supported-promql-features/",
      "published_at": "2021-06-25T22:05:07Z",
      "updated_at": "2021-03-16T04:42:39Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic supports PromQL-style queries, and our query builder offers a PromQL-style query mode that translates PromQL syntax queries into the closest NRQL approximation. Although the method of approximation means that a handful of edge cases are not fully supported, it provides coverage for an overwhelming majority of queries, supporting over 99.5% of queries across the 7.8 million top Grafana dashboard downloads. Read on to learn about how we work with PromQL queries, as well as differences between standard PromQL and our PromQL-like query language we want you to be aware of. Important For general information about Prometheus queries and operators, see the Prometheus.io documentation. Supported features We support the following aggregation, arithmetic, mathematical, and rate-like functions. As we continue to expand support for Prometheus and PromQL, this list will be updated. Aggregation operators and functions Aggregation operators: avg() count() min() max() quantile() stddev() stdvar() sum() topk() Aggregation functions: histogram_quantile() <aggregation>_over_time() functions: avg_over_time count_over_time min_over_time max_over_time quantile_over_time stdev_over_time stvar_over_time sum_over_time Arithmetic binary operators + (addition) - (subtraction) * (multiplication) / (division) % (percent) ^ (power/exponents) Logical operators and or Date/time functions day_of_month() day_of_week() days_in_month() hour() minute() month() time() timestamp() year() Mathematical functions abs() ceil() clamp_max() clamp_min() exp() floor() ln() log10() log2() round() sqrt() Rate-like functions delta() deriv() idelta() increase() irate() rate() Predictive functions predict_linear Time-series selectors We offer support for PromQL time-series selectors including the following: instant vector selectors range series selectors offset modifier Important We only support offset queries if every vector in the query has the same offset value. PromQL troubleshooting This section describes differences in behavior between PromQL and our PromQL-style query behavior and how to work with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the query builder. Metric types Prometheus recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase() on counters, but queries in Prometheus still work most of the time even if they dont follow those instructions. However, because NRDB converts PromQL-style accumulating counters to delta counters, our implementation is unforgiving when using these functions on the wrong data type and will produce different or incorrect answers. For this reason, it's best to follow all Prometheus recommendations when working with our PromQL-style queries, even if you don't follow these recommendations in Prometheus. Limits In order to ensure the stability and performance of our system for all users, we place some limits on what queries can be run. In all cases, we enforce a limit of 366 steps in range queries. We also default to only returning 100 timeseries from queries by default. If you want to see more (or fewer), you need to explicitly add a topk() to your query. (Note that the topk() implementation in our PromQL-style query is different from that of Prometheus.) We limit the total memory a query can use. This means that requests for large numbers of time steps or large numbers of time series may be rejected, particularly if they are combined with an aggregation like unique count or quantile which require significantly more memory to compute than simple arithmetic aggregations. Range vector selectors (sliding windows and smoothing behavior) We provide support for sliding window timeseries aggregations. For more information, see our NRQL syntax, clauses, and functions resource and our sliding windows deep dive. For information on translating between NRQL and our PromQL-style language, see Translate PromQL queries to NRQL. Query range and data scraping intervals The range of your query in PromQL must be larger than the duration of the step size of the query to avoid the error \"TIMESERIES bucket size is larger than the current time window\". We inspect data up to one minute old when servicing instant queries. If your scrape interval is greater than 1 minute, some queries may result in No data found. Avoid this by sending data at least once per minute. If the timeseries unit for your NRQL query is less than the scrape interval for your application, some periods will lack data, and the resulting graph may be jagged or contain peaks and valleys. In general, set the step size to your scrape interval, or higher.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.61177,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Query</em> range <em>and</em> <em>data</em> scraping intervals",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": " with and around these differences. This is particularly relevant if you want to use advanced queries and our PromQL-style mode in the <em>query</em> builder. Metric types <em>Prometheus</em> recommendations note that you should only use some functions, like delta(), on gauges, and only use others like rate() and increase"
      },
      "id": "603e9523e7b9d292ec2a07ce"
    },
    {
      "sections": [
        "Translate PromQL queries to NRQL",
        "Tip",
        "Prometheus and New Relic metric types",
        "Mapping between NRQL and our PromQL-style queries",
        "PromQL-style query example",
        "NRQL query example",
        "Filter examples",
        "PromQL-style to NRQL query examples",
        "For more help"
      ],
      "title": "Translate PromQL queries to NRQL",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "View and query data"
      ],
      "external_id": "fcf45fb8fb49f9d22f74574c2e7032533377e584",
      "image": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/images/PROMQL-query-2.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/view-query-data/translate-promql-queries-nrql/",
      "published_at": "2021-06-26T14:28:52Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Do you have a PromQL query youd like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style query language to explore your Prometheus OpenMetrics integration data along with other data sent to New Relic. Tip To run PromQL-style queries in New Relic One, go to the query builder advanced PromQL-style mode. Prometheus and New Relic metric types The different metric types supported by Prometheus and New Relic are related to each other: New Relic Prometheus Description Count Counter The Prometheus counter is a cumulative sum while the New Relic count is a delta sum. For example, if you see 2 requests in the first reporting period and 3 requests in the second reporting period. The Prometheus counter will report 2 and then 5, while the New Relic count will report 2 and then 3. Gauge Gauge A Prometheus gauge is similar to a New Relic gauge. Multiple counts Histogram Prometheus automatically maps a histogram to a set of counters. In New Relic, these counters should be changed to deltas and reported as counts. Gauges and counts Summary Prometheus represents a Summary with a given basename as the following time series: a basename_sum a basename_count and 0 or more of basename{quantile=\".xx\"...} metrics New Relic maps the _sum as a Summary, the _count as a Counter, and each quantile metric as a Gauge. Summary (No equivalent in Prometheus) New Relic has a distinct metric type called a summary that is different than the Prometheus summary. It is designed for reporting aggregated discrete events so that you can query the count, sum, min, max, and average values. Mapping between NRQL and our PromQL-style queries Tip To see how New Relic translates PromQL-style queries to NRQL, write a query in the query builder PromQL-style tab, then switch to the NRQL tab. This table shows the mapping between NRQL and our PromQL-style queries when exploring data. For more contextual information, see the examples. Description Mapping between NRQL and PromQL-style queries Search for attributes: Explore the attributes on the container_memory_usage_bytes metric. PromQL: container_memory_usage_bytes Copy NRQL: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy Find attribute's value: Explore the current value of the container_memory_usage_bytes metric for unique id attributes. PromQL: sum(container_memory_usage_bytes) by (id) Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy Visualize the attribute's value: Chart the value of the container_memory_usage_bytes metric with the given id attribute value. PromQL: container_memory_usage_bytes{id=\"/\"} Copy NRQL: FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = '/' TIMESERIES Copy PromQL-style query example 1. Start your query. When exploring your data for a particular metric in PromQL, such as memory by container usage in bytes, you can start with a query such as: container_memory_usage_bytes Copy This will chart all the unique metric timeseries for the input metric. 2. Filter the query results. Looking at the data, you can add more query parameters to filter down the number of metric timeseries. For example, if you only want timeseries where the id is /, the PromQL-style query will be: container_memory_usage_bytes{id=\"/\"} Copy PromQL-style example: To filter the data, run this PromQL-style query: container_memory_usage_bytes { id=\"/\"}. NRQL query example 1. Query available metrics. To explore your data, start by looking at all the available metrics. Use the following NRQL query: FROM Metric SELECT uniques(metricName) Copy 2. Find unique attributes. Once you have found the metric you want to review, such as container_memory_usage_bytes, you can find the unique attributes with the following query: FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes' Copy The results will show each available attribute key and the value type (string, boolean, or number). 3. Aggregate and chart the metrics. To chart metrics using NRQL, you first need an aggregation function. For example, you can use latest for gauges, sum for counts, and average for summaries. As the following chart shows, all the unique timeseries are aggregated into one unique timeseries by default: one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT keyset() WHERE metricName = 'container_memory_usage_bytes'. 4. View metrics by ID. To view the unique metric timeseries with various id values, run the following query: FROM Metric SELECT latest(container_memory_usage_bytes) FACET id Copy one.newrelic.com > Query your data: This example shows the data you see after running FROM Metric SELECT latest(container_memory_usage_bytes) FACET id. 5. Add the selected ID to the query. Next you can select an id value and put it in the NRQL where clause. FROM Metric SELECT latest(container_memory_usage_bytes) WHERE id = \"/\" timeseries Copy one.newrelic.com > Query your data: This example shows the data displayed after running From Metric select latest(container_memory_usage_bytes) where id = \"/\" timeseries. Filter examples Both our PromQL-style query language and NRQL provide syntax to filter down the number of unique metric timeseries. PromQL-style uses brackets to filter. NRQL uses a WHERE clause. Here are some example queries: Description PromQL-style and NRQL queries Select data with specific values. PromQL: go_memstats_heap_alloc_bytes{job=\"apiserver\", instance=\"1234\"}) Copy NRQL: To only select data with specific values in NRQL, use the WHERE clause with =. In this example, all data must have the selected value for job and handler. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE job = 'apiserver' AND instance = '1234' TIMESERIES Copy Select data with multiple values. PromQL: go_memstats_heap_alloc_bytes{environment=~\"staging|testing|development\",method!=\"GET\"} Copy NRQL: In NRQL use the in clause to select multiple values for an attribute and the != sign to select all values but the one listed. In this example, the environment can be staging, testing, or development, and the method cannot be GET. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHERE environment IN ('staging', 'testing', 'development') AND method != 'GET' TIMESERIES Copy Select data using partial string values. PromQL: go_memstats_heap_alloc_bytes{job=~\"api.*\"} Copy NRQL: In NRQL use the LIKE clause to match part of a string value. In this example, all data will be returned where the job attributes start with api. FROM Metric SELECT latest(go_memstats_heap_alloc_bytes) WHEREe job LIKE 'api%' TIMESERIES Copy PromQL-style to NRQL query examples You can simulate the following PromQL-style queries with NRQL queries: Description PromQL-style and NRQL queries Measure the per second rate over the last minute of the http_request_total metric. PromQL: sum(rate(http_requests_total[1m])) Copy NRQL: FROM Metric SELECT rate(sum(http_request_total), 1 second) TIMESERIES 1 minute Copy Chart the difference of the two metrics, then divide by 1024. PromQL: (instance_memory_limit_bytes - instance_memory_usage_bytes) / 1024 Copy NRQL: FROM Metric SELECT (latest(instance_memory_limit_bytes) - latest(instance_memory_usage_bytes)) / 1024 TIMESERIES Copy Provide the summed rate per 30-second interval by each handler. PromQL: sum(rate(http_requests_total[30s])) by (handler) Copy NRQL: FROM Metric SELECT rate(sum(http_requests_total), 30 seconds) FACET handler TIMESERIES Copy Chart the difference in the two metrics where the instance is named foo and the fstype is either ext4 or xfs. PromQL: (node_filesystem_free_bytes{instance='foo',fstype=~\"ext4|xfs\"} / node_filesystem_size_bytes{instance='foo',fstype=~\"ext4|xfs\"}) Copy NRQL: FROM Metric SELECT latest(node_filesystem_free_bytes) / latest(node_filesystem_size_bytes) WHERE instance = 'foo' AND fstype IN ('ext4', 'xfs') Copy For more help",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 181.60962,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Translate PromQL <em>queries</em> to NRQL",
        "sections": "<em>Prometheus</em> <em>and</em> New Relic metric types",
        "tags": "<em>View</em> <em>and</em> <em>query</em> <em>data</em>",
        "body": "Do you have a PromQL <em>query</em> youd like to convert to NRQL? This document provides examples that show you how to convert some common PromQL queries to NRQL queries. You can use our PromQL-style <em>query</em> language to explore your <em>Prometheus</em> OpenMetrics integration <em>data</em> along with other <em>data</em> sent to New"
      },
      "id": "603ead4528ccbcbecfeba77b"
    },
    {
      "sections": [
        "Send Prometheus metric data to New Relic",
        "Prometheus OpenMetrics or remote write integration?",
        "Prometheus remote write integration",
        "Scale your data and get moving quickly",
        "How it works",
        "Remote write compatibility and requirements",
        "Prometheus OpenMetrics integrations",
        "Reduce overhead and scale your data",
        "Kubernetes",
        "Docker",
        "OpenMetrics integrations compatibility and requirements",
        "Important",
        "What's next"
      ],
      "title": "Send Prometheus metric data to New Relic",
      "type": "docs",
      "tags": [
        "Integrations",
        "Prometheus integrations",
        "Get started"
      ],
      "external_id": "c43eafc49c9c82cbf8642897c868c9602cecc6b9",
      "image": "https://docs.newrelic.com/static/3b6e65cd4f0d292124399b59a6195a0a/8c557/Prometheus-remote-write-dashboard.png",
      "url": "https://docs.newrelic.com/docs/integrations/prometheus-integrations/get-started/send-prometheus-metric-data-new-relic/",
      "published_at": "2021-06-26T14:26:55Z",
      "updated_at": "2021-06-26T14:26:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This page provides an overview of New Relic's Prometheus integration options and how they work. The information here will help you choose from among our options based on which one best fits your unique business needs. Prometheus OpenMetrics or remote write integration? We currently offer two integration options: Prometheus remote write integration and Prometheus OpenMetrics integration for Kubernetes or Docker. We recommend getting started with the remote write integration if you already have a Prometheus server install base. If you find it hard to manage your Prometheus cluster, or if you are getting started with integrating Prometheus Metrics, you should use OpenMetrics. Prometheus remote write integration Prometheus OpenMetrics for Kubernetes or Docker Benefits Use this if you currently have Prometheus servers and want an easy way to access your combined metrics from New Relic. It only takes one line of yaml in your Prometheus configuration. You'll be able to access your metrics through both New Relic and Prometheus. You don't need to make any additional adjustments for data to remain available in Prometheus. Federation: Allows you to combine data from multiple servers into a single source. Prometheus High Availability support: We de-duplicate data from HA-pairs on ingest. Use this if youre looking for an alternative or replacement to a Prometheus server that stores all your metrics directly in New Relic. You wont have to manage any Prometheus servers yourself. You don't need local storage. Keep in mind You will still need to manage your Prometheus servers, although you should be able to reduce your storage retention, and therell be fewer query loads to the server. Slightly more complex setup. No support for High Availability replicas. The Kubernetes operator is not available for enhanced operations automation. Recommendations Evaluate your observability needs to manage your data volumes better: The scrape interval is the biggest factor influencing data volumes: select it based on your observability needs. For example, changing from 15s (default value) to 30s can reduce data volumes by 50%. Set your filters and configure data to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our Prometheus integrations: You can use Grafana or other query tools via New Relic's Prometheus' API. You benefit from more nuanced security and user management options as part of New Relic One. The New Relic Telemetry Data Platform can be the centralized long-term data store for all your Prometheus metrics, allowing you to observe all your data in one place. You can execute queries to scale, supported by New Relic. Prometheus remote write integration The Prometheus remote write integration allows you to forward telemetry data from your existing Prometheus servers to New Relic. Once integrated, you can leverage the full range of options for setup and management, from raw data to queries, dashboards, and more. Scale your data and get moving quickly With the Prometheus remote write integration, you can: Store and visualize crucial metrics on a single platform Combine and group data across your entire software stack Get a fully connected view of the relationship between data about your software stack and the behaviors and outcomes youre monitoring Connect your Grafana dashboards (optional). Prometheus remote write dashboard How it works Signup for New Relic is fast and free  we won't even ask for a credit card number. Once logged in, you can get data flowing with a few simple steps: Generate your remote_write URL. Add the new remote_write URL to the configuration file for your Prometheus server. Restart your Prometheus server. Check for your data. Query and explore! Read the setup docs Add Prometheus data Remote write compatibility and requirements New Relic supports the Prometheus remote write integration for Prometheus versions 2.15.0 or newer. Prometheus OpenMetrics integrations New Relics Prometheus OpenMetrics integrations for Docker and Kubernetes allow you to scrape Prometheus endpoints and send the data to New Relic, so you can store and visualize crucial metrics on one platform. With these integrations, you can: Automatically identify a static list of endpoints. Collect metrics that are important to your business. Query and visualize this data in the New Relic UI. Connect your Grafana dashboards (optional). Kubernetes OpenMetrics dashboard Reduce overhead and scale your data Collect, analyze, and visualize your metrics data from any source, alongside your telemetry data, so you can correlate issues all in one place. Out-of-the-box integrations for open-source tools like Prometheus make it easy to get started, and eliminate the cost and complexity of hosting, operating, and managing additional monitoring systems. Prometheus OpenMetrics integrations gather all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. To learn more about how to scale your data without the hassles of managing Prometheus and a separate dashboard tool, see New Relic's Prometheus OpenMetrics integration blog post. Kubernetes In a Kubernetes environment, New Relic automatically discovers the endpoints in the same way that the Prometheus Kubernetes collector does it. The integration looks for the prometheus.io/scrape annotation or label. You can also identify additional static endpoints in the configuration. Docker The Prometheus OpenMetrics integration gathers all your data in one place, and New Relic stores the metrics from Prometheus. This integration helps remove the overhead of managing storage and availability of the Prometheus server. OpenMetrics integrations compatibility and requirements For Kubernetes and Docker OpenMetrics integrations, you should be aware of the following compatibility and requirements information. Kubernetes New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2 and Kubernetes versions 1.9 or higher. The integration was tested using Kubernetes 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For more details, see the metrics API documentation. Important Recommendation: Always run the scraper with one replica. Adding more replicas will result in duplicated data. Docker New Relic has contributed the Prometheus integration to the open source community under an Apache 2.0 license. This integration supports Prometheus protocol version 2. The integration was tested using Docker 1.9, 1.11, and 1.13 on kops, GKE, and minikube. Limits apply to the metrics you send. For details, see the metrics API documentation. What's next Ready to get moving? Here are some suggested next steps: Read the how-to for completing the remote write integration. Read the how-to for completing the Prometheus OpenMetrics integration. Both integration options generate dimensional metrics that are subject to the same rate limits described in the Metric API. Learn about Grafana support options. Explore the range of other options available as part of the Telemetry Data Platform.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 132.38684,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Send <em>Prometheus</em> metric <em>data</em> to New Relic",
        "sections": "<em>Prometheus</em> OpenMetrics <em>integrations</em>",
        "tags": "<em>Prometheus</em> <em>integrations</em>",
        "body": ") to 30s can reduce <em>data</em> volumes by 50%. Set your filters and configure <em>data</em> to target (see metrics or targets). Balance remote write(s) between one or more new relic accounts or sub-accounts to manage rate limits. Regardless of the option you chose, with our <em>Prometheus</em> <em>integrations</em>: You can use Grafana"
      },
      "id": "603ea41964441f0d824e8874"
    }
  ],
  "/docs/introduction-new-relic-agent-sdk": [
    {
      "sections": [
        "Our EU and US region data centers",
        "Requirements",
        "Regions and availability",
        "Regions and account hierarchy",
        "Hierarchy example for partnership accounts",
        "Create an EU region account",
        "API endpoints for EU region accounts",
        "Access New Relic One",
        "Billing and pricing",
        "Operational access and processing",
        "Verify your account is based in EU region"
      ],
      "title": "Our EU and US region data centers",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Welcome to New Relic",
        "Get started"
      ],
      "external_id": "38baae8599707418dbb5d42e05001e202b1bd28c",
      "image": "https://docs.newrelic.com/static/d22ffcc8535edcfb0d8bc64ff3444874/c1b63/Eu_account_hierarchy.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/our-eu-us-region-data-centers/",
      "published_at": "2021-06-26T08:41:51Z",
      "updated_at": "2021-06-26T08:41:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's global data hosting structure consists of two regions: the European Union (EU) region and the United States (US) region. Selecting your preferred region during the account setup process allows you to specify the region where your performance monitoring data will be hosted. You can also create accounts in each region. Requirements Access to the New Relic EU region requires the latest agent version. For new customers: Install the most recent agent version. For existing customers: Update to the most recent agent version. Minimum agent version required: C SDK 1.0.0 or higher Go 2.0.0 or higher Java 4.0.0 or higher .NET 8.0.0 or higher Node.js 3.0.0 or higher PHP 8.0.0.204 or higher Python 3.0.0.89 or higher Ruby 5.0.0.342 or higher Regions and availability Your choice of data center is not limited by your geographic location. You can choose to create an account with data hosted in either the EU or US region, regardless of where you or your systems reside. You can also create accounts in each region. New Relic offers almost all the same active products, features, support offerings, and performance levels in the EU region as what is offered in the US region. Exceptions: The following are not supported with an EU region account: Infinite Tracing is not available. APM's weekly performance reports are not available. The Plugins product is unavailable and is not supported. Errors inbox is not available. Log patterns Deprecated products and features are not available. New Relic's Incident Intelligence service operates solely in the U.S. Whether you store your data in New Relics US region data center or New Relics EU region data center, by using New Relic Incident Intelligence, you consent that New Relic may move your data to, and process your data in, the US region. Regions and account hierarchy You can create accounts in each region. If your data is currently being hosted in the US region, you must create a new account to store data in the EU region. You cannot view EU data from a US account, or US data from an EU account. The data collected remains separate. The data cannot be aggregated or migrated between accounts. For standard accounts, you can only have one master account. For more information, see Manage apps or users with sub-accounts. For partnership accounts, no changes to the partnership owner account are required. However, data cannot be shared across regions, so a partnership requires a master account for each region. Hierarchy example for partnership accounts With partnership accounts, a new master account must be created for any data to be host in the EU region. This hierarchy illustrates how global accounts are structured with partnership owner accounts. Data is not aggregated beyond the master account. Example hierarchy for partnership organizations. Because data cannot be shared across regions, a partnership will require a master account for each region. Create an EU region account To create a New Relic account in the EU region: Go to the New Relic signup page. OR If you have a specific offer from a New Relic partner, follow that link directly. Follow the online steps to create your account. From the Select your region dropdown, select European Union. Agree to the Terms of Service. When you receive an email confirmation message, select the link to confirm your account and sign in to New Relic. Then install or update to the most recent agent version. API endpoints for EU region accounts If you have an EU region account, use the appropriate endpoints to access the following New Relic APIs: API EU endpoint Browser source maps API sourcemaps.service. eu .newrelic.com Copy Infrastructure Alert API infra-api. eu .newrelic.com Copy Dashboard API rpm. eu .newrelic.com/api/explore/dashboards/list Copy Insert API insights-collector. eu01 .nr-data.net Copy Insights Query API insights-api. eu .newrelic.com Copy Mobile apps rpm. eu .newrelic.com/mobile Copy NerdGraph GraphiQL API api. eu .newrelic.com/graphiql Copy Partner API The partner API is a global API with no regional data differences. Use this endpoint for both EU and US accounts: rpm.newrelic.com/api/v2/partners/ Copy REST API api. eu .newrelic.com Copy Synthetics API synthetics. eu .newrelic.com/synthetics/api Copy Trace API trace-api. eu .newrelic.com/trace/v1 Copy Metric API metric-api. eu .newrelic.com/metric/v1 Copy Log API log-api. eu .newrelic.com/log/v1 Copy Access New Relic One If your accounts report data to the EU data center, use the following link to go to New Relic One: one.eu.newrelic.com. Billing and pricing New Relic's account billing process and pricing options are the same for both the EU and US regions. Operational access and processing Customer Data is hosted in the region selected during account creation. Systems Operations Data is stored in the US region. All other information, including account information (such as license subscription information, billing, and internal monitoring) is hosted in the US region and replicated in the EU region. New Relic may access and process Customer Data in the United States and such other jurisdictions where New Relic has affiliates and subsidiaries, including as may be necessary to maintain, secure, or perform the services, to provide technical support, or as necessary to comply with law or a binding order of a government body. Customer Data from existing New Relic accounts cannot be transferred or shared across regions, and new data generated cannot be shared with existing accounts, even in partnership accounts. Verify your account is based in EU region Use either of these options to verify whether your account data is hosted in the EU region's data center: In APM, mouse over the application name to view the URL. If it begins with rpm.eu.newrelic.com/, it is an EU-based account. Check your New Relic license key. If it begins with EU, it is an EU-based account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 70.3734,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Access <em>New</em> <em>Relic</em> One",
        "tags": "Welcome <em>to</em> <em>New</em> <em>Relic</em>",
        "body": " create accounts in each region. Requirements Access to the <em>New</em> <em>Relic</em> EU region requires the latest <em>agent</em> version. For <em>new</em> customers: Install the most recent <em>agent</em> version. For existing customers: Update to the most recent <em>agent</em> version. Minimum <em>agent</em> version required: C <em>SDK</em> 1.0.0 or higher Go 2.0.0"
      },
      "id": "6044586c64441f844b378edd"
    },
    {
      "sections": [
        "Collect data from any source",
        "Agent APIs",
        "Telemetry SDK",
        "Trace API",
        "Metric API",
        "Event API",
        "Log API"
      ],
      "title": "Collect data from any source",
      "type": "developer",
      "tags": [
        "Agent API",
        "Telemetry SDK",
        "Trace API",
        "Metric API",
        "Event API"
      ],
      "external_id": "5bfb043fffe42ea4a78d5a90bf8e92aa8b8f8c33",
      "image": "",
      "url": "https://developer.newrelic.com/collect-data/collect-data-from-any-source/",
      "published_at": "2021-06-30T01:41:09Z",
      "updated_at": "2021-06-25T01:40:38Z",
      "document_type": "page",
      "popularity": 1,
      "info": "Open source emitters. APIs. New Relic agents. Get data from anywhere. ",
      "body": "New Relic products report a lot of data out of the box. When you use products like APM, Browser, Mobile, Infrastructure monitoring, or an integration, by default you receive performance data. But you may want to bring data into New Relic that isn't collected by default. Maybe you want an API-based solution that doesn't require install of an agent. Maybe you want to bring telemetry data from another analysis service into New Relic. This page describes several ways to get data into New Relic. Step 1 of 6 Agent APIs If you use our APM, Browser, or Mobile agents to report data, you can use their associated APIs to report custom data. For example, if you monitor your application with our APM Python agent, you can use the Python agent API to set up custom instrumentation. See the agent APIs. Step 2 of 6 Telemetry SDK Our Telemetry SDKs are language wrappers for our Trace API and Metric API (and eventually our Log API and Event API). These SDKs let you easily send metrics and trace data to New Relic without needing to install an agent. For customers, we offer open-source exporters and integrations that use the Telemetry SDKs to send metrics and trace data: Istio adaptor Prometheus OpenMetrics (for Docker | for Kubernetes) OpenCensus exporter (for Go | for Python) DropWizard exporter Micrometer exporter Want to build your own solution? See our Telemetry SDK docs. Step 3 of 6 Trace API Our Trace API lets you send distributed tracing data to New Relic and consolidate tracing data from multiple sources in one place. We accept trace data in two formats: Zipkin format New Relic format (if you dont have Zipkin-format data, youd use this) bash Copy $ curl -i -X POST https://trace-api.newrelic.com/trace/v1 \\ > -H \"Content-Type: application/json\" \\ > -H \"Api-Key: $INSIGHTS_INSERT_API_KEY\" \\ > -H 'Data-Format: newrelic' \\ > -H 'Data-Format-Version: 1' \\ > -d '[ $ { $ \"common\": { $ \"attributes\": { $ \"service.name\": \"Test Service A\", $ \"host\": \"host123.test.com\" $ } $ }, $ \"spans\": [ $ { $ \"trace.id\": \"123456\", $ \"id\": \"ABC\", $ \"attributes\": { $ \"duration.ms\": 12.53, $ \"name\": \"/home\" $ } $ }, $ { $ \"trace.id\": \"123456\", $ \"id\": \"DEF\", $ \"attributes\": { $ \"service.name\": \"Test Service A\", $ \"host\": \"host456.test.com\", $ \"duration.ms\": 2.97, $ \"name\": \"/auth\", $ \"parent.id\": \"ABC\" $ } $ } $ ] $ } $ ]' Step 4 of 6 Metric API You can use our Metric API to send metric data to New Relic from any source. bash Copy $ curl -i -X POST https://metric-api.newrelic.com/metric/v1 \\ > -H \"Content-Type: application/json\" \\ > -H \"Api-Key: $INSIGHTS_INSERT_API_KEY\" \\ > -d '[ $ { $ \"metrics\": [ $ { $ \"name\": \"memory.heap\", $ \"type\": \"gauge\", $ \"value\": 2.3, $ \"timestamp\": 1531414060739, $ \"attributes\": { $ \"host.name\": \"dev.server.com\" $ } $ } $ ] $ } $ ]' Step 5 of 6 Event API For sending arbitrary events to New Relic, you can use our Event API. We save these events as a new event type, which can then be queried via NRQL. (Eventually, the Telemetry SDKs will support the Event API.) bash Copy $ curl -i -X POST https://insights-collector.newrelic.com/v1/accounts/$ACCOUNT_ID/events \\ > -H \"Content-Type: application/json\" \\ > -H \"x-insert-key: $INSIGHTS_INSERT_API_KEY\" \\ > -d '[ $ { $ \"eventType\": \"LoginEvent\", $ \"service\": \"login-service\", $ \"customerId\": \"xyz\" $ } $ ]' Step 6 of 6 Log API If our existing logging integrations dont meet your needs, you can use our Log API to send any arbitrary log data to New Relic. (Eventually, the Telemetry SDKs will support the Log API.) bash Copy $ curl -i -X POST https://log-api.newrelic.com/log/v1 \\ > -H \"Content-Type: application/json\" \\ > -H \"Api-Key: $INSIGHTS_INSERT_API_KEY\" \\ > -d '[ $ \"logs\": [ $ { $ \"timestamp\": 1593538496000, $ \"message\": \"User xyz logged in\", $ \"service\": \"login-service\", $ \"hostname\": \"login.example.com\" $ } $ ] $ ]'",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 67.98484,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Agent</em> APIs",
        "info": "Open source emitters. APIs. <em>New</em> <em>Relic</em> <em>agents</em>. Get data from anywhere. ",
        "tags": "<em>Agent</em> API",
        "body": "-based solution that doesn&#x27;t require install of an <em>agent</em>. Maybe you want to bring telemetry data from another analysis service into <em>New</em> <em>Relic</em>. This page describes several ways to get data into <em>New</em> <em>Relic</em>. Step 1 of 6 <em>Agent</em> APIs If you use our APM, Browser, or Mobile agents to report data, you can use"
      },
      "id": "6091fa39e7b9d2cedb506906"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/",
      "sections": [
        "Welcome to New Relic",
        "Create a free account",
        "Start collecting data",
        "Set up alerts",
        "Telemetry Data Platform",
        "Introduction to Telemetry Data Platform",
        "Data explorer",
        "Dashboards",
        "Log management",
        "APIs",
        "Manage data",
        "Build on New Relic One",
        "Full-Stack Observability",
        "Introduction to Full-Stack Observability",
        "APM",
        "Browser",
        "Distributed Tracing",
        "Infrastructure",
        "Logs in context",
        "Mobile",
        "Serverless",
        "Synthetics",
        "Workloads",
        "Alerts and Applied Intelligence (AI)",
        "Introduction to Alerts",
        "Introduction to Applied Intelligence",
        "Incident Intelligence",
        "Incident Workflows",
        "Proactive Detection",
        "New Relic integrations",
        "Back-end, front-end, and mobile applications",
        "Infrastructure and cloud platforms",
        "Open-source monitoring systems",
        "Security, privacy, and legal information",
        "Data privacy",
        "Security compliance",
        "Security bulletins",
        "Licenses"
      ],
      "published_at": "2021-06-30T01:37:42Z",
      "title": "New Relic Documentation",
      "updated_at": "2021-05-23T01:37:22Z",
      "type": "docs",
      "external_id": "807276d5fdd805ba0247a1ce1016af986fb31995",
      "popularity": 1,
      "body": "Welcome to New Relic If you're new, follow these three steps to create an account and get going. (It's free!) Scroll on for more about our Telemetry Data Platform, Full-Stack Observability, and Applied Intelligence. Or, to get a wider view of our platform's capabilities, read the Intro to New Relic, and use our solutions and best practices guides. Create a free account No credit card required. Start collecting data Our UI guides you through setup and install. Set up alerts Get notified quickly about changes in your system. Telemetry Data Platform Ingest, visualize, and alert on all your telemetry data in one place. Introduction to Telemetry Data Platform How to manage all your monitoring in one place. Data explorer Query and build charts with NRQL, our PromQL-style syntax, or our visual chart builder. Dashboards Combine data from anywhere in our platform into customized dashboards. Log management Get logs alongside your telemetry data with our fast, scalable log management. APIs Find APIs to send data in, get data out, or manage the New Relic platform. Manage data Monitor and control your data usage. Build on New Relic One Learn how to build custom apps on our platform at developer.newrelic.com Full-Stack Observability Analyze and troubleshoot problems easily across your entire software stack. Introduction to Full-Stack Observability Get deep insight into everything from infrastructure to server code to end-user apps. APM Get real-time and trending data about your app's performance and stability. Browser Measure website performance, track errors, and see how users interact with your site. Distributed Tracing Track requests through your distributed system to find trends and anomalies. Infrastructure Monitor your infrastructurehosts, cloud providers, container services, backend services, orchestrators, and more. Logs in context Link your log data to APM agent data so you can pinpoint where things are going wrong. Mobile Understand user journeys in your Android and iOS apps and troubleshoot crashes. Serverless Monitor AWS Lambda, Azure Functions, and Google Cloud Functions. Synthetics Simulate user activity to detect outages and fix poor website performance. Workloads Group or monitor entities across your entire stack based on a team or a set of responsibilities. Alerts and Applied Intelligence (AI) Automatically detect anomalies, correlate issues, and reduce alert noise. Introduction to Alerts Get notified about important changes in your system based on any data you connect to New Relic. Introduction to Applied Intelligence Recognize issues sooner, resolve problems faster, and reduce noise for better incident management. Incident Intelligence Get an overview of all your incidents. See sources and related events, and find out how they all correlate. Incident Workflows Enrich your incidents with New Relic data before sending them to your notification platform. Proactive Detection Get notified by Slack or webhook of unusual app behavior. New Relic integrations Integrations connect the technologies in your stack to New Relic. Here are a few of our 370+ integrations: Back-end, front-end, and mobile applications Android Browser (JavaScript) C SDK Go iOS Java .NET Node.js PHP Python Ruby Synthetics Infrastructure and cloud platforms Apache AWS Azure Google Cloud Kafka Kubernetes Linux Microsoft SQL MongoDB MySQL NGINX PostgreSQL Redis Windows Open-source monitoring systems Dropwizard Istio JMX Kamon Micrometer OpenCensus OpenTelemetry Prometheus StatsD See all 360+ integrations Security, privacy, and legal information Find out how we ensure security, data privacy, and compliance, and find our terms of service. Data privacy New Relic takes your data privacy seriously. Our principles-based approach aims to go beyond the legal requirements for consent. Security compliance Whether your data is in transit to New Relic or at rest in our storage, strong encryption measures can help protect against unauthorized access or theft of valuable data. Security bulletins Security bulletins are a way for us to let you know about security vulnerabilities, remediation strategies, and applicable updates. Licenses All about New Relic's licenses, Usage Plan, and policies. Find out what open source software we use in our products.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 67.349945,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>New</em> <em>Relic</em> Documentation",
        "sections": "Welcome <em>to</em> <em>New</em> <em>Relic</em>",
        "body": " alert noise. <em>Introduction</em> to Alerts Get notified about important changes in your system based on any data you connect to <em>New</em> <em>Relic</em>. <em>Introduction</em> to Applied Intelligence Recognize issues sooner, resolve problems faster, and reduce noise for better incident management. Incident Intelligence Get"
      },
      "id": "5be4e5d38e9c0f3585d72edf"
    }
  ],
  "/docs/introduction-new-relic-mobile-unity": [
    {
      "sections": [
        "Our EU and US region data centers",
        "Requirements",
        "Regions and availability",
        "Regions and account hierarchy",
        "Hierarchy example for partnership accounts",
        "Create an EU region account",
        "API endpoints for EU region accounts",
        "Access New Relic One",
        "Billing and pricing",
        "Operational access and processing",
        "Verify your account is based in EU region"
      ],
      "title": "Our EU and US region data centers",
      "type": "docs",
      "tags": [
        "Using New Relic",
        "Welcome to New Relic",
        "Get started"
      ],
      "external_id": "38baae8599707418dbb5d42e05001e202b1bd28c",
      "image": "https://docs.newrelic.com/static/d22ffcc8535edcfb0d8bc64ff3444874/c1b63/Eu_account_hierarchy.png",
      "url": "https://docs.newrelic.com/docs/using-new-relic/welcome-new-relic/get-started/our-eu-us-region-data-centers/",
      "published_at": "2021-06-26T08:41:51Z",
      "updated_at": "2021-06-26T08:41:50Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's global data hosting structure consists of two regions: the European Union (EU) region and the United States (US) region. Selecting your preferred region during the account setup process allows you to specify the region where your performance monitoring data will be hosted. You can also create accounts in each region. Requirements Access to the New Relic EU region requires the latest agent version. For new customers: Install the most recent agent version. For existing customers: Update to the most recent agent version. Minimum agent version required: C SDK 1.0.0 or higher Go 2.0.0 or higher Java 4.0.0 or higher .NET 8.0.0 or higher Node.js 3.0.0 or higher PHP 8.0.0.204 or higher Python 3.0.0.89 or higher Ruby 5.0.0.342 or higher Regions and availability Your choice of data center is not limited by your geographic location. You can choose to create an account with data hosted in either the EU or US region, regardless of where you or your systems reside. You can also create accounts in each region. New Relic offers almost all the same active products, features, support offerings, and performance levels in the EU region as what is offered in the US region. Exceptions: The following are not supported with an EU region account: Infinite Tracing is not available. APM's weekly performance reports are not available. The Plugins product is unavailable and is not supported. Errors inbox is not available. Log patterns Deprecated products and features are not available. New Relic's Incident Intelligence service operates solely in the U.S. Whether you store your data in New Relics US region data center or New Relics EU region data center, by using New Relic Incident Intelligence, you consent that New Relic may move your data to, and process your data in, the US region. Regions and account hierarchy You can create accounts in each region. If your data is currently being hosted in the US region, you must create a new account to store data in the EU region. You cannot view EU data from a US account, or US data from an EU account. The data collected remains separate. The data cannot be aggregated or migrated between accounts. For standard accounts, you can only have one master account. For more information, see Manage apps or users with sub-accounts. For partnership accounts, no changes to the partnership owner account are required. However, data cannot be shared across regions, so a partnership requires a master account for each region. Hierarchy example for partnership accounts With partnership accounts, a new master account must be created for any data to be host in the EU region. This hierarchy illustrates how global accounts are structured with partnership owner accounts. Data is not aggregated beyond the master account. Example hierarchy for partnership organizations. Because data cannot be shared across regions, a partnership will require a master account for each region. Create an EU region account To create a New Relic account in the EU region: Go to the New Relic signup page. OR If you have a specific offer from a New Relic partner, follow that link directly. Follow the online steps to create your account. From the Select your region dropdown, select European Union. Agree to the Terms of Service. When you receive an email confirmation message, select the link to confirm your account and sign in to New Relic. Then install or update to the most recent agent version. API endpoints for EU region accounts If you have an EU region account, use the appropriate endpoints to access the following New Relic APIs: API EU endpoint Browser source maps API sourcemaps.service. eu .newrelic.com Copy Infrastructure Alert API infra-api. eu .newrelic.com Copy Dashboard API rpm. eu .newrelic.com/api/explore/dashboards/list Copy Insert API insights-collector. eu01 .nr-data.net Copy Insights Query API insights-api. eu .newrelic.com Copy Mobile apps rpm. eu .newrelic.com/mobile Copy NerdGraph GraphiQL API api. eu .newrelic.com/graphiql Copy Partner API The partner API is a global API with no regional data differences. Use this endpoint for both EU and US accounts: rpm.newrelic.com/api/v2/partners/ Copy REST API api. eu .newrelic.com Copy Synthetics API synthetics. eu .newrelic.com/synthetics/api Copy Trace API trace-api. eu .newrelic.com/trace/v1 Copy Metric API metric-api. eu .newrelic.com/metric/v1 Copy Log API log-api. eu .newrelic.com/log/v1 Copy Access New Relic One If your accounts report data to the EU data center, use the following link to go to New Relic One: one.eu.newrelic.com. Billing and pricing New Relic's account billing process and pricing options are the same for both the EU and US regions. Operational access and processing Customer Data is hosted in the region selected during account creation. Systems Operations Data is stored in the US region. All other information, including account information (such as license subscription information, billing, and internal monitoring) is hosted in the US region and replicated in the EU region. New Relic may access and process Customer Data in the United States and such other jurisdictions where New Relic has affiliates and subsidiaries, including as may be necessary to maintain, secure, or perform the services, to provide technical support, or as necessary to comply with law or a binding order of a government body. Customer Data from existing New Relic accounts cannot be transferred or shared across regions, and new data generated cannot be shared with existing accounts, even in partnership accounts. Verify your account is based in EU region Use either of these options to verify whether your account data is hosted in the EU region's data center: In APM, mouse over the application name to view the URL. If it begins with rpm.eu.newrelic.com/, it is an EU-based account. Check your New Relic license key. If it begins with EU, it is an EU-based account.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 82.73412,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Access <em>New</em> <em>Relic</em> One",
        "tags": "Welcome <em>to</em> <em>New</em> <em>Relic</em>",
        "body": "<em>New</em> <em>Relic</em>&#x27;s global data hosting structure consists of two regions: the European Union (EU) region and the United States (US) region. Selecting your preferred region during the account setup process allows you to specify the region where your performance monitoring data will be hosted. You can also"
      },
      "id": "6044586c64441f844b378edd"
    },
    {
      "sections": [
        "Introduction to New Relic APIs",
        "Tip",
        "NerdGraph (GraphQL)",
        "REST APIs by capability",
        "Alerts",
        "APM",
        "Browser monitoring",
        "Infrastructure monitoring",
        "Mobile monitoring",
        "Synthetic monitoring",
        "Telemetry APIs for core data types",
        "Account management, admin, and usage APIs",
        "Other APIs",
        "Insights",
        "Plugins",
        "See APIs in action"
      ],
      "title": "Introduction to New Relic APIs",
      "type": "docs",
      "tags": [
        "APIs",
        "Get started",
        "Intro to APIs"
      ],
      "external_id": "01e9799a214baad5de04de6146483f6dbbc198aa",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/intro-apis/introduction-new-relic-apis/",
      "published_at": "2021-06-25T20:18:04Z",
      "updated_at": "2021-06-02T16:33:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic offers a variety of APIs and SDKs you can use to: Retrieve data from New Relic. Send data to New Relic. Adjust settings. This document provides examples and reference information for our API endpoints. For developer-focused content on how to use and customize New Relic, see developer.newrelic.com. Tip To use APIs and SDKs, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. NerdGraph (GraphQL) NerdGraph is New Relic's GraphQL-format API, an efficient and flexible query language that lets you request exactly the data you need, without over-fetching or under-fetching. NerdGraph is the preferred API for querying New Relic data and making a range of feature configurations. To get started, see Introduction to NerdGraph. REST APIs by capability New Relic capabilities, like APM, infrastructure monitoring, or alerts, are often used together, and sometimes they overlap in functionality. This is why multiple APIs may be relevant to each area. Some API functionality will depend on your access to features and data. Tip To learn more about different API key types, see Understand New Relic API keys. Alerts Use the REST API for alerts and the API Explorer to: Create and manage policies, conditions, and notification channels. Create alert conditions based on NRQL queries. Create alert conditions based on data from other New Relic capabilities. APM API resources for application monitoring include: Resource Details REST API REST API features include: Retrieve APM data, including metrics, Apdex, error rates, and host data. Report deployments. Change the app name in the UI. Agent APIs Every APM language agent has an API that lets you customize the agent's default behavior, including reporting custom data. APM agent APIs include: C SDK API Go agent API Java agent API .NET agent API Node.js agent API PHP agent API Python agent API Ruby agent API Query API To query APM data, use the Query API. Account management APIs For APIs related to accounts and subscription usage, see the account-related APIs. Browser monitoring The Browser API resources include: Resource Details Browser agent API Use the Browser agent API for tasks such as: Report custom end user data to browser monitoring. Monitor asynchronous browser activity using SPA API calls. Insert custom data into New Relic dashboards . Manage source maps. REST API With the REST API you can: Retrieve page load timing data and throughput. Add or list apps monitored by browser monitoring. Manage alerts conditions for your browser data. Query API To retrieve browser monitoring data, use the Query API. Account management APIs For APIs related to accounts and subscription usage, see the account-related APIs. Infrastructure monitoring The Infrastructure API resources include: Resource Details Query API To retrieve infrastructure data, use the Query API. This API can also be used to retrieve subscription usage data. Infrastructure alert API To manage your infrastructure alerts, use the Infrastructure alert API. Integrations SDK To make your own custom integrations for reporting data to infrastructure monitoring, use the Integrations SDK. NerdGraph You can use NerdGraph (our GraphQL API) to query your cloud integration data and make changes to cloud integration settings. Mobile monitoring Mobile API resources include: Resource Details Mobile agent APIs Mobile APIs let you custom instrument your own code and send events to New Relic. See the platform-specific documentation: iOS Android Unity REST API Use the REST API for such tasks as: Retrieve a list of monitored apps. Get subscription usage data. Get metric names and data. Get crash count and crash rate data. Manage New Relic alerts conditions for your mobile apps. Query API To retrieve Mobile data from New Relic, use the Query API. Account management APIs For account-related APIs, see Account APIs. Synthetic monitoring Synthetics API resources include: Resource Details Synthetics REST API The Synthetics REST API functionality includes: Create and manage synthetics monitors. Manage synthetics alert notifications. Add labels to monitors, and retrieve monitors with specific labels. Query API To retrieve synthetics event data, use the Query API. Alerts API To create and manage alert conditions that target synthetics monitors, use the Alerts API. Telemetry APIs for core data types We offer several APIs that allow you to get our core data types (metrics, logs, traces, and events) into New Relic without the use of an installed agent. Data type Description Trace API Send distributed tracing data to New Relic. Event API Send event data to New Relic. Metric API Send metrics to New Relic from any source (including other telemetry monitoring services). Log API Send your log data to New Relic. Account management, admin, and usage APIs Like any other New Relic product or service, you want to be confident that your APIs protect you and your customers' data privacy. The following are API resources related to New Relic account administration and usage. For more information about API capabilities, see the specific New Relic API. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website. Resource Details REST API REST API features include: Find your API keys, account ID, and information needed to use the REST API. Return a list of account users (original user model only). Get SLA report data for browser and application monitoring. Subscription usage You can use the Query API to retrieve subscription usage data. This can be helpful to see how usage compares to your current subscription level, or for doing departmental chargebacks. Partner API To retrieve information about your New Relic partner account, sub-accounts, and users, use the Partner API. Other APIs Insights New Relic Insights was the name of our original product that governed custom event reporting and querying. The features associated with Insights have been rolled into our New Relic One platform (learn more), but there are still some APIs and original pricing plans that use the term \"Insights\" for these historical reasons. Insights-related APIs include: Resource Details Event API To report custom events, use the Event API. Query API To query your data using NRQL-format queries, you can use the Query API. Note that this API is deprecated and NerdGraph is preferred for querying your data. Dashboard API See the Insights Dashboard API. Plugins Use the REST API for New Relic plugins and the API Explorer to: Get a list of plugins, including their names, IDs, and GUIDs. List one or more plugin components, their output, and their metric timeslice data. Developers and New Relic partners can also use New Relic's Plugin API to write an agent in any language that can work directly with the API for plugins. This allows you to send your own metric data to our plugins and view data received from the API in New Relic. See APIs in action For more on how you as a developer can optimize your ability to solve problems using New Relic, go to developer.newrelic.com.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 79.30746,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Introduction</em> <em>to</em> <em>New</em> <em>Relic</em> APIs",
        "sections": "<em>Introduction</em> <em>to</em> <em>New</em> <em>Relic</em> APIs",
        "tags": "Intro <em>to</em> APIs",
        "body": " documentation: iOS Android <em>Unity</em> REST API Use the REST API for such tasks as: Retrieve a list of monitored apps. Get subscription usage data. Get metric names and data. Get crash count and crash rate data. Manage <em>New</em> <em>Relic</em> alerts conditions for your <em>mobile</em> apps. Query API To retrieve <em>Mobile</em> data from"
      },
      "id": "609fa5cf196a67066022b194"
    },
    {
      "sections": [
        "Security for mobile apps",
        "Data collection",
        "Secure data endpoints",
        "Unique identifiers",
        "No remote updates",
        "Data storage",
        "Instrumentation added to your code",
        "User's IP address"
      ],
      "title": "Security for mobile apps",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "New Relic Mobile",
        "Get started"
      ],
      "external_id": "c1f31e708e4710eb4823467a43ab30af1f29243c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started/security-mobile-apps/",
      "published_at": "2021-06-25T22:48:23Z",
      "updated_at": "2021-05-05T05:08:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "To protect your mobile application's security and your users' data privacy, New Relic only records performance data, as described in this document. We do not collect any data used or stored by the monitored app. For more information about New Relic's security measures, see our security and data privacy documentation, or visit the New Relic security website. Data collection When you install New Relic, our mobile monitoring capabilities become part of your iOS or Android app. These capabilities live within your application's \"sandbox,\" so they cannot access anything other than performance data from your mobile app. We do not collect performance data about the device itself, such as battery level. Our mobile SDK agent collects and sends specific data to the New Relic collector, including: Mobile data collected Comments Devices Length of application session Wireless carrier's name The device's model name and manufacturer, and its operating system version Certain package, class, method, and thread names A unique instance identifier Requests and responses URLs of HTTP requests, along with HTTP status code, response time, and size of the request and response body Operating system error code for network failures (HTTP requests that fail to complete) The first 2KB of the response body when the HTTP request receives a 4xx or 5xx response status code Android only: A stack trace when the HTTP request receives a 4xx or 5xx response status code The agent sends all data using HTTPS encryption and validates the collector's SSL certificate. This prevents common data sniffing and server spoofing attacks. The agent also removes the query string, fragment identifier, username, and password from each URL before sending the data. Secure data endpoints Our mobile SDK agent sends harvested data to the collectors for processing. You can redirect those data posts to proxy or delegate servers for secure data handling. Android: You can use APIs to specify the URI authority of harvest and crash collector data endpoints. For more information, see the Android agent configuration and feature flags documentation. iOS: For more information, see the iOS agent configuration and feature flags documentation. Unique identifiers Our mobile SDK agent assigns a unique identifier to each installed app instance in order to track discrete installs, identify recurring sessions, and correlate performance over time. Mobile agent Identifiers Android Our Android agent generates a cryptographically strong UUID and stores it in the app's SharedPreferences. For more information, see ourAndroid compatibility and requirements documentation. iOS The security measures used for iOS depend on the agent version. In versions 5.3.5 or higher, the iOS agent uses the IdentifierForVendor property to provide a unique device ID. In versions 5.3.4 or lower, the iOS agent used the SecureUDID open source library. SecureUDID is used by many third party libraries and is an accepted industry standard that does not violate Apple App store guidelines. SecureUDID does not use device hardware identifiers such as IMEI. Note that our mobile SDK does not collect IDFA (Identity For Advertisers). For more information, see ouriOS compatibility and requirements documentation. No remote updates New Relic does not have the ability to update mobile agents remotely. Using the agent will not introduce any code into your mobile app without your knowledge. Data storage Our mobile SDK agent stores configuration information using your app's normal preferences or settings API on the mobile device. This configuration includes your: Application token Application version number Android or iOS SDK agent version number Settings such as the maximum number of HTTP requests to track per minute Performance data is buffered in memory. It is never written to the device's storage. Server-side data storage for mobile apps is handled in the same way as all other applications monitored by New Relic. For more information, see our security documentation about hosting and data storage. In general, we retain performance data according to the more generous time period of either your web or your mobile subscription. We also retain aggregate records of the number of active instances of your application. Instrumentation added to your code Our mobile SDK agent injects code into certain method calls within your application in order to collect performance data. This can have the effect of adding stack frames to your application's call graph where our code executes. This allows us to time and monitor the inputs and outputs of various APIs. This added code has been reviewed and tested for security-related flaws, and it incorporates best practices related to secure coding. Because our code runs within your application's process, it is subject to the same rights and restrictions as your own code. In addition, our iOS agent registers an NSURLProtocol handler to track NSURLConnection-based networking activity. This instrumentation is compatible with other custom NSURLProtocol handlers your application may register. The handler is registered within a single application process, so it is unable to monitor networking requests originating from other applications or the underlying operating system. User's IP address Our mobile SDK agent captures the user's IP address to enrich data for additional user information. The IP address is used as a lookup value that maps to additional details and allows our customers to diagnose performance issues. IP address lookup values include: App name Country code Region Postal code Latitude Longitude Area code For more information about events and attributes for mobile monitoring, see ourdata dictionary. New Relic does not retain the user's IP address after the attributes have been mapped. The IP address value is cached in memory for up to six hours before being discarded. If you have questions or concerns about this use of IP addresses with regards to your own regulatory obligations for notice and consent, please contact your privacy or legal teams.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 77.59708,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security for <em>mobile</em> apps",
        "sections": "Security for <em>mobile</em> apps",
        "tags": "<em>New</em> <em>Relic</em> <em>Mobile</em>",
        "body": "To protect your <em>mobile</em> application&#x27;s security and your users&#x27; data privacy, <em>New</em> <em>Relic</em> only records performance data, as described in this document. We do not collect any data used or stored by the monitored app. For more information about <em>New</em> <em>Relic</em>&#x27;s security measures, see our security and data"
      },
      "id": "603eb1c564441fa7e44e88a5"
    }
  ],
  "/docs/ios-device-id-obfuscation": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/attribute-dictionary/",
      "sections": [
        "New Relic data dictionary",
        "AjaxRequest",
        "AwsLambdaInvocation",
        "AwsLambdaInvocationError",
        "BrowserInteraction",
        "BrowserTiming",
        "ContainerSample",
        "DistributedTraceSummary",
        "InfrastructureEvent",
        "JavaScriptError",
        "Metric",
        "Mobile",
        "MobileCrash",
        "MobileHandledException",
        "MobileRequest",
        "MobileRequestError",
        "MobileSession",
        "NetworkSample",
        "NrAuditEvent",
        "NrConsumption",
        "NrDailyUsage",
        "NrIntegrationError",
        "NrMTDConsumption",
        "NrUsage",
        "PageAction",
        "PageView",
        "PageViewTiming",
        "ProcessSample",
        "Span",
        "StorageSample",
        "SyntheticCheck",
        "SyntheticRequest",
        "SyntheticsPrivateLocationStatus",
        "SyntheticsPrivateMinion",
        "SystemSample",
        "Transaction",
        "TransactionError",
        "WorkloadStatus"
      ],
      "published_at": "2021-06-30T01:43:19Z",
      "title": "New Relic data dictionary",
      "updated_at": "2021-06-30T01:43:18Z",
      "type": "docs",
      "external_id": "cbca3a897621bcbb31159067d6d4ec27c5178fe4",
      "document_type": "views_page_content",
      "popularity": 1,
      "body": "Displaying 0 of 37 results Clear AjaxRequest Data source : Browser agent An AjaxRequest event is created automatically when an Ajax request occurs during a BrowserInteraction event. The event attributes track geographic and browser info. Attribute name Definition Events appId ID The ID of your application, as recorded by New Relic. AjaxRequest BrowserInteraction BrowserTiming PageAction PageView PageViewTiming appName The name of the application that handled the request as shown in New Relic Browser. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming asn Autonomous System Number: a unique number identifying a group of IP networks that serves the content to the end user. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError MobileHandledException MobileRequest MobileRequestError MobileSession PageAction PageView PageViewTiming Span asnLatitude The latitude of the geographic center of the postal code where the Autonomous System Network is registered. This is not the end user's latitude. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming asnLongitude The longitude of the geographic center of the postal code where the Autonomous System Network is registered. This is not the end user's longitude. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming asnOrganization The organization that owns the Autonomous System Number. Often an ISP, sometimes a private company or institution. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming browserInteractionId ID A unique value generated for each browser interaction captured by the New Relic agent. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError browserInteractionName The name of the interaction. This is either the targetGroupedUrl or the custom name set via the API. AjaxRequest BrowserInteraction BrowserTiming city The city in which the event occurred, such as Portland or Seattle. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError MobileHandledException MobileSession PageAction PageView PageViewTiming countryCode The country from which the browser initiated the page load. For a list of country codes, see ISO 3166-1 alpha-2. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming deviceType The type of device that loaded the page: mobile, tablet, or desktop. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageView eventId ID A value that you can link to multiple BrowserInteraction events so you can view the interactions that occurred surrounding a specific event. For example, you can see the browser interactions that occurred prior to a JS error. AjaxRequest BrowserInteraction BrowserTiming groupedPageURL The grouped URL of the view that made the AJAX request. For example: myapp.com/acct/*/dash. AjaxRequest BrowserTiming groupedRequestUrl The grouped URL of the AJAX request. For example: myapp.com/acct/*/ajax. AjaxRequest Span hostname The fully qualified domain name (FQDN) of the request URL. AjaxRequest httpMethod enum The HTTP method of the AJAX request. Example: POST. AjaxRequest httpResponseCode enum The HTTP response code. Example: 200. AjaxRequest jsDuration seconds (s) The total duration, in seconds, spent on JavaScript execution. (This attribute doesn't exist for initial page load events.) AjaxRequest BrowserInteraction BrowserTiming pageUrl The URL of the page that was loaded for the PageView. For example: http://www.newrelic.com. This URL does not include query parameters. AjaxRequest BrowserTiming JavaScriptError PageAction PageView PageViewTiming Span parentEventId ID A unique value generated for each interaction with the page. You can use this value to group interactions together. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError port enum The request port. AjaxRequest Span priority Likelihood this event will be saved. AjaxRequest regionCode The specific administrative division within a country where the PageView event occurred. In the United States, regions correspond to state codes, such as WA or NY. Outside the United States, a country's regions correspond to numerical codes. In the United States, regions correspond to state codes ; for example, WA or NY. Outside the United States, a country's regions correspond to numerical codes . AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming requestBodySize bytes (B) The payload size of the request body, in bytes. AjaxRequest requestUrl The URL of the AJAX request. For example: myapp.com/acct/1/ajax. AjaxRequest responseBodySize bytes (B) The payload size of the response body, in bytes. AjaxRequest session A unique identifier for a single session. The session cookie expires when the user closes the browser (for example, they fully exit Chrome.) A new session identifier will be assigned when the user opens up a new instance of the browser. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming Span timeSinceBrowserInteractionStart seconds (s) The time in seconds between the start of the BrowserInteraction and the start of the request. AjaxRequest BrowserTiming timeToLastCallbackEnd seconds (s) The duration, in seconds, from the start of the request (timestamp) to the end of the last callback. This is not just an additive function; the callback time can overlap with the wait time. AjaxRequest BrowserTiming timeToLoadEventStart seconds (s) The time, in seconds, from the start of the AJAX request to the start of its load event. This value represents the duration of the AJAX request with single page app (SPA) monitoring. For more information, see the Mozilla developer documentation about XMLHttpRequest load events. AjaxRequest timeToSettle seconds (s) The time, in seconds, from the start of the request to when all resulting callbacks (including callbacks of subsequent AJAX requests) are complete. AjaxRequest BrowserTiming timestamp The time (date, hour, minute, second) at which the interaction occurred. AjaxRequest BrowserInteraction BrowserTiming PageAction PageView PageViewTiming Span userAgentName The browsers name, such as Chrome and Firefox, obtained from the User-Agent header of an HTTP request. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming userAgentOS The browsers reported operating system, such as Windows or Linux, that it is running on. This is obtained from the User-Agent header of an HTTP request. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming userAgentVersion The browsers reported software version, which is obtained from the User-Agent header of an HTTP request. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming AwsLambdaInvocation Data source : AWS Lambda This event is reported by New Relic monitoring for AWS Lambda. This event captures overall function timing and associated metadata. A single AwsLambdaInvocation event is generated for each invocation. Attribute name Definition Events aws.lambda.arn The Amazon Resource Name (ARN) of the instrumented Lambda function. AwsLambdaInvocation AwsLambdaInvocationError aws.lambda.coldStart A Boolean indicating if the AWS Lambda invocation is a cold start. AwsLambdaInvocation AwsLambdaInvocationError aws.lambda.eventSource.arn The Amazon Resource Name (ARN) of the entity that invoked the instrumented Lambda function. AwsLambdaInvocation AwsLambdaInvocationError aws.requestId ID AWS identifier of the invocation. AwsLambdaInvocation AwsLambdaInvocationError databaseCallCount count The number of database calls made by this transaction. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError databaseDuration seconds (s) The database response time in seconds. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError duration seconds (s) The total invocation time for the transaction, in seconds. (Data source: AWS Lambda) AwsLambdaInvocation AwsLambdaInvocationError externalCallCount count The number of external calls made by this transaction. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError externalDuration seconds (s) The total response time of all external (out-of-process) services, in seconds. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError newRelic.ingestPoint Where the data point entered the platform (such as browser.spans, or api.traces). AwsLambdaInvocation AwsLambdaInvocationError Span parent.account ID If a distributed tracing payload is received, this is the account identifier for the transaction's upstream caller. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError parent.app ID If a distributed tracing payload is received, this is the application identifier. APM agents retrieve this value in the connect response under the key primary_application_id. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError parent.transportType When a distributed tracing payload is received, the method of transport for the payload. Example values: Unknown, HTTP, HTTPS, Kafka, JMS, IronMQ, AMQP, Queue, or Other. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError parent.type If a distributed trace payload was received, the parent's data source type. Example values: App, Browser, Mobile. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError request.headers.accept The types as read from the HTTP Accept request header. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError request.headers.contentLength bytes (B) Incoming request size in bytes as read from the Content-Length HTTP request header. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError request.headers.contentType Incoming request content-type as read from the HTTP request header Content-Type. Example value: application/octet-stream. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError request.headers.host The name from the HTTP host request header. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError request.headers.referer The incoming request referer as read from the Referer request header. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError request.headers.userAgent The contents of the User-Agent HTTP header. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError request.method The HTTP request method used. Example values: POST, GET. AwsLambdaInvocation AwsLambdaInvocationError Span Transaction TransactionError response.headers.contentLength bytes (B) The outgoing response size in bytes as read from the Content-Length response header. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError response.headers.contentType For an HTTP response, the data type of the returned response. Example values: text/html, application/json. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError response.status bytes (B) The response code for an HTTP request AwsLambdaInvocation totalTime seconds (s) The sum of all async components' duration, in seconds. An async component is a method or function where there is no instrumented encapsulating method or function. AwsLambdaInvocation Transaction traceId ID The unique ID (a randomly generated string) used to identify a single request as it crosses inter- and intra- process boundaries. This ID allows the linking of spans in a distributed trace. Included when distributed tracing is enabled. AwsLambdaInvocation AwsLambdaInvocationError MobileRequest MobileRequestError Span type The New Relic event type. Example values: Transaction, Span. AwsLambdaInvocation AwsLambdaInvocationError AwsLambdaInvocationError Data source : AWS Lambda This event is reported by New Relic monitoring for AWS Lambda. It's generated when an error occurs during a Lambda function invocation. Attribute name Definition Events aws.lambda.arn The Amazon Resource Name (ARN) of the instrumented Lambda function. AwsLambdaInvocation AwsLambdaInvocationError aws.lambda.coldStart A Boolean indicating if the AWS Lambda invocation is a cold start. AwsLambdaInvocation AwsLambdaInvocationError aws.lambda.eventSource.arn The Amazon Resource Name (ARN) of the entity that invoked the instrumented Lambda function. AwsLambdaInvocation AwsLambdaInvocationError aws.requestId ID AWS identifier of the invocation. AwsLambdaInvocation AwsLambdaInvocationError databaseCallCount count The number of database calls made by this transaction. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError databaseDuration seconds (s) The database response time in seconds. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError duration seconds (s) The total invocation time for the transaction, in seconds. (Data source: AWS Lambda) AwsLambdaInvocation AwsLambdaInvocationError error.class The class name or type for the error. This will be server and platform specific. AwsLambdaInvocationError TransactionError error.message The error message for the transaction. This will be server and platform specific. AwsLambdaInvocationError TransactionError externalCallCount count The number of external calls made by this transaction. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError externalDuration seconds (s) The total response time of all external (out-of-process) services, in seconds. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError newRelic.ingestPoint Where the data point entered the platform (such as browser.spans, or api.traces). AwsLambdaInvocation AwsLambdaInvocationError Span parent.account ID If a distributed tracing payload is received, this is the account identifier for the transaction's upstream caller. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError parent.app ID If a distributed tracing payload is received, this is the application identifier. APM agents retrieve this value in the connect response under the key primary_application_id. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError parent.transportType When a distributed tracing payload is received, the method of transport for the payload. Example values: Unknown, HTTP, HTTPS, Kafka, JMS, IronMQ, AMQP, Queue, or Other. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError parent.type If a distributed trace payload was received, the parent's data source type. Example values: App, Browser, Mobile. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError request.headers.accept The types as read from the HTTP Accept request header. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError request.headers.contentLength bytes (B) Incoming request size in bytes as read from the Content-Length HTTP request header. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError request.headers.contentType Incoming request content-type as read from the HTTP request header Content-Type. Example value: application/octet-stream. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError request.headers.host The name from the HTTP host request header. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError request.headers.referer The incoming request referer as read from the Referer request header. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError request.headers.userAgent The contents of the User-Agent HTTP header. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError request.method The HTTP request method used. Example values: POST, GET. AwsLambdaInvocation AwsLambdaInvocationError Span Transaction TransactionError response.headers.contentLength bytes (B) The outgoing response size in bytes as read from the Content-Length response header. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError response.headers.contentType For an HTTP response, the data type of the returned response. Example values: text/html, application/json. AwsLambdaInvocation AwsLambdaInvocationError Transaction TransactionError stackTrace The error stack trace. The format will be different depending on the agent language. AwsLambdaInvocationError traceId ID The unique ID (a randomly generated string) used to identify a single request as it crosses inter- and intra- process boundaries. This ID allows the linking of spans in a distributed trace. Included when distributed tracing is enabled. AwsLambdaInvocation AwsLambdaInvocationError MobileRequest MobileRequestError Span transactionName Name of the transaction in which the error occurred. Example value: Controller/customers/show. Value may be 'Unknown' if an error occurs outside of a transaction. AwsLambdaInvocationError TransactionError type The New Relic event type. Example values: Transaction, Span. AwsLambdaInvocation AwsLambdaInvocationError BrowserInteraction Data source : Browser agent A BrowserInteraction represents a unit of work in a browser session, triggered by a user interacting with the webpage. It captures information about the session, AJAX calls and custom JavaScript timing that occurred as a result of the interaction. Initial load and route changes are captured as special types of Browser interactions, and are used for SPA monitoring. Attribute name Definition Events actionText The text of the HTML element that was clicked when a browser interaction started. BrowserInteraction ajaxCount count A count of all XHRs included in the timing of a SPA interaction. BrowserInteraction appId ID The ID of your application, as recorded by New Relic. AjaxRequest BrowserInteraction BrowserTiming PageAction PageView PageViewTiming appName The name of the application that handled the request as shown in New Relic Browser. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming asn Autonomous System Number: a unique number identifying a group of IP networks that serves the content to the end user. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError MobileHandledException MobileRequest MobileRequestError MobileSession PageAction PageView PageViewTiming Span asnLatitude The latitude of the geographic center of the postal code where the Autonomous System Network is registered. This is not the end user's latitude. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming asnLongitude The longitude of the geographic center of the postal code where the Autonomous System Network is registered. This is not the end user's longitude. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming asnOrganization The organization that owns the Autonomous System Number. Often an ISP, sometimes a private company or institution. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming backendTransactionName The name of the backend transaction that served the initial page load. BrowserInteraction browserInteractionId ID A unique value generated for each browser interaction captured by the New Relic agent. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError browserInteractionName The name of the interaction. This is either the targetGroupedUrl or the custom name set via the API. AjaxRequest BrowserInteraction BrowserTiming category The type of interaction; either initial page load, route change, or custom. BrowserInteraction city The city in which the event occurred, such as Portland or Seattle. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError MobileHandledException MobileSession PageAction PageView PageViewTiming countryCode The country from which the browser initiated the page load. For a list of country codes, see ISO 3166-1 alpha-2. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming deviceType The type of device that loaded the page: mobile, tablet, or desktop. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageView domain The domain portion of the request URL. BrowserInteraction JavaScriptError PageView PageViewTiming duration seconds (s) The total time elapsed of the interaction event BrowserInteraction eventId ID A value that you can link to multiple BrowserInteraction events so you can view the interactions that occurred surrounding a specific event. For example, you can see the browser interactions that occurred prior to a JS error. AjaxRequest BrowserInteraction BrowserTiming firstContentfulPaint firstContentfulPaint is the point when the browser renders the first bit of content from the DOM, which may be text, an image, SVG, or a <canvas> element. Google's User-centric Performance Metrics contains detailed information about its Paint Timing API and firstContentfulPaint. See Compatibility and requirements for New Relic Browser for additional information about firstContentfulPaint browser compatibility. BrowserInteraction PageView firstPaint firstPaint marks the point when the browser renders anything that is visually different from what was on the screen prior to navigation. This includes non-default background paint and the enclosing box of an iframe. Google's User-centric Performance Metrics contains detailed information about its Paint Timing API and firstPaint. See Compatibility and requirements for New Relic Browser for additional information about firstPaint browser compatibility. BrowserInteraction PageView jsDuration seconds (s) The total duration, in seconds, spent on JavaScript execution. (This attribute doesn't exist for initial page load events.) AjaxRequest BrowserInteraction BrowserTiming monitorAccountId The Synthetics account from which you are running the monitor. BrowserInteraction JavaScriptError monitorId ID A unique number identifying a particular monitor. BrowserInteraction JavaScriptError SyntheticCheck monitorJobId ID The ID of a single Synthetics monitor run, which began at a specific time and originated from a specific location. BrowserInteraction JavaScriptError parentEventId ID A unique value generated for each interaction with the page. You can use this value to group interactions together. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError previousGroupedUrl The grouped version of the URL in the browser at the start of the interaction. BrowserInteraction previousRouteName The route name of the page at the start of the interaction. This is the last value passed by setCurrentRouteName before the start of the interaction. BrowserInteraction previousURL The ungrouped URL in the browser at the start of the interaction. BrowserInteraction regionCode The specific administrative division within a country where the PageView event occurred. In the United States, regions correspond to state codes, such as WA or NY. Outside the United States, a country's regions correspond to numerical codes. In the United States, regions correspond to state codes ; for example, WA or NY. Outside the United States, a country's regions correspond to numerical codes . AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming session A unique identifier for a single session. The session cookie expires when the user closes the browser (for example, they fully exit Chrome.) A new session identifier will be assigned when the user opens up a new instance of the browser. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming Span targetGroupedUrl The grouped version of the URL in the browser at the end of the interaction. BrowserInteraction targetRouteName The route name for the page at the end of the interaction. The last value passed by setCurrentRouteName before the end of the interaction. BrowserInteraction targetUrl The ungrouped URL in the browser at the end of the interaction. BrowserInteraction timeToConnectEnd seconds (s) The time, in seconds, from the start of the interaction to the connectEnd, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToConnectStart seconds (s) The time, in seconds, from the start of the interaction to the connectStart, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToDomComplete seconds (s) The time, in seconds, from the start of the interaction to the domComplete, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToDomContentLoadedEventEnd seconds (s) The time, in seconds, from the start of the interaction to the domContentLoadedEventEnd, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToDomContentLoadedEventStart seconds (s) The time, in seconds, from the start of the interaction to the domContentLoadedEventStart, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToDomInteractive seconds (s) The time, in seconds, from the start of the interaction to the domInteractive, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToDomLoading seconds (s) The time, in seconds, from the start of the interaction to the domLoading, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToDomainLookupEnd seconds (s) The time, in seconds, from the start of the interaction to the domainLookupEnd, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToDomainLookupStart seconds (s) The time, in seconds, from the start of the interaction to the domainLookupStart, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToFetchStart seconds (s) The time, in seconds, from the start of the interaction to the fetchStart, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToLoadEventEnd seconds (s) The time, in seconds, from the start of the interaction to the loadEventEnd, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToLoadEventStart seconds (s) The time, in seconds, from the start of the interaction to the loadEventStart, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information, see our documentation about instrumentation for the Navigation Timing API. BrowserInteraction timeToRedirectEnd seconds (s) The time, in seconds, from the start of the interaction to the redirectEnd, as defined by the Navigation Timing API. This attribute exists only for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToRedirectStart seconds (s) The time, in seconds, from the start of the interaction to the redirectStart, as defined by the Navigation Timing API. This attribute exists only for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToRequestStart seconds (s) The time, in seconds, from the start of the interaction to the requestStart, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToResponseEnd seconds (s) The time, in seconds, from the start of the interaction to the responseEnd, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToResponseStart seconds (s) The time, in seconds, from the start of the interaction to the responseStart, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToSecureConnectionStart seconds (s) The time, in seconds, from the start of the interaction to the secureConnectionStart, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToUnloadEventEnd seconds (s) The time, in seconds, from the start of the interaction to the unloadEventEnd, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timeToUnloadEventStart seconds (s) The time, in seconds, from the start of the interaction to the unloadEventStart, as defined by the Navigation Timing API. This attribute exists for initial page load events, not route changes. For more information about the Navigation Timing API, see Instrumentation for Browser monitoring. BrowserInteraction timestamp The time (date, hour, minute, second) at which the interaction occurred. AjaxRequest BrowserInteraction BrowserTiming PageAction PageView PageViewTiming Span trigger The cause of the route change or page load. The default values are click, submit, popstate, or initial page load. For a custom event created with the API, the default value for trigger will be api. This value can also be set via the API. BrowserInteraction userAgentName The browsers name, such as Chrome and Firefox, obtained from the User-Agent header of an HTTP request. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming userAgentOS The browsers reported operating system, such as Windows or Linux, that it is running on. This is obtained from the User-Agent header of an HTTP request. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming userAgentVersion The browsers reported software version, which is obtained from the User-Agent header of an HTTP request. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming BrowserTiming Data source : Browser agent BrowserTiming is a custom event that captures SPA timing data for browser interactions started using the custom createTracer SPA API method. BrowserTiming contains many of the same attributes used by other events, especially AjaxRequest. Attribute name Definition Events appId ID The ID of your application, as recorded by New Relic. AjaxRequest BrowserInteraction BrowserTiming PageAction PageView PageViewTiming appName The name of the application that handled the request as shown in New Relic Browser. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming asn Autonomous System Number: a unique number identifying a group of IP networks that serves the content to the end user. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError MobileHandledException MobileRequest MobileRequestError MobileSession PageAction PageView PageViewTiming Span asnLatitude The latitude of the geographic center of the postal code where the Autonomous System Network is registered. This is not the end user's latitude. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming asnLongitude The longitude of the geographic center of the postal code where the Autonomous System Network is registered. This is not the end user's longitude. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming asnOrganization The organization that owns the Autonomous System Number. Often an ISP, sometimes a private company or institution. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming browserInteractionId ID A unique value generated for each browser interaction captured by the New Relic agent. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError browserInteractionName The name of the interaction. This is either the targetGroupedUrl or the custom name set via the API. AjaxRequest BrowserInteraction BrowserTiming browserTimingName The name of the event. This is taken from the name argument of createTracer. BrowserTiming city The city in which the event occurred, such as Portland or Seattle. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError MobileHandledException MobileSession PageAction PageView PageViewTiming countryCode The country from which the browser initiated the page load. For a list of country codes, see ISO 3166-1 alpha-2. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming deviceType The type of device that loaded the page: mobile, tablet, or desktop. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageView eventId ID A value that you can link to multiple BrowserInteraction events so you can view the interactions that occurred surrounding a specific event. For example, you can see the browser interactions that occurred prior to a JS error. AjaxRequest BrowserInteraction BrowserTiming groupedPageURL The grouped URL of the view that made the AJAX request. For example: myapp.com/acct/*/dash. AjaxRequest BrowserTiming jsDuration seconds (s) The total duration, in seconds, spent on JavaScript execution. (This attribute doesn't exist for initial page load events.) AjaxRequest BrowserInteraction BrowserTiming pageUrl The URL of the page that was loaded for the PageView. For example: http://www.newrelic.com. This URL does not include query parameters. AjaxRequest BrowserTiming JavaScriptError PageAction PageView PageViewTiming Span parentEventId ID A unique value generated for each interaction with the page. You can use this value to group interactions together. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError regionCode The specific administrative division within a country where the PageView event occurred. In the United States, regions correspond to state codes, such as WA or NY. Outside the United States, a country's regions correspond to numerical codes. In the United States, regions correspond to state codes ; for example, WA or NY. Outside the United States, a country's regions correspond to numerical codes . AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming session A unique identifier for a single session. The session cookie expires when the user closes the browser (for example, they fully exit Chrome.) A new session identifier will be assigned when the user opens up a new instance of the browser. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming Span timeSinceBrowserInteractionStart seconds (s) The time in seconds between the start of the BrowserInteraction and the start of the request. AjaxRequest BrowserTiming timeToLastCallbackEnd seconds (s) The duration, in seconds, from the start of the request (timestamp) to the end of the last callback. This is not just an additive function; the callback time can overlap with the wait time. AjaxRequest BrowserTiming timeToSettle seconds (s) The time, in seconds, from the start of the request to when all resulting callbacks (including callbacks of subsequent AJAX requests) are complete. AjaxRequest BrowserTiming timeToTracedCallbackStart seconds (s) The time in seconds from the start of the custom tracer until the start of the traced callback. This attribute is unique to the BrowserTiming event. BrowserTiming timestamp The time (date, hour, minute, second) at which the interaction occurred. AjaxRequest BrowserInteraction BrowserTiming PageAction PageView PageViewTiming Span tracedCallbackDuration seconds (s) The duration in seconds of the traced callback. This attribute is unique to the BrowserTiming event. BrowserTiming userAgentName The browsers name, such as Chrome and Firefox, obtained from the User-Agent header of an HTTP request. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming userAgentOS The browsers reported operating system, such as Windows or Linux, that it is running on. This is obtained from the User-Agent header of an HTTP request. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming userAgentVersion The browsers reported software version, which is obtained from the User-Agent header of an HTTP request. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming ContainerSample Data source : Infrastructure This event is reported by the New Relic Infrastructure agent. It collects data from all the Docker containers on the host (which may or may not be running). It includes the container's ID, name, image, image name, and metrics about CPU, memory and networking. We take a snapshot of this data every 15 seconds for each container and package it into this event, which is then sent to New Relic. This data appears on the Containers UI page. Attribute name Definition Events StorageDataAvailableBytes bytes (B) Data space available in the Storage Driver. Only Device Mapper driver is supported. ContainerSample StorageDataTotalBytes bytes (B) Total Data space in the Storage Driver. Only Device Mapper driver is supported. ContainerSample StorageDataUsagePercent percentage (%) Percent of Data space used in the Storage Driver. Only Device Mapper driver is supported. ContainerSample StorageDataUsedBytes bytes (B) Data space used by the Storage Driver. Only Device Mapper driver is supported. ContainerSample StorageMetadataAvailableBytes bytes (B) Metadata space available in the Storage Driver. Only Device Mapper driver is supported. ContainerSample StorageMetadataTotalBytes bytes (B) Total Metadata space in the Storage Driver. Only Device Mapper driver is supported. ContainerSample StorageMetadataUsagePercent percentage (%) Percent of Metadata space used in the Storage Driver. Only Device Mapper driver is supported. ContainerSample StorageMetadataUsedBytes bytes (B) Metadata space used by the Storage Driver. Only Device Mapper driver is supported. ContainerSample commandLine The command line used in the container. ContainerSample containerId ID The unique Docker container ID. ContainerSample cpuKernelPercent percentage (%) CPU time percentage used in kernel space. ContainerSample cpuLimitCores count Number of cores available for the container. ContainerSample cpuPercent percentage (%) CPU usage percentage used. ContainerSample cpuShares count Number of CPU shares assigned to the container. ContainerSample cpuThrottlePeriods count Total number of periods throttled. ContainerSample cpuThrottleTimeMs milliseconds (ms) Total throttling time in milliseconds. ContainerSample cpuUsedCores percentage (%) CPU usage per core. ContainerSample cpuUsedCoresPercent percentage (%) CPU usage percentage per core. ContainerSample cpuUserPercent percentage (%) CPU time percentage used in user space. ContainerSample criticalViolationCount count The number of times that alert conditions violated critical thresholds, causing critical violations and opening incidents. If this attribute does not exist on the sample, it has zero violations. ContainerSample NetworkSample ProcessSample StorageSample SystemSample image ID The Docker image ID for the image the container is based on. ContainerSample imageName The Docker image name for the container. ContainerSample label_KEY Docker labels associated with this container (where KEY represents a custom label's key value). ContainerSample memoryCacheBytes count The amount of memory used by the container that can be associated precisely with a block on a block device. ContainerSample memoryKernelUsageBytes bytes (B) The amount of current kernel memory allocation. ContainerSample memoryResidentSizeBytes bytes (B) The amount of memory that doesn't correspond to anything on disk: stacks, heaps, and anonymous memory maps. ContainerSample memorySizeLimitBytes bytes (B) The total amount of memory the container is allowed to use. ContainerSample memorySoftLimitBytes bytes (B) The soft limit of memory usage equivalent to the memory reservation of the container. ContainerSample memorySwapLimitBytes bytes (B) The total amount of memory the container is using, including swap. ContainerSample memorySwapLimitUsagePercent percentage (%) This metric is calculated as the percentage of memorySwapUsageBytes over memorySwapLimitBytes, if the limit exists. ContainerSample memorySwapOnlyUsageBytes bytes (B) The amount of swap memory the container is using. This memory doesn't include non-swap memory. ContainerSample memorySwapUsageBytes bytes (B) The amount of memory swap the container is using, including swap. ContainerSample memoryUsageBytes bytes (B) This metric doesn't account for swap usage. ContainerSample memoryUsageLimitPercent percentage (%) This metric is calculated as the memoryUsageBytes percentage over memorySizeLimitBytes, if the limit exists. ContainerSample name The Docker container name. ContainerSample networkRxBytes bytes (B) Total number of received bytes. ContainerSample networkRxBytesPerSecond rate Number of received bytes per second. ContainerSample networkRxDropped count Total number of received packets dropped. ContainerSample networkRxDroppedPerSecond rate Number of received packets dropped per second. ContainerSample networkRxError count Total number of received packets with error. ContainerSample networkRxErrorsPerSecond rate Number of received packets with error per second. ContainerSample networkRxPackets count Total number of received packets. ContainerSample networkRxPacketsPerSecond rate Number of received packets with error per second. ContainerSample networkTxBytesPerSecond rate Number of transmitted bytes per second. ContainerSample networkTxDropped count Total number of transmitted packets dropped. ContainerSample networkTxDroppedPerSecond rate Number of transmitted packets dropped per second. ContainerSample networkTxErrors count Total number of transmitted packets with error. ContainerSample networkTxErrorsPerSecond rate Number of transmitted packets with error per second. ContainerSample networkTxPackets count Total number of transmitted packets. ContainerSample networkTxPacketsPerSecond rate Number of transmitted packets per second. ContainerSample networksTxBytes count Total number of bytes transmitted. ContainerSample restartCount count The number of times the container was restarted. ContainerSample state It can be: created, restarting, running, removing, paused, exited, or dead. ContainerSample status Holds the current container state. ContainerSample warningViolationCount count The number of times that alert conditions violated warning thresholds, causing warning violations. ContainerSample NetworkSample ProcessSample StorageSample SystemSample DistributedTraceSummary Data source : Distributed Tracing This event contains summary data about a distributed trace and provides an aggregated view of distributed tracing data. DistributedTraceSummary events are generated by New Relic and are triggered by distributed tracing data from agents or the Trace API. Attribute name Definition Events accountIds A comma delimited list of newrelic accountIds that took part in this trace. DistributedTraceSummary backend.duration.ms milliseconds (ms) The total elapsed time in milliseconds of all backend services in this trace. DistributedTraceSummary backend.timestamp milliseconds (ms) The timestamp of the first span in this trace from a backend entity. In distributed tracing, any events that are not from client-side applications contributed to the backend duration. DistributedTraceSummary duration.ms The duration of the entire distributed trace, including both backend and client-side entities. the earliest span to the latest. DistributedTraceSummary entityCount count The number of unique entities that took part in this trace. DistributedTraceSummary entityGuids A comma delimited list of entity GUIDs for entities that participated in this trace. These GUIDs are assigned by New Relic for the New Relic-monitored entity (host, application, etc.). Each GUID is stored as a Base64 encoded value. DistributedTraceSummary errorCount count The number of events in this distributed trace that were identified as errors. DistributedTraceSummary newRelic.traceFilter.type The name of the trace filter used by the Infinite Tracing trace observer to select this trace. DistributedTraceSummary Span root.entity.accountId The New Relic account ID that the root entity of this trace reports data to. DistributedTraceSummary root.entity.guid The entity GUID associated with the root entity of this trace. DistributedTraceSummary root.entity.name The name of the root entity of this trace. DistributedTraceSummary root.span.duration.ms milliseconds (ms) The elapsed time in milliseconds of the root span of this trace. The root of a distributed trace is the first span, and will have a null value for parent.id. DistributedTraceSummary root.span.id The unique identifier of the root span of this trace. The root of a distributed trace is the first span, and it has a null value for parent.id. DistributedTraceSummary root.span.name The name of the root span of this trace. DistributedTraceSummary root.span.timestamp milliseconds (ms) The timestamp of the root span of this trace. The root of a distributed trace is the first span, and will have a null value for parent.id. DistributedTraceSummary spanCount count The number of events in this distributed trace. Events in a distributed trace can have several event types, including Span, Transaction, and TransactionError. DistributedTraceSummary timestamp milliseconds (ms) The timestamp of the root span in this distributed trace. DistributedTraceSummary trace.id ID The unique ID (a randomly generated string) used to identify a single request as it crosses inter- and intra- process boundaries. This ID allows the linking of spans in a distributed trace. Included when distributed tracing is enabled. DistributedTraceSummary MobileRequest MobileRequestError Span Transaction TransactionError InfrastructureEvent Data sources : InfrastructureCloudTrail InfrastructureEvent describes changes (deltas) that occur in a system's live state. When an inventory or system state is added, removed, or changed, Infrastructure will produce an InfrastructureEvent that logs that activity. Attribute name Definition Events category A New Relic category used to organize events in the UI. For example: automation, notification, and service. InfrastructureEvent changeType A simple classification of the type of change made to the entity: added, modified, or removed. InfrastructureEvent changedPath The fully specified name of the item that changed. This is constructed by taking the source and adding one or more additional path elements that uniquely identify the item that changed. InfrastructureEvent deltaId Delta refers to a recorded change in the system. The deltaId is a number used by New Relic to organize incoming inventory change data. InfrastructureEvent eventId The unique ID of the event, generated by New Relic. InfrastructureEvent format The type of infrastructure event. Each format type includes attributes that may be used to render the event in the UI. InfrastructureEvent newStatus The new agent status: disconnected or connected. InfrastructureEvent newValue If a change is made to the entity, this attribute contains the new value of the inventory that was changed. This will have no value if no change has been made. The value will display approximately 4K bytes of data. InfrastructureEvent oldValue If a change is made to the entity, this attribute contains the old value of the inventory that was changed. This will be blank if no change has been made. InfrastructureEvent provider For integrations that use generic event types (like the DatastoreSample event), the provider value specifies the source of the data (the service, or a sub-category of data from that service). Some Insights events are generic and are used by several integrations. For example, the DatastoreSample event is used by several integrations, including the AWS DynamoDB integration and the AWS RDS integration. In these cases, the provider attribute value represents the source of that attribute. This will usually be the service that data comes from or, for integrations that use several provider values, a certain sub-category of data from that service. When a provider value is present for a generic event, that event will have additional integration-specific attributes attached to it. Heres an example of an Insights NRQL query that returns the attributes present for a DatastoreSample event reported by the AWS RDS integration: SELECT * from DatastoreSample where provider = 'RdsDbCluster' InfrastructureEvent source The fully specified origin of this inventory item. This is typically in the form category/plugin, where plugin is the generic word used for the tool that gathered this data. InfrastructureEvent summary A summary of the change that happened. Uses a human-friendly string, such as Agent disconnected. InfrastructureEvent violationUpdateType The type of change to the violation: For example: open or closed. InfrastructureEvent JavaScriptError Data source : Browser agent As JavaScript errors are triggered, we capture details as events. The JavaScriptError event contains information to help you segment errors to understand how they impact performance. Attribute name Definition Events appId ID The identification number for the reporting browser agent. JavaScriptError appName The name of the application that handled the request as shown in New Relic Browser. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming asn Autonomous System Number: a unique number identifying a group of IP networks that serves the content to the end user. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError MobileHandledException MobileRequest MobileRequestError MobileSession PageAction PageView PageViewTiming Span asnLatitude The latitude of the geographic center of the postal code where the Autonomous System Network is registered. This is not the end user's latitude. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming asnLongitude The longitude of the geographic center of the postal code where the Autonomous System Network is registered. This is not the end user's longitude. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming asnOrganization The organization that owns the Autonomous System Number. Often an ISP, sometimes a private company or institution. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming browserInteractionId ID A unique value generated for each browser interaction captured by the New Relic agent. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError browserStackHash ID An unique identifier generated for a stack trace. The browserStackHash for a stack trace is different across different browsers. An identical stack trace will generate the same identifier. JavaScriptError city The city in which the event occurred, such as Portland or Seattle. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError MobileHandledException MobileSession PageAction PageView PageViewTiming countryCode The country from which the browser initiated the page load. For a list of country codes, see ISO 3166-1 alpha-2. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming deviceType The type of device that loaded the page: mobile, tablet, or desktop. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageView domain The domain portion of the request URL. BrowserInteraction JavaScriptError PageView PageViewTiming entityGuid The unique identifier of the monitor referenced in New Relic One. JavaScriptError SyntheticCheck SyntheticRequest errorClass The error type of the JavaScript Error object. Examples: ReferenceError, SyntaxError, and UncaughtException. JavaScriptError errorMessage The error message that was delivered. JavaScriptError firstErrorInSession A value to indicate whether or not this was the first JS error in the session. Example: true. JavaScriptError monitorAccountId The Synthetics account from which you are running the monitor. BrowserInteraction JavaScriptError monitorId ID A unique number identifying a particular monitor. BrowserInteraction JavaScriptError SyntheticCheck monitorJobId ID The ID of a single Synthetics monitor run, which began at a specific time and originated from a specific location. BrowserInteraction JavaScriptError pageUrl The URL of the page that was loaded for the PageView. For example: http://www.newrelic.com. This URL does not include query parameters. AjaxRequest BrowserTiming JavaScriptError PageAction PageView PageViewTiming Span parentEventId ID A unique value generated for each interaction with the page. You can use this value to group interactions together. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError regionCode The specific administrative division within a country where the PageView event occurred. In the United States, regions correspond to state codes, such as WA or NY. Outside the United States, a country's regions correspond to numerical codes. In the United States, regions correspond to state codes ; for example, WA or NY. Outside the United States, a country's regions correspond to numerical codes . AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming releaseIds ID The releases in which the app was updated. Example: {\\\"jQuery\\\":\\\"v3.1.1\\\",\\\"multiverse\\\":\\\"96e9ac7\\\"}. JavaScriptError requestUri The URI of the requested resource. JavaScriptError session A unique identifier for a single session. The session cookie expires when the user closes the browser (for example, they fully exit Chrome.) A new session identifier will be assigned when the user opens up a new instance of the browser. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming Span stackHash ID A unique identifier generated by the Browser agent for a stack trace. The stackHash for a stack trace is the same across different browsers. An identical stack trace will generate the same identifier. JavaScriptError stackTrace A collection of the active stack frames when the error occurred. JavaScriptError stackTraceGzip A compressed version of the stackTrace attribute. JavaScriptError timestamp The time that the error occurred, in Unix time. JavaScriptError transactionName The full metric name of the transaction in which the error occurred, or Unknown if the error occurs outside of a transaction. JavaScriptError userAgentName The browsers name, such as Chrome and Firefox, obtained from the User-Agent header of an HTTP request. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming userAgentOS The browsers reported operating system, such as Windows or Linux, that it is running on. This is obtained from the User-Agent header of an HTTP request. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming userAgentVersion The browsers reported software version, which is obtained from the User-Agent header of an HTTP request. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError PageAction PageView PageViewTiming Metric Data source : Metrics Represents a metric data point (e.g., a measurement over a range of time, or a sample at a specific point in time) with multiple attributes attached, which allow for in-depth analysis and querying. This metric data comes from our Metric API, our Telemetry SDKs, and some of our open-source exporters/integrations. Attribute name Definition Events endTimestamp milliseconds (ms) The end of the time range associated with the metric, in Unix time, in milliseconds. This is calculated by adding the metric interval to the timestamp of the metric (timestamp + interval.ms). Metric interval.ms milliseconds (ms) The length of the time window. Metric metricName Name of the metric. Metric newrelic.source The source of this data. For example: metricAPI. Metric timestamp milliseconds (ms) The start time for the metric in Unix time, in milliseconds. Metric Mobile Data source : Mobile A Mobile event is created when a crash occurs, when an interaction ends or has run for 1 second, or if a session completes after the app is closed, backgrounded, or has run for 10 minutes. Mobile events were once the only event type and were generated for every event, but now there are several specialized event types. Recommendation: Upgrade to the most recent New Relic Mobile agent version to take full advantage of the new event types. Attribute name Definition Events category The type of data, either session or interaction. Mobile MobileSession interactionDuration For interaction category events only. This is the total time for the interaction to render on the device. In addition to render time, this usually includes all external calls associated with the interaction. Currently, this attribute is measured in seconds for Android devices and in milliseconds for iOS devices. Mobile name For interaction category events only. This is the label of the interaction associated with the event. It is by default assigned by New Relic. For example: ApplicationsListFragment or Display iOS_Example.MasterViewController. Mobile reportedTimestampMs For interaction category events only. The UTC based timestamp for when the event was sent to New Relic. This is different from the attribute timestamp, which is when the event began. Mobile MobileCrash Data source : Mobile The MobileCrash event is created when an app crashes. MobileCrash includes attributes such as crash line number, class, and crash message. Attribute name Definition Events appBuild Indicates the technical build number of the app binary. As a developer, you can use this attribute to identify specific builds of your app. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appId ID A unique identifier for a monitored app, based on the app token. For example: 35091. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appName The name of the monitored app. For example: My Mobile App - iOS. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appToken The mobile application license token. MobileCrash appVersion The version number of the monitored app. For example: 2.2.9. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appVersionId ID An identifier for the specific version of the app. For example: 1713477. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession architecture The processor architecture of the device. For example: armv7 or arm64. MobileCrash asnOwner The telecom owner of the ASN. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession bundleId ID The unique string used to identify the application. MobileCrash MobileSession carrier The network over which the app transferred data, such as Wi-Fi, Verizon, or Sprint. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession crashException The exception associated with the crash, if one is present. For example: java.lang.NullPointerException. MobileCrash crashFingerprint ID The New Relic-generated fingerprint used to uniquely identify the crash and other crashes identical to this one. MobileCrash crashLocationFile The file in which the crash occurred. MobileCrash crashMessage The message associated with the crash, if one is present. MobileCrash deviceManufacturer The manufacturer of the device, such as Motorola or HTC. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession deviceModel The model number of the device, such as XT1039 or SM-G900F. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession deviceName The device's name. MobileCrash deviceUuid ID A unique identifier assigned at the time of app installation by New Relic. It is only reset if a user deletes and then reinstalls the app. For example: B8B0BC30-0235-11E4-9191-0800200C9A66. Dupliate of uuid. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession diskAvailable bytes (B) Space available on the device, in bytes. MobileCrash interactionHistory The client interactions with the application that led to the crash. MobileCrash isFirstOccurrence A boolean value indicating whether or not this was the first occurrence of the crash. MobileCrash lastInteraction The last interaction before a crash or harvest event, if one is present. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession memUsageMb megabytes (MB) The total amount of memory, in MB, used by the application. Updated every 60 seconds. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession modelNumber The model of the device. This is the same as the session-level deviceModel attribute. MobileCrash networkStatus The type of network that the device was on at the time of crash, such as wifi or LTE. MobileCrash newRelicVersion The version number of the agent running on the application. For example: 4.232.0. Duplicate of newRelicAgentVersion. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession occurrenceId ID The ID for this instance of the crash. MobileCrash orientation The orientation of the device, such as landscape or portrait. MobileCrash osBuild For Android only. The specific build of the Android OS. MobileCrash MobileHandledException osMajorVersion The simplified version number of the app's host operating system, such as iOS 11, as compared to iOS 11.0.4. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession osName The name of the app's host operating system, for example, iOS or Android. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession osVersion The exact version number of the app's host operating system, such as iOS 11.0.4, as compared to iOS 11. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession parentProcess The parent process that launched the crashing process. MobileCrash parentProcessId ID The parent identification number (PID) of the parent process. MobileCrash platform The platform type of the New Relic Mobile agent, such as native or Cordova. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession processId ID The PID of the previously running process on the device. MobileCrash processName The name of the previously running process. MobileCrash processPath The path to the binary. MobileCrash reportedTimestampMs The UTC timestamp for when the event was received by New Relic. (This is different from timestamp, which is when the MobileSession event began that crashed.) MobileCrash runTime For Android only. The Android Runtime version where the exception/crash was generated. MobileCrash MobileHandledException sessionCrashed A boolean value indicating whether or not that session crashed. MobileCrash MobileSession sessionId ID A unique identifier for a single user session. A new sessionId is created each time the app is brought into the foreground. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession symbolicated A boolean value indicating whether or not the crash was properly symbolicated. MobileCrash timeSinceLastInteraction milliseconds (ms) The time, in milliseconds, since the interaction before a crash event. MobileCrash userImageUuids ID The array of build UUIDs for applications and libraries. MobileCrash uuid ID A unique identifier assigned by New Relic for a specific app on a particular device. It is only reset if a user deletes and then reinstalls the app. For example: B8B0BC30-0235-11E4-9191-0800200C9A66. Dupliate of deviceUuid. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession MobileHandledException Data source : Mobile MobileHandledException is sent when an exception is caught and is used for non-fatal exceptions reported to New Relic using the recordHandledException API for Android or iOS. Attribute name Definition Events appBuild Indicates the technical build number of the app binary. As a developer, you can use this attribute to identify specific builds of your app. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appId ID A unique identifier for a monitored app, based on the app token. For example: 35091. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appName The name of the monitored app. For example: My Mobile App - iOS. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appVersion The version number of the monitored app. For example: 2.2.9. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appVersionId ID An identifier for the specific version of the app. For example: 1713477. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession asn Autonomous System Number: a unique number identifying a group of IP networks that serves the content to the end user. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError MobileHandledException MobileRequest MobileRequestError MobileSession PageAction PageView PageViewTiming Span asnOwner The telecom owner of the ASN. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession carrier The network over which the app transferred data, such as Wi-Fi, Verizon, or Sprint. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession city The city in which the event occurred, such as Portland or Seattle. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError MobileHandledException MobileSession PageAction PageView PageViewTiming countryCode The country from which the device ran the application. For a list of country codes, see ISO 3166-1 alpha-2. MobileHandledException MobileRequest MobileRequestError MobileSession device The specific type of the device: iPhone 8, iPad Pro, etc. Duplicate of deviceType. MobileHandledException MobileRequest MobileRequestError MobileSession deviceManufacturer The manufacturer of the device, such as Motorola or HTC. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession deviceModel The model number of the device, such as XT1039 or SM-G900F. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession deviceType The specific type of device: iPhone 8, iPad Pro, etc. Duplicate of device. MobileHandledException MobileRequest MobileRequestError MobileSession deviceUuid ID A unique identifier assigned at the time of app installation by New Relic. It is only reset if a user deletes and then reinstalls the app. For example: B8B0BC30-0235-11E4-9191-0800200C9A66. Dupliate of uuid. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession exceptionAppBuildUuid ID The build uuid of the application binary in which the exception was caught. MobileHandledException exceptionCause The unsymbolicated, platform-specific cause of the exception. MobileHandledException exceptionLocation New Relic defined location of an exception. Contains a combination of exception file name, class, line number, and method. MobileHandledException exceptionLocationClass The class that generated the exception. Only present if symbolication succeeded. MobileHandledException exceptionLocationFile The class that generated the exception. Only present if symbolication succeeded. MobileHandledException exceptionLocationLibraryOffset For XCFramework agent only. The library offset of the library that generated the exception. Only present if symbolication succeeded. MobileHandledException exceptionLocationLine Comes from the exception: The line number where the exception was generated. Only present if symbolication succeeded. MobileHandledException exceptionLocationMethod The method that generated the exception. Only present if symbolication succeeded. MobileHandledException exceptionMessage The unsymbolicated message from the exception. It can be user-generated or a generic system message. For Android, this is the Throwable message. MobileHandledException exceptionName The unsymbolicated exception type. MobileHandledException fingerprint ID The New Relic-generated identifier used to group like exceptions. MobileHandledException handledExceptionUuid ID The unique ID of the exception event. MobileHandledException lastInteraction The last interaction before a crash or harvest event, if one is present. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession libraryName For XCFramework agent only. The library name where the exception was generated. MobileHandledException libraryStartAddr For XCFramework agent only. The library start address where the exception was generated. MobileHandledException memUsageMb megabytes (MB) The total amount of memory, in MB, used by the application. Updated every 60 seconds. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession newRelicVersion The version number of the agent running on the application. For example: 4.232.0. Duplicate of newRelicAgentVersion. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession occurrenceTimestamp Agent-reported epoch timestamp of the handled exception. MobileHandledException osBuild For Android only. The specific build of the Android OS. MobileCrash MobileHandledException osMajorVersion The simplified version number of the app's host operating system, such as iOS 11, as compared to iOS 11.0.4. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession osName The name of the app's host operating system, for example, iOS or Android. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession osVersion The exact version number of the app's host operating system, such as iOS 11.0.4, as compared to iOS 11. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession platform The platform type of the New Relic Mobile agent, such as native or Cordova. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession regionCode The specific region within a country where the monitored app is located. In the United States, regions are states. The regionCode is based on IP address and may not always match your region. MobileHandledException MobileRequest MobileRequestError MobileSession runTime For Android only. The Android Runtime version where the exception/crash was generated. MobileCrash MobileHandledException sessionId ID A unique identifier for a single user session. A new sessionId is created each time the app is brought into the foreground. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession timestamp Epoch timestamp of the handled exception. This exception timestamp represents the time New Relic created the event, if it's older than two days or some other unexpected time. MobileHandledException uuid ID A unique identifier assigned by New Relic for a specific app on a particular device. It is only reset if a user deletes and then reinstalls the app. For example: B8B0BC30-0235-11E4-9191-0800200C9A66. Dupliate of deviceUuid. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession MobileRequest Data source : Mobile A MobileRequest event is created when an HTTP request successfully completes, resulting in a response code below 400. Attribute name Definition Events appBuild Indicates the technical build number of the app binary. As a developer, you can use this attribute to identify specific builds of your app. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appId ID A unique identifier for a monitored app, based on the app token. For example: 35091. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appName The name of the monitored app. For example: My Mobile App - iOS. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appVersion The version number of the monitored app. For example: 2.2.9. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appVersionId ID An identifier for the specific version of the app. For example: 1713477. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession asn Autonomous System Number: a unique number identifying a group of IP networks that serves the content to the end user. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError MobileHandledException MobileRequest MobileRequestError MobileSession PageAction PageView PageViewTiming Span asnOwner The telecom owner of the ASN. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession bytesReceived bytes (B) Optional: If the application received a response from the requestUrl, the size of that response in bytes. MobileRequest MobileRequestError bytesSent bytes (B) Optional: If the application sent a request to the requestUrl, the size of that request in bytes. MobileRequest MobileRequestError carrier The network over which the app transferred data, such as Wi-Fi, Verizon, or Sprint. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession connectionType The type of connection which the device was using, such as 2G or 3G. MobileRequest MobileRequestError countryCode The country from which the device ran the application. For a list of country codes, see ISO 3166-1 alpha-2. MobileHandledException MobileRequest MobileRequestError MobileSession device The specific type of the device: iPhone 8, iPad Pro, etc. Duplicate of deviceType. MobileHandledException MobileRequest MobileRequestError MobileSession deviceGroup The category of the device, such as iPhone or Tablet. MobileRequest MobileRequestError MobileSession deviceManufacturer The manufacturer of the device, such as Motorola or HTC. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession deviceModel The model number of the device, such as XT1039 or SM-G900F. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession deviceSize The display size of the device: Small, normal, large, xlarge. MobileRequest MobileRequestError deviceType The specific type of device: iPhone 8, iPad Pro, etc. Duplicate of device. MobileHandledException MobileRequest MobileRequestError MobileSession deviceUuid ID A unique identifier assigned at the time of app installation by New Relic. It is only reset if a user deletes and then reinstalls the app. For example: B8B0BC30-0235-11E4-9191-0800200C9A66. Dupliate of uuid. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession duration seconds (s) Optional: The time to complete the request, measured in fractional seconds. MobileRequest MobileRequestError guid ID The unique identifier for the segment. This is equivalent to spanID in OpenTracing semantics. MobileRequest MobileRequestError Span lastInteraction The last interaction before a crash or harvest event, if one is present. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession memUsageMb megabytes (MB) The total amount of memory, in MB, used by the application. Updated every 60 seconds. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession newRelicVersion The version number of the agent running on the application. For example: 4.232.0. Duplicate of newRelicAgentVersion. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession osMajorVersion The simplified version number of the app's host operating system, such as iOS 11, as compared to iOS 11.0.4. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession osName The name of the app's host operating system, for example, iOS or Android. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession osVersion The exact version number of the app's host operating system, such as iOS 11.0.4, as compared to iOS 11. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession platform The platform type of the New Relic Mobile agent, such as native or Cordova. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession regionCode The specific region within a country where the monitored app is located. In the United States, regions are states. The regionCode is based on IP address and may not always match your region. MobileHandledException MobileRequest MobileRequestError MobileSession requestDomain The domain that the application attempted to access when the event occurred. MobileRequest MobileRequestError requestFingerprint ID The New Relic-generated identifier used to group like request events. MobileRequest requestMethod The REST method (GET, PUT, POST, etc.) that the application attempted when the event occurred. MobileRequest MobileRequestError requestPath The path that the application attempted to access when the event occurred. MobileRequest MobileRequestError requestUrl The URL that the application attempted to access when the event occurred. MobileRequest MobileRequestError requestUuid ID A unique identifer for the request event. MobileRequest MobileRequestError responseTime seconds (s) The time between the request and the response in fractional seconds. MobileRequest MobileRequestError sessionId ID A unique identifier for a single user session. A new sessionId is created each time the app is brought into the foreground. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession statusCode Optional: The HTTP status code for the HTTP event. MobileRequest MobileRequestError timestamp The UTC epoch time at which an event began. MobileRequest MobileRequestError MobileSession trace.id ID The unique ID (a randomly generated string) used to identify a single request as it crosses inter- and intra- process boundaries. This ID allows the linking of spans in a distributed trace. Included when distributed tracing is enabled. DistributedTraceSummary MobileRequest MobileRequestError Span Transaction TransactionError traceId ID The unique ID (a randomly generated string) used to identify a single request as it crosses inter- and intra- process boundaries. This ID allows the linking of spans in a distributed trace. Included when distributed tracing is enabled. AwsLambdaInvocation AwsLambdaInvocationError MobileRequest MobileRequestError Span uuid ID A unique identifier assigned by New Relic for a specific app on a particular device. It is only reset if a user deletes and then reinstalls the app. For example: B8B0BC30-0235-11E4-9191-0800200C9A66. Dupliate of deviceUuid. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession MobileRequestError Data source : Mobile A MobileRequestError is used for HTTP errors or network failures. HTTP errors are HTTP requests that have a status code greater than 400. A network failure is a HTTP request that results in no response. The event is sent when the HTTP request completes. Attribute name Definition Events appBuild Indicates the technical build number of the app binary. As a developer, you can use this attribute to identify specific builds of your app. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appId ID A unique identifier for a monitored app, based on the app token. For example: 35091. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appName The name of the monitored app. For example: My Mobile App - iOS. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appVersion The version number of the monitored app. For example: 2.2.9. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appVersionId ID An identifier for the specific version of the app. For example: 1713477. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession asn Autonomous System Number: a unique number identifying a group of IP networks that serves the content to the end user. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError MobileHandledException MobileRequest MobileRequestError MobileSession PageAction PageView PageViewTiming Span asnOwner The telecom owner of the ASN. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession bytesReceived bytes (B) Optional: If the application received a response from the requestUrl, the size of that response in bytes. MobileRequest MobileRequestError bytesSent bytes (B) Optional: If the application sent a request to the requestUrl, the size of that request in bytes. MobileRequest MobileRequestError carrier The network over which the app transferred data, such as Wi-Fi, Verizon, or Sprint. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession connectionType The type of connection which the device was using, such as 2G or 3G. MobileRequest MobileRequestError countryCode The country from which the device ran the application. For a list of country codes, see ISO 3166-1 alpha-2. MobileHandledException MobileRequest MobileRequestError MobileSession device The specific type of the device: iPhone 8, iPad Pro, etc. Duplicate of deviceType. MobileHandledException MobileRequest MobileRequestError MobileSession deviceGroup The category of the device, such as iPhone or Tablet. MobileRequest MobileRequestError MobileSession deviceManufacturer The manufacturer of the device, such as Motorola or HTC. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession deviceModel The model number of the device, such as XT1039 or SM-G900F. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession deviceSize The display size of the device: Small, normal, large, xlarge. MobileRequest MobileRequestError deviceType The specific type of device: iPhone 8, iPad Pro, etc. Duplicate of device. MobileHandledException MobileRequest MobileRequestError MobileSession deviceUuid ID A unique identifier assigned at the time of app installation by New Relic. It is only reset if a user deletes and then reinstalls the app. For example: B8B0BC30-0235-11E4-9191-0800200C9A66. Dupliate of uuid. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession duration seconds (s) Optional: The time to complete the request, measured in fractional seconds. MobileRequest MobileRequestError errorType Either HTTPError or NetworkFailure, depending on whether the error is a result of a failed request to a host or a failure on the cellular network. MobileRequestError guid ID The unique identifier for the segment. This is equivalent to spanID in OpenTracing semantics. MobileRequest MobileRequestError Span lastInteraction The last interaction before a crash or harvest event, if one is present. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession memUsageMb megabytes (MB) The total amount of memory, in MB, used by the application. Updated every 60 seconds. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession networkError The error message associated with the iOS NSURL Error code. See networkErrorCode for more information. MobileRequestError networkErrorCode If the error is a network error, this is the iOS network error code. For Android applications, this is the mapped value. MobileRequestError newRelicVersion The version number of the agent running on the application. For example: 4.232.0. Duplicate of newRelicAgentVersion. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession osMajorVersion The simplified version number of the app's host operating system, such as iOS 11, as compared to iOS 11.0.4. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession osName The name of the app's host operating system, for example, iOS or Android. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession osVersion The exact version number of the app's host operating system, such as iOS 11.0.4, as compared to iOS 11. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession platform The platform type of the New Relic Mobile agent, such as native or Cordova. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession regionCode The specific region within a country where the monitored app is located. In the United States, regions are states. The regionCode is based on IP address and may not always match your region. MobileHandledException MobileRequest MobileRequestError MobileSession requestDomain The domain that the application attempted to access when the event occurred. MobileRequest MobileRequestError requestErrorFingerprint ID The New Relic-generated identifier used to group like request error events. MobileRequestError requestMethod The REST method (GET, PUT, POST, etc.) that the application attempted when the event occurred. MobileRequest MobileRequestError requestPath The path that the application attempted to access when the event occurred. MobileRequest MobileRequestError requestUrl The URL that the application attempted to access when the event occurred. MobileRequest MobileRequestError requestUuid ID A unique identifer for the request event. MobileRequest MobileRequestError responseBody Optional: The response that is sent from the requestDomain for the HTTP error, up to 4096 bytes. MobileRequestError responseTime seconds (s) The time between the request and the response in fractional seconds. MobileRequest MobileRequestError sessionId ID A unique identifier for a single user session. A new sessionId is created each time the app is brought into the foreground. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession statusCode Optional: The HTTP status code for the HTTP event. MobileRequest MobileRequestError timestamp The UTC epoch time at which an event began. MobileRequest MobileRequestError MobileSession trace.id ID The unique ID (a randomly generated string) used to identify a single request as it crosses inter- and intra- process boundaries. This ID allows the linking of spans in a distributed trace. Included when distributed tracing is enabled. DistributedTraceSummary MobileRequest MobileRequestError Span Transaction TransactionError traceId ID The unique ID (a randomly generated string) used to identify a single request as it crosses inter- and intra- process boundaries. This ID allows the linking of spans in a distributed trace. Included when distributed tracing is enabled. AwsLambdaInvocation AwsLambdaInvocationError MobileRequest MobileRequestError Span uuid ID A unique identifier assigned by New Relic for a specific app on a particular device. It is only reset if a user deletes and then reinstalls the app. For example: B8B0BC30-0235-11E4-9191-0800200C9A66. Dupliate of deviceUuid. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession MobileSession Data source : Mobile A MobileSession event is sent when an app is closed, backgrounded, or when 10 minutes of active use has elapsed. This is the source of the general session data used by the other New Relic Mobile events. MobileSession captures attributes such as device type, device OS, and geographical information. Attribute name Definition Events appBuild Indicates the technical build number of the app binary. As a developer, you can use this attribute to identify specific builds of your app. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appId ID A unique identifier for a monitored app, based on the app token. For example: 35091. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appName The name of the monitored app. For example: My Mobile App - iOS. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appVersion The version number of the monitored app. For example: 2.2.9. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appVersionId ID An identifier for the specific version of the app. For example: 1713477. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession asn Autonomous System Number: a unique number identifying a group of IP networks that serves the content to the end user. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError MobileHandledException MobileRequest MobileRequestError MobileSession PageAction PageView PageViewTiming Span asnOwner The telecom owner of the ASN. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession bundleId ID The unique string used to identify the application. MobileCrash MobileSession carrier The network over which the app transferred data, such as Wi-Fi, Verizon, or Sprint. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession category The type of data, either session or interaction. Mobile MobileSession city The city in which the event occurred, such as Portland or Seattle. AjaxRequest BrowserInteraction BrowserTiming JavaScriptError MobileHandledException MobileSession PageAction PageView PageViewTiming countryCode The country from which the device ran the application. For a list of country codes, see ISO 3166-1 alpha-2. MobileHandledException MobileRequest MobileRequestError MobileSession device The specific type of the device: iPhone 8, iPad Pro, etc. Duplicate of deviceType. MobileHandledException MobileRequest MobileRequestError MobileSession deviceGroup The category of the device, such as iPhone or Tablet. MobileRequest MobileRequestError MobileSession deviceManufacturer The manufacturer of the device, such as Motorola or HTC. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession deviceModel The model number of the device, such as XT1039 or SM-G900F. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession deviceType The specific type of device: iPhone 8, iPad Pro, etc. Duplicate of device. MobileHandledException MobileRequest MobileRequestError MobileSession deviceUuid ID A unique identifier assigned at the time of app installation by New Relic. It is only reset if a user deletes and then reinstalls the app. For example: B8B0BC30-0235-11E4-9191-0800200C9A66. Dupliate of uuid. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession install Indicates true only if the current session is the first session after app install. MobileSession lastInteraction The last interaction before a crash or harvest event, if one is present. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession memUsageMb megabytes (MB) The total amount of memory, in MB, used by the application. Updated every 60 seconds. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession newRelicAgent The New Relic agent running on the application. For example: the iOSAgent or the androidAgent. MobileSession newRelicVersion The version number of the agent running on the application. For example: 4.232.0. Duplicate of newRelicAgentVersion. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession osMajorVersion The simplified version number of the app's host operating system, such as iOS 11, as compared to iOS 11.0.4. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession osName The name of the app's host operating system, for example, iOS or Android. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession osVersion The exact version number of the app's host operating system, such as iOS 11.0.4, as compared to iOS 11. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession platform The platform type of the New Relic Mobile agent, such as native or Cordova. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession regionCode The specific region within a country where the monitored app is located. In the United States, regions are states. The regionCode is based on IP address and may not always match your region. MobileHandledException MobileRequest MobileRequestError MobileSession sessionCrashed A boolean value indicating whether or not that session crashed. MobileCrash MobileSession sessionDuration seconds (s) The length of time for which the user used the application in seconds. If the session crashes, sessionDuration is not captured (although other events and attributes are still recorded). For sessions longer than 10 minutes, events in the Interaction and Custom event categories are sent to Insights while the session is ongoing, and therefore do not have sessionDuration attributes. Events recorded near the end of the session will include the duration, as will the Session event category. MobileSession sessionId ID A unique identifier for a single user session. A new sessionId is created each time the app is brought into the foreground. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession timeSinceLoad seconds (s) The time, in seconds, from the beginning of the mobile session to the time the event occurred. MobileSession timestamp The UTC epoch time at which an event began. MobileRequest MobileRequestError MobileSession upgradeFrom Indictates previous version number only if this is the first launch after app upgrade. MobileSession uuid ID A unique identifier assigned by New Relic for a specific app on a particular device. It is only reset if a user deletes and then reinstalls the app. For example: B8B0BC30-0235-11E4-9191-0800200C9A66. Dupliate of deviceUuid. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession NetworkSample Data source : Infrastructure NetworkSample event captures the descriptive and state information for each network device associated with a server. It includes the device's interface and address information, as well as current usage data. New Relic samples this data every 10 seconds for each attached network interface and packages it into a NetworkSample event, then sends the raw data to New Relic's collectors every 60 seconds. Attribute name Definition Events agentName The name of the agent (Infrastructure). NetworkSample ProcessSample StorageSample SystemSample agentVersion The version of the New Relic Infrastructure agent. NetworkSample ProcessSample StorageSample SystemSample criticalViolationCount count The number of times that alert conditions violated critical thresholds, causing critical violations and opening incidents. If this attribute does not exist on the sample, it has zero violations. ContainerSample NetworkSample ProcessSample StorageSample SystemSample entityID ID New Relic's unique ID number for the entity that is reporting data. This is used by New Relic to distinguish between customers and their entities. NetworkSample ProcessSample StorageSample SystemSample fullHostname The fully qualified (DNS) hostname. NetworkSample ProcessSample StorageSample SystemSample hardwareAddress The unique hardware address of the interface. NetworkSample hostname The short version of the entity's name. NetworkSample ProcessSample StorageSample SystemSample interfaceName The interface name as reported by the operating system. NetworkSample ipV4Address The IP version 4 address. NetworkSample ipV6Address The IP version 6 address. NetworkSample kernelVersion The Linux kernel version, in string format. This attribute is available only for systems on a Linux platform. NetworkSample ProcessSample StorageSample SystemSample linuxDistribution The name of the Linux distribution the server is using. This attribute is available only for systems on a Linux platform. NetworkSample ProcessSample StorageSample SystemSample operatingSystem The operating system on which the agent is installed. NetworkSample ProcessSample StorageSample SystemSample receiveBytesPerSecond bytes (B) The number of bytes per second received during the sampling period. NetworkSample receiveDroppedPerSecond count The number of received packets per second dropped during the sampling period. NetworkSample receiveErrorsPerSecond count The number of receive errors per second on the interface during the sampling period. NetworkSample receivePacketsPerSecond count The number of packets per second (as defined by OS) received during the sampling period. NetworkSample state The state of the entity: either up or down. NetworkSample timestamp The time (date, hour, minute, second) at which the interaction occurred. NetworkSample ProcessSample StorageSample SystemSample transmitBytesPerSecond bytes (B) The number of bytes sent per second during the sampling period. NetworkSample transmitDroppedPerSecond count The number of dropped send packets per second during the sampling period. NetworkSample transmitErrorsPerSecond count The number of send errors per second on the interface during the sampling period. NetworkSample transmitPacketsPerSecond count The number of packets per second as defined by OS) sent during this sampling period. NetworkSample warningViolationCount count The number of times that alert conditions violated warning thresholds, causing warning violations. ContainerSample NetworkSample ProcessSample StorageSample SystemSample windowsFamily The Windows family indicates w",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 277.47964,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>InfrastructureEvent</em>",
        "body": " MobileSession app<em>Id</em> <em>ID</em> A unique identifier for a monitored app, based on the app token. For example: 35091. MobileCrash MobileHandledException MobileRequest MobileRequestError MobileSession appName The name of the monitored app. For example: My Mobile App - <em>iOS</em>. MobileCrash MobileHandledException"
      },
      "id": "603f53b164441f41894e8875"
    },
    {
      "sections": [
        "tvOS installation and configuration",
        "Tip",
        "Installing your tvOS application",
        "Configuring your tvOS application",
        "Executing a demo crash (optional)",
        "Changing the logging level (optional)",
        "For more help"
      ],
      "title": "tvOS installation and configuration",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "New Relic Mobile iOS",
        "tvOS"
      ],
      "external_id": "04798a275a7591bfbafb5437194cfbab4b33d8e3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-ios/tvos/tvos-installation-configuration/",
      "published_at": "2021-06-26T14:43:43Z",
      "updated_at": "2021-05-16T01:00:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Tip Owner or Admins Follow these instructions to install and configure New Relic Mobile functionality with your tvOS applications. Tip tvOS apps using Cocoapods have separate installation procedures. Installing your tvOS application As part of the installation process, New Relic Mobile automatically generates an application token. This is a 40-character hexadecimal string for authenticating each mobile app you monitor in New Relic Mobile. For Admins with existing New Relic accounts, follow these steps to install and configure your application. (If you do not have a New Relic account, see New Relic Mobile.) Go to one.newrelic.com and click Mobile. If applicable: From the Mobile Apps list, select Add a new app From the Get Started page, select tvOS as the platform for mobile monitoring. Type a name for your mobile app, and select Continue. Continue with the steps to configure New Relic Mobile. To complete the configuration process for a new mobile app later: Go to one.newrelic.com and click Mobile, then select See Instructions next to your mobile app name. To upgrade an existing tvOS installation: Go to one.newrelic.com > Mobile > (selected app) > Settings > Installation. Configuring your tvOS application These procedures to configure your tvOS app also appear on the Get Started page in New Relic. Download and unzip the tvOS SDK for New Relic Mobile. To add the New Relic tvOS Mobile Framework to your Xcode project: Use Finder to drag the NewRelicAgentTVOS.framework folder into your Xcode project, and drop it onto your Project in the Project Navigator window. Follow the prompts to copy items into destination and to create folder references. Add the SystemConfiguration.framework, libc++.tbd, and libz.tbd libraries to your Linker settings. To start the agent: Import the New Relic Mobile Agent header at the top of your prefix.pch. Add + [ NewRelic startWithApplicationToken: < appToken>] to the top of -application:didFinishLaunchingWithOptions: in your AppDelegate.m using the unique application token that is automatically generated. Add a build script to your target's Build Phases and paste the following, replacing PUT_NEW_RELIC_APP_TOKEN_HERE with your application token: SCRIPT=`/usr/bin/find \"${SRCROOT}\" -name newrelic_postbuild.sh | head -n 1` /bin/sh \"${SCRIPT}\" \"PUT_NEW_RELIC_APP_TOKEN_HERE\" Copy Clean and build your app, and then run it in the simulator or other device. Within a few minutes you will begin to see data for your iOS app: Go to one.newrelic.com > Mobile > (selected app). If you don't, see No data appears. Executing a demo crash (optional) If you have trouble getting your app to crash, the New Relic agent provides an API to execute a demo crash. Recommendation: Add one of these lines of code to a button click event handler as applicable: [NewRelic crashNow]; Copy OR [NewRelic crashNow:@\"<reason>\"]; Copy Changing the logging level (optional) Six log levels are available for mobile apps monitoring: none error warning info verbose ALL To increase your logging level in the app, add this method call before calling startWithApplicationToken: [NRLogger setLogLevels:NRLogLevelALL]; Copy For more help Additional documentation resources include: Mobile Apps index (standard menu functions available from your list of mobile applications in New Relic Mobile) Mobile Apps Overview page (a summary of your mobile app's performance) Customizing your mobile app settings (Settings tab options to view your mobile app's application token, rename it, change its alert thresholds, or install New Relic Mobile updates) Upgrading New Relic Mobile for tvOS apps (standard upgrade procedures) No data appears (basic troubleshooting steps)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.10951,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "tvOS <em>installation</em> and configuration",
        "sections": "tvOS <em>installation</em> and configuration",
        "tags": "New Relic Mobile <em>iOS</em>",
        "body": " or other <em>device</em>. Within a few minutes you will begin to see data for your <em>iOS</em> app: Go to one.newrelic.com &gt; Mobile &gt; (selected app). If you don&#x27;t, see No data appears. Executing a demo crash (optional) If you have trouble getting your app to crash, the New Relic agent provides an API to execute a demo"
      },
      "id": "60441ac5e7b9d26bb55799b6"
    },
    {
      "sections": [
        "Upgrading New Relic Mobile's tvOS SDK",
        "Tip",
        "Contents",
        "Replacing your tvOS framework",
        "For more help"
      ],
      "title": "Upgrading New Relic Mobile's tvOS SDK",
      "type": "docs",
      "tags": [
        "Mobile monitoring",
        "New Relic Mobile iOS",
        "tvOS"
      ],
      "external_id": "f5d6d3d356952d185b96aa409605b79e3ace8ec9",
      "image": "https://docs.newrelic.com/static/a3b6801675529d8f4eba123cf08e8f1f/c1b63/Mobile_tvOS_replace-framework_1.png",
      "url": "https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile-ios/tvos/upgrading-new-relic-mobiles-tvos-sdk/",
      "published_at": "2021-06-26T14:43:44Z",
      "updated_at": "2021-05-16T01:00:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Tip Owner or Admins You must be the account Admin to install, configure, and upgrade New Relic Mobile functionality with your tvOS applications. For information about the latest version, refer to the release notes. Contents Replacing your tvOS framework Admins: You must replace the earlier version of your tvOS agent framework before upgrading to a newer version of the tvOS SDK for New Relic Mobile. Here is an example of the workflow to remove your existing tvOS agent framework so you can replace it with a newer version. From the Project Navigator (CMD 1) in Xcode, search for NewRelicAgentTVOS.framework. Right-click or control-click NewRelicAgentTVOS.framework, and select Show in Finder. Drag NewRelicAgentTVOS.framework to the trash. Verify that the Xcode project highlights the reference to NewRelicAgentTVOS.framework in red. Right-click or control-click NewRelicAgentTVOS.framework, and select Delete to remove the obsolete reference from the project. Follow standard installation procedures for tvOS app monitoring. For more help Additional documentation resources include: tvOS installation and configuration (standard installation and configuration procedures) CocoaPods installation and configuration (procedures specific to CocoaPods) Customizing your mobile app settings (Settings tab options to view your mobile app's application token, rename it, or install New Relic Mobile updates)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.10951,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "New Relic Mobile <em>iOS</em>",
        "body": "Tip Owner or Admins You must be the account Admin to install, configure, and upgrade New Relic Mobile functionality with your tv<em>OS</em> applications. For information about the latest version, refer to the release notes. Contents Replacing your tv<em>OS</em> framework Admins: You must replace the earlier version"
      },
      "id": "604416cc64441f805c378ecf"
    }
  ],
  "/docs/licenses/index": [
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-06-25T22:44:37Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0  2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0  2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0  2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0  2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0  2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright  2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright  1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright  1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright  2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright  1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright  1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 65.11467,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "Licenses for plugin developers",
        "Important",
        "Limited access to legacy plugins",
        "Licenses for developing plugins"
      ],
      "title": "Licenses for plugin developers",
      "type": "docs",
      "tags": [
        "Plugins",
        "Plugin developer resources",
        "Developer reference"
      ],
      "external_id": "04388d903244009eb60f0da612f3a21d899300f0",
      "image": "",
      "url": "https://docs.newrelic.com/docs/plugins/plugin-developer-resources/developer-reference/licenses-plugin-developers/",
      "published_at": "2021-06-25T22:25:50Z",
      "updated_at": "2021-03-13T02:39:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important For an even better experience than plugins, go to: newrelic.com/integrations: Integrate the on-host and cloud systems you already use with New Relic, so you can filter and analyze data, create dashboards, and set alerts within a single platform. developer.newrelic.com: Use developer tools to collect data from any source, automate workflows, build apps, and use our APIs. Limited access to legacy plugins As of December 2, 2020, plugin access has been limited to accounts that have accessed a legacy plugin in the past 30 days. The legacy plugin experience will reach end of life (EoL) as of June 16, 2021. For more information, see our Explorers Hub post. Licenses for developing plugins In addition to the Plugins licenses used by New Relic, the software licenses associated with developing plugins depend on the plugin itself. For example: Plugins developed with the Java SDK for Plugins use the same MIT and Apache 2.0 licenses as New Relic APM's Java agent. Plugins developed with the Ruby SDK for Plugins use the same MIT licenses as New Relic APM's Ruby agent. Plugins developed with the .NET SDK for New Relic Plugins use the same MIT licenses as New Relic APM's .NET agent. Collectors for Plugins use the same MySQL Java licenses as New Relic. For more information, see Data collector licenses. Plugins that do not use an SDK fall under the licensing agreements that the individual plugin author uses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 60.468098,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Licenses</em> for plugin developers",
        "sections": "<em>Licenses</em> for plugin developers",
        "body": ") as of June 16, 2021. For more information, see our Explorers Hub post. <em>Licenses</em> for developing plugins In addition to the Plugins <em>licenses</em> used by New Relic, the software <em>licenses</em> associated with developing plugins depend on the plugin itself. For example: Plugins developed with the Java SDK"
      },
      "id": "604435cd64441f7eb5378ec7"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2021-06-26T04:46:32Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 59.872288,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic APM <em>licenses</em>",
        "sections": "New Relic APM <em>licenses</em>",
        "tags": "<em>Licenses</em>",
        "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view <em>licenses</em> for... See... APM agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;product-or-service-<em>licenses</em>&#x2F;new-relic-apm"
      },
      "id": "603e7895e7b9d24e832a07d5"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-06-26T08:50:43Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.36313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "FIT instrumentation end user license agreement"
      ],
      "title": "FIT instrumentation end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "e8e10baf120678407d08c9f78ab708d271cf0223",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement/",
      "published_at": "2021-06-26T04:34:18Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NEW RELIC, INC. FIT INSTRUMENTATION END USER LICENSE AGREEMENT In connection with the work provided by New Relics Field Instrumentation Team, you may be provided with certain custom-created software to enable, optimize, or enhance your use of New Relics Services. By downloading, installing, authorizing installation, or using the FIT Instrumentation with your Underlying Software (defined below), you (Customer) agree to the terms and conditions herein (Agreement) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (New Relic), (collectively the Parties). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the Terms of Service) available here: http://newrelic.com/terms IF YOU DO NOT AGREE TO THIS AGREEMENT, PLEASE DO NOT USE THE FIT INSTRUMENTATION. 1. DEFINITIONS FIT Instrumentation means the New Relic custom-made software, including but not limited to connectors, extensions, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (Underlying Software). For the purposes of the Terms of Service, the FIT Instrumentation shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE FIT INSTRUMENTATION 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the FIT Instrumentation solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the FIT Instrumentation and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the FIT Instrumentation in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (Feedback) regarding the FIT Instrumentation, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the FIT Instrumentation is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that FIT Instrumentation will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an Upgrade Event). Customer shall hold New Relic harmless from any claims or damages arising from Customers Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the FIT Instrumentation except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the FIT Instrumentation; (iii) decompile, disassemble or reverse engineer any software underlying the FIT Instrumentation; (iv) use the FIT Instrumentation to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other partys use and enjoyment of the Services; (v) use the FIT Instrumentation to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the FIT Instrumentation; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend usage of the Services by any user, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time at https://docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential Information means: (a) the FIT Instrumentation, and any features, results or output produced by, and other information relating to the FIT Instrumentation (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. FIT INSTRUMENTATION IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. FIT INSTRUMENTATION IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE. , OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE FIT INSTRUMENTATION THEREFROM. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES OR LIMITATIONS ON APPLICABLE STATUTORY RIGHTS OF A CONSUMER, SO THE ABOVE EXCLUSION AND LIMITATIONS MAY NOT APPLY TO THE CUSTOMER. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE FIT INSTRUMENTATION OR FOR ANY ERROR OR DEFECT IN THE FIT INSTRUMENTATION OR THE SERVICES, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT, STRICT LIABILITY, OR OTHERWISE, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC'S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the FIT Instrumentation is not an official product and has not been commercially released for sale by New Relic; (b) the FIT Instrumentation may not operate properly, being in final form, or fully functional; (c) the FIT Instrumentation may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the FIT Instrumentation fully functional; (e) the information obtained using the FIT Instrumentation may not be accurate; (f) use of the FIT Instrumentation may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the FIT Instrumentation; and (h) New Relic has the right unilaterally to abandon development of the FIT Instrumentation, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the \"Documentation\").This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the FIT Instrumentation. In the absence of a separate agreement between New Relic and Customer with respect to the FIT Instrumentation, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such FIT Instrumentation. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the FIT Instrumentation only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.77925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "FIT instrumentation end user <em>license</em> agreement",
        "sections": "FIT instrumentation end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " Acceptable Use Policy as may be published and updated from time to time at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential <em>Information</em> means: (a) the FIT Instrumentation, and any features, results"
      },
      "id": "603e9f3ee7b9d206e32a0800"
    },
    {
      "sections": [
        "New Relic Agent Software Notice"
      ],
      "title": "New Relic Agent Software Notice",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "2bf9501c2767105130d3808f1bf3a91a032d903e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice/",
      "published_at": "2021-06-26T04:34:19Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This software is  2008-2021 New Relic, Inc. and its licensors. This software is solely for use with New Relics proprietary SaaS service (New Relic Service), so to use the software you must have a valid account for the New Relic Service under a separate agreement with New Relic (Subscription Agreement). You may only use the software to support your use of the New Relic Service as permitted in the Subscription Agreement. Without a Subscription Agreement, you may not use the software. All other use is prohibited. New Relic and its suppliers retain all right, title and interest (including intellectual property rights) in the software. The Subscription Agreement will control in event of a conflict with this notice. Unless otherwise agreed by New Relic in your Subscription Agreement: You may not use, copy, distribute or sublicense the software, use the software on behalf of third parties, reverse engineer or decompile the software, modify or create derivative works of the software, use the software for competitive analysis or benchmarking, or remove or obscure any proprietary notices in the software. The software is provided AS IS and New Relic disclaims all warranties, whether express, implied, statutory or otherwise, including warranties of merchantability, fitness for a particular purpose, title or noninfringement. To the full extent permitted by law, New Relic will have no liability arising from or related to the software or under this notice for any direct, indirect, special, incidental, or consequential damages of any kind, even if advised of their possibility in advance, and regardless of legal theory (whether contract, tort, negligence, strict liability or otherwise). The software may contain third-party open source software (OSS) as described here and at https://github.com/newrelic. To the extent required by the OSS license, that license will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile and as set forth: https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started and https://docs.newrelic.com/docs/browser/new-relic-browser/installation/install-new-relic-browser-agent Software versions New Relic makes available under an OSS license (such as Apache 2.0) are governed by the terms of the applicable OSS license. For a current list of New Relic software versions released as OSS please visit https://opensource.newrelic.com/.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.77925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " and at https:&#x2F;&#x2F;github.com&#x2F;newrelic. To the extent required by the OSS <em>license</em>, that <em>license</em> will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile"
      },
      "id": "603eb73828ccbc1f99eba74a"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-06-26T08:50:43Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.36313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2021-06-25T22:51:29Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (Customer) agree to the terms and conditions herein (Agreement) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (New Relic\"), (collectively the Parties). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the Terms of Service) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS Add-on means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (Underlying Software). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (Feedback) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an Upgrade Event). Customer shall hold New Relic harmless from any claims or damages arising from Customers Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other partys use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential Information means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELICS AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.7909,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential <em>Information</em> means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    },
    {
      "sections": [
        "New Relic Agent Software Notice"
      ],
      "title": "New Relic Agent Software Notice",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "2bf9501c2767105130d3808f1bf3a91a032d903e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice/",
      "published_at": "2021-06-26T04:34:19Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This software is  2008-2021 New Relic, Inc. and its licensors. This software is solely for use with New Relics proprietary SaaS service (New Relic Service), so to use the software you must have a valid account for the New Relic Service under a separate agreement with New Relic (Subscription Agreement). You may only use the software to support your use of the New Relic Service as permitted in the Subscription Agreement. Without a Subscription Agreement, you may not use the software. All other use is prohibited. New Relic and its suppliers retain all right, title and interest (including intellectual property rights) in the software. The Subscription Agreement will control in event of a conflict with this notice. Unless otherwise agreed by New Relic in your Subscription Agreement: You may not use, copy, distribute or sublicense the software, use the software on behalf of third parties, reverse engineer or decompile the software, modify or create derivative works of the software, use the software for competitive analysis or benchmarking, or remove or obscure any proprietary notices in the software. The software is provided AS IS and New Relic disclaims all warranties, whether express, implied, statutory or otherwise, including warranties of merchantability, fitness for a particular purpose, title or noninfringement. To the full extent permitted by law, New Relic will have no liability arising from or related to the software or under this notice for any direct, indirect, special, incidental, or consequential damages of any kind, even if advised of their possibility in advance, and regardless of legal theory (whether contract, tort, negligence, strict liability or otherwise). The software may contain third-party open source software (OSS) as described here and at https://github.com/newrelic. To the extent required by the OSS license, that license will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile and as set forth: https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started and https://docs.newrelic.com/docs/browser/new-relic-browser/installation/install-new-relic-browser-agent Software versions New Relic makes available under an OSS license (such as Apache 2.0) are governed by the terms of the applicable OSS license. For a current list of New Relic software versions released as OSS please visit https://opensource.newrelic.com/.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.77925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " and at https:&#x2F;&#x2F;github.com&#x2F;newrelic. To the extent required by the OSS <em>license</em>, that <em>license</em> will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile"
      },
      "id": "603eb73828ccbc1f99eba74a"
    }
  ],
  "/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice": [
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-06-26T08:50:43Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.36313,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    },
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2021-06-25T22:51:29Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (Customer) agree to the terms and conditions herein (Agreement) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (New Relic\"), (collectively the Parties). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the Terms of Service) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS Add-on means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (Underlying Software). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (Feedback) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an Upgrade Event). Customer shall hold New Relic harmless from any claims or damages arising from Customers Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other partys use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential Information means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELICS AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.7909,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential <em>Information</em> means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    },
    {
      "sections": [
        "FIT instrumentation end user license agreement"
      ],
      "title": "FIT instrumentation end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "e8e10baf120678407d08c9f78ab708d271cf0223",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement/",
      "published_at": "2021-06-26T04:34:18Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NEW RELIC, INC. FIT INSTRUMENTATION END USER LICENSE AGREEMENT In connection with the work provided by New Relics Field Instrumentation Team, you may be provided with certain custom-created software to enable, optimize, or enhance your use of New Relics Services. By downloading, installing, authorizing installation, or using the FIT Instrumentation with your Underlying Software (defined below), you (Customer) agree to the terms and conditions herein (Agreement) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (New Relic), (collectively the Parties). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the Terms of Service) available here: http://newrelic.com/terms IF YOU DO NOT AGREE TO THIS AGREEMENT, PLEASE DO NOT USE THE FIT INSTRUMENTATION. 1. DEFINITIONS FIT Instrumentation means the New Relic custom-made software, including but not limited to connectors, extensions, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (Underlying Software). For the purposes of the Terms of Service, the FIT Instrumentation shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE FIT INSTRUMENTATION 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the FIT Instrumentation solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the FIT Instrumentation and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the FIT Instrumentation in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (Feedback) regarding the FIT Instrumentation, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the FIT Instrumentation is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that FIT Instrumentation will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an Upgrade Event). Customer shall hold New Relic harmless from any claims or damages arising from Customers Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the FIT Instrumentation except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the FIT Instrumentation; (iii) decompile, disassemble or reverse engineer any software underlying the FIT Instrumentation; (iv) use the FIT Instrumentation to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other partys use and enjoyment of the Services; (v) use the FIT Instrumentation to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the FIT Instrumentation; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend usage of the Services by any user, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time at https://docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential Information means: (a) the FIT Instrumentation, and any features, results or output produced by, and other information relating to the FIT Instrumentation (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. FIT INSTRUMENTATION IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. FIT INSTRUMENTATION IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE. , OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE FIT INSTRUMENTATION THEREFROM. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES OR LIMITATIONS ON APPLICABLE STATUTORY RIGHTS OF A CONSUMER, SO THE ABOVE EXCLUSION AND LIMITATIONS MAY NOT APPLY TO THE CUSTOMER. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE FIT INSTRUMENTATION OR FOR ANY ERROR OR DEFECT IN THE FIT INSTRUMENTATION OR THE SERVICES, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT, STRICT LIABILITY, OR OTHERWISE, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC'S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the FIT Instrumentation is not an official product and has not been commercially released for sale by New Relic; (b) the FIT Instrumentation may not operate properly, being in final form, or fully functional; (c) the FIT Instrumentation may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the FIT Instrumentation fully functional; (e) the information obtained using the FIT Instrumentation may not be accurate; (f) use of the FIT Instrumentation may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the FIT Instrumentation; and (h) New Relic has the right unilaterally to abandon development of the FIT Instrumentation, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the \"Documentation\").This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the FIT Instrumentation. In the absence of a separate agreement between New Relic and Customer with respect to the FIT Instrumentation, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such FIT Instrumentation. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the FIT Instrumentation only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.77925,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "FIT instrumentation end user <em>license</em> agreement",
        "sections": "FIT instrumentation end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " Acceptable Use Policy as may be published and updated from time to time at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential <em>Information</em> means: (a) the FIT Instrumentation, and any features, results"
      },
      "id": "603e9f3ee7b9d206e32a0800"
    }
  ],
  "/docs/licenses/license-information/faq/new-relic-one-pricing-plan-frequently-asked-questions": [
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-06-26T04:35:29Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customers websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated Documentation page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customers Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relics customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 121.70634,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-06-25T22:53:06Z",
      "updated_at": "2021-05-22T17:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into the Insights event database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 121.70598,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ebacc64441f77774e8872"
    },
    {
      "sections": [
        "Services licenses",
        "Contents",
        "Java internal services",
        "Java crash data API",
        "CoffeeScript",
        "JavaScript",
        "Crash reporting",
        "Label services",
        "Infrastructure Monitoring services",
        ".NET support uploader service",
        "Ruby gems",
        "Storage services",
        "Go packages"
      ],
      "title": "Services licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "49a2ad450d31dacfc2aae690ca947d2326d18761",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/other-licenses/services-licenses/",
      "published_at": "2021-06-26T08:50:43Z",
      "updated_at": "2021-04-06T00:26:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Contents Java internal services Java crash data API Framework License Apache TomCat Apache 2.0 Bean Validation Apache 2.0 Cassandra Driver Core Apache 2.0 ClassMate Apache 2.0 Hamcrest BSD Mockito MIT Netty Apache 2.0 Objenesis MIT SnakeYAML Apache 2.0 Spring Boot Apache 2.0 Java internal services Library License json_simple Apache 2.0 newrelic-api New Relic newrelic-api New Relic antlr BSD aopalliance Public Domain asm-analysis BSD asm-commons BSD asm-tree BSD asm-util BSD asm BSD c3p0 EPLv1.0 cglib-nodep Apache 2.0 cglib Apache 2.0 cglib Apache 2.0 cal10n-api MIT logback-classic EPLv1.0 logback-core EPLv1.0 guava-jetty-service Apache 2.0 zkclient Apache 2.0 reporter-config-base Apache 2.0 reporter-config3 Apache 2.0 reporter-config Apache 2.0 aws-java-sdk-acm Apache 2.0 aws-java-sdk-api-gateway Apache 2.0 aws-java-sdk-applicationautoscaling Apache 2.0 aws-java-sdk-autoscaling Apache 2.0 aws-java-sdk-cloudformation Apache 2.0 aws-java-sdk-cloudfront Apache 2.0 aws-java-sdk-cloudhsm Apache 2.0 aws-java-sdk-cloudsearch Apache 2.0 aws-java-sdk-cloudtrail Apache 2.0 aws-java-sdk-cloudwatch Apache 2.0 aws-java-sdk-cloudwatchmetrics Apache 2.0 aws-java-sdk-codecommit Apache 2.0 aws-java-sdk-codedeploy Apache 2.0 aws-java-sdk-codepipeline Apache 2.0 aws-java-sdk-cognitoidentity Apache 2.0 aws-java-sdk-cognitoidp Apache 2.0 aws-java-sdk-cognitosync Apache 2.0 aws-java-sdk-config Apache 2.0 aws-java-sdk-core Apache 2.0 aws-java-sdk-datapipeline Apache 2.0 aws-java-sdk-devicefarm Apache 2.0 aws-java-sdk-directconnect Apache 2.0 aws-java-sdk-directory Apache 2.0 aws-java-sdk-discovery Apache 2.0 aws-java-sdk-dms Apache 2.0 aws-java-sdk-dynamodb Apache 2.0 aws-java-sdk-ec2 Apache 2.0 aws-java-sdk-ecr Apache 2.0 aws-java-sdk-ecs Apache 2.0 aws-java-sdk-efs Apache 2.0 aws-java-sdk-elasticache Apache 2.0 aws-java-sdk-elasticbeanstalk Apache 2.0 aws-java-sdk-elasticloadbalancing Apache 2.0 aws-java-sdk-elasticloadbalancingv2 Apache 2.0 aws-java-sdk-elasticsearch Apache 2.0 aws-java-sdk-elastictranscoder Apache 2.0 aws-java-sdk-emr Apache 2.0 aws-java-sdk-events Apache 2.0 aws-java-sdk-gamelift Apache 2.0 aws-java-sdk-glacier Apache 2.0 aws-java-sdk-iam Apache 2.0 aws-java-sdk-importexport Apache 2.0 aws-java-sdk-inspector Apache 2.0 aws-java-sdk-iot Apache 2.0 aws-java-sdk-kinesis Apache 2.0 aws-java-sdk-kms Apache 2.0 aws-java-sdk-lambda Apache 2.0 aws-java-sdk-logs Apache 2.0 aws-java-sdk-machinelearning Apache 2.0 aws-java-sdk-marketplacecommerceanalytics Apache 2.0 aws-java-sdk-marketplacemeteringservice Apache 2.0 aws-java-sdk-models Apache 2.0 aws-java-sdk-opsworks Apache 2.0 aws-java-sdk-rds Apache 2.0 aws-java-sdk-redshift Apache 2.0 aws-java-sdk-route53 Apache 2.0 aws-java-sdk-s3 Apache 2.0 aws-java-sdk-servicecatalog Apache 2.0 aws-java-sdk-ses Apache 2.0 aws-java-sdk-simpledb Apache 2.0 aws-java-sdk-simpleworkflow Apache 2.0 aws-java-sdk-snowball Apache 2.0 aws-java-sdk-sns Apache 2.0 aws-java-sdk-sqs Apache 2.0 aws-java-sdk-ssm Apache 2.0 aws-java-sdk-storagegateway Apache 2.0 aws-java-sdk-sts Apache 2.0 aws-java-sdk-support Apache 2.0 aws-java-sdk-swf-libraries Apache 2.0 aws-java-sdk-waf Apache 2.0 aws-java-sdk-workspaces Apache 2.0 aws-java-sdk Apache 2.0 jmespath-java Apache 2.0 AppleJavaExtensions BSD jcommander Apache 2.0 high-scale-lib MIT hppc Apache 2.0 clover Paid stream Apache 2.0 metrics-core Apache 2.0 cassandra-driver-core Apache 2.0 cassandra-driver-core Apache 2.0 uuid MIT grabbag MIT speed4j Apache 2.0 yamlbeans BSD jackson-annotations Apache 2.0 jackson-core Apache 2.0 jackson-databind Apache 2.0 jackson-dataformat-cbor Apache 2.0 jackson-dataformat-csv Apache 2.0 jackson-dataformat-xml Apache 2.0 jackson-dataformat-yaml Apache 2.0 jackson-datatype-guava Apache 2.0 jackson-datatype-jdk7 Apache 2.0 jackson-datatype-jdk8 Apache 2.0 jackson-datatype-joda Apache 2.0 jackson-datatype-jsr310 Apache 2.0 jackson-jaxrs-base Apache 2.0 jackson-jaxrs-json-provider Apache 2.0 jackson-module-afterburner Apache 2.0 jackson-module-jaxb-annotations Apache 2.0 jackson-module-kotlin Apache 2.0 woodstox-core Apache 2.0 classmate Apache 2.0 zjsonpatch Apache 2.0 caffeine Apache 2.0 waffle-jna EPLv1.0 docker-java Apache 2.0 docker-java Apache 2.0 btf Apache 2.0 jackson-coreutils Apache 2.0 json-schema-core Apache 2.0 json-schema-validator Apache 2.0 msg-simple Apache 2.0 uri-template Apache 2.0 jamm Apache 2.0 jffi Apache 2.0 jnr-constants Apache 2.0 jnr-ffi Apache 2.0 jnr-ffi Apache 2.0 jnr-posix EPLv1.0 jnr-x86asm MIT embedded-redis Apache 2.0 memoryfilesystem MIT guava-retrying Apache 2.0 guava-retrying Apache 2.0 snowball-stemmer BSD system-rules CPL uuid Apache 2.0 wiremock Apache 2.0 auto-common Apache 2.0 annotations LGPLv2.1 annotations LGPLv2.1 bcel-findbugs LGPLv2.1 findbugs LGPLv2.1 jFormatString BSD jFormatString BSD jsr305 BSD jsr305 BSD jsr305 BSD gson Apache 2.0 spymemcached MIT guava-testlib Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guava Apache 2.0 guice-multibindings Apache 2.0 guice Apache 2.0 truth Apache 2.0 concurrent-trees Apache 2.0 concurrentlinkedhashmap-lru Apache 2.0 json-simple Apache 2.0 libphonenumber Apache 2.0 locality-uuid BSD h2 EPLv1.0 annotations Apache 2.0 json-path-assert Apache 2.0 json-path Apache 2.0 json-path Apache 2.0 jzlib BSD bonecp-provider Apache 2.0 bonecp Apache 2.0 junixsocket-common Apache 2.0 junixsocket-native-common Apache 2.0 disruptor Apache 2.0 geoip-api LGPLv2.1 c3p0 LGPLv2.1 mchange-commons-java LGPLv2.1 archaius-core Apache 2.0 hystrix-core Apache 2.0 hystrix-servo-metrics-publisher Apache 2.0 rxjava-core Apache 2.0 servo-core Apache 2.0 kafka-clients Apache 2.0 kafka_2.10 Apache 2.0 kafka_2.11 Apache 2.0 common-cassandra Apache 2.0 timeslice_utils New Relic mockito-kotlin MIT compress-lzf Apache 2.0 checkstyle Apache 2.0 dagger-compiler Apache 2.0 dagger Apache 2.0 okhttp Apache 2.0 okio Apache 2.0 javapoet Apache 2.0 javawriter Apache 2.0 jaxb-impl CDDLv1 thrift-server Apache 2.0 finagle-core_2.10 Apache 2.0 finagle-core_2.11 Apache 2.0 finagle-http_2.10 Apache 2.0 finagle-http_2.11 Apache 2.0 jsr166e Creative Commons util-app_2.10 Apache 2.0 util-app_2.11 Apache 2.0 util-cache_2.10 Apache 2.0 util-cache_2.11 Apache 2.0 util-codec_2.10 Apache 2.0 util-codec_2.11 Apache 2.0 util-collection_2.10 Apache 2.0 util-collection_2.11 Apache 2.0 util-core_2.10 Apache 2.0 util-core_2.11 Apache 2.0 util-function_2.10 Apache 2.0 util-function_2.11 Apache 2.0 util-hashing_2.10 Apache 2.0 util-hashing_2.11 Apache 2.0 util-jvm_2.10 Apache 2.0 util-jvm_2.11 Apache 2.0 util-lint_2.10 Apache 2.0 util-lint_2.11 Apache 2.0 util-logging_2.10 Apache 2.0 util-logging_2.11 Apache 2.0 util-registry_2.10 Apache 2.0 util-registry_2.11 Apache 2.0 util-stats_2.10 Apache 2.0 util-stats_2.11 Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 HikariCP Apache 2.0 commons-beanutils Apache 2.0 commons-cli Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-io Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 commons-pool Apache 2.0 unix-socket-factory Apache 2.0 unix-socket-factory Apache 2.0 jflex BSD dom4j BSD airline Apache 2.0 metrics-annotation Apache 2.0 metrics-core Apache 2.0 metrics-healthchecks Apache 2.0 metrics-httpclient Apache 2.0 metrics-jdbi Apache 2.0 metrics-jersey2 Apache 2.0 metrics-jetty9 Apache 2.0 metrics-json Apache 2.0 metrics-jvm Apache 2.0 metrics-logback Apache 2.0 metrics-servlets Apache 2.0 dropwizard-java8-jdbi Apache 2.0 dropwizard-client Apache 2.0 dropwizard-configuration Apache 2.0 dropwizard-core Apache 2.0 dropwizard-db Apache 2.0 dropwizard-jackson Apache 2.0 dropwizard-jdbi Apache 2.0 dropwizard-jersey Apache 2.0 dropwizard-jetty Apache 2.0 dropwizard-lifecycle Apache 2.0 dropwizard-logging Apache 2.0 dropwizard-metrics Apache 2.0 dropwizard-servlets Apache 2.0 dropwizard-testing Apache 2.0 dropwizard-util Apache 2.0 dropwizard-validation Apache 2.0 netty-all Apache 2.0 netty-buffer Apache 2.0 netty-codec-http Apache 2.0 netty-codec-socks Apache 2.0 netty-codec Apache 2.0 netty-common Apache 2.0 netty-handler-proxy Apache 2.0 netty-handler Apache 2.0 netty-resolver Apache 2.0 netty-transport-native-epoll Apache 2.0 netty-transport Apache 2.0 netty Apache 2.0 ratpack-core Apache 2.0 ratpack-groovy-test Apache 2.0 ratpack-groovy Apache 2.0 ratpack-guice Apache 2.0 ratpack-test Apache 2.0 rxjava Apache 2.0 fastutil Apache 2.0 janino BSD activation Apache 2.0 javax.annotation-api CDDLv1 jsr250-api CDDLv1 javax.inject Apache 2.0 mail CDDLv1 mailapi CDDLv1 javax.servlet-api Apache 2.0 servlet-api Apache 2.0 javax.transaction-api CDDLv1 validation-api Apache 2.0 javax.websocket-api CDDLv1 javax.ws.rs-api CDDLv1 jaxb-api CDDLv1 stax-api CDDLv1 jaxen Apache 2.0 jline BSD jline BSD joda-time Apache 2.0 junit-dep CPL junit CPL junit EPLv1.0 kafka_2.10 Apache 2.0 kafka_2.9.2 Apache 2.0 apache-log4j-extras Apache 2.0 log4j Apache 2.0 mysql-connector-java GPLv2 with Classpath Exception byte-buddy Apache 2.0 byte-buddy Apache 2.0 jna-platform LGPLv2.1 jna Apache 2.0 jcip-annotations Apache 2.0 lz4 Apache 2.0 accessors-smart Apache 2.0 asm Apache 2.0 json-smart Apache 2.0 json-smart Apache 2.0 primitive GPLv2 with Classpath Exception compiler Apache 2.0 lang Apache 2.0 beaver-cc BSD ehcache-core Apache 2.0 ehcache Apache 2.0 jopt-simple MIT jopt-simple MIT opencsv Apache 2.0 quality-check Apache 2.0 scannotation Apache 2.0 super-csv Apache 2.0 uadetector-core Apache 2.0 uadetector-resources Apache 2.0 argparse4j MIT spymemcached MIT kalium Apache 2.0 ST4 BSD antlr-runtime BSD antlr-runtime BSD antlr BSD stringtemplate BSD ant-launcher Apache 2.0 ant Apache 2.0 cassandra-all Apache 2.0 cassandra-all Apache 2.0 cassandra-thrift Apache 2.0 cassandra-thrift Apache 2.0 commons-collections4 Apache 2.0 commons-compress Apache 2.0 commons-csv Apache 2.0 commons-dbcp2 Apache 2.0 commons-io Apache 2.0 commons-lang3 Apache 2.0 commons-math3 Apache 2.0 commons-math Apache 2.0 commons-pool2 Apache 2.0 curator-client Apache 2.0 curator-framework Apache 2.0 curator-recipes Apache 2.0 curator-test Apache 2.0 derby Apache 2.0 geronimo-servlet_3.0_spec Apache 2.0 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 ivy Apache 2.0 kafka-clients Apache 2.0 kafka_2.11 Apache 2.0 libthrift Apache 2.0 tomcat-embed-core Apache 2.0 tomcat-embed-el Apache 2.0 tomcat-embed-logging-juli Apache 2.0 tomcat-embed-websocket Apache 2.0 tomcat-jdbc Apache 2.0 tomcat-juli Apache 2.0 zookeeper Apache 2.0 aspectjweaver EPLv1.0 assertj-core Apache 2.0 assertj-core Apache 2.0 evo-inflector Apache 2.0 bcmail-jdk15on MIT bcpkix-jdk15on MIT bcprov-jdk15on MIT ohc-core Apache 2.0 cassandra-unit-spring LGPLv3 cassandra-unit-spring LGPLv3 cassandra-unit LGPLv3 cassandra-unit LGPLv3 groovy-all Apache 2.0 groovy-all Apache 2.0 groovy Apache 2.0 groovy Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 easymock Apache 2.0 ecj EPLv1.0 javax.activation CDDLv1 javax.annotation CDDLv1 javax.mail.glassfish CDDLv1 javax.servlet CDDLv1 javax.transaction CDDLv1 org.objectweb.asm BSD jetty-setuid-java EPLv1.0 jetty-schemas Apache 2.0 javax-websocket-client-impl Apache 2.0 javax-websocket-server-impl Apache 2.0 websocket-api Apache 2.0 websocket-client Apache 2.0 websocket-common Apache 2.0 websocket-server Apache 2.0 websocket-servlet Apache 2.0 apache-jsp Apache 2.0 jetty-annotations CDDLv1 jetty-client Apache 2.0 jetty-continuation Apache 2.0 jetty-http Apache 2.0 jetty-io Apache 2.0 jetty-jmx Apache 2.0 jetty-jndi Apache 2.0 jetty-plus Apache 2.0 jetty-proxy Apache 2.0 jetty-security Apache 2.0 jetty-server Apache 2.0 jetty-servlet Apache 2.0 jetty-servlets Apache 2.0 jetty-util Apache 2.0 jetty-webapp Apache 2.0 jetty-xml Apache 2.0 sigar Apache 2.0 aopalliance-repackaged CDDLv1 javax.inject CDDLv1 hk2-api CDDLv1 hk2-locator CDDLv1 hk2-utils CDDLv1 osgi-resource-locator CDDLv1 jersey-guava CDDLv1 jersey-client CDDLv1 jersey-apache-connector CDDLv1 jersey-container-servlet-core CDDLv1 jersey-container-servlet CDDLv1 jersey-client CDDLv1 jersey-common CDDLv1 jersey-server CDDLv1 jersey-bean-validation CDDLv1 jersey-metainf-services CDDLv1 jersey-media-jaxb CDDLv1 jersey-client CDDLv1 jersey-test-framework-provider-inmemory CDDLv1 jersey-test-framework-core CDDLv1 javax.el CDDLv1 hamcrest-all BSD hamcrest-core BSD hamcrest-integration BSD hamcrest-library BSD HdrHistogram Public Domain hector-core MIT hibernate-commons-annotations LGPLv2.1 hibernate-jpa-2.0-api LGPLv2.1 hibernate-jpa-2.1-api LGPLv2.1 hibernate-c3p0 Apache 2.0 hibernate-core Apache 2.0 hibernate-ehcache Apache 2.0 hibernate-entitymanager Apache 2.0 hibernate-jmx Apache 2.0 hibernate-validator Apache 2.0 freebuilder Apache 2.0 org.jacoco.agent EPLv1.0 org.jacoco.ant EPLv1.0 org.jacoco.core EPLv1.0 org.jacoco.report EPLv1.0 javassist Apache 2.0 jboss-logging-annotations LGPLv2.1 jboss-logging Apache 2.0 jboss-transaction-api_1.1_spec LGPLv2.1 jboss-transaction-api_1.2_spec LGPLv2.1 jandex Apache 2.0 jdbi Apache 2.0 job-dsl-core Apache 2.0 version-number MIT kotlin-annotation-processing Apache 2.0 kotlin-reflect Apache 2.0 kotlin-runtime Apache 2.0 kotlin-stdlib Apache 2.0 kotlin-test-junit Apache 2.0 kotlin-test Apache 2.0 annotations Apache 2.0 jolokia-core Apache 2.0 jruby-complete LGPLv2.1 LGPLv2.1 json Public Domain json Apache 2.0 xstream BSD annotations Apache 2.0 mapdb Apache 2.0 mapdb Apache 2.0 jbcrypt ISC mockserver-client-java Apache 2.0 mockserver-core Apache 2.0 mockserver-logging Apache 2.0 mockserver-netty Apache 2.0 mockito-all MIT mockito-core MIT mockito-core MIT apache-el Apache 2.0 apache-jsp Apache 2.0 alpn-boot Apache 2.0 etcd4j Apache 2.0 msgpack Apache 2.0 objenesis Apache 2.0 objenesis Apache 2.0 jmh-core GPLv2 with Classpath Exception jmh-generator-annprocess GPLv2 with Classpath Exception jol-core GPLv2 with Classpath Exception asm-analysis BSD asm-analysis BSD asm-commons BSD asm-debug-all BSD asm-tree BSD asm-util BSD asm-util BSD asm BSD asm Public Domain postgresql BSD powermock-api-mockito Apache 2.0 powermock-api-support Apache 2.0 powermock-core Apache 2.0 powermock-module-junit4-common Apache 2.0 powermock-module-junit4 Apache 2.0 powermock-reflect Apache 2.0 lombok MIT reactive-streams Public Domain reflections WTFPL scala-java8-compat_2.11 Scala scala-parser-combinators_2.11 Scala scala-xml_2.11 Scala scala-library Scala scala-reflect Scala scalatest_2.10 Apache 2.0 scalatest_2.11 Apache 2.0 native-lib-loader BSD jsonassert Apache 2.0 jcl-over-slf4j MIT jul-to-slf4j MIT log4j-log4j12 MIT log4j-over-slf4j MIT slf4j-api MIT slf4j-ext MIT slf4j-log4j12 MIT slf4j-simple MIT spock-core Apache 2.0 spock-core Apache 2.0 spring-boot-actuator Apache 2.0 spring-boot-autoconfigure Apache 2.0 spring-boot-configuration-processor Apache 2.0 spring-boot-starter-actuator Apache 2.0 spring-boot-starter-aop Apache 2.0 spring-boot-starter-data-jpa Apache 2.0 spring-boot-starter-data-rest Apache 2.0 spring-boot-starter-jdbc Apache 2.0 spring-boot-starter-jetty Apache 2.0 spring-boot-starter-logging Apache 2.0 spring-boot-starter-test Apache 2.0 spring-boot-starter-tomcat Apache 2.0 spring-boot-starter-web Apache 2.0 spring-boot-starter Apache 2.0 spring-boot-test-autoconfigure Apache 2.0 spring-boot-test Apache 2.0 spring-boot Apache 2.0 spring-data-commons Apache 2.0 spring-data-jpa Apache 2.0 spring-data-rest-core Apache 2.0 spring-data-rest-webmvc Apache 2.0 spring-hateoas Apache 2.0 spring-plugin-core Apache 2.0 spring-aop Apache 2.0 spring-aspects Apache 2.0 spring-beans Apache 2.0 spring-context-support Apache 2.0 spring-context Apache 2.0 spring-core Apache 2.0 spring-core Apache 2.0 spring-expression Apache 2.0 spring-jdbc Apache 2.0 spring-orm Apache 2.0 spring-test Apache 2.0 spring-test Apache 2.0 spring-tx Apache 2.0 spring-web Apache 2.0 spring-webmvc Apache 2.0 xz Public Domain wasabi Apache 2.0 snappy-java Apache 2.0 xmlunit-core Apache 2.0 xmlunit-legacy Apache 2.0 snakeyaml Apache 2.0 JUnitParams Apache 2.0 postgresql BSD postgresql BSD jedis MIT scala-library Scala ion-java Apache 2.0 timeslice_service-thrift New Relic timeslice_service New Relic xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3_min Public Domain zkclient Apache 2.0 zookeeper Apache 2.0 CoffeeScript Library License CoffeeScript MIT coffin MIT JavaScript JavaScript License ie_html5/ie_html5.js MIT Angular.js MIT Angular UI directives for Bootstrap MIT Chart.js MIT Chosen JQuery plugin MIT Fullcalendar MIT Moment MIT Crash reporting Software License Apple OS X OS X Yosemite Xcode & Apple SDK Xcode license Label services Service License BoneCP Apache 2.0 Gradle Apache 2.0 Hibernate Apache 2.0 Spring Apache 2.0 Infrastructure Monitoring services Service License antlr BSD aopalliance Public Domain asm MIT cglib Apache 2.0 ch.qos.logback EPLv1.0 com.101tec Apache 2.0 com.amazonaws Apache 2.0 com.cenqua.clover Paid com.fasterxml.jackson.core Apache 2.0 com.fasterxml Apache 2.0 com.google.code.findbugs BSD com.google.guava Apache 2.0 com.googlecode.json-simple Apache 2.0 com.intellij Apache 2.0 New Relic com.squareup.dagger Apache 2.0 com.squareup Apache 2.0 com.yammer.metrics Apache 2.0 commons-codec Apache 2.0 commons-configuration Apache 2.0 commons-lang Apache 2.0 commons-logging Apache 2.0 dom4j BSD io.netty Apache 2.0 javax.annotation CDDLv1 javax.inject Apache 2.0 javax.servlet.jsp CDDLv1 javax.servlet Apache 2.0 javax.validation Apache 2.0 javax.websocket CDDLv1 jline BSD joda-time Apache 2.0 junit CPL log4j Apache 2.0 mysql GPLv2 with Classpath Exception net.sf.jopt-simple MIT org.apache.commons Apache 2.0 org.apache.httpcomponents Apache 2.0 org.apache.ivy Apache 2.0 org.apache.thrift Apache 2.0 org.apache.tomcat.embed Apache 2.0 org.apache.zookeeper Apache 2.0 org.codehaus.groovy Apache 2.0 org.codehaus.jackson Apache 2.0 org.easymock Apache 2.0 org.eclipse.jetty.orbit CDDLv1 org.eclipse.jetty.toolchain Apache 2.0 org.eclipse.jetty.websocket Apache 2.0 org.eclipse.jetty CDDLv1 org.glassfish.jersey.core CDDLv1 org.glassfish.web CDDLv1 org.glassfish CDDLv1 org.hamcrest BSD org.hibernate Apache 2.0 org.javassist Apache 2.0 org.jboss.logging LGPLv2.1 org.jenkins-ci.plugins Apache 2.0 org.jenkins-ci Apache 2.0 org.jetbrains Apache 2.0 org.jolokia Apache 2.0 org.jvnet.hudson BSD org.mockito MIT org.objenesis Apache 2.0 org.ow2.asm MIT org.reflections WTFPL org.scala-lang Scala org.slf4j MIT org.springframework.boot Apache 2.0 org.springframework Apache 2.0 org.xerial.snappy Apache 2.0 org.yaml Apache 2.0 xml-apis Apache 2.0 xmlpull Public Domain xmlunit Apache 2.0 xpp3 Public Domain .NET support uploader service Library License Amazon AWS SDK for Java Apache 2.0 Apache Commons Codec Apache 2.0 Gson Apache 2.0 Jetty Apache 2.0 Ruby gems Ruby gems Gem License aasm MIT actionmailer MIT actionpack MIT activeadmin MIT actionview MIT activejob MIT active_model_serializers MIT activemodel MIT activerecord MIT activerecord-deprecated_finders MIT activerecord-mysql2-adapter MIT activeresource MIT activesupport MIT addressable Apache 2.0 angularjs-rails MIT arbre MIT arel MIT arel MIT atomic Apache 2.0 atomic Apache-2.0 attr_required MIT awesome_print MIT backports MIT better_errors MIT binding_of_caller MIT bourbon MIT brwsr MIT browser-timezone-rails MIT builder MIT bundler MIT byebug MIT callsite MIT capistrano MIT capistrano-bundler MIT capistrano-ext MIT capistrano-rbenv MIT capistrano-stats MIT capybara MIT celluloid MIT CFPropertyList MIT chunky_png MIT ci_reporter MIT ci_reporter_rspec MIT clockwork MIT codeclimate-test-reporter MIT coderay MIT coderay MIT coffee-rails MIT coffee-script MIT coffee-script-source MIT columnize Ruby compass MIT compass-core MIT compass-import-once MIT compass-rails MIT concurrent-ruby MIT crack MIT crypt Unlicensed daemons MIT dalli MIT database_cleaner MIT debug_inspector MIT debugger-linecache MIT diff-lcs MIT docile MIT dotenv MIT dotenv-deployment MIT erubis MIT etcd MIT ethon MIT eventmachine Ruby excon MIT execjs MIT factory_girl MIT factory_girl_rails MIT faraday MIT faraday-middleware MIT faye-websocket MIT ffi MIT ffi BSD fission MIT fog MIT fog-atmos MIT fog-aws MIT fog-brightbox MIT fog-core MIT fog-ecloud MIT fog-google MIT fog-json MIT fog-local MIT fog-powerdns MIT fog-profitbricks MIT fog-radosgw MIT fog-riakcs MIT fog-sakuracloud MIT fog-serverlove MIT fog-softlayer MIT fog-storm_on_demand MIT fog-terremark MIT fog-vmfusion MIT fog-voxel MIT fog-xml MIT font-awesome-rails MIT foreman MIT formatador MIT formtastic MIT formtastic i18n MIT fssm MIT globalid MIT guard MIT guard-rspec MIT haml MIT haml-rails MIT has scope MIT hashie MIT highline Ruby hike MIT hipchat-api MIT histogram MIT hitimes ISC hiredis BSD http_parser.rb MIT httpclient Ruby httpclient Ruby httparty MIT i18n MIT inflecto MIT inherited resources MIT intercom-rails MIT ipaddress MIT jasmine MIT jasmine-core MIT jasmine-rails MIT jbuilder MIT jira MIT jira-ruby OSL-3.0 journey MIT jquery-rails MIT jquery-turbolinks MIT jquery-ui-rails MIT json Ruby jwt MIT kaminari MIT kgio LGPLv3 kgio LGPLv3 kgio LGPL-v2.1+ libv8 MIT listen MIT logger Ruby loofah MIT lumberjack MIT macaddr MIT mail MIT mailcatcher MIT memoist MIT metaclass MIT meta_request MIT method_source MIT mime-types MIT mini_portile MIT minitest MIT minitest-rails MIT mixlib-log Apache 2.0 mocha MIT mono_logger MIT multi_json MIT multi_xml MIT multipart-post MIT mysql2 MIT net-http-persistent MIT net-scp MIT net-sftp MIT net-ssh MIT net-ssh-gateway MIT netrc MIT nokogiri MIT oauth2 MIT oj MIT omniauth MIT omniauth-oauth2 MIT pagerduty MIT papers MIT pg BSD pg Ruby phantomjs BSD phantomjs-binaries Unlicensed poltergeist MIT polyamorous MIT polyglot MIT polyglot MIT protected_attributes MIT pry MIT pry-nav MIT pry-rails MIT pry-stack_explorer MIT puma BSD quiet assets MIT rack MIT rack-cache MIT rack-contrib MIT rack-oauth2 MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-protection MIT rack-ssl MIT rack-ssl-enforcer MIT rack-test MIT rails MIT rails-deprecated_sanitizer MIT rails-dom-testing MIT rails-html-sanitizer MIT rails-observers MIT rails_12factor MIT rails_serve_static_assets MIT rails_stdout_logging MIT railties MIT raindrops LGPLv3 raindrops LGPLv3 raindrops LGPLv2.1+ rake MIT rake MIT rake MIT rake MIT ransack MIT rb-fsevent MIT rb-inotify MIT rdoc Ruby redcarpet MIT redis MIT redis-namespace MIT redis-queue MIT ref MIT remote_syslog_logger MIT request_store MIT responders MIT resque MIT resque-cleaner MIT resque-pool MIT resque-scheduler MIT resque-status MIT rest-client MIT restforce MIT rspec MIT rspec-core MIT rspec-expectations MIT rspec-mocks MIT rspec-rails MIT rspec-support MIT rspec_junit_formatter MIT rubyntlm MIT ruby-saml MIT rufus-scheduler MIT safe_yaml MIT salesforce_bulk_query BSD sass MIT sass-rails MIT sass-rails MIT sdoc MIT secure_headers Apache 2.0 sequel MIT serveza MIT settingslogic MIT shoulda MIT shoulda-context MIT shoulda-matchers MIT simplecov MIT simplecov-html MIT simplecov-rcov MIT sinatra MIT sinatra-activerecord MIT sinatra-contrib MIT skinny MIT slop MIT spring MIT sprockets MIT sprockets-rails MIT sqlite3 BSD sshkit GPL-No Distro sys-uname Ruby syslog_protocol MIT systemu BSDL term-ansicolor GPL-No Distro terminal-table MIT therubyracer MIT thin Ruby thor MIT thread_safe Apache-2.0 thrift Apache 2.0 thrift-rack MIT tilt MIT timecop MIT timers MIT tins MIT treetop MIT turbolinks MIT twitter-bootstrap-rails MIT typhoeus MIT tzinfo MIT uglifier MIT unicorn Ruby unicorn-rails MIT uuid MIT uuidtools Apache-2.0 vegas MIT webmock MIT web-console MIT xml-simple Ruby xpath MIT yard MIT Storage services Library License Amazon AWS SDK for Java Apache 2.0 Amazon AWS Command Line Interface Apache 2.0 Go packages Library License Go-Mysql-Driver Mozilla Public License 2.0 gocql BSD 3-Clause groupcache Apache 2.0 Migrate MIT go-sqlite3 MIT Revel MIT Pathtree MIT Ansicolor MIT fsnotify BSD 3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 115.60083,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Services <em>licenses</em>",
        "sections": "Services <em>licenses</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": "We love open-source software, and use the following in various New Relic services. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Contents Java internal"
      },
      "id": "603ea24364441f91fb4e8864"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-06-26T14:30:46Z",
      "updated_at": "2021-05-22T17:22:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.31245,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-06-25T22:52:16Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (PRC Use). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.42978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "Overview of limits",
        "View limits and manage data",
        "Account-level limits",
        "Data ingest APIs",
        "Other agent and integration limits",
        "Manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-06-26T14:31:42Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its sub-accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data Want to understand your accounts limit violations using the UI? See View limits. Want to manage your data for organization or billing purposes? See Manage data. Account-level limits The following table includes general max limits that apply across all New Relic accounts. Specific New Relic tools, like agents and integrations, have their own limits and configurations, and might be lower than these theoretical maximum limits. Limited condition Limit Rate of NRDB record * ingest 55 million per account per minute Max NRDB records * ingested per API call 1MB (10^6 bytes) Max attribute value size 1KB (10^3 bytes) Max payload size 1MB (10^6 bytes) Max total attributes per data type (including default attributes) 254 (less for some tools; for example, 64 for agents) Number of unique custom data types 250 per account per day (applies to custom events because that's source of new data types) APM limits Agent instances: 50K per account Agent instances per app: 10K APM apps/services: 10K per second Browser: number of page views 1M per minute per app Rate of metric timeslice data (used by APM, browser, mobile) Ingest: 2 million per minute Rate of names: 4 million per minute per account Number per monitored app: 300K Mobile monitoring: number of crashes reported 10K per hour Infrastructure agents, integrations Number of infrastructure agents and/or integrations: 5K per account Gross number of new monitored containers: 5K per hour per account Query limits NRDB records * inspected: 100 billion per account per hour Rate of queries: 20 per account per second See other query limits * NRDB records refers to database records for our core data types, which includes events, metrics (dimensional), logs, and distributed tracing (span) data, all stored in the New Relic database (NRDB). This does not include metric timeslice data. Data ingest APIs Our ingest APIs have additional limits that may override the more general account-level limits. Note that these limits also apply to our tools that use these APIs (like our Telemetry SDKs or our open source telemetry integrations). Metric API (dimensional metrics) Event API Log API Trace API Other agent and integration limits To find limits for our other agents and integrations, which will override more general account-level limits, see the docs for those tools: you can search our solutions here. Some default reporting limits are located in these tools' configuration docs. Manage data Want to manage your New Relic data ingest and storage to improve data organization or reduce billing? See Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.42737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings": [
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-06-25T22:52:16Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (PRC Use). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.42978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Acceptable use policy",
        "You will not, and not to allow third parties, in connection with your use of the New Relic Properties to:",
        "Harm New Relics Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2021-06-26T14:29:58Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relics mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (AUP). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the New Relic Properties) and is part of the New Relic documentation, which can be found at https://docs.newrelic.com (the Documentation). You will not, and not to allow third parties, in connection with your use of the New Relic Properties to: Harm New Relics Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relics or a third partys intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using botnets, robots, spiders and offline readers); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other hidden text; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including scraping or using any data mining methods); Sharing your New Relic Properties account or login credentials with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other users account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by spoofing or phishing); Collecting or harvesting any personally identifiable information, including account names, from any other users account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other peoples private and confidential information without their express permission; Using anyones name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (HIPAA); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any personal information as such term is defined under the Childrens Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, drivers license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; or (vii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R.  120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if youre a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term content means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, you may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and we means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.42737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Social Security numbers, drivers <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "Overview of limits",
        "View limits and manage data",
        "Account-level limits",
        "Data ingest APIs",
        "Other agent and integration limits",
        "Manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-06-26T14:31:42Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its sub-accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data Want to understand your accounts limit violations using the UI? See View limits. Want to manage your data for organization or billing purposes? See Manage data. Account-level limits The following table includes general max limits that apply across all New Relic accounts. Specific New Relic tools, like agents and integrations, have their own limits and configurations, and might be lower than these theoretical maximum limits. Limited condition Limit Rate of NRDB record * ingest 55 million per account per minute Max NRDB records * ingested per API call 1MB (10^6 bytes) Max attribute value size 1KB (10^3 bytes) Max payload size 1MB (10^6 bytes) Max total attributes per data type (including default attributes) 254 (less for some tools; for example, 64 for agents) Number of unique custom data types 250 per account per day (applies to custom events because that's source of new data types) APM limits Agent instances: 50K per account Agent instances per app: 10K APM apps/services: 10K per second Browser: number of page views 1M per minute per app Rate of metric timeslice data (used by APM, browser, mobile) Ingest: 2 million per minute Rate of names: 4 million per minute per account Number per monitored app: 300K Mobile monitoring: number of crashes reported 10K per hour Infrastructure agents, integrations Number of infrastructure agents and/or integrations: 5K per account Gross number of new monitored containers: 5K per hour per account Query limits NRDB records * inspected: 100 billion per account per hour Rate of queries: 20 per account per second See other query limits * NRDB records refers to database records for our core data types, which includes events, metrics (dimensional), logs, and distributed tracing (span) data, all stored in the New Relic database (NRDB). This does not include metric timeslice data. Data ingest APIs Our ingest APIs have additional limits that may override the more general account-level limits. Note that these limits also apply to our tools that use these APIs (like our Telemetry SDKs or our open source telemetry integrations). Metric API (dimensional metrics) Event API Log API Trace API Other agent and integration limits To find limits for our other agents and integrations, which will override more general account-level limits, see the docs for those tools: you can search our solutions here. Some default reporting limits are located in these tools' configuration docs. Manage data Want to manage your New Relic data ingest and storage to improve data organization or reduce billing? See Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.42737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/government-addendum": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-06-26T14:30:46Z",
      "updated_at": "2021-05-22T17:22:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.31244,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-06-25T22:52:16Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (PRC Use). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.42978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Acceptable use policy",
        "You will not, and not to allow third parties, in connection with your use of the New Relic Properties to:",
        "Harm New Relics Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2021-06-26T14:29:58Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relics mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (AUP). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the New Relic Properties) and is part of the New Relic documentation, which can be found at https://docs.newrelic.com (the Documentation). You will not, and not to allow third parties, in connection with your use of the New Relic Properties to: Harm New Relics Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relics or a third partys intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using botnets, robots, spiders and offline readers); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other hidden text; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including scraping or using any data mining methods); Sharing your New Relic Properties account or login credentials with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other users account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by spoofing or phishing); Collecting or harvesting any personally identifiable information, including account names, from any other users account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other peoples private and confidential information without their express permission; Using anyones name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (HIPAA); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any personal information as such term is defined under the Childrens Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, drivers license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; or (vii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R.  120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if youre a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term content means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, you may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and we means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.42737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Social Security numbers, drivers <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-06-26T14:30:46Z",
      "updated_at": "2021-05-22T17:22:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.31244,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-06-25T22:52:16Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (PRC Use). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.42978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Acceptable use policy",
        "You will not, and not to allow third parties, in connection with your use of the New Relic Properties to:",
        "Harm New Relics Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2021-06-26T14:29:58Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relics mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (AUP). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the New Relic Properties) and is part of the New Relic documentation, which can be found at https://docs.newrelic.com (the Documentation). You will not, and not to allow third parties, in connection with your use of the New Relic Properties to: Harm New Relics Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relics or a third partys intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using botnets, robots, spiders and offline readers); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other hidden text; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including scraping or using any data mining methods); Sharing your New Relic Properties account or login credentials with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other users account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by spoofing or phishing); Collecting or harvesting any personally identifiable information, including account names, from any other users account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other peoples private and confidential information without their express permission; Using anyones name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (HIPAA); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any personal information as such term is defined under the Childrens Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, drivers license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; or (vii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R.  120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if youre a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term content means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, you may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and we means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.42737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Social Security numbers, drivers <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/new-relics-provision-services": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-06-26T14:30:46Z",
      "updated_at": "2021-05-22T17:22:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.31244,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "The People's Republic of China"
      ],
      "title": "The People's Republic of China",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "c3c078b8139d695b928d2001cd6c6c9318c43599",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/peoples-republic-china/",
      "published_at": "2021-06-25T22:52:16Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important Information Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory requirements in the PRC. New Relic does not provide support for, including but not limited to, the deployment, access, or use of the Service(s) or Software in the PRC, or otherwise in respect of assets in the PRC (PRC Use). Without overriding any express prohibitions that you may have agreed to as part of your agreement with New Relic, you (and you as an agent of the respective New Relic Customer (\"Customer\")) acknowledge that any PRC use is subject to the PRC national firewall system and may be subject to outages and other interference outside of the control of New Relic. Accordingly, you acknowledge and agree (on behalf of yourself and including any Customer on whose behalf you use the Service(s) or Software) that any PRC Use is at your sole risk and is fully excepted from all representations and warranties, including, but not limited to, any terms that assure the confidentiality, integrity, availability or privacy of your data. New Relic makes no other representations and hereby expressly disclaims any and all warranties in respect of PRC Use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.42978,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "Important <em>Information</em> Regarding the Peoples Republic of China and the Use of New Relic New Relic is not authorized to do business in the Peoples Republic of China (PRC). The Service(s) and Software are not designed, tested, or certified for compliance with any operational or regulatory"
      },
      "id": "603eb3a328ccbcb488eba76b"
    },
    {
      "sections": [
        "Acceptable use policy",
        "You will not, and not to allow third parties, in connection with your use of the New Relic Properties to:",
        "Harm New Relics Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2021-06-26T14:29:58Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relics mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (AUP). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the New Relic Properties) and is part of the New Relic documentation, which can be found at https://docs.newrelic.com (the Documentation). You will not, and not to allow third parties, in connection with your use of the New Relic Properties to: Harm New Relics Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relics or a third partys intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using botnets, robots, spiders and offline readers); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other hidden text; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including scraping or using any data mining methods); Sharing your New Relic Properties account or login credentials with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other users account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by spoofing or phishing); Collecting or harvesting any personally identifiable information, including account names, from any other users account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other peoples private and confidential information without their express permission; Using anyones name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (HIPAA); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any personal information as such term is defined under the Childrens Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, drivers license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; or (vii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R.  120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if youre a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term content means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, you may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and we means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.42737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Social Security numbers, drivers <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    }
  ],
  "/docs/licenses/license-information/general-usage-licenses/peoples-republic-china": [
    {
      "sections": [
        "Global Technical Support offerings",
        "Support plans",
        "Support plan for New Relic One pricing and packaging model",
        "Original New Relic support plan",
        "Support resources",
        "Support channels",
        "Community forum",
        "Github",
        "Diagnostic tools",
        "Support ticket",
        "Scope of support",
        "Support includes",
        "Support does not include",
        "Unsupported or incompatible environments & frameworks",
        "Software customizations",
        "Custom applications",
        "Custom scripts & queries",
        "End of Life",
        "Beta or Limited Release",
        "Troubleshooting of customer environment",
        "Troubleshooting third-party tools & services",
        "Some account-related functions",
        "Product training",
        "Consultancy services",
        "Open source support",
        "Open source project categories",
        "Open source support includes",
        "Open source support does not include",
        "Support videos",
        "For more help"
      ],
      "title": "Global Technical Support offerings",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b988cdcfb8ae304e36bdd3195f1afdb0092bbc32",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/global-technical-support-offerings/",
      "published_at": "2021-06-26T14:30:46Z",
      "updated_at": "2021-05-22T17:22:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Support Plan offers a variety of resources based on your service subscription. Check out the Support Plan information, resources, channels, and scope of support below. Support plans These Support Plans apply only to your paid service subscription under an existing New Relic agreement. If you have questions about these New Relic Support Plans, contact your New Relic account representative. Support plan for New Relic One pricing and packaging model The below New Relic One Support Plan applies only to a customers paid subscription to New Relic One (Full Stack Observability). Standard Pro Enterprise BENEFITS Explorers Hub Community Documentation Support Portal Access @ support.newrelic.com Communication Method Community Forum Community Forum, Ticket, Chat Community Forum, Ticket, Chat, Phone, Slack Support Hours 24x7/365 24x7/365 Initial Support Response SLA 2 hours critical, 8 hours standard 1 hour critical, 3 hours standard On-Boarding On-demand video Training Webinar/Virtual Training Designated Technical Account Manager Designated Support Customer Experience Manager Priority Ticket Routing Critical Date/Event Support Support Escalation 1-Click Away Notes: If you have not upgraded or changed to the New Relic One pricing plan, your existing support plan still applies. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Original New Relic support plan The below Support Plan applies only to a customers paid service subscription to non-New Relic One Products (our original product-based pricing plan). Silver Gold Platinum Priority Essential Plus BENEFITS Documentation New Relic University Support Portal Access at support.newrelic.com Explorers Hub Community Communication Methods Explorers Hub Explorers Hub, Ticket Explorers Hub, Ticket, Phone Explorers Hub, Ticket, Phone, Slack Explorers Hub, Ticket, Phone, Slack Support Hours 24/7x365 24/7x365 24/7x365 24/7x365 Initial Support Response Time 2 hours critical, 8 hours standard 2 hours critical, 4 hours standard 1 hours critical, 3 hours standard 1 hours critical, 3 hours standard Priority Ticket Routing Designated Support Customer Manager Expert Services Support Solutions Architect NRU Instructor Led Training Quarterly Health Check, Office Hours Notes: Silver tier applies to customers with $1 to $9,999 annual spend. Gold tier applies to customers with $10,000 to $99,999 annual spend. Platinum tier applies to customers with $100,000 annual spend and above. Contact your Account Manger regarding Priority Support. Initial Support Response Time begins when the request is received by the New Relic support system. Critical means customers business operations are severely impacted due to New Relic with no available workaround; or there is a critical security issue. This Support Plan is subject to change at any time; changes will take immediate effect. Support resources We're here to help you get everything you need from the New Relic One Platform. To begin with, we recommend that all New Relic users become familiar with these resources: New Relic Status Page: Get updates on any incidents New Relic Documentation: Comprehensive guidance for using our platform New Relic Community forum: Thousands of customer questions asked and answered New Relic Diagnostics: Diagnose and troubleshoot installation and configuration New Relic Open Source: Discover, research, and contribute to our open source projects New Relic Security Overview: Our approach to handling security issues You may find these resources helpful too: New Relic Developers: Resources for building custom observability applications New Relic University: A range of online training for New Relic users of every level New Relic on GitHub: Discuss issues and features related to our Open Source projects We are committed to providing documentation and tools to assist with installation, configuration, and diagnostics of New Relics distributed software as described here: New Relic Installation, configuration, and requirements Support channels If you need assistance with New Relic Products, you are in good hands with several support channels available to you depending on the service level associated with your New Relic account. For more information about service levels, please refer to our Support Plan. Community forum The New Relic Community Forum is 100% free and open to anyone with a New Relic account. The community is a place where many customer questions have already been asked and answered. Answers come from our community of experienced users, New Relic Support Engineers, and dozens of other Relics who help answer questions and solve problems. If you want to ask a question, check the community - if your question has not already been answered, members of the community can help. Github We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. You can find all our open source projects in our Github repo. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. For more information on support for Open Source projects, visit our Open Source Support Policy. Diagnostic tools New Relic offers a diagnostics utility that can automatically detect common problems with New Relic agents. If Diagnostics detects a problem, it suggests troubleshooting steps. New Relic Diagnostics can also automatically attach troubleshooting data to a New Relic Support ticket. We have also made available Troubleshooting Frameworks that step users through common troubleshooting questions. Support ticket Support is now available in the New Relic One Platform! Just click on the question mark at the top right of your New Relic One screen to surface contextual documentation and resources. Depending on the Service Level associated with your New Relic account, you may be eligible for ticketed support and can open a ticket without leaving the New Relic One Platform. As an alternative, customers eligible for ticketed support may also open a support ticket from the New Relic Support page We are available 24 hours a day, 7 days a week, 365 days a year to help you troubleshoot issues related to the New Relic One Platform and generally available New Relic Products as outlined below. Scope of support You can have confidence that the Products we make Generally Available are fully tested with the compatible environments outlined in New Relic Documentation. New Relics Global Technical Support provides assistance with the New Relic One Platform, and the features and capabilities inherent in the Telemetry Data Platform, Full Stack Observability, and Applied Intelligence Product lines. For issues within Third Party tools, or when tools in your infrastructure aren't working together properly, Global Technical Support may reach a point where we must refer New Relic users to such Third Party or community for assistance. Support includes Troubleshooting problems on the New Relic One Platform Assistance with issues during installation & upgrade in compatible environments Guidance on implementation and configuration in compatible environments Troubleshooting problems with ingesting data into New Relic General usage and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments & frameworks Our Products are fully tested with the compatible environments and installation frameworks, and were here to help you through issues that may arise with our Products within these compatible environments and frameworks. We cannot support installation or configuration of our Products in environments or frameworks that do not meet established compatibility requirements. But if you're looking for help customizing New Relic for your particular environment, New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Software customizations We are eager to help troubleshoot issues with the Products and features we make generally available, and those categorized as New Relics Open Source Community Plus Projects. New Relics Global Technical Support does not support customizations, modifications, or extensions to our code. Customizations or extensions to New Relics Open Source Projects in other Project categories are supported by the developer community in GitHub. New Relics Expert Services is a team of highly skilled consultants that can assist with unique configurations or environments. Custom applications With New Relic One, users have the ability to extend beyond the curated dashboards and design custom applications tailored to your business. New Relics Global Technical Support team does not support custom applications. The New Relic Developer site provides guidance on building custom apps, and here are a growing number of open source apps that you can use to get started. Custom scripts & queries We are happy to help troubleshoot issues related to the New Relic One platform that may be causing issues with a script or query. We cannot provide solutions for specific script or query use cases. New Relic Documentation and New Relic University offer resources on how to construct custom scripts and queries. End of Life New Relic may EOL products in accordance with the EOL policy. We recommend upgrading to our newest versions so you can take advantage of recent capabilities and bug fixes. More details are available in our published End of Life Policy. Beta or Limited Release Our support team covers generally available New Relic Products. Products that are in Beta or Limited Release status are not considered generally available. If you are invited to participate in a Beta program, or are using a Limited Release component, your account team will be your point of contact for questions. Please contact your account representative directly. Troubleshooting of customer environment We want to help every customer get the most of their New Relic experience within what are increasingly complex environments. However, we cant help with things we didnt build. We cannot assist with administration, configuration, or troubleshooting of a customer environment. When in doubt, you can get in touch with us, and well help verify whether an issue is with our Product within a supported environment so you know where to go next. Troubleshooting third-party tools & services New Relic integrates well with many Third Party tools and services; however, we cannot support tools and services not provided or licensed by New Relic. Well do our best to determine whether an issue is with New Relics Products or caused by something outside of our control and purview. Issues with installation or configuration of the Third Party tools and services themselves should be directed to the respective owner of that Third Party tool or service or to the developer community. The Community and GitHub are great resources for assistance with Third Party tools and services as well. Some account-related functions For security reasons, some account-related Product functions must be conducted by the New Relic user designated as the account owner, such as Enabling SSO and High-Security Mode, adding users, and upgrading user permissions. Product training We are here to help you solve problems you may encounter on the road to instrumenting everything. Global Technical Support cannot provide user training on New Relic Products. New Relic offers a well-curated library of documentation and in-depth tutorials organized by Product, skill level, learning format, and solutions to help you navigate the observability journey. Check out New Relic University! Consultancy services Global Technical Support is here to help our valued customers as outlined in these support offerings. If you need help with something that falls outside of the Scope of Support, New Relics Expert Services is a team of highly skilled consultants that can help you navigate the challenges of building modern software and adopting the latest technologies, so you can focus on what you do best: delivering an incredible experience to your customers. Open source support We want everyone to monitor their systems, and we're contributing our technology back to the open-source community to make that happen. We're committed to open standards, open-sourcing all of our instrumentation, and engaging engineers where they are, in the communities they already belong to. Open source project categories New Relic Open Source Projects are assigned to one of five different categories. These categories determine the support options available for a project as listed below: Community plus projects: Actively maintained by New Relic. Support requests can be made through Github, Community, and Ticketed Support channels, depending on the service level associated with the New Relic account. Community projects: Actively maintained by New Relic. Support requests can be made through Github or Community. New Relic One catalog: Support requests can be made through the Github channel. Issues/Pull Requests should be directed to the relevant Github repository. Example code: Project support is through Github channel. Issues/Pull Requests should be directed to the relevant Github repository. New Relic experimental: Projects have no ongoing maintenance, development or support. Archived: Projects are read-only, are not actively maintained, and do not have support. Open source support includes Support for Community Plus Projects from New Relics Global Technical Support includes: Troubleshooting problems with the Community Plus Projects on the New Relic One Platform Assistance with issues with Community Plus Projects during installation & upgrade in compatible environments Guidance on implementation and configuration of Community Plus Projects in compatible environments Troubleshooting problems with ingesting data with Community Plus Projects into New Relic General usage and best practice guidelines with Community Plus Projects Identifying bugs in Community Plus Projects Assistance in English or Japanese Only (Japanese customer Terms of Service) Open source support does not include Open source projects assigned to categories other than the Community Plus category Unsupported environments & frameworks Code development End of Life Beta or Limited Release Troubleshooting of customer environment Troubleshooting third-party tools and services Product training Consultancy services Support videos For a library of additional videos, webinars, and other information about using New Relic features, visit New Relic University and newrelic.com/resources. For more help Recommendations for learning more: See the Docs site's landing page for Licenses documentation. Browse New Relic's Explorers Hub for community discussions about New Relic licenses.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 229.31244,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Troubleshooting problems with ingesting data into New Relic <em>General</em> <em>usage</em> and best practice guidelines Identifying bugs with New Relic Products Assistance in English or Japanese Only (Japanese customer Terms of Service) Support does not include Unsupported or incompatible environments &amp; frameworks Our Products"
      },
      "id": "603ea419e7b9d27b942a07b4"
    },
    {
      "sections": [
        "Acceptable use policy",
        "You will not, and not to allow third parties, in connection with your use of the New Relic Properties to:",
        "Harm New Relics Properties and interests, such as:",
        "Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as:",
        "Violate any applicable law or regulation or for high-risk purposes, such as:",
        "New Relic Properties do not include Third-Party Services",
        "Updates, Contact Information and Violations"
      ],
      "title": "Acceptable use policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "b006ab295dae6522e8c76fcd47b3a0d4a45938e4",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/acceptable-use-policy/",
      "published_at": "2021-06-26T14:29:58Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relics mission is to instrument, measure and improve the internet to help our customers create more perfect software, experiences and businesses. We strive to keep our resources operating efficiently, so our services are available to all subscribers. Because you have access to shared resources, we have put these rules in place to ensure everyone has a great experience. For example, you as a tenant would not want other tenants to engage in the types of activities described below. To help us do this, we have put some rules in place regarding your use of the New Relic Properties and created this Acceptable Use Policy (AUP). This AUP applies if you use any New Relic product, service, software, website, forum, page or system (collectively, the New Relic Properties) and is part of the New Relic documentation, which can be found at https://docs.newrelic.com (the Documentation). You will not, and not to allow third parties, in connection with your use of the New Relic Properties to: Harm New Relics Properties and interests, such as: Uploading, transmitting or otherwise provide content that infringes New Relics or a third partys intellectual property, privacy or other rights, violates applicable laws or regulations or contains viruses, worms, harmful code, malware or other harmful materials; Hosting, selling, reselling, renting, exploiting, sublicensing, leasing, or otherwise providing the New Relic Properties or any portion thereof or use such for time sharing purposes or on a service bureau basis without our express written permission; Modifying, disabling, or compromising the integrity or performance of the New Relic Properties or related systems, networks, or data; including by: Attempting to compromise the integrity of the New Relic Properties, including probing, scanning or testing the vulnerability of any part of the New Relic Properties without proper authorization; Overwhelming our infrastructure (such as by using botnets, robots, spiders and offline readers); Going beyond the use parameters for any given service as described in the corresponding Documentation; Using metatags or other hidden text; Drastically exceeding your contracted rate of use as set forth in your order or the Documentation; or Consuming an unreasonable amount of storage. Accessing any unauthorized part of the New Relic Properties, or accessing or searching any part of the New Relic Properties by means other than those provided or authorized by New Relic (including scraping or using any data mining methods); Sharing your New Relic Properties account or login credentials with any other individual; Deciphering or decrypting transmissions, circumventing any access, authentication or copy restrictions, or otherwise attempting to compromise the security of the New Relic Properties (including any other users account); Accessing the New Relic Properties in order to build a similar or competitive website, application or service; or Attempting to do anything else that may result in some form of adverse impact to the New Relic Properties or use of the New Relic Properties by any of our other customers. Harass others or engage in activity that is unlawful, invasive, infringing, defamatory, fraudulent or violates anyone's legal rights, such as: Posting or transmitting abusive messages, defamatory, libelous, false or misleading statements, hate speech or messages that incite or threaten violence, or stalk or harass others; Promoting, encouraging, or facilitating hate speech, violence, discrimination based on race, color, sexual orientation, marital status, gender or identity expression, parental status, religion or creed, national origin or ancestry, sex, age, physical or mental disability, veteran status, genetic information, citizenship and/or any other characteristic protected by law. You are not permitted to use New Relic Properties if you are an entity identified by nationally-recognized non-profits as engaging in such activities. Attempting to modify or gain unauthorized use of or access to, another user's account, website, application, system, equipment or data; Misrepresenting yourself, impersonating another person, falsely implying any sponsorship or association with New Relic or affiliation with any third party, engaging in fraud, hiding or attempt to hide your identity or disguising the origin of any content (including by spoofing or phishing); Collecting or harvesting any personally identifiable information, including account names, from any other users account or the New Relic Properties, or using the New Relic Properties to violate the privacy of others; Including, publishing or posting other peoples private and confidential information without their express permission; Using anyones name or trademarks without their express written permission; Using the New Relic Properties to generate or send unsolicited communications, advertising, chain letters, or spam; Soliciting our users for commercial purposes, unless expressly permitted in writing by New Relic; Disparaging anyone; or Disclosing any confidential information obtained through any method contrary to this AUP. Violate any applicable law or regulation or for high-risk purposes, such as: Using the New Relic Properties in violation of any applicable law or regulation, including data, privacy, and export control laws in applicable jurisdictions; Using the New Relic Properties in any situation for which they are not designed, manufactured or intended, such as for use in life support, emergency or mission critical circumstances, or in any activities where use or failure of the New Relic Properties could lead to death, personal injury or property or environmental damage. For example, you may not use, or permit any other person to use, the New Relic Properties in connection with aircraft or other modes of human mass transportation or nuclear or chemical facilities, life support systems, implantable medical equipment, motor vehicles, or weaponry systems; or Processing or submitting any personal data that could be legally considered sensitive in any applicable jurisdiction, including, but not limited to: (i) patient, medical, or other protected health information regulated by the Health Insurance Portability and Accountability Act (as amended and supplemented) (HIPAA); (ii) personal data about individuals under the age of 16, which for the avoidance of doubt includes any personal information as such term is defined under the Childrens Online Privacy Protection Act; (iii) government issued identification numbers, including Social Security numbers, drivers license numbers and other state-issued identification numbers; (iv) financial account information, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account, including the combination of a username or email address along with a password or security question and answer that would permit access to an online account; (vi) special categories of sensitive personal data, (such as defined under Regulation (EU) 2016/679 of the European Parliament), including personal data revealing racial or ethnic origin, political opinions, religious beliefs, trade union membership, physical or mental health or condition, sexual life, sexual orientation, genetic data, biometric data, or the commission or alleged commission any crime or offense; or (vii) any data similar to the above protected under foreign or domestic laws, including without limitation any data subject to regulation under the International Traffic in Arms Regulations (ITAR), 22 C.F.R.  120-130. You represent and warrant to New Relic that you have all necessary rights, consents, and permissions to use and submit data that you send to the New Relic Properties, all without violating or infringing any applicable laws, third-party rights (including intellectual property, publicity, or privacy rights), or any terms or policies governing such data. New Relic Properties do not include Third-Party Services If you choose to use any Third Party Services, your use of Third-Party Services is wholly subject to your separate agreement with the relevant provider. New Relic bears no responsibility or liability for Third-Party Services. If you enable a Third-Party Service with the New Relic Properties, New Relic may access and exchange Customer Data with the Third-Party Service on your behalf and instruction. Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a User integrates or enables for use with the Service, including third-party applications and plug-ins. Open source software that New Relic makes separately available for download (e.g. community tools) is, as required, governed by the terms of the applicable open source license. The license for any open source software identified as included in New Relic Properties will, as required, wholly apply to your use of that open source software. Updates, Contact Information and Violations We will occasionally need to modify this AUP to help us continue to provide you with a great experience while using the New Relic Properties. In the event we modify this AUP, we will do so by posting a revised version, and any changes will be effective immediately if youre a new user of the New Relic Properties and thirty (30) days after posting for all other users. If you continue using the New Relic Properties after we update this AUP, you agree to the latest version of this AUP. You can report a violation of this AUP to: AUP@newrelic.com. Or by mail at: Attn: Legal New Relic, Inc. 188 Spear Street, Suite 1200 San Francisco, CA 94105 This AUP, and our customers compliance with it, is essential for enabling us to provide you and our other customers with the New Relic Properties, which we take very seriously. You are wholly and solely responsible for appropriate configuration of systems and software that you own or can control to ensure your compliance with this AUP. So, if we determine in our sole discretion that you have violated this AUP, we may, without limiting any other remedies available to us, permanently or temporarily suspend, limit, or terminate your access to the New Relic Properties without notice or liability. This right applies even if the breach is unintentional or unauthorized if we believe that any such suspension, limitation, or termination is necessary to ensure compliance with laws, or to protect the rights, safety, privacy, security, or property of us or others. In this AUP, the term content means: (1) any information, data, text, software, code, scripts, music, sound, photos, graphics, videos, messages, tags, interactive features, or other materials that you post, upload, share, submit, or otherwise provide in any manner to the services and (2) any other materials, content, or data you provide to New Relic or use with the New Relic Properties. As used in this AUP, you may refer to an individual user or the legal entity an individual user is employed by that has contracted with New Relic, and we means New Relic.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.42737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Updates, Contact <em>Information</em> and Violations",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": " Social Security numbers, drivers <em>license</em> numbers and other state-issued identification numbers; (iv) financial account <em>information</em>, including financial account numbers or payment card data (including credit card or debit card numbers); (v) credentials granting access to an online account"
      },
      "id": "603e93bf28ccbc99f3eba7bc"
    },
    {
      "sections": [
        "New Relic data usage limits and policies",
        "Overview of limits",
        "View limits and manage data",
        "Account-level limits",
        "Data ingest APIs",
        "Other agent and integration limits",
        "Manage data"
      ],
      "title": "New Relic data usage limits and policies",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "General usage licenses"
      ],
      "external_id": "fc32c25b40a030ffa0fad6bfc95be7fca1360ee1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/general-usage-licenses/new-relic-data-usage-limits-policies/",
      "published_at": "2021-06-26T14:31:42Z",
      "updated_at": "2021-03-16T04:21:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data usage spikes in one New Relic account from impacting other customers' accounts, we have various data volume and rate limits in place. We reserve the right to enforce these limits to protect our system and to avoid issues for you and other customers. If your New Relic account, whether by configuration or by error, exceeds one of these limits, it or its sub-accounts might experience one or both of the following: Sampling of data Temporary pause or cessation of data collection To learn more about how hitting a limit can affect your data, see View limits. If you have further questions about these limits, your contract, or a limit you've reached, contact your New Relic account representative. We can work with you to adjust any rate limits to meet your needs. View limits and manage data Want to understand your accounts limit violations using the UI? See View limits. Want to manage your data for organization or billing purposes? See Manage data. Account-level limits The following table includes general max limits that apply across all New Relic accounts. Specific New Relic tools, like agents and integrations, have their own limits and configurations, and might be lower than these theoretical maximum limits. Limited condition Limit Rate of NRDB record * ingest 55 million per account per minute Max NRDB records * ingested per API call 1MB (10^6 bytes) Max attribute value size 1KB (10^3 bytes) Max payload size 1MB (10^6 bytes) Max total attributes per data type (including default attributes) 254 (less for some tools; for example, 64 for agents) Number of unique custom data types 250 per account per day (applies to custom events because that's source of new data types) APM limits Agent instances: 50K per account Agent instances per app: 10K APM apps/services: 10K per second Browser: number of page views 1M per minute per app Rate of metric timeslice data (used by APM, browser, mobile) Ingest: 2 million per minute Rate of names: 4 million per minute per account Number per monitored app: 300K Mobile monitoring: number of crashes reported 10K per hour Infrastructure agents, integrations Number of infrastructure agents and/or integrations: 5K per account Gross number of new monitored containers: 5K per hour per account Query limits NRDB records * inspected: 100 billion per account per hour Rate of queries: 20 per account per second See other query limits * NRDB records refers to database records for our core data types, which includes events, metrics (dimensional), logs, and distributed tracing (span) data, all stored in the New Relic database (NRDB). This does not include metric timeslice data. Data ingest APIs Our ingest APIs have additional limits that may override the more general account-level limits. Note that these limits also apply to our tools that use these APIs (like our Telemetry SDKs or our open source telemetry integrations). Metric API (dimensional metrics) Event API Log API Trace API Other agent and integration limits To find limits for our other agents and integrations, which will override more general account-level limits, see the docs for those tools: you can search our solutions here. Some default reporting limits are located in these tools' configuration docs. Manage data Want to manage your New Relic data ingest and storage to improve data organization or reduce billing? See Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.42737,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic data <em>usage</em> limits and policies",
        "sections": "New Relic data <em>usage</em> limits and policies",
        "tags": "<em>General</em> <em>usage</em> <em>licenses</em>",
        "body": "This document lists some important account-level limits and links to other limit-related docs. Overview of limits We strive to keep our resources operating efficiently so that our services are available to all our users. To prevent data <em>usage</em> spikes in one New Relic account from impacting other"
      },
      "id": "603eb1c528ccbc0311eba7c7"
    }
  ],
  "/docs/licenses/license-information/other-licenses/services-licenses": [
    {
      "sections": [
        "Add-on end user license agreement",
        "New Relic, Inc. Add-on End User License Agreement"
      ],
      "title": "Add-on end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "a01c225ca30f95dab7db856cd946c76de557c31f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/add-end-user-license-agreement/",
      "published_at": "2021-06-25T22:51:29Z",
      "updated_at": "2021-03-16T06:19:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic, Inc. Add-on End User License Agreement In connection with the work provided by New Relic's Expert Services, Field Instrumentation Team, or Sales team, you may be provided with certain custom-created software to visualize, enable, optimize, or enhance your use of New Relic's Services. By downloading, installing, authorizing installation, or using the Add-on (defined below) with your Underlying Software (defined below), you (Customer) agree to the terms and conditions herein (Agreement) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (New Relic\"), (collectively the Parties). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the Terms of Service) available here: newrelic.com/termsandconditions/terms. IF YOU DO NOT AGREE TO THIS AGREEMENT, YOUR SOLE REMEDY IS TO NOT USE THE ADD-ON. 1. DEFINITIONS Add-on means the New Relic applications and/or software, including but not limited to connectors, extensions, UI extension, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (Underlying Software). For the purposes of the Terms of Service, the Add-on shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE PROGRAMMABILITY ADD-ON 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the Add-on solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the Add-on and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the Add-on in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (Feedback) regarding the Add-on, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the Add-on is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that Add-on will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an Upgrade Event). Customer shall hold New Relic harmless from any claims or damages arising from Customers Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the Add-on except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the Add-on; (iii) decompile, disassemble or reverse engineer any software underlying the Add-on; (iv) use the Add-on to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other partys use and enjoyment of the Services; (v) use the Add-on to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the Add-on; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend Customer usage of the Services, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time on the New Relic web site at docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential Information means: (a) the Programmability Add-on, and any features, results or output produced by, and other information relating to the Add-on (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. THE ADD-ON IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. THE ADD-ON IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE, OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE ADD-ON THEREFROM. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE ADD-ON OR FOR ANY ERROR OR DEFECT IN THE ADD-ON OR THE SERVICES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELICS AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND CUSTOMER. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the Add-on is not an official product and has not been commercially released for sale by New Relic; (b) the Add-on may not operate properly, being in final form, or fully functional; (c) the Add-on may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the Add-on fully functional; (e) the information obtained using the Add-on may not be accurate; (f) use of the Add-on may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the Add-on; and (h) New Relic has the right unilaterally to abandon development of the Add-on, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the Documentation). This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the Add-on. In the absence of a separate agreement, upon agreement between New Relic and Customer with respect to the Add-On, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such Add-On. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the Add-on only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.7909,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Add-on end user <em>license</em> agreement",
        "sections": "Add-on end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": "&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential <em>Information</em> means: (a) the Programmability Add-on, and any features, results or output produced by, and other <em>information</em> relating to the Add-on (including, without limitation"
      },
      "id": "603ec23328ccbccf1beba79a"
    },
    {
      "sections": [
        "FIT instrumentation end user license agreement"
      ],
      "title": "FIT instrumentation end user license agreement",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "e8e10baf120678407d08c9f78ab708d271cf0223",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/fit-instrumentation-end-user-license-agreement/",
      "published_at": "2021-06-26T04:34:18Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "NEW RELIC, INC. FIT INSTRUMENTATION END USER LICENSE AGREEMENT In connection with the work provided by New Relics Field Instrumentation Team, you may be provided with certain custom-created software to enable, optimize, or enhance your use of New Relics Services. By downloading, installing, authorizing installation, or using the FIT Instrumentation with your Underlying Software (defined below), you (Customer) agree to the terms and conditions herein (Agreement) with New Relic, Inc., a Delaware corporation with offices located at 188 Spear Street, Suite 1200 San Francisco, CA 94105 (New Relic), (collectively the Parties). Capitalized terms not defined herein shall have the meanings set forth in the New Relic Terms of Service (the Terms of Service) available here: http://newrelic.com/terms IF YOU DO NOT AGREE TO THIS AGREEMENT, PLEASE DO NOT USE THE FIT INSTRUMENTATION. 1. DEFINITIONS FIT Instrumentation means the New Relic custom-made software, including but not limited to connectors, extensions, and plugins, provided by New Relic to Customer to enable the Services to operate with certain third party or Customer software or systems (Underlying Software). For the purposes of the Terms of Service, the FIT Instrumentation shall be treated like an Agent, subject to the separate terms herein. 2. USE OF THE FIT INSTRUMENTATION 2.1 Software Evaluation License. Subject to the terms herein, New Relic grants to Customer a limited, non-exclusive, non-transferrable, non-sublicensable right to install, use, and configure the FIT Instrumentation solely as needed to enable the Services for internal evaluation purposes. New Relic reserves all rights and licenses not expressly granted herein. 2.2 Ownership. As between the Parties, New Relic owns all right, title, and interest to the FIT Instrumentation and Feedback, including but not limited to any intellectual property and proprietary rights therein. Customer retains all right, title, and interest in any Customer Data processed by the FIT Instrumentation in connection with the Services. 2.3 Feedback. Customer agrees to provide feedback, suggestions, ideas, requests or recommendations (Feedback) regarding the FIT Instrumentation, and hereby irrevocably assigns all intellectual property and proprietary rights it holds in the Feedback to New Relic. 2.4 Notice. Customer acknowledges and agrees that the FIT Instrumentation is made to work with the Underlying Software as configured at the time of creation and there is no guarantee that FIT Instrumentation will continue to work in the event Customer changes, replaces, upgrades versions of, updates, or otherwise changes the Underlying Software (an Upgrade Event). Customer shall hold New Relic harmless from any claims or damages arising from Customers Upgrade Event. 2.5 Restrictions. Customer will not: (i) use the FIT Instrumentation except as permitted hereunder; (ii) distribute, sell, sublicense, or otherwise transfer the FIT Instrumentation; (iii) decompile, disassemble or reverse engineer any software underlying the FIT Instrumentation; (iv) use the FIT Instrumentation to damage, disable, overburden or impair any New Relic server or network(s) connected to any New Relic server or interfere with any other partys use and enjoyment of the Services; (v) use the FIT Instrumentation to defraud, defame, abuse, harass, stalk, threaten or infringe the rights of privacy or other intellectual property rights of others or otherwise violate any applicable law; (vi) circumvent or modify any security technologies included as part of the FIT Instrumentation; or (vii) attempt or permit any third party to do any of the foregoing. New Relic may suspend usage of the Services by any user, without notice, pending any investigation of misuse. These restrictions may be supplemented or superceded (to the extent they conflict) by the New Relic Acceptable Use Policy as may be published and updated from time to time at https://docs.newrelic.com/docs/licenses/license-information/acceptable-use-policy/acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential Information means: (a) the FIT Instrumentation, and any features, results or output produced by, and other information relating to the FIT Instrumentation (including, without limitation, all Feedback); and (b) any business or technical information of New Relic including but not limited to any technical information, research, development, know-how that a reasonable person would understand to be confidential. 3.2 Restrictions. Customer will not use or disclose any Confidential Information, except as necessary for the performance of this Agreement. Customer will use all reasonable efforts to protect Confidential Information from unauthorized use or disclosure, but in no event less than the efforts that it ordinarily uses with respect to its own proprietary information. Customer may disclose Confidential Information to those of its employees who have a bona fide need to know such Confidential Information for the performance of this Agreement; provided that each such employee first executes a written agreement that contains use and nondisclosure restrictions at least as protective as those set forth herein. Confidential Information shall not include any information that: (a) is or becomes generally known to the public through no fault or breach of this Agreement by Customer; (b) is rightfully known by Customer at the time of disclosure without an obligation of confidentiality; (c) is independently developed by Customer without access or use of any Confidential Information; or (d) is rightfully obtained from a third party without restriction on use or disclosure. 4. DISCLAIMER 4.1 DISCLAIMER. FIT INSTRUMENTATION IS PROVIDED AS-IS AND AS-AVAILABLE AND NEW RELIC DISCLAIMS AND MAKES NO WARRANTIES, EITHER EXPRESS OR IMPLIED, INCLUDING ANY IMPLIED WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, TITLE OR NON-INFRINGEMENT AND WHETHER OR NOT ARISING THROUGH A COURSE OF DEALING. FIT INSTRUMENTATION IS NOT GUARANTEED TO BE ERROR-FREE, COMPATIBLE WITH THE UNDERLYING SOFTWARE. , OR THAT CUSTOMER WILL ACHIEVE ANY RESULTS FROM USE OF THE FIT INSTRUMENTATION THEREFROM. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES OR LIMITATIONS ON APPLICABLE STATUTORY RIGHTS OF A CONSUMER, SO THE ABOVE EXCLUSION AND LIMITATIONS MAY NOT APPLY TO THE CUSTOMER. 4.2 LIMITATION OF LIABILITY. TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE TO CUSTOMER OR ANY THIRD PARTY FOR DAMAGES OF ANY KIND, INCLUDING, WITHOUT LIMITATION, INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, OR CONSEQUENTIAL DAMAGES (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) ARISING OUT OF OR IN CONNECTION WITH THIS AGREEMENT OR THE INSTALLATION, USE OR INABILITY TO USE THE FIT INSTRUMENTATION OR FOR ANY ERROR OR DEFECT IN THE FIT INSTRUMENTATION OR THE SERVICES, WHETHER SUCH LIABILITY ARISES FROM ANY CLAIM BASED UPON CONTRACT, WARRANTY, TORT, STRICT LIABILITY, OR OTHERWISE, WHETHER OR NOT NEW RELIC HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH LOSS OR DAMAGE. NEW RELIC'S AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS AGREEMENT WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE PARTIES HAVE AGREED THAT THESE LIMITATIONS WILL SURVIVE AND APPLY EVEN IF ANY LIMITED REMEDY SPECIFIED IN THIS AGREEMENT IS FOUND TO HAVE FAILED OF ITS ESSENTIAL PURPOSE. 4.3 EVALUATION VERSION. Customer agrees and acknowledges that: (a) the FIT Instrumentation is not an official product and has not been commercially released for sale by New Relic; (b) the FIT Instrumentation may not operate properly, being in final form, or fully functional; (c) the FIT Instrumentation may contain errors, security vulnerabilities, design flaws, or other problems; (d) it may not be possible to make the FIT Instrumentation fully functional; (e) the information obtained using the FIT Instrumentation may not be accurate; (f) use of the FIT Instrumentation may result in unexpected results, loss of data, delays or other unpredictable damages or loss; (g) New Relic is under no obligation to release a commercial version of the FIT Instrumentation; and (h) New Relic has the right unilaterally to abandon development of the FIT Instrumentation, at any time and without any obligation or liability to Customer. 5. GENERAL PROVISIONS 5.1 Terms of Service; Documentation. This Agreement shall be considered a part of the New Relic documentation, located at: https://docs.newrelic.com (the \"Documentation\").This Agreement and the Terms of Service constitute the entire and exclusive agreement between New Relic and Customer with respect to the FIT Instrumentation. In the absence of a separate agreement between New Relic and Customer with respect to the FIT Instrumentation, the terms and conditions of this Agreement shall govern the relationship between New Relic and Customer with respect to such FIT Instrumentation. To the extent of a conflict between the Agreement and the Terms of Service, this Agreement shall govern with respect to the FIT Instrumentation only.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.77924,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "FIT instrumentation end user <em>license</em> agreement",
        "sections": "FIT instrumentation end user <em>license</em> agreement",
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " Acceptable Use Policy as may be published and updated from time to time at https:&#x2F;&#x2F;docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>license</em>-<em>information</em>&#x2F;acceptable-use-policy&#x2F;acceptable-use-policy. 3. CONFIDENTIALITY 3.1 Definition. Confidential <em>Information</em> means: (a) the FIT Instrumentation, and any features, results"
      },
      "id": "603e9f3ee7b9d206e32a0800"
    },
    {
      "sections": [
        "New Relic Agent Software Notice"
      ],
      "title": "New Relic Agent Software Notice",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Distributed licenses"
      ],
      "external_id": "2bf9501c2767105130d3808f1bf3a91a032d903e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/distributed-licenses/new-relic-agent-software-notice/",
      "published_at": "2021-06-26T04:34:19Z",
      "updated_at": "2021-03-16T04:20:32Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This software is  2008-2021 New Relic, Inc. and its licensors. This software is solely for use with New Relics proprietary SaaS service (New Relic Service), so to use the software you must have a valid account for the New Relic Service under a separate agreement with New Relic (Subscription Agreement). You may only use the software to support your use of the New Relic Service as permitted in the Subscription Agreement. Without a Subscription Agreement, you may not use the software. All other use is prohibited. New Relic and its suppliers retain all right, title and interest (including intellectual property rights) in the software. The Subscription Agreement will control in event of a conflict with this notice. Unless otherwise agreed by New Relic in your Subscription Agreement: You may not use, copy, distribute or sublicense the software, use the software on behalf of third parties, reverse engineer or decompile the software, modify or create derivative works of the software, use the software for competitive analysis or benchmarking, or remove or obscure any proprietary notices in the software. The software is provided AS IS and New Relic disclaims all warranties, whether express, implied, statutory or otherwise, including warranties of merchantability, fitness for a particular purpose, title or noninfringement. To the full extent permitted by law, New Relic will have no liability arising from or related to the software or under this notice for any direct, indirect, special, incidental, or consequential damages of any kind, even if advised of their possibility in advance, and regardless of legal theory (whether contract, tort, negligence, strict liability or otherwise). The software may contain third-party open source software (OSS) as described here and at https://github.com/newrelic. To the extent required by the OSS license, that license will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile and as set forth: https://docs.newrelic.com/docs/mobile-monitoring/new-relic-mobile/get-started and https://docs.newrelic.com/docs/browser/new-relic-browser/installation/install-new-relic-browser-agent Software versions New Relic makes available under an OSS license (such as Apache 2.0) are governed by the terms of the applicable OSS license. For a current list of New Relic software versions released as OSS please visit https://opensource.newrelic.com/.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.77924,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Distributed</em> <em>licenses</em>",
        "body": " and at https:&#x2F;&#x2F;github.com&#x2F;newrelic. To the extent required by the OSS <em>license</em>, that <em>license</em> will apply to the OSS when used on a stand-alone basis. For avoidance of doubt, you may copy and distribute New Relic agents pursuant to your Subscription Agreement for New Relic Browser and New Relic Mobile"
      },
      "id": "603eb73828ccbc1f99eba74a"
    }
  ],
  "/docs/licenses/license-information/product-definitions/legacy-product-definitions": [
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-06-26T04:35:29Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customers websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated Documentation page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customers Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relics customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.95703,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One pricing: <em>Definitions</em>",
        "sections": "New Relic One pricing: <em>Definitions</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " pricing, invoicing related <em>information</em>, and <em>product</em>-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing."
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2021-06-26T04:40:08Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing plan. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the Product Usage Ratio) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Products list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.8739,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "sections": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "tags": "<em>License</em> <em>information</em>",
        "body": " will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more <em>information</em> about units of measures, see <em>Product</em> <em>definitions</em>. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units"
      },
      "id": "603ea32a28ccbc7e22eba768"
    },
    {
      "sections": [
        "New Relic One pricing plan: Frequently asked questions",
        "Frequently asked questions",
        "Q: Where can I find more information about New Relic One and related products?",
        "Q: Is there a limit on the number of concurrent sessions or IP addresses that may be used with a user account?",
        "Q: What terms govern my use of the Products?",
        "Q: How are the number of full users (also referred to as Monthly Provisioned Users) calculated for each month?",
        "Q: Can I mix and match New Relic One Standard, Pro, and Enterprise users in a billing account?",
        "Q: Can Event extended retention, additional Synthetics Checks, or additional New Relic Edge data be added to my account?",
        "Q: What is included in New Relic's Free Tier of Products?",
        "Q: How does the Free Tier impact my pay-as-you-go (PAYG) or Annual Pool of Funds (APOF) subscription?"
      ],
      "title": "New Relic One pricing plan: Frequently asked questions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "FAQ"
      ],
      "external_id": "cc702038ec7eb4983faa152108a4a233bd285264",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/faq/new-relic-one-pricing-plan-frequently-asked-questions/",
      "published_at": "2021-06-26T14:29:57Z",
      "updated_at": "2021-03-29T20:50:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "These are some frequently asked questions about the New Relic One pricing plan that arise for agreement-level language. This is meant to serve as a supplement to the main New Relic One pricing docs. Frequently asked questions These FAQs are meant to serve as a supplement to the main New Relic One pricing docs. Q: Where can I find more information about New Relic One and related products? A: For a high level description of user entitlements for Telemetry Data Platform, Full Stack Observability, and Applied Intelligence, please visit our pricing page. For more in-depth information, see New Relic One pricing. Q: Is there a limit on the number of concurrent sessions or IP addresses that may be used with a user account? A: Yes. See Manage users. Q: What terms govern my use of the Products? A: If a Customer has paid New Relic during a rolling 12-month period, then Customers usage of the Products is covered by the Paid Terms of Service. If a Customer has not paid New Relic during a 12-month period, then Customers usage of the Products is covered by the Unpaid Terms of Service. Q: How are the number of full users (also referred to as Monthly Provisioned Users) calculated for each month? A: Please see Calculation details. Q: Can I mix and match New Relic One Standard, Pro, and Enterprise users in a billing account? A: No. For the New Relic One pricing plan, you can only have one subscription per organization (group of accounts that share the same billing ID). Q: Can Event extended retention, additional Synthetics Checks, or additional New Relic Edge data be added to my account? A: For additional retention for Events or Logs beyond the Standard Data Retention, contact your New Relic account executive. Note: Minimum requirement for Extended Retention is that the Customer must have a Annual Pool of Funds subscription which includes all of the following: Telemetry Data Platform, Full Stack Observability Pro or above. Telemetry Data Platform Pricing is affected. Q: What is included in New Relic's Free Tier of Products? A: Please see this pricing doc for a description of what's included. Customers use of the Free Tier shall be governed by the terms and conditions described in the Unpaid Terms of Service. If Customers usage exceeds the Free Tier, Customer is fully responsible for fees incurred in excess of the Free Tier as described in the Pay-as-you-go program and as described in the Paid Terms of Service. Q: How does the Free Tier impact my pay-as-you-go (PAYG) or Annual Pool of Funds (APOF) subscription? A: The Free Tier usage will be deducted automatically from the Monthly Product Usage for Pay-as-you-go or Annual pool of funds subscriptions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.31297,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Q: Where can I find more <em>information</em> about New Relic One and related <em>products</em>?",
        "tags": "<em>License</em> <em>information</em>",
        "body": " pricing docs. Q: Where can I find more <em>information</em> about New Relic One and related products? A: For a high level description of user entitlements for Telemetry Data Platform, Full Stack Observability, and Applied Intelligence, please visit our pricing page. For more in-depth <em>information</em>, see New"
      },
      "id": "6044e6e5196a67b568960f3e"
    }
  ],
  "/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions": [
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-06-25T22:53:06Z",
      "updated_at": "2021-05-22T17:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into the Insights event database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 220.95639,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Original <em>product</em>-based pricing <em>definitions</em>",
        "sections": "Original <em>product</em>-based pricing <em>definitions</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": "This is a glossary of terms that appear in contracts for our original <em>product</em>-based pricing. For New Relic One pricing plan terms, see New Relic One pricing <em>definitions</em>. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app"
      },
      "id": "603ebacc64441f77774e8872"
    },
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2021-06-26T04:40:08Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing plan. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the Product Usage Ratio) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Products list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 136.8739,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "sections": "<em>Product</em>-based pricing usage and New Relic Platform Pricing Usage Plan",
        "tags": "<em>License</em> <em>information</em>",
        "body": " will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more <em>information</em> about units of measures, see <em>Product</em> <em>definitions</em>. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units"
      },
      "id": "603ea32a28ccbc7e22eba768"
    },
    {
      "sections": [
        "New Relic One pricing plan: Frequently asked questions",
        "Frequently asked questions",
        "Q: Where can I find more information about New Relic One and related products?",
        "Q: Is there a limit on the number of concurrent sessions or IP addresses that may be used with a user account?",
        "Q: What terms govern my use of the Products?",
        "Q: How are the number of full users (also referred to as Monthly Provisioned Users) calculated for each month?",
        "Q: Can I mix and match New Relic One Standard, Pro, and Enterprise users in a billing account?",
        "Q: Can Event extended retention, additional Synthetics Checks, or additional New Relic Edge data be added to my account?",
        "Q: What is included in New Relic's Free Tier of Products?",
        "Q: How does the Free Tier impact my pay-as-you-go (PAYG) or Annual Pool of Funds (APOF) subscription?"
      ],
      "title": "New Relic One pricing plan: Frequently asked questions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "FAQ"
      ],
      "external_id": "cc702038ec7eb4983faa152108a4a233bd285264",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/faq/new-relic-one-pricing-plan-frequently-asked-questions/",
      "published_at": "2021-06-26T14:29:57Z",
      "updated_at": "2021-03-29T20:50:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "These are some frequently asked questions about the New Relic One pricing plan that arise for agreement-level language. This is meant to serve as a supplement to the main New Relic One pricing docs. Frequently asked questions These FAQs are meant to serve as a supplement to the main New Relic One pricing docs. Q: Where can I find more information about New Relic One and related products? A: For a high level description of user entitlements for Telemetry Data Platform, Full Stack Observability, and Applied Intelligence, please visit our pricing page. For more in-depth information, see New Relic One pricing. Q: Is there a limit on the number of concurrent sessions or IP addresses that may be used with a user account? A: Yes. See Manage users. Q: What terms govern my use of the Products? A: If a Customer has paid New Relic during a rolling 12-month period, then Customers usage of the Products is covered by the Paid Terms of Service. If a Customer has not paid New Relic during a 12-month period, then Customers usage of the Products is covered by the Unpaid Terms of Service. Q: How are the number of full users (also referred to as Monthly Provisioned Users) calculated for each month? A: Please see Calculation details. Q: Can I mix and match New Relic One Standard, Pro, and Enterprise users in a billing account? A: No. For the New Relic One pricing plan, you can only have one subscription per organization (group of accounts that share the same billing ID). Q: Can Event extended retention, additional Synthetics Checks, or additional New Relic Edge data be added to my account? A: For additional retention for Events or Logs beyond the Standard Data Retention, contact your New Relic account executive. Note: Minimum requirement for Extended Retention is that the Customer must have a Annual Pool of Funds subscription which includes all of the following: Telemetry Data Platform, Full Stack Observability Pro or above. Telemetry Data Platform Pricing is affected. Q: What is included in New Relic's Free Tier of Products? A: Please see this pricing doc for a description of what's included. Customers use of the Free Tier shall be governed by the terms and conditions described in the Unpaid Terms of Service. If Customers usage exceeds the Free Tier, Customer is fully responsible for fees incurred in excess of the Free Tier as described in the Pay-as-you-go program and as described in the Paid Terms of Service. Q: How does the Free Tier impact my pay-as-you-go (PAYG) or Annual Pool of Funds (APOF) subscription? A: The Free Tier usage will be deducted automatically from the Monthly Product Usage for Pay-as-you-go or Annual pool of funds subscriptions.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 118.31297,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Q: Where can I find more <em>information</em> about New Relic One and related <em>products</em>?",
        "tags": "<em>License</em> <em>information</em>",
        "body": " pricing docs. Q: Where can I find more <em>information</em> about New Relic One and related products? A: For a high level description of user entitlements for Telemetry Data Platform, Full Stack Observability, and Applied Intelligence, please visit our pricing page. For more in-depth <em>information</em>, see New"
      },
      "id": "6044e6e5196a67b568960f3e"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Data Security",
        "2. Data Access",
        "3. Server Security",
        "4. Network Security",
        "5. Security Audits",
        "6. Security and Incident Response",
        "7. Disaster Recovery",
        "8. Copies and Removal",
        "9. Disclosure by Law"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-06-26T04:36:34Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Data Security 1.1. New Relic shall establish and maintain data security procedures and other safeguards designed to protect against the loss, theft or other unauthorized access or alteration of Customer Data in the possession or under the control of New Relic or to which New Relic has access, which are no less rigorous than accepted security standards in the industry. 1.2. New Relic shall maintain an information security policy that outlines a definition of information security and its overall objectives; a framework for setting control objectives and controls, including the structure of risk assessment and risk management; a brief explanation of the compliance requirements, and procedures for managing information security incidents. 2. Data Access 2.1. Access to Customer Data stored on New Relics systems shall not be granted to members of New Relic unless they have been uniquely identified and have sufficient credentials. 2.2. Access permissions shall be established in a manner that allows for the minimum access level(s) required for each employee. 2.3. Access to Customer Data shall be logged with sufficient information to determine the nature and scope of any inappropriate access. 3. Server Security 3.1. New Relic shall establish and follow reasonable server configuration guidelines and processes to prevent unauthorized access to Customer Data. 3.2. New Relic shall establish and follow reasonable configuration change management procedures for its servers containing Customer Data. 4. Network Security 4.1. New Relic network architecture shall be designed to limit site access and restrict the availability of information services that are considered to be vulnerable to attack. 4.2. New Relic shall utilize SSL certificates for all Internet activity. By default, Customer Data transmitted to and from the New Relic network shall be sent over encrypted medium or an encrypted format. 4.3. New Relic network shall use IDS technologies for network intrusion detection. 4.4. Access to New Relic systems containing Customer Data shall be restricted to authorized personnel. 5. Security Audits 5.1. New Relic shall conduct at least annually a SOC 2 or industry equivalent audit. New Relic shall provide to Customer audit results upon request, and shall explain and provide remediation plans to correct any problems to the extent reasonably possible. 6. Security and Incident Response 6.1. New Relic shall maintain an Information Security Incident Response plan, and make that plan available to Customer if requested. 6.2. In the event of an actual theft, loss, or unauthorized access of Customer Data by New Relics personnel and/or any unauthorized individual or entity, New Relic shall: (a) investigate such breach, (b) attempt to cure such breach, and (c) provide notification to Customer that describes such breach. 7. Disaster Recovery 7.1. New Relic shall have in effect a disaster recovery plan designed to respond to both a component failure of New Relic equipment within its data center and a catastrophic loss of service. This plan shall include documented policies and procedures to restore service in the event of either type of failure. 7.2. New Relic shall establish and follow backup and restore procedures for servers containing Customer Data. 8. Copies and Removal 8.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of this Agreement for any reason: (a) New Relic shall, and shall cause its personnel, to cease and desist all access and use of any Customer Data, (b) New Relic shall delete all copies of Customer Data within ninety (90) days. 9. Disclosure by Law 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.40988,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Relic has access, which are no less rigorous than accepted security standards in the industry. 1.2. New Relic shall maintain an <em>information</em> security policy that outlines a definition of <em>information</em> security and its overall objectives; a framework for setting control objectives and controls"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2021-06-26T04:36:33Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to New Relic Full Stack Observability Pro or Enterprise Products, the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to New Relic Full Stack Observability Standard, any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customers access or use of the Service in accordance with this Agreement. This describes Customers sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.39645,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-06-26T04:35:29Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customers websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated Documentation page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customers Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relics customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.006454,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/security-policy": [
    {
      "sections": [
        "Service level availability commitment",
        "New Relic Service Level Availability"
      ],
      "title": "Service level availability commitment",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5489f268010887c72446f12059a1a0e4279ad960",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/service-level-availability-commitment/",
      "published_at": "2021-06-26T04:36:33Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you subscribe to New Relic Full Stack Observability Pro or Enterprise Products, the service level availability commitments available to you are those set forth (i) in the Terms, or (ii) as set forth on this page. If you subscribe to New Relic Full Stack Observability Standard, any service level availability commitment or related remedies contained in your Terms are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. If you subscribe to any other New Relic Products on a product-based pricing basis, any service level availability commitment are contained in (i) your Terms, or (ii) if your Terms explicitly references this Service Level Availability commitment applying to the Service purchased in an Order, this page for the Services you use. Capitalized terms not defined below shall take on the meaning set forth in your Terms. New Relic Service Level Availability The Service will be considered available so long as Customer is able to log in to its interface and view Customer Data (\"Service Availability\"). The applicable Service Availability will be calculated as a percentage of: (1) the total number of minutes in a month after (2) subtracting any periods of unavailability during such month from the total number of minutes in a month. New Relic will use commercially reasonable efforts to maintain Service Availability of at least 99.8% during any calendar month. In the event the Service Availability drops below: (i) 98.5% for two consecutive calendar months during the Subscription Term, or (ii) 96.5% in any single calendar month, Customer may request to terminate the relevant Service with no penalty. Such termination will be effective as of the end of the then-current billing period and no additional fees will be charged. The service level within New Relic's control is the Service Availability, not, for example, the transmission of data over the public Internet. Service Availability calculations will exclude unavailability arising from any: (a) planned maintenance periods; (b) emergency maintenance that is necessary to prevent imminent harm to the Service; (c) force majeure events; (d) Third-Party Services, Customer application, equipment, software or other technology, or Customer or its User's use of the Service, in violation of the Agreement or not in accordance with the Documentation; or (e) suspension, limitation, and/or termination of Customers access or use of the Service in accordance with this Agreement. This describes Customers sole and exclusive remedy for failures of Service Availability. Customer may request the Service Availability attainment for the previous month by filing a support ticket on the New Relic support site.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.39645,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>"
      },
      "id": "603ea44828ccbcd9c8eba7b7"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2021-06-26T04:35:29Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (New Relic) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the Policy) governing your use and participation in New Relics program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the Pre-Release Service(s)). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as you or your) as of your date of first use of the Pre-Release Service(s) (the Effective Date), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated Documentation page of the New Relic website (the Documentation) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (Feedback). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (Confidential Information). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., personal information of children as defined by the Childrens Online Privacy Protection Act, protected health information as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, special categories of data as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relics request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively Unintended Effects); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED AS IS. TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELICS AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorneys fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.89651,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-06-26T04:35:29Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customers websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated Documentation page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customers Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relics customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.006454,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    }
  ],
  "/docs/licenses/license-information/referenced-policies/service-level-availability-commitment": [
    {
      "sections": [
        "Security policy",
        "New Relic Security Policy",
        "1. Data Security",
        "2. Data Access",
        "3. Server Security",
        "4. Network Security",
        "5. Security Audits",
        "6. Security and Incident Response",
        "7. Disaster Recovery",
        "8. Copies and Removal",
        "9. Disclosure by Law"
      ],
      "title": "Security policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "291509f6b521ca34ac9d49039518e7da8b883518",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/security-policy/",
      "published_at": "2021-06-26T04:36:34Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The below Security Policy applies only to customers with an existing New Relic agreement in place that explicitly references this Security Policy applying to the Service purchased in an Order. Capitalized terms not defined below shall take on the meaning set forth in such New Relic agreement. New Relic Security Policy 1. Data Security 1.1. New Relic shall establish and maintain data security procedures and other safeguards designed to protect against the loss, theft or other unauthorized access or alteration of Customer Data in the possession or under the control of New Relic or to which New Relic has access, which are no less rigorous than accepted security standards in the industry. 1.2. New Relic shall maintain an information security policy that outlines a definition of information security and its overall objectives; a framework for setting control objectives and controls, including the structure of risk assessment and risk management; a brief explanation of the compliance requirements, and procedures for managing information security incidents. 2. Data Access 2.1. Access to Customer Data stored on New Relics systems shall not be granted to members of New Relic unless they have been uniquely identified and have sufficient credentials. 2.2. Access permissions shall be established in a manner that allows for the minimum access level(s) required for each employee. 2.3. Access to Customer Data shall be logged with sufficient information to determine the nature and scope of any inappropriate access. 3. Server Security 3.1. New Relic shall establish and follow reasonable server configuration guidelines and processes to prevent unauthorized access to Customer Data. 3.2. New Relic shall establish and follow reasonable configuration change management procedures for its servers containing Customer Data. 4. Network Security 4.1. New Relic network architecture shall be designed to limit site access and restrict the availability of information services that are considered to be vulnerable to attack. 4.2. New Relic shall utilize SSL certificates for all Internet activity. By default, Customer Data transmitted to and from the New Relic network shall be sent over encrypted medium or an encrypted format. 4.3. New Relic network shall use IDS technologies for network intrusion detection. 4.4. Access to New Relic systems containing Customer Data shall be restricted to authorized personnel. 5. Security Audits 5.1. New Relic shall conduct at least annually a SOC 2 or industry equivalent audit. New Relic shall provide to Customer audit results upon request, and shall explain and provide remediation plans to correct any problems to the extent reasonably possible. 6. Security and Incident Response 6.1. New Relic shall maintain an Information Security Incident Response plan, and make that plan available to Customer if requested. 6.2. In the event of an actual theft, loss, or unauthorized access of Customer Data by New Relics personnel and/or any unauthorized individual or entity, New Relic shall: (a) investigate such breach, (b) attempt to cure such breach, and (c) provide notification to Customer that describes such breach. 7. Disaster Recovery 7.1. New Relic shall have in effect a disaster recovery plan designed to respond to both a component failure of New Relic equipment within its data center and a catastrophic loss of service. This plan shall include documented policies and procedures to restore service in the event of either type of failure. 7.2. New Relic shall establish and follow backup and restore procedures for servers containing Customer Data. 8. Copies and Removal 8.1. In addition to any obligations of New Relic in the Agreement, upon expiration or termination of this Agreement for any reason: (a) New Relic shall, and shall cause its personnel, to cease and desist all access and use of any Customer Data, (b) New Relic shall delete all copies of Customer Data within ninety (90) days. 9. Disclosure by Law 9.1. In the event the New Relic is required by law, regulation, or legal process to disclose any Customer Data, New Relic shall (a) give Customer, to the extent possible, reasonable advance notice prior to disclosure so Customer may contest the disclosure or seek a protective order, and (b) reasonably limit the disclosure to the minimum amount that is legally required to be disclosed.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 213.40988,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Security <em>policy</em>",
        "sections": "Security <em>policy</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Relic has access, which are no less rigorous than accepted security standards in the industry. 1.2. New Relic shall maintain an <em>information</em> security policy that outlines a definition of <em>information</em> security and its overall objectives; a framework for setting control objectives and controls"
      },
      "id": "603ea3dbe7b9d22b802a0802"
    },
    {
      "sections": [
        "New Relic Pre-release policy",
        "New Relic Pre-Release Policy",
        "1. Introduction",
        "2. License and Restrictions",
        "3. Confidential Information",
        "4. Disclaimers and Acknowledgement",
        "5. Limitation of Liability",
        "6. Indemnity",
        "7. Export Restrictions",
        "8. Termination"
      ],
      "title": "New Relic Pre-release policy",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Referenced policies"
      ],
      "external_id": "5ca324e942906913fc346245cfd80d405e90ad1f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/referenced-policies/new-relic-pre-release-policy/",
      "published_at": "2021-06-26T04:35:29Z",
      "updated_at": "2021-03-13T01:17:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Without overriding any express provisions that you may have agreed to in a separate written beta or pre-release agreement executed by you and New Relic, the following terms apply to your use of any New Relic Pre-Release Service(s). New Relic Pre-Release Policy Through your use of any New Relic, Inc. (New Relic) Pre-Release Services (as further defined), you signify that you have read, understood, and accept the terms of this Pre-Release Policy (the Policy) governing your use and participation in New Relics program for making beta, alpha and pre-release versions of New Relic software (i.e. agents, SDKs, APIs, integrations, private locations, code, etc.), services, features, user-interfaces, and platforms made available for evaluation purposes (the Pre-Release Service(s)). This Policy is effective on behalf of yourself or the entity(ies) associated with your New Relic account (collectively referred to herein as you or your) as of your date of first use of the Pre-Release Service(s) (the Effective Date), which means that your choice to use the Pre-Release Service(s) means that you have the legal authority to use the Pre-Release Service(s) personally or on behalf of the company or organization associated with your New Relic account in accordance with this Policy. Please read this Policy carefully as your use of Pre-Release Service(s) may have unintended consequences and materially impact your data or use of the New Relic services and products. If you do not agree to this Policy, your sole remedy is to not make use of the Pre-Release Services. 1. Introduction This Policy and any New Relic technical guides and documentation made available with the Pre-Release Service(s) or from the dedicated Documentation page of the New Relic website (the Documentation) enables you to test experimental features and products before they are made generally available. New Relic reserves the right to withhold or discontinue any Pre-Release Service(s), which may be designated as an alpha, beta, pilot, limited release, developer preview, technology preview, non-production, evaluation, or by a similar description, in its sole discretion and provides no guarantee that any Pre-Release Service(s) will eventually be made commercially or generally available. Pre-Release Service(s) are versions provided before they are generally available and may have bugs, stability issues, or other problems. Use of the Pre-Release Service(s) may result in unexpected results, loss of data, outages, or other damage to your systems or networks. New Relic advises against using the Pre-Release Service(s) in production. No support is offered with or for Pre-Release Service(s). You hereby release New Relic from any liability arising from or related to your use of the Pre-Release Service(s). In connection with your use of the Pre-Release Services, you may provide feedback, comments, and/or suggestions based on or relating to the Pre-Release Services (Feedback). The Pre-Release Services and any related information, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback, and information related to any business or technical information of New Relic, including without limitation product plans, costs, prices, finances, marketing plans, business opportunities, research, development, and negotiations are considered confidential information of New Relic (Confidential Information). 2. License and Restrictions 2.1 Evaluation License. Subject to your compliance with the Policy, New Relic hereby grants you a limited, non-exclusive, non-transferable, revocable license, during the term of this Policy, to use the Pre-Release Service(s) in accordance with the Documentation solely for the purpose of internal evaluation and supplying Feedback to New Relic. 2.2 License Restrictions. You may not (and may not authorize or enable a third party to): (i) reverse engineer, decompile, disassemble, or otherwise attempt to discover the source code or other trade secrets in the Pre-Release Service(s); (ii) access the Pre-Release Service(s) in order to build a similar or competitive application, service, feature or other competitive purpose; (iii) remove or destroy any copyright notices or other proprietary marks contained on or in the Pre-Release Service(s); (iv) use the Pre-Release Service(s) for any benchmarking purposes or in connection with a service bureau, timeshare, service provider, or like activity where you operate the Pre-Release Service(s) on behalf of a third party; (v) mirror, frame, copy, modify, host, rent, lease, sell, commercialize, sublicense, assign or otherwise transfer the Pre-Release Service(s), or the access or use of the Pre-Release Service(s); (vi) use the Pre-Release Service(s) in a manner that may violate or infringe the intellectual property, data protection, or other proprietary right of a third party; (vii) use or access the Pre-Release Service(s) in an unauthorized manner and/or in a manner that violates any applicable law, rule, contract, or guideline (including but not limited to our Documentation or Community Guidelines); (viii) use the Pre-Release Service(s) to transmit worms, viruses, malicious code, security vulnerabilities, or otherwise negatively impact network operations, third parties, or New Relic; (ix) use any data mining or similar data gathering and extraction methods in connection with the Pre-Release Service(s); or (x) use the Pre-Release Service(s) to process sensitive personal information, e.g., personal information of children as defined by the Childrens Online Privacy Protection Act, protected health information as defined by the Health Insurance Portability and Accountability Act of 1996, government issued identification numbers, financial account information, payment card data, special categories of data as described in the EU General Data Protection Regulation (GDPR), or other information subject to regulatory, statutory, or contractual restrictions. New Relic reserves the right, but not the obligation, to monitor or review your use of the Pre-Release Service(s) at any time and may investigate any suspected violations of this Policy. 2.3 Intellectual Property Rights. You acknowledge and agree that, as between you and New Relic, New Relic owns all right, title, and interest in and to the Pre-Release Service(s), Confidential Information, and Feedback, including but not limited to all intellectual property and proprietary rights therein. New Relic and its licensors reserve all rights and licenses not expressly granted herein. 2.4 Feedback. You acknowledge and agree that all Feedback will be the sole and exclusive property of New Relic. You hereby irrevocably transfer and assign to New Relic all right, title, and interest in all Feedback, including but not limited to all intellectual property and proprietary rights therein. 3. Confidential Information You will not use or disclose any Confidential Information, except as necessary for the performance of this Policy, and you will use reasonable efforts to protect Confidential Information from unauthorized use or disclosure. You may disclose Confidential Information only to those employees with a bona fide need to know, provided that each employee has signed a written agreement with nondisclosure restrictions at least as protective of the Confidential Information as those set forth herein. For the purposes of this section, information will not be deemed Confidential Information if it: (a) is or becomes generally known to the public through no fault or breach of this Policy; (b) is rightfully known by you at the time of disclosure without an obligation of confidentiality; (c) is independently developed by you without access to or use of the Confidential Information; or (d) is rightfully obtained by you from a third party without restriction on use or disclosure. Upon New Relics request, you agree to destroy all Confidential Information in your possession within 30 days of termination of this Policy. 4. Disclaimers and Acknowledgement 4.1 Acknowledgement of Pre-Release Service(s). You acknowledge and agree that: (a) the Pre-Release Service(s) is not an official product and has not been commercially released for sale by New Relic; (b) the Pre-Release Service(s) may not operate properly, be in final form, or fully function; (c) the Pre-Release Service(s) may contain errors, design flaws, security vulnerabilities, or other problems; (d) it may not be possible to make the Pre-Release Service(s) fully functional or secure; (e) the information obtained using the Pre-Release Service(s) may not be accurate; (f) use of the Pre-Release Service(s) may result in unexpected results, vulnerabilities, loss of data, project delays, or other unpredictable damage or loss, including without limitation to your use of New Relic services and products not governed by this Policy (collectively Unintended Effects); (g) New Relic is under no obligation to release a commercial or generally available version of the Pre-Release Service(s); and (h) New Relic has the right unilaterally to abandon development of the Pre-Release Service(s) at any time and without any obligation or liability to you. 4.2 Your Data. You acknowledge and agree that you should not rely on the Pre-Release Service(s) for any reason. You further acknowledge and agree that you are solely responsible for maintaining and protecting all data and information that is stored, retrieved, or otherwise processed by the Pre-Release Service(s) in accordance with your contractual obligations and requirements under applicable law. Without limiting the foregoing, you are responsible for all costs and expenses required to backup and restore any data and information that is lost or corrupted as a result of your use of the Pre-Release Service(s) or any Unintended Effects. You understand and agree that your Data may be transferred to the United States for storage, processing, and use by New Relic to provide the Pre-Release Service(s). For any data that you process using the Pre-Release Service(s), you represent and warrant that you have all necessary rights and consent to do so. New Relic has no obligation to store any data you process using the Pre-Release Service(s) and shall have no liability for the deletion or accuracy of such data. 4.3 Warranty Disclaimers. YOU ACKNOWLEDGE THAT PRE-RELEASE SERVICE(S) ARE PROVIDED AS IS. TO THE EXTENT PERMITTED BY APPLICABLE LAW, NEW RELIC DISCLAIMS ALL WARRANTIES, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO ANY WARRANTIES REGARDING MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE, NON-INFRINGEMENT, SECURITY, AND ANY WARRANTIES ARISING OUT OF THE COURSE OF DEALING OR USAGE OF TRADE. NEW RELIC MAKES NO WARRANTY THAT PRE-RELEASE SERVICE(S) WILL MEET YOUR REQUIREMENTS OR WILL BE ACCURATE, RELIABLE, ERROR-FREE, UNINTERRUPTED, TIMELY, OR SECURE. YOUR USE OF PRE-RELEASE SERVICE(S) IS AT YOUR OWN RISK. 4.4 Modifications. New Relic may change the Policy from time to time as our business changes and technology evolves, and future versions of our generally available services may not be compatible with Pre-Release Service(s) built using previous versions. The most current version of this Policy will be posted in the Documentation. Any changes to this Policy will be effective immediately for all users in instances to comply with applicable law, for new users of a Pre-Release Service(s) and, for all other users, any changes to this Policy will be effective as of fifteen (15) days after posting notice of such changes. If we determine in our sole discretion that an update is material, we will provide notice of such material change to you through your New Relic account, the Pre-Release Service(s), the Documentation, our blogs or community forums, and/or by email to the email address of your account administrator. We may require you to provide consent to the updated Policy in a specified manner before further use of the Pre-Release Service(s) is permitted. If you do not agree to any change(s), your sole remedy is to stop using the Pre-Release Service(s) and any such termination by you shall be without penalty except as specified in this Policy. Otherwise, your use of any Pre-Release Service(s) after any such update shall constitute acceptance of the then-current Policy. 5. Limitation of Liability TO THE EXTENT PERMITTED BY APPLICABLE LAW, IN NO EVENT WILL NEW RELIC, ITS AFFILIATES, OFFICERS, EMPLOYEES, AGENTS, SUPPLIERS OR LICENSORS BE LIABLE FOR ANY INDIRECT, SPECIAL, INCIDENTAL, PUNITIVE, EXEMPLARY OR CONSEQUENTIAL (INCLUDING LOSS OF USE, DATA, BUSINESS, OR PROFITS) DAMAGES, REGARDLESS OF LEGAL THEORY, WHETHER OR NOT NEW RELIC HAS BEEN WARNED OF THE POSSIBILITY OF SUCH DAMAGES, AND EVEN IF A REMEDY FAILS OF ITS ESSENTIAL PURPOSE. NEW RELICS AGGREGATE LIABILITY FOR ALL CLAIMS RELATING TO THIS POLICY, THE PRE-RELEASE SERVICES, OR ANY UNINTENDED EFFECTS WILL BE LIMITED TO FIFTY U.S. DOLLARS (U.S. $50). THE LIMITATIONS OF DAMAGES SET FORTH ABOVE ARE FUNDAMENTAL ELEMENTS OF THE BASIS OF THE BARGAIN BETWEEN NEW RELIC AND YOU. 6. Indemnity You agree to indemnify and hold New Relic, its parents, subsidiaries, affiliates, officers, agents, employees, and licensors harmless from any claims, fees, fines, demands, losses, liabilities, damages, and costs, including reasonable attorneys fees, arising from or related to your use of the Pre-Release Service(s), including but not limited to allegations arising from your breach of any terms herein and/or allegations that data processed by the Pre-Release Service(s) violates or infringes the privacy, data protection, or intellectual property rights of a third party. 7. Export Restrictions You acknowledge that the software in the Pre-Release Service(s) licensed hereunder may be subject to the export control laws and regulations of the U.S. and other countries. You agree that you will not export or re-export the Pre-Release Service(s), any part thereof, or any process or service that is the direct product of the Pre-Release Service(s) to any country, person or entity subject to U.S. export restrictions. 8. Termination Your access to the Pre-Release Service(s) will terminate upon the earliest of: (i) New Relic making the Pre-Release Service(s) or a successor version of the Pre-Release Service(s) generally available or available for commercial release; (ii) New Relic ceasing to make the Pre-Release Service(s) available to you; or (iii) New Relic terminating this Policy. New Relic may terminate this Policy for any reason upon notice to you. Sections 2.2, 2.3, 2.4, 3, 4, 5, 6, and 7 will survive your use of the Pre-Release Service(s).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 212.89651,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Pre-release <em>policy</em>",
        "sections": "2. <em>License</em> and Restrictions",
        "tags": "<em>License</em> <em>information</em>",
        "body": " Services and any related <em>information</em>, including without limitation features, functionality, designs, user interface, capabilities, specifications, architectural diagrams, usage data, APIs, deployment schedules, email lists, bug databases, know how, source code, potential features, Feedback"
      },
      "id": "6044e71b28ccbc984d2c60c8"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-06-26T04:35:29Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customers websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated Documentation page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customers Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relics customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 124.006454,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": ". Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, <em>information</em>, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer"
      },
      "id": "6044e6e528ccbc26f22c6084"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/data-collector-licenses": [
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-06-26T04:38:52Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.83559,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-06-26T04:37:38Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.83243,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-06-26T04:37:39Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (Terms) set forth the terms and conditions under which you (Customer) may use New Relic Diagnostics (the Software), as made available by New Relic, Inc. (New Relic). By clicking accept or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then Customer means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customers use of New Relic products to which Customer has a separate subscription (New Relic Products). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. Usage Data means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered Customer Data, your Data or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customers election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMERS OWN RISK. SOFTWARE IS PROVIDED AS IS, WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELICS ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customers access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customers written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Departments Table of Denial Orders or U.S. Treasury Departments list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relics prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customers consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.37134,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-06-26T04:36:34Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.84608,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-06-26T04:38:52Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.83559,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-06-26T04:37:38Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.83243,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-premium-support": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-06-26T04:36:34Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.84608,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Priority Support"
      ],
      "title": "New Relic Priority Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "f700b8b349627e66540b23ed020c7d9a46e19580",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-priority-support/",
      "published_at": "2021-06-26T04:38:52Z",
      "updated_at": "2021-03-16T04:43:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Priority Support document as a PDF (172 KB). The services described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.83559,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Priority Support document as a PDF (172 KB). The <em>services</em> described in this document only apply to initial orders entered after August 1, 2019. If you have questions about New Relic Priority Support, contact your New Relic account representative."
      },
      "id": "603ea90ae7b9d2e9d42a07d1"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-06-26T04:37:39Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (Terms) set forth the terms and conditions under which you (Customer) may use New Relic Diagnostics (the Software), as made available by New Relic, Inc. (New Relic). By clicking accept or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then Customer means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customers use of New Relic products to which Customer has a separate subscription (New Relic Products). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. Usage Data means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered Customer Data, your Data or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customers election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMERS OWN RISK. SOFTWARE IS PROVIDED AS IS, WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELICS ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customers access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customers written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Departments Table of Denial Orders or U.S. Treasury Departments list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relics prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customers consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.37134,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/special-services-licenses/new-relic-priority-support": [
    {
      "sections": [
        "Data collector licenses"
      ],
      "title": "Data collector licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "8eeb19ae388c1f4fd6856084084467a037861675",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/data-collector-licenses/",
      "published_at": "2021-06-26T04:36:34Z",
      "updated_at": "2021-03-16T06:20:41Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Package Licenses annotations Apache 2.0 antlr BSD apache-log4j-extras Apache 2.0 apache-mime4j Apache 2.0 bonecp Apache 2.0 bonecp-provider Apache 2.0 bytelist MIT c3p0 Eclipse 1.0 commons-beanutils Apache 2.0 commons-codec Apache 2.0 commons-collections Apache 2.0 commons-configuration Apache 2.0 commons-dbcp Apache 2.0 commons-dbutils Apache 2.0 commons-digester Apache 2.0 commons-io Apache 2.0 commons-lang Apache 2.0 commons-lang3 Apache 2.0 commons-logging Apache 2.0 commons-math Apache 2.0 commons-pool Apache 2.0 constantine MIT dom4j BSD ehcache Apache 2.0 gson Apache 2.0 guava Apache 2.0 hamcrest-core BSD-3-Clause hibernate-commons-annotations GNU LGPL 2.1 hibernate-core GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 hibernate-c3p0 GNU LGPL 2.1 hibernate-ehcache GNU LGPL 2.1 hibernate-jpa-2.0-api GNU LGPL 2.1 httpclient Apache 2.0 httpcore Apache 2.0 httpmime Apache 2.0 hystrix Apache 2.0 jackson-core-asl Apache 2.0 jackson-mapper-asl Apache 2.0 jaffl GNU LGPL 3.0 java-driver Apache 2.0 javassist MPL 1.1, GNU GPL 2.1, Apache 2.0 javax.servlet Apache 2.0, Eclipse 1.0 javax.servlet-api CDDL + GPLv2 with classpath exception jboss-logging GNU LGPL 2.1 jboss-transaction-api_1.1_spec GNU LGPL 2.1 jcodings Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 jedis MIT jetty Apache 2.0, Eclipse 1.0 jffi GNU LGPL 3.0 jline BSD 2-Clause joni MIT jnr-netdb GNU LGPL 3.0 jnr-posix Common Public License 1.0, GNU GPL 2, GNU LGPL 2.1 joda-time Apache 2.0 jopt-simple MIT json-simple Apache 2.0 jsr305 Apache 2.0 junit Common Public License 1.0 kafka Apache 2.0 libthrift Apache 2.0 log4j Apache 2.0 logback-classic Eclipse 1.0 logback-core Eclipse 1.0 metrics-annotation Apache 2.0 mockito-all MIT msgpack Apache 2.0 mysql-connector-java GNU GPL 2 reflections WTFPL scala-library Apache 2.0 scannotation Apache 2.0 slf4j-api MIT slf4j-over-log4j MIT snakeyaml Apache 2.0 snappy Apache 2.0 syslog4j LGPL xml-apis Apache 2.0 yamlbeans MIT zkclient Apache 2.0 zookeeper Apache 2.0",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.84608,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Data collector <em>licenses</em>",
        "sections": "Data collector <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic data collectors. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the <em>license</em> we&#x27;ve chosen to use. Package <em>Licenses</em> annotations"
      },
      "id": "603eb41d64441fd7f74e8893"
    },
    {
      "sections": [
        "New Relic Premium Support"
      ],
      "title": "New Relic Premium Support",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "db095214148ca053cdabd9815dc74b04299dbe7b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-premium-support/",
      "published_at": "2021-06-26T04:37:38Z",
      "updated_at": "2021-03-16T04:14:38Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Download the New Relic Premium Support document as a PDF (248 KB). The services described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.83241,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": "Download the New Relic Premium Support document as a PDF (248 KB). The <em>services</em> described in this document only apply to initial orders entered after April 1, 2018. If you have questions about New Relic Premium Support, contact your New Relic account representative."
      },
      "id": "603ea8c964441fe48b4e88af"
    },
    {
      "sections": [
        "New Relic Diagnostics licenses",
        "Proprietary license",
        "New Relic Diagnostics license terms",
        "Open-source licenses"
      ],
      "title": "New Relic Diagnostics licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Special services licenses"
      ],
      "external_id": "6d86f8f63d372be98270fadc8634f9da8da2a893",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/special-services-licenses/new-relic-diagnostics-licenses/",
      "published_at": "2021-06-26T04:37:39Z",
      "updated_at": "2021-03-13T03:25:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Proprietary license New Relic Diagnostics license terms These New Relic Diagnostics License Terms (Terms) set forth the terms and conditions under which you (Customer) may use New Relic Diagnostics (the Software), as made available by New Relic, Inc. (New Relic). By clicking accept or downloading or using the Software, you agree to be bound by these Terms. If you are agreeing to these Terms on behalf of your company, then Customer means your company and you are binding your company to these Terms. 1. License Grant. Subject to all of the terms and conditions of these Terms, New Relic grants Customer a limited, non-exclusive, non-transferable, non-sublicensable license to use the Software in accordance with its documentation in support of Customers use of New Relic products to which Customer has a separate subscription (New Relic Products). Unless otherwise specified, there is no fee for use of the Software. 2. License Restrictions. Customer will not (a) sell, rent, sublicense, transfer, time-share or otherwise provide access to any copies of the Software, or portions thereof, to a third party; (b) modify, decompile, disassemble or reverse engineer the Software; (c) use the Software to develop services or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance information or analysis (including benchmarks) relating to the Software. 3. Ownership. Except for the limited license rights expressly provided herein, New Relic and its suppliers have and will retain all right, title and interest in and to the Software, and all copies, updates, modifications and derivative works thereof. Customer acknowledges that it is obtaining only a limited license right to the Software and no ownership rights are being conveyed to Customer under these Terms or otherwise. 4. Usage Data. Customer agrees that New Relic and its affiliates have the right to collect Usage Data from Customer through the Software and use Usage Data to support, operate and improve New Relic products and services and for other lawful business purposes. Usage Data means diagnostics data related to the use of the Software with New Relic Products, including, without limitation, configuration information, details on diagnostics tasks, New Relic account and application ID numbers and New Relic license keys. For clarity, Usage Data is not considered Customer Data, your Data or any other similar term as used in any applicable subscription or license agreement for New Relic Products. At Customers election, Customer may disable the collection of Usage Data as described in the Software documentation. 5. Disclaimers. New Relic is not obligated to provide any support or maintenance for the Software. ALL USE OF SOFTWARE IS AT CUSTOMERS OWN RISK. SOFTWARE IS PROVIDED AS IS, WITH ALL FAULTS AND WITHOUT WARRANTY OF ANY KIND, EXPRESS, IMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, WARRANTIES OF TITLE OR NON-INFRINGEMENT. CUSTOMER MAY HAVE OTHER STATUTORY RIGHTS, HOWEVER, THE DURATION OF STATUTORILY REQUIRED WARRANTIES, IF ANY, WILL BE LIMITED TO THE FULLEST EXTENT PERMITTED BY LAW. 6. Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, IN NO EVENT WILL NEW RELIC BE LIABLE FOR ANY DIRECT, INDIRECT, SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES OF ANY KIND, REGARDLESS OF THE FORM OF ACTION, WHETHER IN CONTRACT, TORT (INCLUDING NEGLIGENCE), STRICT LIABILITY OR OTHERWISE, EVEN IF INFORMED OF THE POSSIBILITY OF SUCH DAMAGES IN ADVANCE. TO THE EXTENT ANY OF THE ABOVE LIMITATIONS ARE NOT ENFORCEABLE AT APPLICABLE LAW, NEW RELICS ENTIRE LIABILITY TO CUSTOMER UNDER THESE TERMS WILL NOT EXCEED $50. THESE LIMITATIONS ON LIABILITY ARE A FUNDAMENTAL BASIS OF THE BARGAIN AND NEW RELIC WOULD NOT BE ABLE TO PROVIDE THE SOFTWARE WITHOUT SUCH LIMITATIONS. THESE LIMITATIONS ON LIABILITY WILL APPLY NOTWITHSTANDING ANY FAILURE OF ESSENTIAL PURPOSE OF ANY LIMITED REMEDY. 7. Changes and Termination. At its discretion, without notice or liability to Customer, New Relic may (a) update, modify or discontinue the Software; (b) modify these Terms, effective upon posting; or (c) terminate or suspend these Terms or Customers access to the Software. If Customer does not agree with any modification, its sole remedy is to terminate its use of the Software. Upon any termination or suspension, Customer must stop using the Software. 8. Third Party Code. The Software may contain or be provided with third party code (including code which may be made available to Customer in source code form). A list of third party code and ownership, use, warranty and modification rights with respect to such code may be identified in the documentation or provided by New Relic upon Customers written request. New Relic is not responsible for applications and services not licensed by New Relic. 9. Export Compliance. Customer acknowledges that the Software is subject to export restrictions by the U.S. government and import restrictions by certain foreign governments. Customer shall not remove or export from the U.S. or allow the export or re-export of any part of the Software or any direct product thereof: (a) into (or to a national or resident of) any embargoed or terrorist-supporting country; (b) to anyone on the U.S. Commerce Departments Table of Denial Orders or U.S. Treasury Departments list of Specially Designated Nationals; (c) to any country to which such export or re-export is restricted or prohibited, or as to which the U.S. government or any agency thereof requires an export license or other governmental approval at the time of export or re-export without first obtaining such license or approval; or (d) otherwise in violation of any export or import restrictions, laws or regulations of any U.S. or foreign agency or authority. Customer agrees to the foregoing and warrants that Customer is not located in, under the control of, or a national or resident of any such prohibited country or on any such prohibited party list. 10. Government End-Users. The Software is commercial computer software. If Customer is an entity of the U.S. government, the use, duplication, reproduction, release, modification, disclosure or transfer of the Software, or any related documentation of any kind, is restricted by a license agreement or by these Terms in accordance with Federal Acquisition Regulation 12.212 for civilian purposes and Defense Federal Acquisition Regulation Supplement 227.7202 for military purposes. The Software was developed fully at private expense. All other use is prohibited. 11. General. These Terms will be governed by and construed under the laws of the State of California and the U.S. without regard to conflicts of law provisions thereof, and without regard to the United Nations Convention on the International Sale of Goods. The jurisdiction and venue for actions arising out of or relating to these Terms shall be in the state and federal courts in San Francisco, California. The parties are independent contractors. Customer may not assign these Terms without New Relics prior written consent and any attempt to do so will be void; New Relic may assign these Terms freely to any party without Customers consent. If any provision of these Terms is held by a court of competent jurisdiction to be unenforceable or invalid for any reason, that provision shall be limited to the minimum extent necessary so that these Terms shall otherwise remain in effect. These Terms are the entire agreement between the parties relating to the Software, and supersede all prior or contemporaneous agreements (oral or written) relating to the Software. Any separate agreement Customer has for New Relic Products does not apply to the Software. Open-source licenses We love open-source software, and use the following in New Relic Diagnostics. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License clbanning/mxj MIT go-yaml/yaml Apache 2.0 StackExchange/wmi MIT go-ole/go-ole MIT shirou/gopsutil BSD-3-Clause shirou/w32 BSD-3-Clause cheggaaa/pb BSD-3-Clause google/uuid BSD-3-Clause",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 202.37134,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Diagnostics <em>licenses</em>",
        "sections": "New Relic Diagnostics <em>licenses</em>",
        "tags": "<em>Special</em> <em>services</em> <em>licenses</em>",
        "body": ") use the Software to develop <em>services</em> or products for sale or include any components of the Software in any product; (d) remove any product identification, proprietary, copyright or other notices in the Software; or (e) publicly disseminate performance <em>information</em> or analysis (including benchmarks"
      },
      "id": "604505ad28ccbc457e2c60b4"
    }
  ],
  "/docs/licenses/license-information/usage-plans/new-relic-one-usage-plan-descriptions": [
    {
      "sections": [
        "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
        "Important",
        "Usage Plan: New Relic Platform Pricing",
        "Eligible Services",
        "Eligible Services (Partners)",
        "Subscriptions with indeterminate pricing or usage quantities",
        "Product Usage Ratio"
      ],
      "title": "Product-based pricing usage and New Relic Platform Pricing Usage Plan",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "e2686dc773c4e844544ce633a9a41a16f15edf5a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan/",
      "published_at": "2021-06-26T04:40:08Z",
      "updated_at": "2021-03-16T04:22:57Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important This doc explains our original product-based pricing plan. For more about pricing changes, see Overview of pricing changes. The following provisions are applicable to existing customers (i) that have existing Terms, and (ii) that to the extent its subscription to the New Relic products reference the usage plans set forth below or where a subscription has indeterminate product pricing or usage quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. Usage Plan: New Relic Platform Pricing The following Eligible Services with New Relic are products that are referenced in specific order forms for New Relic Platform Pricing. For more information, contact your New Relic account representative. Eligible Services Eligible Services Per Unit Unit of Measure Monthly Standard Fee Rate New Relic APM Pro CU Annual* 10,000 Compute Units $166.70 New Relic APM Pro Host Annual* 1 Hosts $149 New Relic Insights Pro Annual 50,000,000 Events $165 New Relic Infrastructure Pro Annual 10,000 Compute Units $12 New Relic Browser Pro Annual 500,000 Page Views $149 New Relic Logs Annual - 8 Days* * 1 Per GB Daily $55 New Relic Logs Annual - 15 Days* * 1 Per GB Daily $65 New Relic Logs Annual - 30 Days* * 1 Per GB Daily $75 New Relic Metrics Annual 1,000 Data Points per Minute $25 New Relic Mobile Enterprise Annual 50,000 Total Users $499 New Relic Serverless for AWS Lambda Annual 1,000,000 AWS Lambda Events $15 New Relic Synthetics Pro Annual 10,000 Checks $69 New Relic Synthetics private locations (for New Relic Synthetics Pro Annual product) Fixed fee N/A $1,000 New Relic Traces Annual 1,000,000 Spans $1 New Relic AI Incident Intelligence Annual 1,000 Incident Events $500 New Relic AI Proactive Detection Annual 1,000,000,000 App Transactions $250 * Customer may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. For more information about units of measures, see Product definitions. Eligible Services (Partners) Eligible services for New Relic Partners Measurement New Relic APM Pro CU Annual* Sold in Units of 8,900 Compute Units New Relic APM Pro Host Annual* Sold in Units of 1 Host New Relic Browser Pro Annual Sold in Units of 500,000 Page Views New Relic Infrastructure Pro Annual Sold in Units of 125,000 Compute Units New Relic Insights Pro Annual Sold in Units of 45,000,000 Events New Relic Logs Annual - 8 Days* * Sold in Units of 3 GB Daily New Relic Logs Annual - 15 Days* * Sold in Units of 2.5 GB Daily New Relic Logs Annual - 30 Days* * Sold in Units of 2 GB Daily New Relic Metrics Annual Sold in Units of 6,000 Data Points per Minute New Relic Mobile Enterprise Annual Sold in Units of 15,000 Total Users New Relic Serverless for AWS Lambda Annual Sold in Units of 10,000,000 AWS Lambda Events New Relic Synthetics Pro with Private Locations Annual Sold in Units of 22,000 Checks New Relic Traces Annual Sold in Units of 149,000,000 Spans * Customer and/or Partner may utilize either New Relic APM Pro Annual or New Relic APM Pro CUs Annual, but not both. Customer and/or Partner will be provisioned New Relic APM Pro CUs Annual by default, unless otherwise stated in the relevant Order Form. * * Customer and/or Partner may utilize only one of New Relic Logs Annual - 8 Days, New Relic Logs Annual - 15 Days, or New Relic Logs Annual - 30 days. For the avoidance of doubt, Customer may not utilize New Relic Logs Annual with varying numbers of days retention concurrently. Customer and/or Partner will be provisioned New Relic Logs Annual - 30 Days by default, unless otherwise stated in the relevant Order Form. Subscriptions with indeterminate pricing or usage quantities Product Usage Ratio Where a Customer subscription to the Products contain no specific pricing or quantities, Product-specific usage for a specific calendar month shall be determined through the calculation of the following ratio (the Product Usage Ratio) where: (1) the numerator shall be the Product specific usage for such calendar month multiplied by such Products list price, and (2) the denominator shall be the the aggregate Customer usage of all Products during such calendar month multiplied by all such Product(s) list price. The Product Usage Ratio shall be the percentage of usage for a specific Product for a calendar month period, or if usage cannot be measured for a period, the last Product Usage Ratio that can be calculated shall be assumed constant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 196.24373,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Product-based pricing <em>usage</em> and New Relic Platform Pricing <em>Usage</em> <em>Plan</em>",
        "sections": "Product-based pricing <em>usage</em> and New Relic Platform Pricing <em>Usage</em> <em>Plan</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " reference the <em>usage</em> <em>plans</em> set forth below or where a subscription has indeterminate product pricing or <em>usage</em> quantities. New customers are eligible for New Relic One pricing as described here. Capitalized terms not defined below shall take on the meaning set forth in such New Relic order form. <em>Usage</em>"
      },
      "id": "603ea32a28ccbc7e22eba768"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-06-26T04:35:29Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customers websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated Documentation page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customers Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relics customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 121.7063,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Usage</em> <em>Plan</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https:&#x2F;&#x2F;newrelic.com&#x2F;termsandconditions&#x2F;unpaid. <em>Usage</em> <em>Plan</em> <em>Usage</em> <em>Plan</em> refers to the Service or Product"
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-06-25T22:53:06Z",
      "updated_at": "2021-05-22T17:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into the Insights event database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 121.70594,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing <em>plan</em> terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app"
      },
      "id": "603ebacc64441f77774e8872"
    }
  ],
  "/docs/licenses/license-information/usage-plans/product-based-pricing-usage-new-relic-platform-pricing-usage-plan": [
    {
      "sections": [
        "New Relic One Usage plan descriptions",
        "Pay As You Go",
        "Annual Pool of Funds",
        "Applicable Invoicing and Order Terms",
        "User Accounts",
        "New Relic One Pro and Enterprise Service Level Availability Commitment",
        "New Relic One Pro and Enterprise Support Plans",
        "Free tier, lite, no-charge, preview access, New Relic One - Data, New Relic One - Standard User subscriptions"
      ],
      "title": "New Relic One Usage plan descriptions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Usage plans"
      ],
      "external_id": "c18e1c6c294914c28bba48f9de025333210ed254",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/usage-plans/new-relic-one-usage-plan-descriptions/",
      "published_at": "2021-06-26T04:40:09Z",
      "updated_at": "2021-03-13T04:19:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Usage Plan applies to (i) your Pay As You Go subscription, or (ii) your Commitment Term for the Annual Pool of Funds subscription. New Relic may modify the Usage Plan from time to time. Any changes to the Usage Plan will become effective immediately for changes that provide a benefit or right to the Customer, all other changes will become effective if Customer assents or upon any new or renewal Commitment Term. Usage Plan - Effective as February 19, 2021: The Order and Usage Plan may contain defined terms that are denoted by capitalization. In the event that a capitalized term is not defined in either the Order or the Usage Plan, such terms shall have the meaning set forth in the New Relic One pricing definitions page. Pay As You Go By electing and subscribing to the Pay As You Go subscription model (Pay As You Go or PAYG), Customer commits to paying for the New Relic Products on a month-to-month consumption basis. Monthly Product Usage will be invoiced regardless if a PO is required or not. Any Customer dispute to Monthly Product Usage from the prior month must be in good-faith and received by New Relic in writing within three (3) business days of the start of the next month or such dispute notice will be considered invalid. The dispute notice will set forth in reasonable detail the information concerning the disputed charges. The parties will use good-faith efforts to promptly resolve any disputed charges. Customers usage of the Products in excess of the Free Tier each month shall be billed in arrears on the first business day of the following month based on the Customers Per Unit usage of each Product each month multiplied by the corresponding rates set forth in an Order and summed (Monthly Product Usage). Annual Pool of Funds By electing and subscribing to the Annual Pool of Funds subscription model (Annual Pool of Funds or APoF) for the Commitment Term, Customer commits in an Order to: (i) paying the Commitment Fee amounts described in the Subscription table and any additional commitment fees set forth; and (ii) the Monthly Discounted Fee Rates applying to Customers Monthly Product Usage. New Relic will invoice the Commitment Fee as per the Billing Terms described in an Order. On a monthly cadence during the Commitment Term, Customers Per Unit usage of the Products will be multiplied by the corresponding Monthly Discounted Rate and summed (Monthly Product Usage). Monthly Product Usage will be deducted from the Commitment Fee amounts that are paid in advance. If Monthly Product Usage exceeds any such remaining unconsumed amounts, Customer will be invoiced for the difference (Additional Usage), including for any Monthly Product Usage during the last month of the Commitment Term. Payment of such invoices will be governed as set forth in the Terms. Any Customer dispute to Monthly Product Usage from the prior month must be in good-faith and received by New Relic in writing within three (3) business days of the start of the next month (including for any Monthly Product Usage for the last month of the Commitment Term) or such dispute notice will be considered invalid. The dispute notice will set forth in reasonable detail the information concerning the disputed charges. The parties will use good-faith efforts to promptly resolve any disputed charges. Any unconsumed balances from the Customer payment of each annual Commitment Fee and any additional payments, if applicable, as set out in an Order will expire and lapse at the end of each year of the Commitment Term. Applicable Invoicing and Order Terms All amounts stated in an Order are non-cancelable payment obligations of the Customer for the Commitment Term regardless of usage. Any fees paid are non-refundable and do not represent a deposit for, or a credit towards, the purchase of other products not specified in an Order or for any purchase after the Commitment Term. Customer acknowledges that a final payment for the full outstanding amount of any remaining unpaid Commitment Fee and/or Monthly Product Usage Fee may be invoiced upon each anniversary date of the term start date. Tax will be added where applicable. New Relic may review Customer's use of the Products at any time. If New Relic identifies any Customer usage of the Product(s) that is not in accordance with the Terms or Documentation, New Relic may suspend such unauthorized usage. All existing purchases and related pricing in effect prior to the execution of an Order shall remain in force. Unless otherwise stated in an Order, an Order does not modify or amend any existing purchases. Additional future products or quantities are not subject to promotional pricing unless otherwise stated in an Order. In the event that a Customer indicates in an Order that it requires a purchase order (PO) for its subscription, Customer agrees to provide the required PO prior to the provisioning of the Products. If a Customer does not indicate a PO is required, Customer agrees that New Relic may issue invoice(s) and is entitled to such payment without a PO reference. All Additional Usage fees will be invoiced regardless of a PO requirement. The Product(s) are deemed accepted upon its provisioning. User Accounts Use of the Products require Customer users to create Login Credentials. Customer user Login Credential information must be accurate, current, and complete. New Relics use and collection of Login Credentials (in accordance with its General Data Privacy Notice) is for account and product management and support of its customers. Customer and Customer users must abide by the New Relic Acceptable Use Policy (AUP) and Login Credentials may not be shared. Each Customer user must have their own user account. Entry into an Order indicates your agreement that the amount of provisioned users (at the rate specified in the Order) applies in lieu of and supersedes any other amount of users of the Products that may be specified in the agreement between Customer and New Relic. New Relic One Pro and Enterprise Service Level Availability Commitment With a subscription to New Relic Full Stack Observability Pro or Enterprise Products, you agree that during the Commitment Term the applicable service level availability commitment set forth on the Service level availability commitment page in the Documentation shall apply to the Products. For clarity, if your agreement with New Relic contains a different service level availability commitment or remedies, the above does not apply to your subscription to the New Relic Full Stack Observability Pro or Enterprise Products. If you subscribe to any other New Relic Products with New Relic One pricing, any service level availability commitment or related remedies contained within your agreement with New Relic are vacated and nullified, and New Relic will use commercially reasonable efforts to make New Relic Full Stack Observability Standard and Telemetry Data Platform available in line with industry standards. New Relic One Pro and Enterprise Support Plans With a subscription to New Relic One Users (Full Stack Observability), New Relic provides an updated support plan commitment. By subscribing to New Relic One Users (Full Stack Observability), you agree that during the Commitment Term the applicable Support Plan set forth on the Support plan page in the Documentation shall apply in lieu of, and supersedes and replaces, any other support related commitments that may be contained within your agreement with New Relic. For New Relic K.K. customers (Japan), the above does not currently apply to the support offerings provided by New Relic to you. Free tier, lite, no-charge, preview access, New Relic One - Data, New Relic One - Standard User subscriptions If you are using New Relics Products in the free tier only, or on a no-charge, or a lite or preview access basis you agree that the Unpaid Terms of Service will apply to such Product usage and replace and supersede any other terms. In addition, if your subscription contains the New Relic One - Standard User (Full Stack Observability Standard) Product, you agree that the Paid Terms of Service will apply to your subscription to the Products set forth in an Order and replace and supersede any other terms.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.80214,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic One <em>Usage</em> <em>plan</em> descriptions",
        "sections": "New Relic One <em>Usage</em> <em>plan</em> descriptions",
        "tags": "<em>License</em> <em>information</em>",
        "body": "The <em>Usage</em> <em>Plan</em> applies to (i) your Pay As You Go subscription, or (ii) your Commitment Term for the Annual Pool of Funds subscription. New Relic may modify the <em>Usage</em> <em>Plan</em> from time to time. Any changes to the <em>Usage</em> <em>Plan</em> will become effective immediately for changes that provide a benefit or right"
      },
      "id": "6044e74ee7b9d2a4515799c8"
    },
    {
      "sections": [
        "New Relic One pricing: Definitions",
        "Account",
        "Commitment Term",
        "Customer Data",
        "Customer Properties",
        "Documentation",
        "GB Ingested",
        "Incident event",
        "Login Credentials",
        "Monthly Provisioned User",
        "Order",
        "Paid Terms of Service",
        "Product(s)",
        "Software",
        "Terms",
        "Third-Party Services",
        "Unpaid Terms of Service",
        "Usage Plan"
      ],
      "title": "New Relic One pricing: Definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "609575acd671fecf7899378157eabc57bc8d68e2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/new-relic-one-pricing-definitions/",
      "published_at": "2021-06-26T04:35:29Z",
      "updated_at": "2021-05-22T17:25:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our New Relic One pricing plan (for a glossary for our original pricing, see Original pricing definitions. Account Account refers to the online account or subaccounts that New Relic provides for customers to manage their use of the Products. Commitment Term Commitment Term means the non-cancelable, committed Subscription Term for the Products. Customer Data Customer Data means the data, information, or content that Customer and its users send to an Account from the Software, the Customer Properties, or Third-Party Services. Customer Properties Customer Properties means Customers websites, infrastructure, networks, mobile applications, or other systems, as well as Customer accounts on Third-Party Services. Documentation Documentation means the New Relic technical guides and documentation made available from the dedicated Documentation page of the New Relic website. GB Ingested A GB Ingested is a measurement of the volume of metrics, events, logs, traces, or other telemetry data sent to or generated by the Products for the benefit of the Customer, including from the Software, the Customer Properties, or Third-Party Services. In this context, a GB is defined as 1 billion bytes. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Login Credentials Login Credentials means the username, email address, password, or other personal information that is provided by a Customer user in order to manage an Account. Monthly Provisioned User A Monthly Provisioned User is any user who can log into Customers Account(s) and access the New Relic One Product functionality as specified in an Order and the Documentation. In our public docs, this is referred to as a full user. Order Order means the purchasing order for access to the Service or related services that: (1) is either executed by the Parties or entered into by you via self-service, and references this Agreement, or (2) is entered into by you and a Channel Partner. Paid Terms of Service Paid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/paid. Product(s) Product(s) mean the purchase of the New Relic subscription products described in the applicable Order and any updates, corrections, bug fixes, modifications, improvements, related services, new features, and functionality (made generally available to New Relics customer base) thereto. Software Software means the distributed software, APIs, scripts, or other code proprietary to New Relic provided with the Products. Terms Terms means the underlying Customer-New Relic agreement and the Order. Third-Party Services Third-Party Services means any third party platform, add-on, service, or product not provided by New Relic and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https://newrelic.com/termsandconditions/unpaid. Usage Plan Usage Plan refers to the Service or Product pricing, invoicing related information, and product-specific terms (e.g. concurrent user account sessions) contained within the Documentation. To learn more about this pricing plan, see New Relic One pricing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 121.70629,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Usage</em> <em>Plan</em>",
        "tags": "<em>License</em> <em>information</em>",
        "body": " and that a user integrates or enables for use with the Products, including third-party applications and plug-ins. Unpaid Terms of Service Unpaid Terms of Service means the legal terms and conditions located at: https:&#x2F;&#x2F;newrelic.com&#x2F;termsandconditions&#x2F;unpaid. <em>Usage</em> <em>Plan</em> <em>Usage</em> <em>Plan</em> refers to the Service or Product"
      },
      "id": "6044e6e528ccbc26f22c6084"
    },
    {
      "sections": [
        "Original product-based pricing definitions",
        "App",
        "App transaction",
        "AWS Lambda event",
        "Check",
        "Compute unit",
        "Datapoints per minute",
        "Event",
        "Host",
        "Incident event",
        "Page view",
        "Per GB daily",
        "Span",
        "User"
      ],
      "title": "Original product-based pricing definitions",
      "type": "docs",
      "tags": [
        "Licenses",
        "License information",
        "Product definitions"
      ],
      "external_id": "42087e53167736831855bf9a4c2967c465677b45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/license-information/product-definitions/legacy-product-definitions/",
      "published_at": "2021-06-25T22:53:06Z",
      "updated_at": "2021-05-22T17:24:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing plan terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app includes a predefined number of users. App transaction An app transaction is an APM application's attempt to process a web or non-web request. In APM these manifest as throughput TIMESERIES or individually as events in the Transaction event type. AWS Lambda event An AWS Lambda event means the row of data collected from the customer's AWS Lambda function by the New Relic agent or sent from an external service into the New Relic platform. It consists of the AwsLambdaInvocation, AwsLambdaInvocationError, or custom event types. Check A check means the single instance of a Synthetics monitor running in New Relic's monitoring network and reporting back response time, and whether the check was a success or failure. Compute unit A compute unit means the measure of resources associated with a unit of computation on a physical or virtual host. Datapoints per minute Datapoints per minute (DPM) refers to the per-minute rate at which individual metric values are sent to the New Relic Metric Ingest API. For billing purposes, datapoints per minute are calculated as a monthly average value by summing the datapoints ingested during a 30 day period and dividing by the number of minutes in that period (43,200). Event An event means the row of data collected from the customer's application by the New Relic agent or sent from an external service into the Insights event database. Host A host means the physical computer or virtual machine instance running a single copy of an operating system. Host usage is tracked monthly by summing the hours that every host in the account is connected to New Relic and dividing by 750. A host is counted if it is connected any time during an hour. Incident event An incident event is an alerting event (open, closed, etc.) created by an alerting engine that is sent into the New Relic Applied Intelligence platform (non-unique) for de-duplication, flapping detection, smart suppression, enrichment, and correlation. Page view A page view means the full page load (triggering an onLoad event) or a recorded URL change (state change). Per GB daily Per GB daily represents a daily average of Log data sent to New Relic over a 30 day period. Span A span represents an operation summary collected from the customer's application via the New Relic APM agent or New Relic Serverless for AWS Lambda agent, or sent from other tracing tools to the New Relic Trace API. User A user means the individual that connects to your app from a single device. Each unique device is considered as a unique user.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 121.70593,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>License</em> <em>information</em>",
        "body": "This is a glossary of terms that appear in contracts for our original product-based pricing. For New Relic One pricing <em>plan</em> terms, see New Relic One pricing definitions. App An app means the application software designed to run on smartphones, tablet computers, and other mobile devices. Each app"
      },
      "id": "603ebacc64441f77774e8872"
    }
  ],
  "/docs/licenses/product-or-service-licenses/miscellaneous/help-center-documentation-licenses": [
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-06-25T22:44:37Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0  2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0  2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0  2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0  2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0  2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright  2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright  1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright  1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright  2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright  1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright  1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 152.58481,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    },
    {
      "sections": [
        "New Relic APM licenses"
      ],
      "title": "New Relic APM licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic APM"
      ],
      "external_id": "e08100d5a48d87c7769f401957826f627bf29779",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm/new-relic-apm-licenses/",
      "published_at": "2021-06-26T04:46:32Z",
      "updated_at": "2021-03-30T21:22:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view licenses for... See... APM agents docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-apm",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 142.66057,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic APM <em>licenses</em>",
        "sections": "New Relic APM <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We use many third-party libraries and tools to create New Relic APM, including a large number contributed by the open source community. To view <em>licenses</em> for... See... APM agents docs.newrelic.com&#x2F;docs&#x2F;<em>licenses</em>&#x2F;<em>product</em>-or-<em>service</em>-<em>licenses</em>&#x2F;new-relic-apm"
      },
      "id": "603e7895e7b9d24e832a07d5"
    },
    {
      "sections": [
        "New Relic Insights licenses"
      ],
      "title": "New Relic Insights licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Insights"
      ],
      "external_id": "3d0f8f4275af8bcf9f6f3ffb02a2e46ff3fc374f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-insights/new-relic-insights-licenses/",
      "published_at": "2021-06-25T23:26:28Z",
      "updated_at": "2021-03-13T04:04:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in New Relic Insights. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Product Licenses New Relic Insights See insights.newrelic.com/licenses",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 139.80205,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "New Relic Insights <em>licenses</em>",
        "sections": "New Relic Insights <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in New Relic Insights. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. <em>Product</em> <em>Licenses</em> New Relic Insights See insights.newrelic.com&#x2F;<em>licenses</em>"
      },
      "id": "6044e84428ccbcf6fa2c60c1"
    }
  ],
  "/docs/licenses/product-or-service-licenses/mobile-app-licenses/android-application-licenses": [
    {
      "sections": [
        "iOS application licenses"
      ],
      "title": "iOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "a6df56e363112e7387e6887f04381a36a5457e84",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/ios-application-licenses/",
      "published_at": "2021-06-26T04:41:09Z",
      "updated_at": "2021-05-05T16:26:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic iOS app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright  2011-2020 Alamofire Software Foundation http://alamofire.org/ Ace BSD Copyright  2010, Ajax.org B.V. ActiveLabel MIT Copyright  2015 Optonaut Alamofire MIT Copyright  2014-2018 Alamofire Software Foundation AlamofireObjectMapper MIT Copyright  2015 Tristan Himmelman Analytics MIT Copyright  2016 Segment.io, Inc. BBlock MIT Copyright  2012 David Keegan BigNumber MIT Copyright  2019 mkrd CDMarkdownKit MIT Copyright  2016-2017 Christopher de Haan contact@christopherdehaan.me ECSlidingViewController MIT Copyright  2013 EdgeCase LoginManagerSDK New Relic License Copyright  2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright  GitHub, Inc. All rights reserved. Copyright  2012, Bitswift, Inc MetricMetadata New Relic License Copyright  2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License Copyright  2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright  2016 Vinh Nguyen NewRelicAgent New Relic License Copyright  2010-2021 New Relic, Inc. All rights reserved. ObjectMapper MIT Copyright  2014 Hearst RadarKit New Relic License  2010-2021 New Relic, Inc. All rights reserved. SSPullToRefresh MIT Copyright  2012-2014 Sam Soffes, http://soff.es UICKeyChainStore MIT Copyright  2011 kishikawa katsumi WidgetLibrary New Relic License  2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright  2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright  2016 JP Simard. jsTimezoneDetect MIT Copyright  2012 Jon Nylander, project maintained at bitbucket.org lodash MIT Copyright  JS Foundation and other contributors js.foundation The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.43608,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "iOS <em>application</em> <em>licenses</em>",
        "sections": "iOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic iOS <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "603e9db1196a670e70a83df3"
    },
    {
      "sections": [
        "tvOS application licenses"
      ],
      "title": "tvOS application licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "Mobile app licenses"
      ],
      "external_id": "0b3ac8ec42cef00f5a4d3ddf354e4be38ad0595f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/mobile-app-licenses/tvos-application-licenses/",
      "published_at": "2021-06-26T04:42:14Z",
      "updated_at": "2021-05-05T16:26:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic for TV app. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. Library License Copyright AFNetworking MIT Copyright  2011-2020 Alamofire Software Foundation (http://alamofire.org/) Analytics MIT Copyright  2016 Segment.io, Inc. CDMarkdownKit MIT Copyright  2016-2017 Christopher de Haan contact@christopherdehaan.me LoginManagerSDK New Relic License  2010-2021 New Relic, Inc. All rights reserved. Mantle MIT Copyright  GitHub, Inc. All rights reserved. Copyright  2012, Bitswift, Inc MetricMetadata New Relic License  2010-2021 New Relic, Inc. All rights reserved. NRCharts New Relic License  2010-2021 New Relic, Inc. All rights reserved. NVActivityIndicatorView MIT Copyright  2016 Vinh Nguyen NewRelicAgent New Relic License  2010-2021 New Relic, Inc. All rights reserved. UICKeyChainStore MIT Copyright  2011 kishikawa katsumi WidgetLibrary New Relic License  2010-2021 New Relic, Inc. All rights reserved. XYPieChart MIT Copyright  2012 Xiaoyang Feng, XYStudio.cc Yams MIT Copyright  2016 JP Simard. iOS-fontawesome CC BY 3.0 & MIT Copyright  2012 Alex Usbergo. All rights reserved. The remainder of the code is covered by the New Relic License agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 218.43607,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "tvOS <em>application</em> <em>licenses</em>",
        "sections": "tvOS <em>application</em> <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic for TV <em>app</em>. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. Library License Copyright"
      },
      "id": "6072d619196a6795b664a75c"
    },
    {
      "sections": [
        "Logs plugin licenses",
        "Plugins for Logs",
        "AWS CloudWatch",
        "Fluentd",
        "Fluent Bit",
        "Kubernetes",
        "Logstash",
        "Go plugins for Logs",
        "Logrus 1.4.0",
        "Java plugins for Logs",
        "Apache Log4j 1.x",
        "Apache Log4j 2.x",
        "Dropwizard 1.3",
        "Logback 1.2"
      ],
      "title": "Logs plugin licenses",
      "type": "docs",
      "tags": [
        "Licenses",
        "Product or service licenses",
        "New Relic Logs"
      ],
      "external_id": "994019d539d8db05675ae7b7e6e48caba02bdd45",
      "image": "",
      "url": "https://docs.newrelic.com/docs/licenses/product-or-service-licenses/new-relic-logs/logs-plugin-licenses/",
      "published_at": "2021-06-25T22:44:37Z",
      "updated_at": "2021-05-05T16:28:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software licenses, and in that case we have listed the license we've chosen to use. For a list of the licenses used for New Relic Logs, see Logs licenses. Plugins for Logs The following licenses are for the plugins used to connects your log data with New Relic Logs. AWS CloudWatch Library License Copyright AWS CloudWatch Apache License 2.0  2019 New Relic, Inc. Fluentd Library License Copyright Fluentd Apache License 2.0  2019 New Relic, Inc. Fluent Bit Library License Copyright Fluent Bit Apache License 2.0  2019 New Relic, Inc. Kubernetes Library License Copyright Kubernetes Apache License 2.0  2019 New Relic, Inc. Logstash Library License Copyright Logstash Apache License 2.0  2019 New Relic, Inc. Go plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Go agent. For Go licenses, see Go agent licenses. Logrus 1.4.0 Library License Copyright Logrus MIT Copyright  2014 Simon Eskildsen Java plugins for Logs The following licenses are for the plugins used link your logs and APM data using New Relic's Java agent. For Java licenses, see Java agent licenses. Apache Log4j 1.x Library License Copyright Apache Log4j 1 Apache License 2.0 Copyright  1999-2005 The Apache Software Foundation Apache Log4j 2.x Library License Copyright Apache Log4j 2 Apache License 2.0 Copyright  1999-2005 The Apache Software Foundation Dropwizard 1.3 Library License Copyright Dropwizard Apache License 2.0 Copyright  2010-2013 Coda Hale and Yammer, Inc., 2014-2016 Dropwizard Team Logback 1.2 Library License Copyright Logback EPL v1.0 Copyright  1999-2017, QOS.ch. All rights reserved. Logback LGPL 2.1 Copyright  1999-2017, QOS.ch. All rights reserved. The remainder of the code is covered by the New Relic license agreement.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 156.14066,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Logs plugin <em>licenses</em>",
        "sections": "Logs plugin <em>licenses</em>",
        "tags": "<em>Product</em> <em>or</em> <em>service</em> <em>licenses</em>",
        "body": "We love open-source software, and use the following in the New Relic Logs plugins. Thank you, open-source community, for making these fine tools! Some of these are listed under multiple software <em>licenses</em>, and in that case we have listed the license we&#x27;ve chosen to use. For a list of the <em>licenses</em>"
      },
      "id": "603ea5b628ccbcc9c6eba76d"
    }
  ]
}