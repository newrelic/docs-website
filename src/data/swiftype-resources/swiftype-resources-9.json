{
  "/docs/correlate-load-balancer-application-metrics-workloads": [
    {
      "sections": [
        "Integrations and managed policies",
        "Recommended policy",
        "Important",
        "Optional policy",
        "Option 1: Use our CloudFormation template",
        "CloudFormation template",
        "Option 2: Manually add permissions",
        "Required by all integrations",
        "ALB permissions",
        "API Gateway permissions",
        "Auto Scaling permissions",
        "Billing permissions",
        "Cloudfront permissions",
        "CloudTrail permissions",
        "DynamoDB permissions",
        "EBS permissions",
        "EC2 permissions",
        "ECS/ECR permissions",
        "EFS permissions",
        "ElastiCache permissions",
        "ElasticSearch permissions",
        "Elastic Beanstalk permissions",
        "ELB permissions",
        "EMR permissions",
        "Health permissions",
        "IAM permissions",
        "IoT permissions",
        "Kinesis Firehose permissions",
        "Kinesis Streams permissions",
        "Lambda permissions",
        "RDS, RDS Enhanced Monitoring permissions",
        "Redshift permissions",
        "Route 53 permissions",
        "S3 permissions",
        "Simple Email Service (SES) permissions",
        "SNS permissions",
        "SQS permissions",
        "Trusted Advisor permissions",
        "VPC permissions",
        "X-Ray monitoring permissions"
      ],
      "title": "Integrations and managed policies",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "Get started"
      ],
      "external_id": "80e215e7b2ba382de1b7ea758ee1b1f0a1e3c7df",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/get-started/integrations-managed-policies/",
      "published_at": "2021-05-04T18:30:29Z",
      "updated_at": "2021-05-04T18:30:28Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In order to use infrastructure integrations, you need to grant New Relic permission to read the relevant data from your account. Amazon Web Services (AWS) uses managed policies to grant these permissions. Recommended policy Important Recommendation: Grant an account-wide ReadOnlyAccess managed policy from AWS. AWS automatically updates this policy when new services are added or existing services are modified. New Relic infrastructure integrations have been designed to function with ReadOnlyAccess policies. For instructions, see Connect AWS integrations to infrastructure. Exception: The Trusted Advisor integration is not covered by the ReadOnlyAccess policy. It requires the additional AWSSupportAccess managed policy. This is also the only integration that requires full access permissions (support:*) in order to correctly operate. We notified Amazon about this limitation. Once it's resolved we'll update documentation with more specific permissions required for this integration. Optional policy If you cannot use the ReadOnlyAccess managed policy from AWS, you can create your own customized policy based on the list of permissions. This allows you to specify the optimal permissions required to fetch data from AWS for each integration. While this option is available, it is not recommended because it must be manually updated when you add or modify your integrations. Important New Relic has no way of identifying problems related to custom permissions. If you choose to create a custom policy, it is your responsibility to maintain it and ensure proper data is being collected. There are two ways to set up your customized policy: You can either use our CloudFormation template, or create own yourself by adding the permissions you need. Option 1: Use our CloudFormation template Our CloudFormation template contains all the permissions for all our AWS integrations. A user different than root can be used in the managed policy. CloudFormation template AWSTemplateFormatVersion: 2010-09-09 Outputs: NewRelicRoleArn: Description: NewRelicRole to monitor AWS Lambda Value: !GetAtt - NewRelicIntegrationsTemplate - Arn Parameters: NewRelicAccountNumber: Type: String Description: The Newrelic account number to send data AllowedPattern: '[0-9]+' Resources: NewRelicIntegrationsTemplate: Type: 'AWS::IAM::Role' Properties: RoleName: !Sub NewRelicTemplateTest AssumeRolePolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Principal: AWS: !Sub 'arn:aws:iam::754728514883:root' Action: 'sts:AssumeRole' Condition: StringEquals: 'sts:ExternalId': !Ref NewRelicAccountNumber Policies: - PolicyName: NewRelicIntegrations PolicyDocument: Version: 2012-10-17 Statement: - Effect: Allow Action: - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticloadbalancing:DescribeTargetGroups' - 'elasticloadbalancing:DescribeTags' - 'elasticloadbalancing:DescribeLoadBalancerAttributes' - 'elasticloadbalancing:DescribeListeners' - 'elasticloadbalancing:DescribeRules' - 'elasticloadbalancing:DescribeTargetGroupAttributes' - 'elasticloadbalancing:DescribeInstanceHealth' - 'elasticloadbalancing:DescribeLoadBalancerPolicies' - 'elasticloadbalancing:DescribeLoadBalancerPolicyTypes' - 'apigateway:GET' - 'apigateway:HEAD' - 'apigateway:OPTIONS' - 'autoscaling:DescribeLaunchConfigurations' - 'autoscaling:DescribeAutoScalingGroups' - 'autoscaling:DescribePolicies' - 'autoscaling:DescribeTags' - 'autoscaling:DescribeAccountLimits' - 'budgets:ViewBilling' - 'budgets:ViewBudget' - 'cloudfront:ListDistributions' - 'cloudfront:ListStreamingDistributions' - 'cloudfront:ListTagsForResource' - 'cloudtrail:LookupEvents' - 'config:BatchGetResourceConfig' - 'config:ListDiscoveredResources' - 'dynamodb:DescribeLimits' - 'dynamodb:ListTables' - 'dynamodb:DescribeTable' - 'dynamodb:ListGlobalTables' - 'dynamodb:DescribeGlobalTable' - 'dynamodb:ListTagsOfResource' - 'ec2:DescribeVolumeStatus' - 'ec2:DescribeVolumes' - 'ec2:DescribeVolumeAttribute' - 'ec2:DescribeInstanceStatus' - 'ec2:DescribeInstances' - 'ec2:DescribeVpnConnections' - 'ecs:ListServices' - 'ecs:DescribeServices' - 'ecs:DescribeClusters' - 'ecs:ListClusters' - 'ecs:ListTagsForResource' - 'ecs:ListContainerInstances' - 'ecs:DescribeContainerInstances' - 'elasticfilesystem:DescribeMountTargets' - 'elasticfilesystem:DescribeFileSystems' - 'elasticache:DescribeCacheClusters' - 'elasticache:ListTagsForResource' - 'es:ListDomainNames' - 'es:DescribeElasticsearchDomain' - 'es:DescribeElasticsearchDomains' - 'es:ListTags' - 'elasticbeanstalk:DescribeEnvironments' - 'elasticbeanstalk:DescribeInstancesHealth' - 'elasticbeanstalk:DescribeConfigurationSettings' - 'elasticloadbalancing:DescribeLoadBalancers' - 'elasticmapreduce:ListInstances' - 'elasticmapreduce:ListClusters' - 'elasticmapreduce:DescribeCluster' - 'elasticmapreduce:ListInstanceGroups' - 'health:DescribeAffectedEntities' - 'health:DescribeEventDetails' - 'health:DescribeEvents' - 'iam:ListSAMLProviders' - 'iam:ListOpenIDConnectProviders' - 'iam:ListServerCertificates' - 'iam:GetAccountAuthorizationDetails' - 'iam:ListVirtualMFADevices' - 'iam:GetAccountSummary' - 'iot:ListTopicRules' - 'iot:GetTopicRule' - 'iot:ListThings' - 'firehose:DescribeDeliveryStream' - 'firehose:ListDeliveryStreams' - 'kinesis:ListStreams' - 'kinesis:DescribeStream' - 'kinesis:ListTagsForStream' - 'rds:ListTagsForResource' - 'rds:DescribeDBInstances' - 'rds:DescribeDBClusters' - 'redshift:DescribeClusters' - 'redshift:DescribeClusterParameters' - 'route53:ListHealthChecks' - 'route53:GetHostedZone' - 'route53:ListHostedZones' - 'route53:ListResourceRecordSets' - 'route53:ListTagsForResources' - 's3:GetLifecycleConfiguration' - 's3:GetBucketTagging' - 's3:ListAllMyBuckets' - 's3:GetBucketWebsite' - 's3:GetBucketLogging' - 's3:GetBucketCORS' - 's3:GetBucketVersioning' - 's3:GetBucketAcl' - 's3:GetBucketNotification' - 's3:GetBucketPolicy' - 's3:GetReplicationConfiguration' - 's3:GetMetricsConfiguration' - 's3:GetAccelerateConfiguration' - 's3:GetAnalyticsConfiguration' - 's3:GetBucketLocation' - 's3:GetBucketRequestPayment' - 's3:GetEncryptionConfiguration' - 's3:GetInventoryConfiguration' - 's3:GetIpConfiguration' - 'ses:ListConfigurationSets' - 'ses:GetSendQuota' - 'ses:DescribeConfigurationSet' - 'ses:ListReceiptFilters' - 'ses:ListReceiptRuleSets' - 'ses:DescribeReceiptRule' - 'ses:DescribeReceiptRuleSet' - 'sns:GetTopicAttributes' - 'sns:ListTopics' - 'sqs:ListQueues' - 'sqs:ListQueueTags' - 'sqs:GetQueueAttributes' - 'tag:GetResources' - 'ec2:DescribeInternetGateways' - 'ec2:DescribeVpcs' - 'ec2:DescribeNatGateways' - 'ec2:DescribeVpcEndpoints' - 'ec2:DescribeSubnets' - 'ec2:DescribeNetworkAcls' - 'ec2:DescribeVpcAttribute' - 'ec2:DescribeRouteTables' - 'ec2:DescribeSecurityGroups' - 'ec2:DescribeVpcPeeringConnections' - 'ec2:DescribeNetworkInterfaces' - 'lambda:GetAccountSettings' - 'lambda:ListFunctions' - 'lambda:ListAliases' - 'lambda:ListTags' - 'lambda:ListEventSourceMappings' - 'cloudwatch:GetMetricStatistics' - 'cloudwatch:ListMetrics' - 'cloudwatch:GetMetricData' - 'support:*' Resource: '*' Copy Option 2: Manually add permissions To create your own policy using available permissions: Add the permissions for all integrations. Add permissions that are specific to the integrations you need The following permissions are used by New Relic to retrieve data for specific AWS integrations: Required by all integrations Important If an integration is not listed on this page, these permissions are all you need. All integrations Permissions CloudWatch cloudwatch:GetMetricStatistics cloudwatch:ListMetrics cloudwatch:GetMetricData Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:DescribeLoadBalancers elasticloadbalancing:DescribeTargetGroups elasticloadbalancing:DescribeTags elasticloadbalancing:DescribeLoadBalancerAttributes elasticloadbalancing:DescribeListeners elasticloadbalancing:DescribeRules elasticloadbalancing:DescribeTargetGroupAttributes elasticloadbalancing:DescribeInstanceHealth elasticloadbalancing:DescribeLoadBalancerPolicies elasticloadbalancing:DescribeLoadBalancerPolicyTypes API Gateway permissions Additional API Gateway permissions: apigateway:GET apigateway:HEAD apigateway:OPTIONS Auto Scaling permissions Additional Auto Scaling permissions: autoscaling:DescribeLaunchConfigurations autoscaling:DescribeAutoScalingGroups autoscaling:DescribePolicies autoscaling:DescribeTags autoscaling:DescribeAccountLimits Billing permissions Additional Billing permissions: budgets:ViewBilling budgets:ViewBudget Cloudfront permissions Additional Cloudfront permissions: cloudfront:ListDistributions cloudfront:ListStreamingDistributions cloudfront:ListTagsForResource CloudTrail permissions Additional CloudTrail permissions: cloudtrail:LookupEvents DynamoDB permissions Additional DynamoDB permissions: dynamodb:DescribeLimits dynamodb:ListTables dynamodb:DescribeTable dynamodb:ListGlobalTables dynamodb:DescribeGlobalTable dynamodb:ListTagsOfResource EBS permissions Additional EBS permissions: ec2:DescribeVolumeStatus ec2:DescribeVolumes ec2:DescribeVolumeAttribute EC2 permissions Additional EC2 permissions: ec2:DescribeInstanceStatus ec2:DescribeInstances ECS/ECR permissions Additional ECS/ECR permissions: ecs:ListServices ecs:DescribeServices ecs:DescribeClusters ecs:ListClusters ecs:ListTagsForResource ecs:ListContainerInstances ecs:DescribeContainerInstances EFS permissions Additional EFS permissions: elasticfilesystem:DescribeMountTargets elasticfilesystem:DescribeFileSystems ElastiCache permissions Additional ElastiCache permissions: elasticache:DescribeCacheClusters elasticache:ListTagsForResource ElasticSearch permissions Additional ElasticSearch permissions: es:ListDomainNames es:DescribeElasticsearchDomain es:DescribeElasticsearchDomains es:ListTags Elastic Beanstalk permissions Additional Elastic Beanstalk permissions: elasticbeanstalk:DescribeEnvironments elasticbeanstalk:DescribeInstancesHealth elasticbeanstalk:DescribeConfigurationSettings ELB permissions Additional ELB permissions: elasticloadbalancing:DescribeLoadBalancers EMR permissions Additional EMR permissions: elasticmapreduce:ListInstances elasticmapreduce:ListClusters elasticmapreduce:DescribeCluster elasticmapreduce:ListInstanceGroups elasticmapreduce:ListInstanceFleets Health permissions Additional Health permissions: health:DescribeAffectedEntities health:DescribeEventDetails health:DescribeEvents IAM permissions Additional IAM permissions: iam:ListSAMLProviders iam:ListOpenIDConnectProviders iam:ListServerCertificates iam:GetAccountAuthorizationDetails iam:ListVirtualMFADevices iam:GetAccountSummary IoT permissions Additional IoT permissions: iot:ListTopicRules iot:GetTopicRule iot:ListThings Kinesis Firehose permissions Additional Kinesis Firehose permissions: firehose:DescribeDeliveryStream firehose:ListDeliveryStreams Kinesis Streams permissions Additional Kinesis Streams permissions: kinesis:ListStreams kinesis:DescribeStream kinesis:ListTagsForStream Lambda permissions Additional Lambda permissions: lambda:GetAccountSettings lambda:ListFunctions lambda:ListAliases lambda:ListTags lambda:ListEventSourceMappings RDS, RDS Enhanced Monitoring permissions Additional RDS and RDS Enhanced Monitoring permissions: rds:ListTagsForResource rds:DescribeDBInstances rds:DescribeDBClusters Redshift permissions Additional Redshift permissions: redshift:DescribeClusters redshift:DescribeClusterParameters Route 53 permissions Additional Route 53 permissions: route53:ListHealthChecks route53:GetHostedZone route53:ListHostedZones route53:ListResourceRecordSets route53:ListTagsForResources S3 permissions Additional S3 permissions: s3:GetLifecycleConfiguration s3:GetBucketTagging s3:ListAllMyBuckets s3:GetBucketWebsite s3:GetBucketLogging s3:GetBucketCORS s3:GetBucketVersioning s3:GetBucketAcl s3:GetBucketNotification s3:GetBucketPolicy s3:GetReplicationConfiguration s3:GetMetricsConfiguration s3:GetAccelerateConfiguration s3:GetAnalyticsConfiguration s3:GetBucketLocation s3:GetBucketRequestPayment s3:GetEncryptionConfiguration s3:GetInventoryConfiguration s3:GetIpConfiguration Simple Email Service (SES) permissions Additional SES permissions: ses:ListConfigurationSets ses:GetSendQuota ses:DescribeConfigurationSet ses:ListReceiptFilters ses:ListReceiptRuleSets ses:DescribeReceiptRule ses:DescribeReceiptRuleSet SNS permissions Additional SNS permissions: sns:GetTopicAttributes sns:ListTopics SQS permissions Additional SQS permissions: sqs:ListQueues sqs:GetQueueAttributes sqs:ListQueueTags Trusted Advisor permissions Additional Trusted Advisor permissions: support:* See also the note about the Trusted Advisor integration and recommended policies. VPC permissions Additional VPC permissions: ec2:DescribeInternetGateways ec2:DescribeVpcs ec2:DescribeNatGateways ec2:DescribeVpcEndpoints ec2:DescribeSubnets ec2:DescribeNetworkAcls ec2:DescribeVpcAttribute ec2:DescribeRouteTables ec2:DescribeSecurityGroups ec2:DescribeVpcPeeringConnections ec2:DescribeNetworkInterfaces ec2:DescribeVpnConnections X-Ray monitoring permissions Additional X-ray monitoring permissions: xray:BatchGet* xray:Get*",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 148.4406,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Integrations <em>and</em> managed policies",
        "sections": "Integrations <em>and</em> managed policies",
        "body": " Permissions CloudWatch cloudwatch:Get<em>Metric</em>Statistics cloudwatch:List<em>Metrics</em> cloudwatch:Get<em>Metric</em>Data Config API config:BatchGetResourceConfig config:ListDiscoveredResources Resource Tagging API tag:GetResources ALB permissions Additional ALB permissions: elasticloadbalancing:Describe<em>Load</em>"
      },
      "id": "6045079fe7b9d27db95799d9"
    },
    {
      "image": "https://docs.newrelic.com/static/775fe6d355fc6f5fd1793f286aa52c54/ae694/Dashboard.png",
      "url": "https://docs.newrelic.com/whats-new/2021/03/aws-cloudwatch/",
      "sections": [
        "Amazon CloudWatch Metric Streams",
        "More metrics, more often - fill gaps in your observability with Amazon CloudWatch Metric Streams and New Relic One"
      ],
      "published_at": "2021-05-04T17:11:28Z",
      "title": "Amazon CloudWatch Metric Streams",
      "updated_at": "2021-04-04T20:34:06Z",
      "type": "docs",
      "external_id": "004ba80238187bccbaa649145cc6dd58d7a90a42",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "More metrics, more often - fill gaps in your observability with Amazon CloudWatch Metric Streams and New Relic One More metrics more often means that you’ll receive Amazon CloudWatch data from all the AWS services you use as soon as they are available. Keep your applications running in the AWS Cloud humming. Less delay equals faster decisions. Correlate, analyze, visualize, and alert on the Amazon CloudWatch metrics for all of the AWS services your AWS cloud-based applications use. Two integration modes are available when you create a new AWS integration. Once you’ve completed setup, you can bring the data from AWS into dashboards right alongside the other New Relic One data available to you. The example below shows metrics from New Relic One’s Synthetic and Infrastructure Monitoring right next to the AWS Application Load Balancer and RDS database Metric Streams data. The Metric Streams data is stored in New Relic One’s Telemetry Data Platform as dimensional metrics of the summary type and can be queried using NRQL and used in Navigator and Lookout. This is the Navigator view grouped by AWS namespaces. Below is the Amazon CloudWatch Metric Streams status dashboard from Infrastructure -> AWS -> Account status dashboard Check out this short video walking through Amazon CloudWatch Metric Streams benefits. To get started: Navigate to Infrastructure Click AWS to set up your AWS integration today",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 143.66798,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Amazon CloudWatch <em>Metric</em> Streams",
        "sections": "More <em>metrics</em>, more often - fill gaps in your observability <em>with</em> Amazon CloudWatch <em>Metric</em> Streams <em>and</em> New Relic One",
        "body": " bring the data from AWS into dashboards right alongside the other New Relic One data available to you. The example below shows <em>metrics</em> from New Relic One’s Synthetic and Infrastructure Monitoring right next to the AWS <em>Application</em> <em>Load</em> <em>Balancer</em> and RDS database <em>Metric</em> Streams data. The <em>Metric</em> Streams"
      },
      "id": "606a22be196a6752ec47e824"
    },
    {
      "sections": [
        "AWS ELB (Classic) monitoring integration",
        "Features",
        "Activate integration",
        "Configuration and polling",
        "View and use data",
        "Metric data",
        "Inventory data",
        "Child inventory data",
        "Tip"
      ],
      "title": "AWS ELB (Classic) monitoring integration",
      "type": "docs",
      "tags": [
        "Integrations",
        "Amazon integrations",
        "AWS integrations list"
      ],
      "external_id": "a2b3ad999c8ed420e698e813618feb7ca30ec75d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/amazon-integrations/aws-integrations-list/aws-elb-classic-monitoring-integration/",
      "published_at": "2021-05-05T00:58:33Z",
      "updated_at": "2021-03-11T10:45:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic infrastructure integrations include an Amazon Elastic Classic Load Balancing (ELB) integration for reporting Classic ELB data to New Relic. This document explains the integration's features, how to activate it, and what data can be reported. Features New Relic's integration for Amazon Elastic Classic Load Balancing (ELB) reports ELB data, including HTTP code message counts, healthy and unhealthy host counts, latency times, and ELB configuration states. AWS integration data is also available for querying and chart creation in New Relic One. Amazon offers three types of load balancers: Classic Load Balancer, Application Load Balancer (ALB), and Network Load Balancer (NLB). New Relic also offers an ALB/NLB integration to monitor the last two types of load balancers. Activate integration To enable this integration, follow standard procedures to connect AWS services to New Relic. Configuration and polling You can change the polling frequency and filter data using configuration options. Default polling information for the AWS ELB integration: New Relic polling interval: 5 minutes Amazon CloudWatch data interval: 1 minute View and use data To view and use this integration's data, go to one.newrelic.com > Infrastructure > AWS and select one of the ELB integration links. You can query and explore your data using the LoadBalancerSample event type, with a provider value of Elb. Metric data The integration collects the following metrics. For additional details about these metrics, see Amazon's ELB Classic Load Balancer metrics documentation. Name Description backendConnectionErrors.Sum Rate of the number of connections per second that were not successfully established between the load balancer and the registered instances. The load balancer retries the connection when there are errors, so this count may exceed the request rate. This count also includes any connection errors related to health checks. healthyHostCount unHealthyHostCount The number of healthy or unhealthy instances registered with your load balancer. A newly registered instance is considered healthy after it passes the first health check. If cross-zone load balancing is enabled, the number of healthy instances for the LoadBalancerName dimension is calculated across all availability zones. Otherwise, it is calculated per availability zone. httpCodeBackend2XX httpCodeBackend3XX httpCodeBackend4XX httpCodeBackend5XX [ HTTP listener] The number of HTTP response codes generated per second by registered instances. This count does not include any response codes generated by the load balancer. httpCodeElb4XX [ HTTP listener] The number of HTTP 4XX client error codes generated by the load balancer per minute. Client errors are generated when a request is malformed or incomplete. httpCodeElb5XX [ HTTP listener] The number of HTTP 5XX server error codes generated by the load balancer per minute. This count does not include any response codes generated by the registered instances. The metric is reported if there are no healthy instances registered to the load balancer, or if the request rate exceeds the capacity of the instances (spillover) or the load balancer. latency.Average latency.Maximum [ HTTP listener] The total time elapsed, in seconds, from the time the load balancer sent the request to a registered instance until the instance started to send the response headers. [ TCP listener] The total time elapsed, in seconds, for the load balancer to successfully establish a connection to a registered instance. Available statistics: aws.elb.latency.p90 aws.elb.latency.p95 aws.elb.latency.p99 requestCount The number of requests completed or connections made per second during the specified interval (1 or 5 minutes). spilloverCount The total number of requests that were rejected per second, due to the surge queue being full. surgeQueueLength.Average, Maximum, Minimum The total number of requests that are pending routing. The load balancer queues a request if it is unable to establish a connection with a healthy instance in order to route the request. The maximum size of the queue is 1,024. Additional requests are rejected when the queue is full. For more information, see SpilloverCount. estimatedAlbActiveConnectionCount.Average, Maximum, Minimum The estimated number of concurrent TCP connections active from clients to the load balancer and from the load balancer to targets. estimatedAlbConsumedLcus.Average, Maximum, Minimum The estimated number of load balancer capacity units (LCU) used by an application load balancer. estimatedAlbNewConnectionCount.Average, Maximum, Minimum The estimated number of new TCP connections established from clients to the load balancer and from the load balancer to targets. estimatedProcessedBytes.Average, Maximum, Minimum The estimated number of bytes processed by an application load balancer. Inventory data The following configuration options are available with the New Relic Amazon ELB integration. Name Description availabilityZone Lists one or more availability zones from the same region as the load balancer. awsRegion The AWS region that the load balancer runs in. canonicalHostedZoneNameId The ID of the Amazon Route 53 hosted zone name associated with the load balancer. canonicalHostedZoneName The name of the Amazon Route 53 hosted zone that is associated with the load balancer. If you specify internal for the Elastic Load Balancing scheme, use DNSNameinstead. For an internal scheme, the load balancer doesn't have a CanonicalHostedZoneName value. createdTime Timestamp with the date and time the load balancer was created. dnsName The public DNS name of the load balancer. instances A JSON string representing the list of IDs of EC2 instances associated with the load balancer. listeners A JSON string representing the list of Listeners associated with the load balancer. loadBalancerName The name of the load balancer. scheme For load balancers attached to an Amazon VPC, this parameter can be used to specify the type of load balancer to use. For more information, see the AWS ElasticLoadBalancing documentation about LoadBalancer properties. securityGroups A JSON string representing the list of security groups assigned to your load balancer within your virtual private cloud (VPC). sourceSecurityGroup The security group that you can use as part of your inbound rules for your load balancer's back-end Amazon EC2 application instances. subnets A JSON string representing the list of subnet IDs in your virtual private cloud (VPC) to attach to your load balancer. Do not specify multiple subnets that are in the same Availability Zone. You can specify the AvailabilityZones or Subnets property, but not both. vpcId The ID of the VPC that the load balancer has been configured in. Child inventory data Tip Data indicated with an asterisk * is only fetched if extended inventory collection is on. Name Description accessLog/enabled * A boolean. If true, access logs are enabled for this load balancer. connectionDraining/enabled * A boolean. Use connection draining to ensure that a Classic Load Balancer does not send requests to unhealthy instances. For more information, see the AWS documentation to configure connection draining for your Classic Load Balancer. connectionDraining/timeout * The number of seconds the load balancer waits before forcibly closing connections to the de-registering instance. Valid values are 1 to 3600. connectionSettings/idleTimeout * For more information, see the AWS documentation to configure the idle connection timeout for your Classic Load Balancer. crossZoneLoadBalancing/enabled * A boolean. For more information, see the AWS documentation to configure cross-zone load balancing for your Classic Load Balancer. healthCheck/healthyThreshold The number of consecutive successful health checks that must occur before declaring an EC2 instance healthy. Valid values: 2 to 10. healthCheck/interval The amount of time in seconds between health checks of an individual instance. Valid values: 5 to 300. healthCheck/target Combination of three properties: The protocol to use to connect with the instance. Valid values: TCP, HTTP, HTTPS, and SSL. The port to use to connect with the instance, as a protocol:port pair. If the load balancer fails to connect with the instance at the specified port within the configured response timeout period, the instance is considered unhealthy. The destination for the HTTP or HTTPS request. healthCheck/timeout The amount of time in seconds to wait when receiving a response from the health check. Valid values: 2 to 60. healthCheck/unhealthyThreshold The number of consecutive failed health checks that must occur before declaring an EC2 instance unhealthy. Valid values: 2 to 10. policies/otherPolicies JSON string representing security policies associated with this load balancer.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 127.302536,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Metric</em> data",
        "body": "&#x27;s data, go to one.newrelic.com &gt; Infrastructure &gt; AWS and select one of the ELB integration links. You can query and explore your data using the <em>LoadBalancer</em>Sample event type, with a provider value of Elb. <em>Metric</em> data The integration collects the following <em>metrics</em>. For additional details about"
      },
      "id": "6044aa46e7b9d218d85799d0"
    }
  ],
  "/docs/create-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk": [
    {
      "sections": [
        "Introduction to Infrastructure Integrations SDK",
        "Important",
        "What is the Integrations SDK?",
        "What data can you report with an on-host integration?",
        "Create a custom integration"
      ],
      "title": "Introduction to Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "0662f6c3fbc151a0c836b54c104e81756b34827f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/introduction-infrastructure-integrations-sdk/",
      "published_at": "2021-05-05T00:25:33Z",
      "updated_at": "2021-03-16T07:28:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "There are several ways New Relic lets you create your own integration: General telemetry (metrics, traces) solutions: Use our Telemetry SDKs. Build a custom New Relic One application that uses your own JavaScript UI functionality. If you have New Relic Infrastructure, you can use our lightweight Flex integration tool (recommended) or use our Integrations SDK to build a complete Infrastructure on-host integration. Important New Relic is transitioning to rely on open source standards like Prometheus for future on-host integrations. Though the infrastructure SDK is the foundation of that transition, some of the tutorials and tools around this SDK might not be up-to-date with the latest developments. What is the Integrations SDK? Our Infrastructure Integrations SDK lets you build an on-host integration that reports custom data from your hosts or services. That data can then be found in New Relic Infrastructure and can be used to create custom queries and charts. What data can you report with an on-host integration? When you build an integration using the Integrations SDK, you assign the entities that report data. In New Relic terms, entity is a purposefully ambiguous term meaning something that reports data to New Relic (for example: a local host, a load balancer, or a database). A single integration can report data from multiple entities, which gives you the ability to report data from more than one service or host instance. There are three types of data an entity can generate: Metrics: Metric data is used for numerical measurement data. Examples: how many requests are in a queue, or the number of hits on a database per minute. Metric data from a custom integration can be queried and used to create dashboards. Inventory: Live system state and configuration information. This data will show up on the Infrastructure Inventory UI page. Events: Events are used to record important activities on a system. Examples: a service starting or a new table being created. Event data will be shown in the Infrastructure Events UI page. Create a custom integration To create an integration using the Integrations SDK, use these resources: See the Go language build tools and tutorial. The tutorial walks you through creating a Redis integration in Go. (Note: Go is not required; it's just the language for which we provide additional build tools. For more information, see Integrations SDK requirements.) See the integration file structure documentation, which describes the files required to create an integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.23376,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Introduction to <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": " Relic <em>Infrastructure</em> and can be used to <em>create</em> custom queries and charts. What data can you report with an on-host integration? When you build an integration using the <em>Integrations</em> <em>SDK</em>, you assign the entities that report data. In New Relic terms, entity is a purposefully ambiguous term meaning"
      },
      "id": "603eb55be7b9d2dcb72a07e8"
    },
    {
      "sections": [
        "Go language integration tutorial and build tools",
        "Integrations tutorial",
        "Important",
        "Tip",
        "Go language integration building package"
      ],
      "title": "Go language integration tutorial and build tools",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "068b7f6bc27cf0a360699121c3cf8c46c9398d6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/go-language-integration-tutorial-build-tools/",
      "published_at": "2021-05-05T03:27:27Z",
      "updated_at": "2021-03-13T01:25:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you build a custom on-host integration. This document explains the build tools and resources available for building an on-host integration with our Go language tools. Integrations tutorial Important The following tutorial is based on integrations using the SDK integration protocol v3. Find more information about the integration protocol v4 in the Github repository. The Go language integration-building tutorial on GitHub gives step-by-step procedures for building a Go language integration that reports Redis data. The tutorial shows how to build an integration using the Linux command line, but you can use the same techniques for a Windows integration with a standard Go install and PowerShell. The make command will not work with PowerShell, but you can use the Go commands inside it as a guide for building your integration. Tip You can create an on-host integration in any language, but Go is the language New Relic uses for its own integrations and build tools. To create an integration in another language, adhere to the integration file structures and JSON output requirements. Go language integration building package The tutorial relies on a New Relic Go language integration-building library package, which provides a set of useful Go functions and data structures. The package gives you tools that: Generate a \"scaffold\" integration structure with all the required fields. Read values from command-line arguments or environment variables. Generate and print JSON data to stdout. For information about file formats and JSON output specifications, see File requirements.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.7187,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go language <em>integration</em> tutorial and build tools",
        "sections": "<em>Integrations</em> tutorial",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you build a custom on-host integration. This document explains the build tools and resources available for building an on-host integration with our Go language tools. <em>Integrations</em> tutorial Important The following tutorial is based on <em>integrations</em>"
      },
      "id": "6044091d28ccbcc5b22c6097"
    },
    {
      "sections": [
        "Not seeing attributes data",
        "Problem",
        "Solution"
      ],
      "title": "Not seeing attributes data",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Troubleshooting"
      ],
      "external_id": "25a763fd32bebfa1cb33b77caf260df9a4d9fe53",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/troubleshooting/not-seeing-attributes/",
      "published_at": "2021-05-05T00:25:33Z",
      "updated_at": "2021-03-16T08:30:13Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your integration is reporting data, but a few inventory attributes are missing. Solution The following inventory key names are used internally by our platform. If customer-provided integrations send inventory items with the same key name, they will remain invisible to the user: pid installed_epoch epoch time size boot_id product_uuid installed_time",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.3016,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Problem Your integration is reporting data, but a few inventory attributes are missing. Solution The following inventory key names are used internally by our platform. If customer-provided <em>integrations</em> send inventory items with the same key name, they will remain invisible to the user: pid installed_epoch epoch time size boot_id product_uuid installed_time"
      },
      "id": "60506c9564441f0e645321ab"
    }
  ],
  "/docs/create-integrations/infrastructure-integrations-sdk/get-started/go-language-integration-tutorial-build-tools": [
    {
      "sections": [
        "Introduction to Infrastructure Integrations SDK",
        "Important",
        "What is the Integrations SDK?",
        "What data can you report with an on-host integration?",
        "Create a custom integration"
      ],
      "title": "Introduction to Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "0662f6c3fbc151a0c836b54c104e81756b34827f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/introduction-infrastructure-integrations-sdk/",
      "published_at": "2021-05-05T00:25:33Z",
      "updated_at": "2021-03-16T07:28:37Z",
      "document_type": "page",
      "popularity": 1,
      "body": "There are several ways New Relic lets you create your own integration: General telemetry (metrics, traces) solutions: Use our Telemetry SDKs. Build a custom New Relic One application that uses your own JavaScript UI functionality. If you have New Relic Infrastructure, you can use our lightweight Flex integration tool (recommended) or use our Integrations SDK to build a complete Infrastructure on-host integration. Important New Relic is transitioning to rely on open source standards like Prometheus for future on-host integrations. Though the infrastructure SDK is the foundation of that transition, some of the tutorials and tools around this SDK might not be up-to-date with the latest developments. What is the Integrations SDK? Our Infrastructure Integrations SDK lets you build an on-host integration that reports custom data from your hosts or services. That data can then be found in New Relic Infrastructure and can be used to create custom queries and charts. What data can you report with an on-host integration? When you build an integration using the Integrations SDK, you assign the entities that report data. In New Relic terms, entity is a purposefully ambiguous term meaning something that reports data to New Relic (for example: a local host, a load balancer, or a database). A single integration can report data from multiple entities, which gives you the ability to report data from more than one service or host instance. There are three types of data an entity can generate: Metrics: Metric data is used for numerical measurement data. Examples: how many requests are in a queue, or the number of hits on a database per minute. Metric data from a custom integration can be queried and used to create dashboards. Inventory: Live system state and configuration information. This data will show up on the Infrastructure Inventory UI page. Events: Events are used to record important activities on a system. Examples: a service starting or a new table being created. Event data will be shown in the Infrastructure Events UI page. Create a custom integration To create an integration using the Integrations SDK, use these resources: See the Go language build tools and tutorial. The tutorial walks you through creating a Redis integration in Go. (Note: Go is not required; it's just the language for which we provide additional build tools. For more information, see Integrations SDK requirements.) See the integration file structure documentation, which describes the files required to create an integration.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 190.23376,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Introduction to <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": " Relic <em>Infrastructure</em> and can be used to <em>create</em> custom queries and charts. What data can you report with an on-host integration? When you build an integration using the <em>Integrations</em> <em>SDK</em>, you assign the entities that report data. In New Relic terms, entity is a purposefully ambiguous term meaning"
      },
      "id": "603eb55be7b9d2dcb72a07e8"
    },
    {
      "sections": [
        "Compatibility and requirements for Infrastructure Integrations SDK",
        "Infrastructure version",
        "Operating systems",
        "Data and file specifications",
        "SDK version changes"
      ],
      "title": "Compatibility and requirements for Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "ea949057459c7c9648a9215928c1fd54c9d6a703",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk/",
      "published_at": "2021-05-04T18:45:04Z",
      "updated_at": "2021-03-13T02:32:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before using the New Relic Infrastructure integrations SDK to create a custom on-host integration, make sure your system meets these requirements. Infrastructure version To use the infrastructure Integrations SDK v4, you must have the Infrastructure agent version 1.13.0 or higher. To use SDK v3, you must have agent version 1.0.888 or higher. For Infrastructure agent version information and options for installation and updates, see the Infrastructure release notes. For notes on SDK versions and changes, see the change log. Operating systems Integrations built with the SDK can be compiled for either Linux or Windows operating systems. Data and file specifications Infrastructure on-host integrations can be created with any programming language, as long as they adhere to the data and file specifications. SDK version changes Infrastructure agent version Details 1.13.0 or higher Changes to integration protocol (v4), including support to dynamically register entities in NR1 and send dimensional metrics. New metric types. See SDK v4 release notes. 1.0.888 or higher Changes to JSON format, including support for multiple entities. The new JSON format is referred to as protocol 2 (described in JSON output documentation and also used in the definition file). Uses newer set of Go language build tools (referenced as GoSDK v3). 1.0.726 (for Linux); 1.0.775 (for Windows) Uses JSON protocol 1 (described in JSON output documentation and also used in the definition file). Uses older set of Go language build tools (referenced as GoSDK v2). If you've built an integration using the older Go language build tools and wish to update, see Upgrade from GoSDK v2 to v3 and Upgrade from v3 to v4. For updating the Infrastructure agent, see Update the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.73914,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Before using the New Relic <em>Infrastructure</em> <em>integrations</em> <em>SDK</em> to <em>create</em> a custom on-host integration, make sure your system meets these requirements. <em>Infrastructure</em> version To use the <em>infrastructure</em> <em>Integrations</em> <em>SDK</em> v4, you must have the <em>Infrastructure</em> agent version 1.13.0 or higher. To use <em>SDK</em> v3"
      },
      "id": "60440de564441fa14f378ecc"
    },
    {
      "sections": [
        "Not seeing attributes data",
        "Problem",
        "Solution"
      ],
      "title": "Not seeing attributes data",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Troubleshooting"
      ],
      "external_id": "25a763fd32bebfa1cb33b77caf260df9a4d9fe53",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/troubleshooting/not-seeing-attributes/",
      "published_at": "2021-05-05T00:25:33Z",
      "updated_at": "2021-03-16T08:30:13Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your integration is reporting data, but a few inventory attributes are missing. Solution The following inventory key names are used internally by our platform. If customer-provided integrations send inventory items with the same key name, they will remain invisible to the user: pid installed_epoch epoch time size boot_id product_uuid installed_time",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.3016,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Problem Your integration is reporting data, but a few inventory attributes are missing. Solution The following inventory key names are used internally by our platform. If customer-provided <em>integrations</em> send inventory items with the same key name, they will remain invisible to the user: pid installed_epoch epoch time size boot_id product_uuid installed_time"
      },
      "id": "60506c9564441f0e645321ab"
    }
  ],
  "/docs/create-integrations/infrastructure-integrations-sdk/get-started/introduction-infrastructure-integrations-sdk": [
    {
      "sections": [
        "Compatibility and requirements for Infrastructure Integrations SDK",
        "Infrastructure version",
        "Operating systems",
        "Data and file specifications",
        "SDK version changes"
      ],
      "title": "Compatibility and requirements for Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "ea949057459c7c9648a9215928c1fd54c9d6a703",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk/",
      "published_at": "2021-05-04T18:45:04Z",
      "updated_at": "2021-03-13T02:32:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before using the New Relic Infrastructure integrations SDK to create a custom on-host integration, make sure your system meets these requirements. Infrastructure version To use the infrastructure Integrations SDK v4, you must have the Infrastructure agent version 1.13.0 or higher. To use SDK v3, you must have agent version 1.0.888 or higher. For Infrastructure agent version information and options for installation and updates, see the Infrastructure release notes. For notes on SDK versions and changes, see the change log. Operating systems Integrations built with the SDK can be compiled for either Linux or Windows operating systems. Data and file specifications Infrastructure on-host integrations can be created with any programming language, as long as they adhere to the data and file specifications. SDK version changes Infrastructure agent version Details 1.13.0 or higher Changes to integration protocol (v4), including support to dynamically register entities in NR1 and send dimensional metrics. New metric types. See SDK v4 release notes. 1.0.888 or higher Changes to JSON format, including support for multiple entities. The new JSON format is referred to as protocol 2 (described in JSON output documentation and also used in the definition file). Uses newer set of Go language build tools (referenced as GoSDK v3). 1.0.726 (for Linux); 1.0.775 (for Windows) Uses JSON protocol 1 (described in JSON output documentation and also used in the definition file). Uses older set of Go language build tools (referenced as GoSDK v2). If you've built an integration using the older Go language build tools and wish to update, see Upgrade from GoSDK v2 to v3 and Upgrade from v3 to v4. For updating the Infrastructure agent, see Update the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.73914,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Before using the New Relic <em>Infrastructure</em> <em>integrations</em> <em>SDK</em> to <em>create</em> a custom on-host integration, make sure your system meets these requirements. <em>Infrastructure</em> version To use the <em>infrastructure</em> <em>Integrations</em> <em>SDK</em> v4, you must have the <em>Infrastructure</em> agent version 1.13.0 or higher. To use <em>SDK</em> v3"
      },
      "id": "60440de564441fa14f378ecc"
    },
    {
      "sections": [
        "Go language integration tutorial and build tools",
        "Integrations tutorial",
        "Important",
        "Tip",
        "Go language integration building package"
      ],
      "title": "Go language integration tutorial and build tools",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "068b7f6bc27cf0a360699121c3cf8c46c9398d6a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/go-language-integration-tutorial-build-tools/",
      "published_at": "2021-05-05T03:27:27Z",
      "updated_at": "2021-03-13T01:25:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you build a custom on-host integration. This document explains the build tools and resources available for building an on-host integration with our Go language tools. Integrations tutorial Important The following tutorial is based on integrations using the SDK integration protocol v3. Find more information about the integration protocol v4 in the Github repository. The Go language integration-building tutorial on GitHub gives step-by-step procedures for building a Go language integration that reports Redis data. The tutorial shows how to build an integration using the Linux command line, but you can use the same techniques for a Windows integration with a standard Go install and PowerShell. The make command will not work with PowerShell, but you can use the Go commands inside it as a guide for building your integration. Tip You can create an on-host integration in any language, but Go is the language New Relic uses for its own integrations and build tools. To create an integration in another language, adhere to the integration file structures and JSON output requirements. Go language integration building package The tutorial relies on a New Relic Go language integration-building library package, which provides a set of useful Go functions and data structures. The package gives you tools that: Generate a \"scaffold\" integration structure with all the required fields. Read values from command-line arguments or environment variables. Generate and print JSON data to stdout. For information about file formats and JSON output specifications, see File requirements.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 188.71869,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Go language <em>integration</em> tutorial and build tools",
        "sections": "<em>Integrations</em> tutorial",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you build a custom on-host integration. This document explains the build tools and resources available for building an on-host integration with our Go language tools. <em>Integrations</em> tutorial Important The following tutorial is based on <em>integrations</em>"
      },
      "id": "6044091d28ccbcc5b22c6097"
    },
    {
      "sections": [
        "Not seeing attributes data",
        "Problem",
        "Solution"
      ],
      "title": "Not seeing attributes data",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Troubleshooting"
      ],
      "external_id": "25a763fd32bebfa1cb33b77caf260df9a4d9fe53",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/troubleshooting/not-seeing-attributes/",
      "published_at": "2021-05-05T00:25:33Z",
      "updated_at": "2021-03-16T08:30:13Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your integration is reporting data, but a few inventory attributes are missing. Solution The following inventory key names are used internally by our platform. If customer-provided integrations send inventory items with the same key name, they will remain invisible to the user: pid installed_epoch epoch time size boot_id product_uuid installed_time",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 151.30159,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Problem Your integration is reporting data, but a few inventory attributes are missing. Solution The following inventory key names are used internally by our platform. If customer-provided <em>integrations</em> send inventory items with the same key name, they will remain invisible to the user: pid installed_epoch epoch time size boot_id product_uuid installed_time"
      },
      "id": "60506c9564441f0e645321ab"
    }
  ],
  "/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-configuration-overview": [
    {
      "sections": [
        "Compatibility and requirements for Infrastructure Integrations SDK",
        "Infrastructure version",
        "Operating systems",
        "Data and file specifications",
        "SDK version changes"
      ],
      "title": "Compatibility and requirements for Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "ea949057459c7c9648a9215928c1fd54c9d6a703",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk/",
      "published_at": "2021-05-04T18:45:04Z",
      "updated_at": "2021-03-13T02:32:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before using the New Relic Infrastructure integrations SDK to create a custom on-host integration, make sure your system meets these requirements. Infrastructure version To use the infrastructure Integrations SDK v4, you must have the Infrastructure agent version 1.13.0 or higher. To use SDK v3, you must have agent version 1.0.888 or higher. For Infrastructure agent version information and options for installation and updates, see the Infrastructure release notes. For notes on SDK versions and changes, see the change log. Operating systems Integrations built with the SDK can be compiled for either Linux or Windows operating systems. Data and file specifications Infrastructure on-host integrations can be created with any programming language, as long as they adhere to the data and file specifications. SDK version changes Infrastructure agent version Details 1.13.0 or higher Changes to integration protocol (v4), including support to dynamically register entities in NR1 and send dimensional metrics. New metric types. See SDK v4 release notes. 1.0.888 or higher Changes to JSON format, including support for multiple entities. The new JSON format is referred to as protocol 2 (described in JSON output documentation and also used in the definition file). Uses newer set of Go language build tools (referenced as GoSDK v3). 1.0.726 (for Linux); 1.0.775 (for Windows) Uses JSON protocol 1 (described in JSON output documentation and also used in the definition file). Uses older set of Go language build tools (referenced as GoSDK v2). If you've built an integration using the older Go language build tools and wish to update, see Upgrade from GoSDK v2 to v3 and Upgrade from v3 to v4. For updating the Infrastructure agent, see Update the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.16666,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Before using the New Relic <em>Infrastructure</em> <em>integrations</em> <em>SDK</em> to <em>create</em> a custom on-host integration, make sure your system meets these requirements. <em>Infrastructure</em> version To use the <em>infrastructure</em> <em>Integrations</em> <em>SDK</em> v4, you must have the <em>Infrastructure</em> agent version 1.13.0 or higher. To use <em>SDK</em> v3"
      },
      "id": "60440de564441fa14f378ecc"
    },
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-05-05T00:27:50Z",
      "updated_at": "2021-03-16T08:29:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent One or more configuration files For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.19626,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    },
    {
      "sections": [
        "On-host integrations: Newer configuration format",
        "Configuration structure",
        "Tip",
        "Important",
        "List of configuration properties",
        "Select an integration to run",
        "name",
        "exec",
        "cli_args",
        "when",
        "Pass configuration to the integration executable",
        "env",
        "config",
        "config_template_path",
        "Configure how the agent executes your integrations",
        "integration_user",
        "interval",
        "inventory_source",
        "labels",
        "timeout",
        "working_dir",
        "Update older integration configuration"
      ],
      "title": "On-host integrations: Newer configuration format ",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bc5bee12812cff6fabc728961e1b342b83f3e471",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-newer-configuration-format/",
      "published_at": "2021-05-05T00:27:53Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. In December 2019, Infrastructure agent version 1.8.0 began supporting a newer configuration format that makes use of a single configuration file (instead of two separate files), and provides other improvements. This document will explain how this newer format works. The older standard configuration format is supported by current Infrastructure agents. For an introduction to configuration, see Config overview. Configuration structure An on-host integration's configuration YAML must have an integrations top-level section containing a YAML array, where each entry represents an integration and its configuration. For each integration entry, only the name property is mandatory. The other properties are optional. Here's an example configuration featuring two integrations: our built-in Docker integration, which requires no configuration, and our MySQL integration: integrations: # New Relic integration that does not require any configuration - name: nri-docker # New Relic integration that gets its configuration from the environment - name: nri-mysql env: PORT: 3306 USERNAME: newrelic PASSWORD: 123456789 # to hide this field, read the secrets management documentation # Any free-form integration executed via a user-provided command - name: my-own-integration exec: python /opt/integrations/my-script.py --host=127.0.0.1 Copy You can have as many configuration YAML files as you want and can group your integration instances. Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Each configuration YAML file can also contain discovery and variables top-level sections. Important This configuration format does not require an agent restart. When saved, changes are detected and implemented immediately. This means that saving intermediate configuration changes may cause the integration to stop working. List of configuration properties This is a list of the general properties used to configure an integration. For more details about using these properties, including example values, see the documentation following the table. Config Description name Name of the integration. This is the only mandatory configuration property across all on-host integrations. If the exec field is not set it will also be the name of the integration executable. cli_args Optional list of command line arguments when name is used to provide the integration executable. Available since agent version 1.13.0. exec Full path to the integration executable, plus arguments. It may be a single-line string or a string array. If left unspecified, the exec field defaults to the name field. env YAML map containing the environment variables to be passed to the integration, where key is the environment variable name, and value is the variable value. config Configuration that is written as an external file and the path that is passed to the integration with the CONFIG_PATH environment variable or the ${config.path} variable placeholder. config_template_path Any external file whose path is passed to the integration with the CONFIG_PATH environment variable or the ${config.path} variable placeholder. Its usage allows applying discovery and secrets binding to any external configuration. integration_user Name of the user who runs the integration. interval Time between consecutive executions of the integration. It must be a number followed by a time unit (s, m or h), without spaces. inventory_source Allows overriding the category and term of the inventory source. labels Map with labels that decorate the data (metrics, events, inventory) reported by the integration. timeout A number followed by a time unit (ms, s, m or h). An integration that hasn't responded in this time period is killed and restarted. working_dir Working directory for the integration binary. when Integration is only executed if the clause evaluates to true. Conditions are defined below. The remainder of this document describes config properties grouped by their functionality: Select an integration to run Pass configuration to the integration command Configure how the agent executes integrations Select an integration to run There are two properties to select which integration will run: name and exec. The only mandatory property across all on-host integrations is name. The remaining properties specified in this document are optional. Example: integrations: - name: nri-docker - name: my-integration exec: /usr/local/bin/my-integration --metrics --inventory Copy name The mandatory name property can work in two ways: If the exec property is set: The name property only provides an identifier for the integration instance. This identifier is used in log messages and to provide a default inventory category/source in the form integration/<name> (for example, integration/nri-redis). This inventory path can be overridden with the inventory_source configuration option. If the exec property is not set: The agent looks for (and executes) an executable with the name value in any of the following folders: Linux: /var/db/newrelic-infra/newrelic-integrations/bin /var/db/newrelic-infra/newrelic-integrations /var/db/newrelic-infra/custom-integrations/bin /var/db/newrelic-infra/custom-integrations Windows C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations\\bin C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations If there is no executable with this name in the above folders the agent logs an error and the integration is not executed. Important In Windows, do not append the .exe extension to the name. The agent does this for you (for example, name: nri-mysql would look for nri-mysql.exe in the above folders). exec The exec optional property specifies the path, command, and command-line arguments of the integration to execute. When none of the path folders or arguments have spaces, it can be written in a single-line string: - name: my-integration exec: /usr/bin/python /opt/integrations/my-script.py --host=127.0.0.1 Copy If any of the path/arguments have spaces that are part of a single element, you can use a YAML array notation: - name: my-integration exec: - C:\\Program Files\\My Integration\\integration.exe - --host - 127.0.0.1 - --port - 8080 Copy The default working directory is the root directory of the agent configuration. It can be overridden with the working_dir property. cli_args The cli_args optional property specifies command line arguments that should be passed to the integration. It is useful when using name as it only provides the integration name identifier (not compatible with exec). - name: my-integration cli_args: [ -interval 10s ] Copy Usual YAML multi-line list format can be used as well: - name: my-integration cli_args: - -interval - 10s Copy when The when property allows to execute the integration only when all the evaluated conditions are successful. Available conditions are: env_exists: Environment variables exist and match value. file_exists: Given file path exists. feature: Provided feature-flag is enabled. Example: integrations: - name: ssh-integration when: file_exists: /var/run/sshd.pid Copy Pass configuration to the integration executable Often integration executables need to receive a configuration to work properly (for example, hostname and port of the monitored system, user credentials, etc.). The Infrastructure agent allows you to configure the integration commands in three ways (which you can combine): Command-line arguments, passed in the exec property. Environment variables, using the env property. Configuration files, whose path needs to be passed through environment variables or command-line arguments (see the config) property. Example: integrations: - name: my-integration exec: /opt/path/bin/script --host 127.0.0.1 --port 8081 - name: nri-mysql env: STATUS_URL: http://10.0.0.3/server-status?auto REMOTE_MONITORING: true Copy env The env property allows you to set environment variables that are passed to the executable. It is a key-value map with the required variables. Example: integrations: - name: nri-postgresql env: DATABASE: postgres PORT: 6432 COLLECTION_LIST: '[\"postgres\"]' COLLECT_DB_LOCK_METRICS: false VERBOSE: 1 Copy If you expect your integration to receive the configuration from the host's environment rather than specifying it explicitly in the configuration file, you need to set the required variables in the Infrastructure agent passthrough_environment global configuration property config This section describes various ways to pass configuration information to an integration. Pass configuration file directly Some integration commands may get their configuration from an external file. If your integration requires a configuration file, nothing prevents you from directly passing its path directly as a command-line argument or an environment variable. Here's an example using configuration of our Flex integration: integrations: - name: nri-flex env: CONFIG_FILE: /etc/nri-flex/configs/http-service.yaml - name: other-integration exec: /opt/integration/integration -f /opt/integration/config.properties Copy The above example assumes that the http-service.yaml and config.properties files exist. We can see that the nri-flex integration is expecting the http-service.yaml complete path via the CONFIG_FILE environment variable and the other-integration expects the complete config.properties path after the -f command-line flag. In the above example, it's necessary for the integration installer/configurator that the configuration files exist in the provided path and that the agent and integrations have read permissions on them. Pass configuration through config section If you prefer to keep your configuration file with the rest of the integration configuration, you can use the config section in the integration entry, which can contain a valid YAML object or just a multi-line string: integrations: - name: nri-flex env: CONFIG_FILE: ${config.path} config: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv - name: other-integration exec: /opt/integration/integration -f ${config.path} config: | example.cfg.verbose=true example.cfg.logger=/var/logs/integration.log example.cfg.hostname=localhost example.cfg.port=9025 Copy In the above examples, every time the nri-flex integration is executed, the agent creates a temporary file with the following contents: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv Copy The above YAML is only a configuration example for the nri-flex integration. The agent ignores its contents; instead, it creates a temporary file and replaces the ${config.path} variable placeholder with its path. When the integration completes execution the temporary file is removed. Also, the agent creates another temporary file before executing the other-integration integration: example.cfg.verbose=true example.cfg.logger=/var/logs/integration.log example.cfg.hostid=localhost example.cfg.port=9025 Copy It replaces the -f ${config.path} command-line placeholder with the temporary path of the written file. By convention, if you do not place the ${config.path} variable in any command-line argument or environment variable value, the agent passes the path of the configuration file via the CONFIG_PATH environment variable: # assuming that nri-example command is prepared to receive the configuration # file via the CONFIG_PATH environment variable integrations: - name: nri-example config: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv Copy Pass secrets and discovery through config section The main benefit for using a config section instead of hardcoding the full path of an external file is that you can insert ${variable} placeholders to apply our auto-discovery feature and secrets management. Here's an example followed by some explanations: variables: my_credentials: vault: http: url: http://my.vault.host/v1/newengine/data/secret headers: X-Vault-Token: my-vault-token discovery: docker: match: label.service: foo integrations: - name: foo-monitor exec: /opt/foo/bin/monitor --config=${config.path} config: | foo.host=${discovery.ip} foo.port=${discovery.port} foo.user=${my_credentials.user} foo.password=${my_credentials.password} Copy Tip (For more details about the variables and discovery sections, please visit the discovery and secrets management documentation). The above example relies on the following premises: There is a Vault service that allows retrieving a JSON object formed by the user and password fields. There may be a variable number of Docker containers labeled with service=foo, which are accessible from the agent host via a discoverable public IP and Port. The user has configured the foo-monitor integration to monitor all the service=foo labeled containers, which share a common user and password. Each instance of the foo-monitor integration requires executing the /opt/foo/bin/monitor executable, passing the text configuration inside the config section via the --config=<path> command-line argument. As example of workflow, imagine that the Vault invocation returns the following JSON: {\"user\":\"monitorer\",\"password\":\"5up3r53cr3t!\"} Copy At the moment of executing the foo-monitor integration, there are three running containers labeled with service=foo: ip: 10.0.0.3, port: 8080 ip: 10.0.0.3, port: 8081 ip: 10.0.0.3, port: 8082 The agent then creates the following three temporary files, using the contents of the config property as a template, but replacing the ${placeholders} by the acquired variables and discovered items (files' path is invented for the sake of simplicity): First match (/tmp/123_discovered): foo.host=10.0.0.3 foo.port=8080 foo.user=monitorer foo.password=5up3r53cr3t! Copy Second match (/tmp/456_discovered): foo.host=10.0.0.3 foo.port=8081 foo.user=monitorer foo.password=5up3r53cr3t! Copy Third match (/tmp/789_discovered) foo.host=10.0.0.3 foo.port=8082 foo.user=monitorer foo.password=5up3r53cr3t! Copy After the config variable placeholders have been replaced and the temporary files have been created, the /opt/foo/bin/monitor executable is executed three times (one per matched container), replacing the ${config.path} command-line placeholder with the temporary file corresponding to each discovered configuration: First match: /opt/foo/bin/monitor --config=/tmp/123_discovered Second match: /opt/foo/bin/monitor --config=/tmp/456_discovered Third match: /opt/foo/bin/monitor --config=/tmp/789_discovered To ensure security and to minimize the chance of leaking secrets to disk, the agent: Creates the files owned by the agent user, for example, root or nri-agent, depending on the user you have configured to run the agent. Sets read permissions only for the owner. Removes the created files when the integration instance finishes its execution. config_template_path If you want to use the secrets management and discovery in the configuration files that you're passing to the integration executable, but you prefer to keep them as an individual file, you can use the config_template_path: <path> option. It works exactly as in the config section: The agent applies secrets management and discovery to the file contents. The agent creates different temporary files that are passed to the integration via the ${config.path} placeholder (or the CONFIG_PATH environment variable). Example: discovery: docker: match: name: /^redis/ integrations: - name: nri-flex env: CONFIG_FILE: ${config.path} config_template_path: /etc/flex-configs/redis.yml Copy In the above example, the redis.yml external file can contain container discovery variable placeholders, like ${discovery.ip} or ${discovery.port}. Configure how the agent executes your integrations The properties of this section modify the way the Infrastructure agent executes and interacts with the integrations, or the way the agent decorates the integrations' data. integration_user The integration commands run as the same user as the agent (usually root or nri-agent). If due to permission restrictions an integration needs to run as another user, its name must be specified in the integration_user property. Example: integrations: - name: dbus-inventory exec: python my-dbus-script.py integration_user: dbus Copy interval The interval option sets the time between consecutive executions of an integration. The accepted format is an integer immediately followed by a time unit (s for seconds, m for minutes, h for hours). The default is 30s, and the minimum accepted value is 15s. Any value lower than 15s is automatically set to 15s. Example: integrations: - name: nri-nginx env: STATUS_URL: http://127.0.0.1/status STATUS_MODULE: discover interval: 20s Copy inventory_source Any inventory item must be catalogued under a category/source taxonomy. By default, each integration inventory is stored as integration/ + name value (for example, integration/nri-apache, integration/nri-mysql). The inventory_source property allows you to override the default taxonomy of inventory data. Example: integrations: - name: nri-nginx - name: nri-apache exec: - /var/db/newrelic-infra/newrelic-integrations/bin/nri-apache - --inventory inventory_source: config/apache Copy In the above example, the nri-nginx inventory, if any, would be visible in the New Relic UI under the integration/nri-nginx source. The nri-apache inventory would be visible under config/apache. labels labels is a key-value map that allows extra metadata to be provided for the integration. The agent uses those labels to decorate the metrics, events, and inventory that it receives from a given integration instance. Example: integrations: - name: nri-apache inventory_source: config/apache labels: env: production role: load_balancer Copy In the above example, the agent decorates all the metrics and events from the nri-apache instance with the following fields: label.env: production label.role: load_balancer Also, the following entries are added to the integration inventory: config/apache/labels/env: production config/apache/labels/role: load_balancer timeout If an integration has not returned any metric (or a heartbeat message as described below) before the time specified in the timeout value, the agent kills the integration process and restarts it after the corresponding interval. The accepted format is an integer number immediately followed (without spaces) by a time unit (ms for milliseconds, s for seconds, m for minutes, h for hours). If a zero (or negative) timeout value is provided, the integration can run forever without being killed by a timeout expiration. For long-running integrations (integrations that keep running, periodically returning metrics/events/inventory), each time the integration submits a metrics/events/inventory payload, the timeout deadline is restarted. That means that long-running integrations must return a valid JSON payload in an interval that is lower than timeout. Returning an empty JSON ({}) is interpreted as a heart-beat message that restarts the timeout, preventing the long-running integration from being killed, even if they don't have information to report. The default is 120s, and the minimum accepted value is 100ms. Any value lower than 100ms is automatically set to 100ms. Example: integrations: - name: nri-jmx arguments: JMX_HOST: jmx-host.localnet JMX_PORT: 7096 COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml\" timeout: 30s Copy working_dir working_dir sets the working directory of the command. If empty or unspecified, the agent runs the command in the Infrastructure agent's current directory. The default is the Infrastructure agent root directory. Example: integrations: - name: my-integration exec: /opt/integration/bin/integration working_dir: /opt/integration/scratch-zone Copy Update older integration configuration In December 2019, the Infrastructure agent version 1.8.0 began using a different configuration format. For details, see Config format differences. The main difference between these formats is that the older configuration format uses two separate configuration files (a INTEGRATION_NAME-definition.yml file and a INTEGRATION_NAME-config.yml file) and the newer version uses a single configuration file. Here are some of the features added by the newer configuration functionality: Flexible configuration via command-line arguments, environment variables, or external files. Ability to group different integrations in the same file. Hot reload: adding a new integration or changing its configuration does not require restarting the agent. Timeouts: if an integration doesn't respond before a user-specified time, the integration process is killed and restarted. Not all on-host integrations come with the newer configuration format, but you can update the configuration to the new format for all on-host integrations to take advantage of the new features. The following YAML shows an example Apache integration configuration using the older configuration format. Note that this configuration will still work with newer agents, but we recommend updating your integrations to take full advantage of features. integration_name: com.newrelic.apache instances: - name: apache-server-metrics command: metrics arguments: status_url: http://127.0.0.1/server-status?auto remote_monitoring: true labels: env: production role: load_balancer - name: apache-server-inventory command: inventory arguments: remote_monitoring: true labels: env: production role: load_balancer Copy To update an older integration configuration to the new format, you must perform two steps: Rename the instances top-level section to integrations. Remove the integration_name top-level section and add it to each integration entry. You are no longer required to keep a separate file for each integration type and you can group your legacy integration entries in the same file as other integrations. The new version of the Apache integration config: integrations: - name: apache-server-metrics integration_name: com.newrelic.apache command: metrics arguments: status_url: http://127.0.0.1/server-status?auto remote_monitoring: true labels: env: production role: load_balancer - name: apache-server-inventory integration_name: com.newrelic.apache command: inventory arguments: remote_monitoring: true labels: env: production role: load_balancer Copy Please note that because the older configuration format doesn't support hot reloading. You will need to restart the Infrastructure agent to remove the old integrations configuration (otherwise the old instances will coexist with the new ones).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.17471,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Newer configuration format ",
        "sections": "On-host <em>integrations</em>: Newer configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. In December 2019, <em>Infrastructure</em> agent version 1.8.0 began supporting a newer configuration format that makes use of a single configuration file (instead of two separate files), and provides other"
      },
      "id": "603e923928ccbcb8dfeba751"
    }
  ],
  "/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-executable-file-json-specifications": [
    {
      "sections": [
        "Compatibility and requirements for Infrastructure Integrations SDK",
        "Infrastructure version",
        "Operating systems",
        "Data and file specifications",
        "SDK version changes"
      ],
      "title": "Compatibility and requirements for Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "ea949057459c7c9648a9215928c1fd54c9d6a703",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk/",
      "published_at": "2021-05-04T18:45:04Z",
      "updated_at": "2021-03-13T02:32:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before using the New Relic Infrastructure integrations SDK to create a custom on-host integration, make sure your system meets these requirements. Infrastructure version To use the infrastructure Integrations SDK v4, you must have the Infrastructure agent version 1.13.0 or higher. To use SDK v3, you must have agent version 1.0.888 or higher. For Infrastructure agent version information and options for installation and updates, see the Infrastructure release notes. For notes on SDK versions and changes, see the change log. Operating systems Integrations built with the SDK can be compiled for either Linux or Windows operating systems. Data and file specifications Infrastructure on-host integrations can be created with any programming language, as long as they adhere to the data and file specifications. SDK version changes Infrastructure agent version Details 1.13.0 or higher Changes to integration protocol (v4), including support to dynamically register entities in NR1 and send dimensional metrics. New metric types. See SDK v4 release notes. 1.0.888 or higher Changes to JSON format, including support for multiple entities. The new JSON format is referred to as protocol 2 (described in JSON output documentation and also used in the definition file). Uses newer set of Go language build tools (referenced as GoSDK v3). 1.0.726 (for Linux); 1.0.775 (for Windows) Uses JSON protocol 1 (described in JSON output documentation and also used in the definition file). Uses older set of Go language build tools (referenced as GoSDK v2). If you've built an integration using the older Go language build tools and wish to update, see Upgrade from GoSDK v2 to v3 and Upgrade from v3 to v4. For updating the Infrastructure agent, see Update the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.16664,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Before using the New Relic <em>Infrastructure</em> <em>integrations</em> <em>SDK</em> to <em>create</em> a custom on-host integration, make sure your system meets these requirements. <em>Infrastructure</em> version To use the <em>infrastructure</em> <em>Integrations</em> <em>SDK</em> v4, you must have the <em>Infrastructure</em> agent version 1.13.0 or higher. To use <em>SDK</em> v3"
      },
      "id": "60440de564441fa14f378ecc"
    },
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-05-05T00:27:50Z",
      "updated_at": "2021-03-16T08:29:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent One or more configuration files For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.19626,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    },
    {
      "sections": [
        "On-host integrations: Newer configuration format",
        "Configuration structure",
        "Tip",
        "Important",
        "List of configuration properties",
        "Select an integration to run",
        "name",
        "exec",
        "cli_args",
        "when",
        "Pass configuration to the integration executable",
        "env",
        "config",
        "config_template_path",
        "Configure how the agent executes your integrations",
        "integration_user",
        "interval",
        "inventory_source",
        "labels",
        "timeout",
        "working_dir",
        "Update older integration configuration"
      ],
      "title": "On-host integrations: Newer configuration format ",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bc5bee12812cff6fabc728961e1b342b83f3e471",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-newer-configuration-format/",
      "published_at": "2021-05-05T00:27:53Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. In December 2019, Infrastructure agent version 1.8.0 began supporting a newer configuration format that makes use of a single configuration file (instead of two separate files), and provides other improvements. This document will explain how this newer format works. The older standard configuration format is supported by current Infrastructure agents. For an introduction to configuration, see Config overview. Configuration structure An on-host integration's configuration YAML must have an integrations top-level section containing a YAML array, where each entry represents an integration and its configuration. For each integration entry, only the name property is mandatory. The other properties are optional. Here's an example configuration featuring two integrations: our built-in Docker integration, which requires no configuration, and our MySQL integration: integrations: # New Relic integration that does not require any configuration - name: nri-docker # New Relic integration that gets its configuration from the environment - name: nri-mysql env: PORT: 3306 USERNAME: newrelic PASSWORD: 123456789 # to hide this field, read the secrets management documentation # Any free-form integration executed via a user-provided command - name: my-own-integration exec: python /opt/integrations/my-script.py --host=127.0.0.1 Copy You can have as many configuration YAML files as you want and can group your integration instances. Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Each configuration YAML file can also contain discovery and variables top-level sections. Important This configuration format does not require an agent restart. When saved, changes are detected and implemented immediately. This means that saving intermediate configuration changes may cause the integration to stop working. List of configuration properties This is a list of the general properties used to configure an integration. For more details about using these properties, including example values, see the documentation following the table. Config Description name Name of the integration. This is the only mandatory configuration property across all on-host integrations. If the exec field is not set it will also be the name of the integration executable. cli_args Optional list of command line arguments when name is used to provide the integration executable. Available since agent version 1.13.0. exec Full path to the integration executable, plus arguments. It may be a single-line string or a string array. If left unspecified, the exec field defaults to the name field. env YAML map containing the environment variables to be passed to the integration, where key is the environment variable name, and value is the variable value. config Configuration that is written as an external file and the path that is passed to the integration with the CONFIG_PATH environment variable or the ${config.path} variable placeholder. config_template_path Any external file whose path is passed to the integration with the CONFIG_PATH environment variable or the ${config.path} variable placeholder. Its usage allows applying discovery and secrets binding to any external configuration. integration_user Name of the user who runs the integration. interval Time between consecutive executions of the integration. It must be a number followed by a time unit (s, m or h), without spaces. inventory_source Allows overriding the category and term of the inventory source. labels Map with labels that decorate the data (metrics, events, inventory) reported by the integration. timeout A number followed by a time unit (ms, s, m or h). An integration that hasn't responded in this time period is killed and restarted. working_dir Working directory for the integration binary. when Integration is only executed if the clause evaluates to true. Conditions are defined below. The remainder of this document describes config properties grouped by their functionality: Select an integration to run Pass configuration to the integration command Configure how the agent executes integrations Select an integration to run There are two properties to select which integration will run: name and exec. The only mandatory property across all on-host integrations is name. The remaining properties specified in this document are optional. Example: integrations: - name: nri-docker - name: my-integration exec: /usr/local/bin/my-integration --metrics --inventory Copy name The mandatory name property can work in two ways: If the exec property is set: The name property only provides an identifier for the integration instance. This identifier is used in log messages and to provide a default inventory category/source in the form integration/<name> (for example, integration/nri-redis). This inventory path can be overridden with the inventory_source configuration option. If the exec property is not set: The agent looks for (and executes) an executable with the name value in any of the following folders: Linux: /var/db/newrelic-infra/newrelic-integrations/bin /var/db/newrelic-infra/newrelic-integrations /var/db/newrelic-infra/custom-integrations/bin /var/db/newrelic-infra/custom-integrations Windows C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations\\bin C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations If there is no executable with this name in the above folders the agent logs an error and the integration is not executed. Important In Windows, do not append the .exe extension to the name. The agent does this for you (for example, name: nri-mysql would look for nri-mysql.exe in the above folders). exec The exec optional property specifies the path, command, and command-line arguments of the integration to execute. When none of the path folders or arguments have spaces, it can be written in a single-line string: - name: my-integration exec: /usr/bin/python /opt/integrations/my-script.py --host=127.0.0.1 Copy If any of the path/arguments have spaces that are part of a single element, you can use a YAML array notation: - name: my-integration exec: - C:\\Program Files\\My Integration\\integration.exe - --host - 127.0.0.1 - --port - 8080 Copy The default working directory is the root directory of the agent configuration. It can be overridden with the working_dir property. cli_args The cli_args optional property specifies command line arguments that should be passed to the integration. It is useful when using name as it only provides the integration name identifier (not compatible with exec). - name: my-integration cli_args: [ -interval 10s ] Copy Usual YAML multi-line list format can be used as well: - name: my-integration cli_args: - -interval - 10s Copy when The when property allows to execute the integration only when all the evaluated conditions are successful. Available conditions are: env_exists: Environment variables exist and match value. file_exists: Given file path exists. feature: Provided feature-flag is enabled. Example: integrations: - name: ssh-integration when: file_exists: /var/run/sshd.pid Copy Pass configuration to the integration executable Often integration executables need to receive a configuration to work properly (for example, hostname and port of the monitored system, user credentials, etc.). The Infrastructure agent allows you to configure the integration commands in three ways (which you can combine): Command-line arguments, passed in the exec property. Environment variables, using the env property. Configuration files, whose path needs to be passed through environment variables or command-line arguments (see the config) property. Example: integrations: - name: my-integration exec: /opt/path/bin/script --host 127.0.0.1 --port 8081 - name: nri-mysql env: STATUS_URL: http://10.0.0.3/server-status?auto REMOTE_MONITORING: true Copy env The env property allows you to set environment variables that are passed to the executable. It is a key-value map with the required variables. Example: integrations: - name: nri-postgresql env: DATABASE: postgres PORT: 6432 COLLECTION_LIST: '[\"postgres\"]' COLLECT_DB_LOCK_METRICS: false VERBOSE: 1 Copy If you expect your integration to receive the configuration from the host's environment rather than specifying it explicitly in the configuration file, you need to set the required variables in the Infrastructure agent passthrough_environment global configuration property config This section describes various ways to pass configuration information to an integration. Pass configuration file directly Some integration commands may get their configuration from an external file. If your integration requires a configuration file, nothing prevents you from directly passing its path directly as a command-line argument or an environment variable. Here's an example using configuration of our Flex integration: integrations: - name: nri-flex env: CONFIG_FILE: /etc/nri-flex/configs/http-service.yaml - name: other-integration exec: /opt/integration/integration -f /opt/integration/config.properties Copy The above example assumes that the http-service.yaml and config.properties files exist. We can see that the nri-flex integration is expecting the http-service.yaml complete path via the CONFIG_FILE environment variable and the other-integration expects the complete config.properties path after the -f command-line flag. In the above example, it's necessary for the integration installer/configurator that the configuration files exist in the provided path and that the agent and integrations have read permissions on them. Pass configuration through config section If you prefer to keep your configuration file with the rest of the integration configuration, you can use the config section in the integration entry, which can contain a valid YAML object or just a multi-line string: integrations: - name: nri-flex env: CONFIG_FILE: ${config.path} config: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv - name: other-integration exec: /opt/integration/integration -f ${config.path} config: | example.cfg.verbose=true example.cfg.logger=/var/logs/integration.log example.cfg.hostname=localhost example.cfg.port=9025 Copy In the above examples, every time the nri-flex integration is executed, the agent creates a temporary file with the following contents: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv Copy The above YAML is only a configuration example for the nri-flex integration. The agent ignores its contents; instead, it creates a temporary file and replaces the ${config.path} variable placeholder with its path. When the integration completes execution the temporary file is removed. Also, the agent creates another temporary file before executing the other-integration integration: example.cfg.verbose=true example.cfg.logger=/var/logs/integration.log example.cfg.hostid=localhost example.cfg.port=9025 Copy It replaces the -f ${config.path} command-line placeholder with the temporary path of the written file. By convention, if you do not place the ${config.path} variable in any command-line argument or environment variable value, the agent passes the path of the configuration file via the CONFIG_PATH environment variable: # assuming that nri-example command is prepared to receive the configuration # file via the CONFIG_PATH environment variable integrations: - name: nri-example config: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv Copy Pass secrets and discovery through config section The main benefit for using a config section instead of hardcoding the full path of an external file is that you can insert ${variable} placeholders to apply our auto-discovery feature and secrets management. Here's an example followed by some explanations: variables: my_credentials: vault: http: url: http://my.vault.host/v1/newengine/data/secret headers: X-Vault-Token: my-vault-token discovery: docker: match: label.service: foo integrations: - name: foo-monitor exec: /opt/foo/bin/monitor --config=${config.path} config: | foo.host=${discovery.ip} foo.port=${discovery.port} foo.user=${my_credentials.user} foo.password=${my_credentials.password} Copy Tip (For more details about the variables and discovery sections, please visit the discovery and secrets management documentation). The above example relies on the following premises: There is a Vault service that allows retrieving a JSON object formed by the user and password fields. There may be a variable number of Docker containers labeled with service=foo, which are accessible from the agent host via a discoverable public IP and Port. The user has configured the foo-monitor integration to monitor all the service=foo labeled containers, which share a common user and password. Each instance of the foo-monitor integration requires executing the /opt/foo/bin/monitor executable, passing the text configuration inside the config section via the --config=<path> command-line argument. As example of workflow, imagine that the Vault invocation returns the following JSON: {\"user\":\"monitorer\",\"password\":\"5up3r53cr3t!\"} Copy At the moment of executing the foo-monitor integration, there are three running containers labeled with service=foo: ip: 10.0.0.3, port: 8080 ip: 10.0.0.3, port: 8081 ip: 10.0.0.3, port: 8082 The agent then creates the following three temporary files, using the contents of the config property as a template, but replacing the ${placeholders} by the acquired variables and discovered items (files' path is invented for the sake of simplicity): First match (/tmp/123_discovered): foo.host=10.0.0.3 foo.port=8080 foo.user=monitorer foo.password=5up3r53cr3t! Copy Second match (/tmp/456_discovered): foo.host=10.0.0.3 foo.port=8081 foo.user=monitorer foo.password=5up3r53cr3t! Copy Third match (/tmp/789_discovered) foo.host=10.0.0.3 foo.port=8082 foo.user=monitorer foo.password=5up3r53cr3t! Copy After the config variable placeholders have been replaced and the temporary files have been created, the /opt/foo/bin/monitor executable is executed three times (one per matched container), replacing the ${config.path} command-line placeholder with the temporary file corresponding to each discovered configuration: First match: /opt/foo/bin/monitor --config=/tmp/123_discovered Second match: /opt/foo/bin/monitor --config=/tmp/456_discovered Third match: /opt/foo/bin/monitor --config=/tmp/789_discovered To ensure security and to minimize the chance of leaking secrets to disk, the agent: Creates the files owned by the agent user, for example, root or nri-agent, depending on the user you have configured to run the agent. Sets read permissions only for the owner. Removes the created files when the integration instance finishes its execution. config_template_path If you want to use the secrets management and discovery in the configuration files that you're passing to the integration executable, but you prefer to keep them as an individual file, you can use the config_template_path: <path> option. It works exactly as in the config section: The agent applies secrets management and discovery to the file contents. The agent creates different temporary files that are passed to the integration via the ${config.path} placeholder (or the CONFIG_PATH environment variable). Example: discovery: docker: match: name: /^redis/ integrations: - name: nri-flex env: CONFIG_FILE: ${config.path} config_template_path: /etc/flex-configs/redis.yml Copy In the above example, the redis.yml external file can contain container discovery variable placeholders, like ${discovery.ip} or ${discovery.port}. Configure how the agent executes your integrations The properties of this section modify the way the Infrastructure agent executes and interacts with the integrations, or the way the agent decorates the integrations' data. integration_user The integration commands run as the same user as the agent (usually root or nri-agent). If due to permission restrictions an integration needs to run as another user, its name must be specified in the integration_user property. Example: integrations: - name: dbus-inventory exec: python my-dbus-script.py integration_user: dbus Copy interval The interval option sets the time between consecutive executions of an integration. The accepted format is an integer immediately followed by a time unit (s for seconds, m for minutes, h for hours). The default is 30s, and the minimum accepted value is 15s. Any value lower than 15s is automatically set to 15s. Example: integrations: - name: nri-nginx env: STATUS_URL: http://127.0.0.1/status STATUS_MODULE: discover interval: 20s Copy inventory_source Any inventory item must be catalogued under a category/source taxonomy. By default, each integration inventory is stored as integration/ + name value (for example, integration/nri-apache, integration/nri-mysql). The inventory_source property allows you to override the default taxonomy of inventory data. Example: integrations: - name: nri-nginx - name: nri-apache exec: - /var/db/newrelic-infra/newrelic-integrations/bin/nri-apache - --inventory inventory_source: config/apache Copy In the above example, the nri-nginx inventory, if any, would be visible in the New Relic UI under the integration/nri-nginx source. The nri-apache inventory would be visible under config/apache. labels labels is a key-value map that allows extra metadata to be provided for the integration. The agent uses those labels to decorate the metrics, events, and inventory that it receives from a given integration instance. Example: integrations: - name: nri-apache inventory_source: config/apache labels: env: production role: load_balancer Copy In the above example, the agent decorates all the metrics and events from the nri-apache instance with the following fields: label.env: production label.role: load_balancer Also, the following entries are added to the integration inventory: config/apache/labels/env: production config/apache/labels/role: load_balancer timeout If an integration has not returned any metric (or a heartbeat message as described below) before the time specified in the timeout value, the agent kills the integration process and restarts it after the corresponding interval. The accepted format is an integer number immediately followed (without spaces) by a time unit (ms for milliseconds, s for seconds, m for minutes, h for hours). If a zero (or negative) timeout value is provided, the integration can run forever without being killed by a timeout expiration. For long-running integrations (integrations that keep running, periodically returning metrics/events/inventory), each time the integration submits a metrics/events/inventory payload, the timeout deadline is restarted. That means that long-running integrations must return a valid JSON payload in an interval that is lower than timeout. Returning an empty JSON ({}) is interpreted as a heart-beat message that restarts the timeout, preventing the long-running integration from being killed, even if they don't have information to report. The default is 120s, and the minimum accepted value is 100ms. Any value lower than 100ms is automatically set to 100ms. Example: integrations: - name: nri-jmx arguments: JMX_HOST: jmx-host.localnet JMX_PORT: 7096 COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml\" timeout: 30s Copy working_dir working_dir sets the working directory of the command. If empty or unspecified, the agent runs the command in the Infrastructure agent's current directory. The default is the Infrastructure agent root directory. Example: integrations: - name: my-integration exec: /opt/integration/bin/integration working_dir: /opt/integration/scratch-zone Copy Update older integration configuration In December 2019, the Infrastructure agent version 1.8.0 began using a different configuration format. For details, see Config format differences. The main difference between these formats is that the older configuration format uses two separate configuration files (a INTEGRATION_NAME-definition.yml file and a INTEGRATION_NAME-config.yml file) and the newer version uses a single configuration file. Here are some of the features added by the newer configuration functionality: Flexible configuration via command-line arguments, environment variables, or external files. Ability to group different integrations in the same file. Hot reload: adding a new integration or changing its configuration does not require restarting the agent. Timeouts: if an integration doesn't respond before a user-specified time, the integration process is killed and restarted. Not all on-host integrations come with the newer configuration format, but you can update the configuration to the new format for all on-host integrations to take advantage of the new features. The following YAML shows an example Apache integration configuration using the older configuration format. Note that this configuration will still work with newer agents, but we recommend updating your integrations to take full advantage of features. integration_name: com.newrelic.apache instances: - name: apache-server-metrics command: metrics arguments: status_url: http://127.0.0.1/server-status?auto remote_monitoring: true labels: env: production role: load_balancer - name: apache-server-inventory command: inventory arguments: remote_monitoring: true labels: env: production role: load_balancer Copy To update an older integration configuration to the new format, you must perform two steps: Rename the instances top-level section to integrations. Remove the integration_name top-level section and add it to each integration entry. You are no longer required to keep a separate file for each integration type and you can group your legacy integration entries in the same file as other integrations. The new version of the Apache integration config: integrations: - name: apache-server-metrics integration_name: com.newrelic.apache command: metrics arguments: status_url: http://127.0.0.1/server-status?auto remote_monitoring: true labels: env: production role: load_balancer - name: apache-server-inventory integration_name: com.newrelic.apache command: inventory arguments: remote_monitoring: true labels: env: production role: load_balancer Copy Please note that because the older configuration format doesn't support hot reloading. You will need to restart the Infrastructure agent to remove the old integrations configuration (otherwise the old instances will coexist with the new ones).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.1747,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Newer configuration format ",
        "sections": "On-host <em>integrations</em>: Newer configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. In December 2019, <em>Infrastructure</em> agent version 1.8.0 began supporting a newer configuration format that makes use of a single configuration file (instead of two separate files), and provides other"
      },
      "id": "603e923928ccbcb8dfeba751"
    }
  ],
  "/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files": [
    {
      "sections": [
        "Compatibility and requirements for Infrastructure Integrations SDK",
        "Infrastructure version",
        "Operating systems",
        "Data and file specifications",
        "SDK version changes"
      ],
      "title": "Compatibility and requirements for Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "ea949057459c7c9648a9215928c1fd54c9d6a703",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk/",
      "published_at": "2021-05-04T18:45:04Z",
      "updated_at": "2021-03-13T02:32:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before using the New Relic Infrastructure integrations SDK to create a custom on-host integration, make sure your system meets these requirements. Infrastructure version To use the infrastructure Integrations SDK v4, you must have the Infrastructure agent version 1.13.0 or higher. To use SDK v3, you must have agent version 1.0.888 or higher. For Infrastructure agent version information and options for installation and updates, see the Infrastructure release notes. For notes on SDK versions and changes, see the change log. Operating systems Integrations built with the SDK can be compiled for either Linux or Windows operating systems. Data and file specifications Infrastructure on-host integrations can be created with any programming language, as long as they adhere to the data and file specifications. SDK version changes Infrastructure agent version Details 1.13.0 or higher Changes to integration protocol (v4), including support to dynamically register entities in NR1 and send dimensional metrics. New metric types. See SDK v4 release notes. 1.0.888 or higher Changes to JSON format, including support for multiple entities. The new JSON format is referred to as protocol 2 (described in JSON output documentation and also used in the definition file). Uses newer set of Go language build tools (referenced as GoSDK v3). 1.0.726 (for Linux); 1.0.775 (for Windows) Uses JSON protocol 1 (described in JSON output documentation and also used in the definition file). Uses older set of Go language build tools (referenced as GoSDK v2). If you've built an integration using the older Go language build tools and wish to update, see Upgrade from GoSDK v2 to v3 and Upgrade from v3 to v4. For updating the Infrastructure agent, see Update the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.16664,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Before using the New Relic <em>Infrastructure</em> <em>integrations</em> <em>SDK</em> to <em>create</em> a custom on-host integration, make sure your system meets these requirements. <em>Infrastructure</em> version To use the <em>infrastructure</em> <em>Integrations</em> <em>SDK</em> v4, you must have the <em>Infrastructure</em> agent version 1.13.0 or higher. To use <em>SDK</em> v3"
      },
      "id": "60440de564441fa14f378ecc"
    },
    {
      "sections": [
        "On-host integrations: Newer configuration format",
        "Configuration structure",
        "Tip",
        "Important",
        "List of configuration properties",
        "Select an integration to run",
        "name",
        "exec",
        "cli_args",
        "when",
        "Pass configuration to the integration executable",
        "env",
        "config",
        "config_template_path",
        "Configure how the agent executes your integrations",
        "integration_user",
        "interval",
        "inventory_source",
        "labels",
        "timeout",
        "working_dir",
        "Update older integration configuration"
      ],
      "title": "On-host integrations: Newer configuration format ",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bc5bee12812cff6fabc728961e1b342b83f3e471",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-newer-configuration-format/",
      "published_at": "2021-05-05T00:27:53Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. In December 2019, Infrastructure agent version 1.8.0 began supporting a newer configuration format that makes use of a single configuration file (instead of two separate files), and provides other improvements. This document will explain how this newer format works. The older standard configuration format is supported by current Infrastructure agents. For an introduction to configuration, see Config overview. Configuration structure An on-host integration's configuration YAML must have an integrations top-level section containing a YAML array, where each entry represents an integration and its configuration. For each integration entry, only the name property is mandatory. The other properties are optional. Here's an example configuration featuring two integrations: our built-in Docker integration, which requires no configuration, and our MySQL integration: integrations: # New Relic integration that does not require any configuration - name: nri-docker # New Relic integration that gets its configuration from the environment - name: nri-mysql env: PORT: 3306 USERNAME: newrelic PASSWORD: 123456789 # to hide this field, read the secrets management documentation # Any free-form integration executed via a user-provided command - name: my-own-integration exec: python /opt/integrations/my-script.py --host=127.0.0.1 Copy You can have as many configuration YAML files as you want and can group your integration instances. Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Each configuration YAML file can also contain discovery and variables top-level sections. Important This configuration format does not require an agent restart. When saved, changes are detected and implemented immediately. This means that saving intermediate configuration changes may cause the integration to stop working. List of configuration properties This is a list of the general properties used to configure an integration. For more details about using these properties, including example values, see the documentation following the table. Config Description name Name of the integration. This is the only mandatory configuration property across all on-host integrations. If the exec field is not set it will also be the name of the integration executable. cli_args Optional list of command line arguments when name is used to provide the integration executable. Available since agent version 1.13.0. exec Full path to the integration executable, plus arguments. It may be a single-line string or a string array. If left unspecified, the exec field defaults to the name field. env YAML map containing the environment variables to be passed to the integration, where key is the environment variable name, and value is the variable value. config Configuration that is written as an external file and the path that is passed to the integration with the CONFIG_PATH environment variable or the ${config.path} variable placeholder. config_template_path Any external file whose path is passed to the integration with the CONFIG_PATH environment variable or the ${config.path} variable placeholder. Its usage allows applying discovery and secrets binding to any external configuration. integration_user Name of the user who runs the integration. interval Time between consecutive executions of the integration. It must be a number followed by a time unit (s, m or h), without spaces. inventory_source Allows overriding the category and term of the inventory source. labels Map with labels that decorate the data (metrics, events, inventory) reported by the integration. timeout A number followed by a time unit (ms, s, m or h). An integration that hasn't responded in this time period is killed and restarted. working_dir Working directory for the integration binary. when Integration is only executed if the clause evaluates to true. Conditions are defined below. The remainder of this document describes config properties grouped by their functionality: Select an integration to run Pass configuration to the integration command Configure how the agent executes integrations Select an integration to run There are two properties to select which integration will run: name and exec. The only mandatory property across all on-host integrations is name. The remaining properties specified in this document are optional. Example: integrations: - name: nri-docker - name: my-integration exec: /usr/local/bin/my-integration --metrics --inventory Copy name The mandatory name property can work in two ways: If the exec property is set: The name property only provides an identifier for the integration instance. This identifier is used in log messages and to provide a default inventory category/source in the form integration/<name> (for example, integration/nri-redis). This inventory path can be overridden with the inventory_source configuration option. If the exec property is not set: The agent looks for (and executes) an executable with the name value in any of the following folders: Linux: /var/db/newrelic-infra/newrelic-integrations/bin /var/db/newrelic-infra/newrelic-integrations /var/db/newrelic-infra/custom-integrations/bin /var/db/newrelic-infra/custom-integrations Windows C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations\\bin C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations If there is no executable with this name in the above folders the agent logs an error and the integration is not executed. Important In Windows, do not append the .exe extension to the name. The agent does this for you (for example, name: nri-mysql would look for nri-mysql.exe in the above folders). exec The exec optional property specifies the path, command, and command-line arguments of the integration to execute. When none of the path folders or arguments have spaces, it can be written in a single-line string: - name: my-integration exec: /usr/bin/python /opt/integrations/my-script.py --host=127.0.0.1 Copy If any of the path/arguments have spaces that are part of a single element, you can use a YAML array notation: - name: my-integration exec: - C:\\Program Files\\My Integration\\integration.exe - --host - 127.0.0.1 - --port - 8080 Copy The default working directory is the root directory of the agent configuration. It can be overridden with the working_dir property. cli_args The cli_args optional property specifies command line arguments that should be passed to the integration. It is useful when using name as it only provides the integration name identifier (not compatible with exec). - name: my-integration cli_args: [ -interval 10s ] Copy Usual YAML multi-line list format can be used as well: - name: my-integration cli_args: - -interval - 10s Copy when The when property allows to execute the integration only when all the evaluated conditions are successful. Available conditions are: env_exists: Environment variables exist and match value. file_exists: Given file path exists. feature: Provided feature-flag is enabled. Example: integrations: - name: ssh-integration when: file_exists: /var/run/sshd.pid Copy Pass configuration to the integration executable Often integration executables need to receive a configuration to work properly (for example, hostname and port of the monitored system, user credentials, etc.). The Infrastructure agent allows you to configure the integration commands in three ways (which you can combine): Command-line arguments, passed in the exec property. Environment variables, using the env property. Configuration files, whose path needs to be passed through environment variables or command-line arguments (see the config) property. Example: integrations: - name: my-integration exec: /opt/path/bin/script --host 127.0.0.1 --port 8081 - name: nri-mysql env: STATUS_URL: http://10.0.0.3/server-status?auto REMOTE_MONITORING: true Copy env The env property allows you to set environment variables that are passed to the executable. It is a key-value map with the required variables. Example: integrations: - name: nri-postgresql env: DATABASE: postgres PORT: 6432 COLLECTION_LIST: '[\"postgres\"]' COLLECT_DB_LOCK_METRICS: false VERBOSE: 1 Copy If you expect your integration to receive the configuration from the host's environment rather than specifying it explicitly in the configuration file, you need to set the required variables in the Infrastructure agent passthrough_environment global configuration property config This section describes various ways to pass configuration information to an integration. Pass configuration file directly Some integration commands may get their configuration from an external file. If your integration requires a configuration file, nothing prevents you from directly passing its path directly as a command-line argument or an environment variable. Here's an example using configuration of our Flex integration: integrations: - name: nri-flex env: CONFIG_FILE: /etc/nri-flex/configs/http-service.yaml - name: other-integration exec: /opt/integration/integration -f /opt/integration/config.properties Copy The above example assumes that the http-service.yaml and config.properties files exist. We can see that the nri-flex integration is expecting the http-service.yaml complete path via the CONFIG_FILE environment variable and the other-integration expects the complete config.properties path after the -f command-line flag. In the above example, it's necessary for the integration installer/configurator that the configuration files exist in the provided path and that the agent and integrations have read permissions on them. Pass configuration through config section If you prefer to keep your configuration file with the rest of the integration configuration, you can use the config section in the integration entry, which can contain a valid YAML object or just a multi-line string: integrations: - name: nri-flex env: CONFIG_FILE: ${config.path} config: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv - name: other-integration exec: /opt/integration/integration -f ${config.path} config: | example.cfg.verbose=true example.cfg.logger=/var/logs/integration.log example.cfg.hostname=localhost example.cfg.port=9025 Copy In the above examples, every time the nri-flex integration is executed, the agent creates a temporary file with the following contents: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv Copy The above YAML is only a configuration example for the nri-flex integration. The agent ignores its contents; instead, it creates a temporary file and replaces the ${config.path} variable placeholder with its path. When the integration completes execution the temporary file is removed. Also, the agent creates another temporary file before executing the other-integration integration: example.cfg.verbose=true example.cfg.logger=/var/logs/integration.log example.cfg.hostid=localhost example.cfg.port=9025 Copy It replaces the -f ${config.path} command-line placeholder with the temporary path of the written file. By convention, if you do not place the ${config.path} variable in any command-line argument or environment variable value, the agent passes the path of the configuration file via the CONFIG_PATH environment variable: # assuming that nri-example command is prepared to receive the configuration # file via the CONFIG_PATH environment variable integrations: - name: nri-example config: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv Copy Pass secrets and discovery through config section The main benefit for using a config section instead of hardcoding the full path of an external file is that you can insert ${variable} placeholders to apply our auto-discovery feature and secrets management. Here's an example followed by some explanations: variables: my_credentials: vault: http: url: http://my.vault.host/v1/newengine/data/secret headers: X-Vault-Token: my-vault-token discovery: docker: match: label.service: foo integrations: - name: foo-monitor exec: /opt/foo/bin/monitor --config=${config.path} config: | foo.host=${discovery.ip} foo.port=${discovery.port} foo.user=${my_credentials.user} foo.password=${my_credentials.password} Copy Tip (For more details about the variables and discovery sections, please visit the discovery and secrets management documentation). The above example relies on the following premises: There is a Vault service that allows retrieving a JSON object formed by the user and password fields. There may be a variable number of Docker containers labeled with service=foo, which are accessible from the agent host via a discoverable public IP and Port. The user has configured the foo-monitor integration to monitor all the service=foo labeled containers, which share a common user and password. Each instance of the foo-monitor integration requires executing the /opt/foo/bin/monitor executable, passing the text configuration inside the config section via the --config=<path> command-line argument. As example of workflow, imagine that the Vault invocation returns the following JSON: {\"user\":\"monitorer\",\"password\":\"5up3r53cr3t!\"} Copy At the moment of executing the foo-monitor integration, there are three running containers labeled with service=foo: ip: 10.0.0.3, port: 8080 ip: 10.0.0.3, port: 8081 ip: 10.0.0.3, port: 8082 The agent then creates the following three temporary files, using the contents of the config property as a template, but replacing the ${placeholders} by the acquired variables and discovered items (files' path is invented for the sake of simplicity): First match (/tmp/123_discovered): foo.host=10.0.0.3 foo.port=8080 foo.user=monitorer foo.password=5up3r53cr3t! Copy Second match (/tmp/456_discovered): foo.host=10.0.0.3 foo.port=8081 foo.user=monitorer foo.password=5up3r53cr3t! Copy Third match (/tmp/789_discovered) foo.host=10.0.0.3 foo.port=8082 foo.user=monitorer foo.password=5up3r53cr3t! Copy After the config variable placeholders have been replaced and the temporary files have been created, the /opt/foo/bin/monitor executable is executed three times (one per matched container), replacing the ${config.path} command-line placeholder with the temporary file corresponding to each discovered configuration: First match: /opt/foo/bin/monitor --config=/tmp/123_discovered Second match: /opt/foo/bin/monitor --config=/tmp/456_discovered Third match: /opt/foo/bin/monitor --config=/tmp/789_discovered To ensure security and to minimize the chance of leaking secrets to disk, the agent: Creates the files owned by the agent user, for example, root or nri-agent, depending on the user you have configured to run the agent. Sets read permissions only for the owner. Removes the created files when the integration instance finishes its execution. config_template_path If you want to use the secrets management and discovery in the configuration files that you're passing to the integration executable, but you prefer to keep them as an individual file, you can use the config_template_path: <path> option. It works exactly as in the config section: The agent applies secrets management and discovery to the file contents. The agent creates different temporary files that are passed to the integration via the ${config.path} placeholder (or the CONFIG_PATH environment variable). Example: discovery: docker: match: name: /^redis/ integrations: - name: nri-flex env: CONFIG_FILE: ${config.path} config_template_path: /etc/flex-configs/redis.yml Copy In the above example, the redis.yml external file can contain container discovery variable placeholders, like ${discovery.ip} or ${discovery.port}. Configure how the agent executes your integrations The properties of this section modify the way the Infrastructure agent executes and interacts with the integrations, or the way the agent decorates the integrations' data. integration_user The integration commands run as the same user as the agent (usually root or nri-agent). If due to permission restrictions an integration needs to run as another user, its name must be specified in the integration_user property. Example: integrations: - name: dbus-inventory exec: python my-dbus-script.py integration_user: dbus Copy interval The interval option sets the time between consecutive executions of an integration. The accepted format is an integer immediately followed by a time unit (s for seconds, m for minutes, h for hours). The default is 30s, and the minimum accepted value is 15s. Any value lower than 15s is automatically set to 15s. Example: integrations: - name: nri-nginx env: STATUS_URL: http://127.0.0.1/status STATUS_MODULE: discover interval: 20s Copy inventory_source Any inventory item must be catalogued under a category/source taxonomy. By default, each integration inventory is stored as integration/ + name value (for example, integration/nri-apache, integration/nri-mysql). The inventory_source property allows you to override the default taxonomy of inventory data. Example: integrations: - name: nri-nginx - name: nri-apache exec: - /var/db/newrelic-infra/newrelic-integrations/bin/nri-apache - --inventory inventory_source: config/apache Copy In the above example, the nri-nginx inventory, if any, would be visible in the New Relic UI under the integration/nri-nginx source. The nri-apache inventory would be visible under config/apache. labels labels is a key-value map that allows extra metadata to be provided for the integration. The agent uses those labels to decorate the metrics, events, and inventory that it receives from a given integration instance. Example: integrations: - name: nri-apache inventory_source: config/apache labels: env: production role: load_balancer Copy In the above example, the agent decorates all the metrics and events from the nri-apache instance with the following fields: label.env: production label.role: load_balancer Also, the following entries are added to the integration inventory: config/apache/labels/env: production config/apache/labels/role: load_balancer timeout If an integration has not returned any metric (or a heartbeat message as described below) before the time specified in the timeout value, the agent kills the integration process and restarts it after the corresponding interval. The accepted format is an integer number immediately followed (without spaces) by a time unit (ms for milliseconds, s for seconds, m for minutes, h for hours). If a zero (or negative) timeout value is provided, the integration can run forever without being killed by a timeout expiration. For long-running integrations (integrations that keep running, periodically returning metrics/events/inventory), each time the integration submits a metrics/events/inventory payload, the timeout deadline is restarted. That means that long-running integrations must return a valid JSON payload in an interval that is lower than timeout. Returning an empty JSON ({}) is interpreted as a heart-beat message that restarts the timeout, preventing the long-running integration from being killed, even if they don't have information to report. The default is 120s, and the minimum accepted value is 100ms. Any value lower than 100ms is automatically set to 100ms. Example: integrations: - name: nri-jmx arguments: JMX_HOST: jmx-host.localnet JMX_PORT: 7096 COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml\" timeout: 30s Copy working_dir working_dir sets the working directory of the command. If empty or unspecified, the agent runs the command in the Infrastructure agent's current directory. The default is the Infrastructure agent root directory. Example: integrations: - name: my-integration exec: /opt/integration/bin/integration working_dir: /opt/integration/scratch-zone Copy Update older integration configuration In December 2019, the Infrastructure agent version 1.8.0 began using a different configuration format. For details, see Config format differences. The main difference between these formats is that the older configuration format uses two separate configuration files (a INTEGRATION_NAME-definition.yml file and a INTEGRATION_NAME-config.yml file) and the newer version uses a single configuration file. Here are some of the features added by the newer configuration functionality: Flexible configuration via command-line arguments, environment variables, or external files. Ability to group different integrations in the same file. Hot reload: adding a new integration or changing its configuration does not require restarting the agent. Timeouts: if an integration doesn't respond before a user-specified time, the integration process is killed and restarted. Not all on-host integrations come with the newer configuration format, but you can update the configuration to the new format for all on-host integrations to take advantage of the new features. The following YAML shows an example Apache integration configuration using the older configuration format. Note that this configuration will still work with newer agents, but we recommend updating your integrations to take full advantage of features. integration_name: com.newrelic.apache instances: - name: apache-server-metrics command: metrics arguments: status_url: http://127.0.0.1/server-status?auto remote_monitoring: true labels: env: production role: load_balancer - name: apache-server-inventory command: inventory arguments: remote_monitoring: true labels: env: production role: load_balancer Copy To update an older integration configuration to the new format, you must perform two steps: Rename the instances top-level section to integrations. Remove the integration_name top-level section and add it to each integration entry. You are no longer required to keep a separate file for each integration type and you can group your legacy integration entries in the same file as other integrations. The new version of the Apache integration config: integrations: - name: apache-server-metrics integration_name: com.newrelic.apache command: metrics arguments: status_url: http://127.0.0.1/server-status?auto remote_monitoring: true labels: env: production role: load_balancer - name: apache-server-inventory integration_name: com.newrelic.apache command: inventory arguments: remote_monitoring: true labels: env: production role: load_balancer Copy Please note that because the older configuration format doesn't support hot reloading. You will need to restart the Infrastructure agent to remove the old integrations configuration (otherwise the old instances will coexist with the new ones).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.1747,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Newer configuration format ",
        "sections": "On-host <em>integrations</em>: Newer configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. In December 2019, <em>Infrastructure</em> agent version 1.8.0 began supporting a newer configuration format that makes use of a single configuration file (instead of two separate files), and provides other"
      },
      "id": "603e923928ccbcb8dfeba751"
    },
    {
      "sections": [
        "On-host integrations: Standard configuration format",
        "Configuration file structure",
        "Definition file",
        "Definition file header",
        "Definition file commands",
        "Configuration file",
        "Tip",
        "Config file field definitions"
      ],
      "title": "On-host integrations: Standard configuration format",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "3b4c5ab77b2b8025ca8de375403f24bc75b3dca7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-standard-configuration-format/",
      "published_at": "2021-05-05T00:23:27Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. This document explains the older, standard configuration format. This format is used by most on-host integrations and is supported by newer Infrastructure agents (which also support a newer, improved configuration format). For an introduction to configuration, see Config overview. Configuration file structure An on-host integration that uses the standard configuration format requires two configuration files: A definition file A configuration file Definition file The definition file has a naming format like INTEGRATION_NAME-definition.yml. This file provides descriptive information about the integration, such as: the version of the JSON protocol it supports, a list of commands it can execute, and arguments that it accepts. It lives in this directory: Linux: /var/db/newrelic-infra/newrelic-integrations Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations Copy Here's an example of an NGINX integration definition file with two command sections on a Linux system: name: com.myorg.nginx protocol_version: 2 description: Collect metric and configuration data from NGINX os: linux commands: metrics: command: - myorg-nginx - --metrics interval: 15 inventory: command: - myorg-nginx - --inventory interval: 120 prefix: integration/myorg-nginx Copy A definition file can be broken down into two parts: The header The commands section Definition file header Here are explanations of a definition file's header elements: Definition header field Description name Required. A unique name name to identify the integration for logging, internal metrics, etc. When the agent loads the config file, New Relic uses the name to look up the integration in the agent's registry. protocol_version Required. The version number of the protocol. New Relic uses this to ensure compatibility between the integration and the agent. If the agent does not recognize an integration's version, it will filter out that integration and create a log message. The current version of the JSON protocol is 2. For more on protocol changes, see SDK changes. description Optional. Human-friendly explanation of what the integration does. os Optional. The operating system where the integration runs. New Relic uses this to filter integrations that you intend to run only on specific operating systems. Default: Run the integration regardless of the os value. To restrict the integration to a specific operating system, use either of these options: linux windows Definition file commands After the header is a list of commands. The commands section defines: One or more independent operating modes for the executable The runtime data required for it to be executed The commands section is a YAML map of command definitions, where each key is the unique alias name of the command in the integration's config file that specifies the executable to run. Definition commands Description command Required. The actual command line to be executed as a YAML array of command parts. These are assembled to run the actual command. For simple commands, the array might be only a single element. Additional command rules include: command arguments: The command and any command line arguments that are shared for all instances of the integration configuration. command execution: The command will be executed in the same directory as the definition file. command path: Any commands available on the host's $PATH can be used. Executables located in the same directory as the definition file, or in a subdirectory of it, can be executed using a relative path. For example: Linux: To run an executable called myorg-nginx in the same directory as the definition file, you could use myorg-nginx or ./myorg-nginx. Linux systems will execute myorg-nginx as if the user used ./myorg-nginx. Windows: To run an executable called myorg-nginx.exe in the same directory as the definition file, you could use \\myorg-nginx.exe or .\\myorg-nginx.exe. Windows systems writing myorg-nginx.exe will be executed as if indicating the current path: .\\myorg-nginx.exe. To use a command installed inside a directory on the host's $PATH, simply use the command name. Example: python. To run any other executable which is neither on the host's $PATH nor within the integration's directory, use an absolute path to the executable. Example: /opt/jdk/bin/java. If the given executable name exists within the integration's directory but also exists elsewhere on the system $PATH, the version in the integration's directory takes precedence. interval Optional. The number of seconds between two consecutive executions of the command, in particular between the end of the previous execution and the start of the next execution. Default for metric polling: 30 seconds. Minimum (floor): 15 seconds. Alerts: For metrics being used for alerts, use an interval of 30 seconds or less. prefix Optional. The categorization of the inventory in the form of category/short_integration_name. Example: integration/myorg-nginx. The prefix is not a platform-specific path. The forward slash is the correct separator between the category and short_integration_name. The prefix can have a maximum of two levels. Otherwise inventory will not be reported. Default value if not set: integration/integration_name. Configuration file The configuration file has a naming format like INTEGRATION_NAME-config.yml. This file specifies which executables to run and the parameters required to run them. It lives in this directory: Linux: /etc/newrelic-infra/integrations.d/ Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d Copy Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Here's an example of a config file with one instance defined. Explanations of these fields are explained below the example. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://127.0.0.1/status labels: environment: production role: load_balancer Copy Another example of a config file with two instances defined. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://one.url/status labels: environment: production role: load_balancer - name: nginx2.myorg.com-metrics command: metrics arguments: status_url: http://another.url/status labels: environment: production role: load_balancer Copy Config file field definitions Config file field Description integration_name Required. This is the header and is used to identify which executables to run. This name must exactly match the name specified in the integration's definition file. Recommendation: To ensure unique names, use reverse domain name notation. name Required. This is the name for the specific invocation (instance) of the integration. This is used to help identify any log messages generated by this integration and is also useful for troubleshooting. command Required. This is the command to be executed. This must exactly match one of the unique alias names specified in the integration's definition file. arguments Optional. A YAML object where: Keys: The argument name. Transformed to upper case when set as environment variable. Values: The argument values. Passed through as is. The arguments are made available to an integration as a set of environment variables. Arguments in the config file cannot be capitalised and should use underscores to separate words. labels Optional. A YAML object where: Keys: The label name. Values: The defined label value. integration_user Optional. String with the name the agent will use for executing the integration binary. Default: depends on the usermode. By default, integrations are executed with the same user that's running the integration agent, nri-agent for privileged and unprivileged mode and root user for root mode. When present, the Infrastructure agent will execute the integration binary as the specified user. For example, to run the integration binary as the root user when running the agent in a usermode different than root, just add integration_user: root to the configuration file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.1747,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Standard configuration format",
        "sections": "On-host <em>integrations</em>: Standard configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. This document explains the older, standard configuration format. This format is used by most on-host <em>integrations</em> and is supported by newer <em>Infrastructure</em> agents (which also support a newer, improved"
      },
      "id": "603e923a196a67581ca83db3"
    }
  ],
  "/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-newer-configuration-format": [
    {
      "sections": [
        "Compatibility and requirements for Infrastructure Integrations SDK",
        "Infrastructure version",
        "Operating systems",
        "Data and file specifications",
        "SDK version changes"
      ],
      "title": "Compatibility and requirements for Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "ea949057459c7c9648a9215928c1fd54c9d6a703",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk/",
      "published_at": "2021-05-04T18:45:04Z",
      "updated_at": "2021-03-13T02:32:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before using the New Relic Infrastructure integrations SDK to create a custom on-host integration, make sure your system meets these requirements. Infrastructure version To use the infrastructure Integrations SDK v4, you must have the Infrastructure agent version 1.13.0 or higher. To use SDK v3, you must have agent version 1.0.888 or higher. For Infrastructure agent version information and options for installation and updates, see the Infrastructure release notes. For notes on SDK versions and changes, see the change log. Operating systems Integrations built with the SDK can be compiled for either Linux or Windows operating systems. Data and file specifications Infrastructure on-host integrations can be created with any programming language, as long as they adhere to the data and file specifications. SDK version changes Infrastructure agent version Details 1.13.0 or higher Changes to integration protocol (v4), including support to dynamically register entities in NR1 and send dimensional metrics. New metric types. See SDK v4 release notes. 1.0.888 or higher Changes to JSON format, including support for multiple entities. The new JSON format is referred to as protocol 2 (described in JSON output documentation and also used in the definition file). Uses newer set of Go language build tools (referenced as GoSDK v3). 1.0.726 (for Linux); 1.0.775 (for Windows) Uses JSON protocol 1 (described in JSON output documentation and also used in the definition file). Uses older set of Go language build tools (referenced as GoSDK v2). If you've built an integration using the older Go language build tools and wish to update, see Upgrade from GoSDK v2 to v3 and Upgrade from v3 to v4. For updating the Infrastructure agent, see Update the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.16664,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Before using the New Relic <em>Infrastructure</em> <em>integrations</em> <em>SDK</em> to <em>create</em> a custom on-host integration, make sure your system meets these requirements. <em>Infrastructure</em> version To use the <em>infrastructure</em> <em>Integrations</em> <em>SDK</em> v4, you must have the <em>Infrastructure</em> agent version 1.13.0 or higher. To use <em>SDK</em> v3"
      },
      "id": "60440de564441fa14f378ecc"
    },
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-05-05T00:27:50Z",
      "updated_at": "2021-03-16T08:29:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent One or more configuration files For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.19624,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    },
    {
      "sections": [
        "On-host integrations: Standard configuration format",
        "Configuration file structure",
        "Definition file",
        "Definition file header",
        "Definition file commands",
        "Configuration file",
        "Tip",
        "Config file field definitions"
      ],
      "title": "On-host integrations: Standard configuration format",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "3b4c5ab77b2b8025ca8de375403f24bc75b3dca7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-standard-configuration-format/",
      "published_at": "2021-05-05T00:23:27Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. This document explains the older, standard configuration format. This format is used by most on-host integrations and is supported by newer Infrastructure agents (which also support a newer, improved configuration format). For an introduction to configuration, see Config overview. Configuration file structure An on-host integration that uses the standard configuration format requires two configuration files: A definition file A configuration file Definition file The definition file has a naming format like INTEGRATION_NAME-definition.yml. This file provides descriptive information about the integration, such as: the version of the JSON protocol it supports, a list of commands it can execute, and arguments that it accepts. It lives in this directory: Linux: /var/db/newrelic-infra/newrelic-integrations Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations Copy Here's an example of an NGINX integration definition file with two command sections on a Linux system: name: com.myorg.nginx protocol_version: 2 description: Collect metric and configuration data from NGINX os: linux commands: metrics: command: - myorg-nginx - --metrics interval: 15 inventory: command: - myorg-nginx - --inventory interval: 120 prefix: integration/myorg-nginx Copy A definition file can be broken down into two parts: The header The commands section Definition file header Here are explanations of a definition file's header elements: Definition header field Description name Required. A unique name name to identify the integration for logging, internal metrics, etc. When the agent loads the config file, New Relic uses the name to look up the integration in the agent's registry. protocol_version Required. The version number of the protocol. New Relic uses this to ensure compatibility between the integration and the agent. If the agent does not recognize an integration's version, it will filter out that integration and create a log message. The current version of the JSON protocol is 2. For more on protocol changes, see SDK changes. description Optional. Human-friendly explanation of what the integration does. os Optional. The operating system where the integration runs. New Relic uses this to filter integrations that you intend to run only on specific operating systems. Default: Run the integration regardless of the os value. To restrict the integration to a specific operating system, use either of these options: linux windows Definition file commands After the header is a list of commands. The commands section defines: One or more independent operating modes for the executable The runtime data required for it to be executed The commands section is a YAML map of command definitions, where each key is the unique alias name of the command in the integration's config file that specifies the executable to run. Definition commands Description command Required. The actual command line to be executed as a YAML array of command parts. These are assembled to run the actual command. For simple commands, the array might be only a single element. Additional command rules include: command arguments: The command and any command line arguments that are shared for all instances of the integration configuration. command execution: The command will be executed in the same directory as the definition file. command path: Any commands available on the host's $PATH can be used. Executables located in the same directory as the definition file, or in a subdirectory of it, can be executed using a relative path. For example: Linux: To run an executable called myorg-nginx in the same directory as the definition file, you could use myorg-nginx or ./myorg-nginx. Linux systems will execute myorg-nginx as if the user used ./myorg-nginx. Windows: To run an executable called myorg-nginx.exe in the same directory as the definition file, you could use \\myorg-nginx.exe or .\\myorg-nginx.exe. Windows systems writing myorg-nginx.exe will be executed as if indicating the current path: .\\myorg-nginx.exe. To use a command installed inside a directory on the host's $PATH, simply use the command name. Example: python. To run any other executable which is neither on the host's $PATH nor within the integration's directory, use an absolute path to the executable. Example: /opt/jdk/bin/java. If the given executable name exists within the integration's directory but also exists elsewhere on the system $PATH, the version in the integration's directory takes precedence. interval Optional. The number of seconds between two consecutive executions of the command, in particular between the end of the previous execution and the start of the next execution. Default for metric polling: 30 seconds. Minimum (floor): 15 seconds. Alerts: For metrics being used for alerts, use an interval of 30 seconds or less. prefix Optional. The categorization of the inventory in the form of category/short_integration_name. Example: integration/myorg-nginx. The prefix is not a platform-specific path. The forward slash is the correct separator between the category and short_integration_name. The prefix can have a maximum of two levels. Otherwise inventory will not be reported. Default value if not set: integration/integration_name. Configuration file The configuration file has a naming format like INTEGRATION_NAME-config.yml. This file specifies which executables to run and the parameters required to run them. It lives in this directory: Linux: /etc/newrelic-infra/integrations.d/ Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d Copy Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Here's an example of a config file with one instance defined. Explanations of these fields are explained below the example. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://127.0.0.1/status labels: environment: production role: load_balancer Copy Another example of a config file with two instances defined. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://one.url/status labels: environment: production role: load_balancer - name: nginx2.myorg.com-metrics command: metrics arguments: status_url: http://another.url/status labels: environment: production role: load_balancer Copy Config file field definitions Config file field Description integration_name Required. This is the header and is used to identify which executables to run. This name must exactly match the name specified in the integration's definition file. Recommendation: To ensure unique names, use reverse domain name notation. name Required. This is the name for the specific invocation (instance) of the integration. This is used to help identify any log messages generated by this integration and is also useful for troubleshooting. command Required. This is the command to be executed. This must exactly match one of the unique alias names specified in the integration's definition file. arguments Optional. A YAML object where: Keys: The argument name. Transformed to upper case when set as environment variable. Values: The argument values. Passed through as is. The arguments are made available to an integration as a set of environment variables. Arguments in the config file cannot be capitalised and should use underscores to separate words. labels Optional. A YAML object where: Keys: The label name. Values: The defined label value. integration_user Optional. String with the name the agent will use for executing the integration binary. Default: depends on the usermode. By default, integrations are executed with the same user that's running the integration agent, nri-agent for privileged and unprivileged mode and root user for root mode. When present, the Infrastructure agent will execute the integration binary as the specified user. For example, to run the integration binary as the root user when running the agent in a usermode different than root, just add integration_user: root to the configuration file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.1747,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Standard configuration format",
        "sections": "On-host <em>integrations</em>: Standard configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. This document explains the older, standard configuration format. This format is used by most on-host <em>integrations</em> and is supported by newer <em>Infrastructure</em> agents (which also support a newer, improved"
      },
      "id": "603e923a196a67581ca83db3"
    }
  ],
  "/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-standard-configuration-format": [
    {
      "sections": [
        "Compatibility and requirements for Infrastructure Integrations SDK",
        "Infrastructure version",
        "Operating systems",
        "Data and file specifications",
        "SDK version changes"
      ],
      "title": "Compatibility and requirements for Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "ea949057459c7c9648a9215928c1fd54c9d6a703",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk/",
      "published_at": "2021-05-04T18:45:04Z",
      "updated_at": "2021-03-13T02:32:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before using the New Relic Infrastructure integrations SDK to create a custom on-host integration, make sure your system meets these requirements. Infrastructure version To use the infrastructure Integrations SDK v4, you must have the Infrastructure agent version 1.13.0 or higher. To use SDK v3, you must have agent version 1.0.888 or higher. For Infrastructure agent version information and options for installation and updates, see the Infrastructure release notes. For notes on SDK versions and changes, see the change log. Operating systems Integrations built with the SDK can be compiled for either Linux or Windows operating systems. Data and file specifications Infrastructure on-host integrations can be created with any programming language, as long as they adhere to the data and file specifications. SDK version changes Infrastructure agent version Details 1.13.0 or higher Changes to integration protocol (v4), including support to dynamically register entities in NR1 and send dimensional metrics. New metric types. See SDK v4 release notes. 1.0.888 or higher Changes to JSON format, including support for multiple entities. The new JSON format is referred to as protocol 2 (described in JSON output documentation and also used in the definition file). Uses newer set of Go language build tools (referenced as GoSDK v3). 1.0.726 (for Linux); 1.0.775 (for Windows) Uses JSON protocol 1 (described in JSON output documentation and also used in the definition file). Uses older set of Go language build tools (referenced as GoSDK v2). If you've built an integration using the older Go language build tools and wish to update, see Upgrade from GoSDK v2 to v3 and Upgrade from v3 to v4. For updating the Infrastructure agent, see Update the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.16664,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Before using the New Relic <em>Infrastructure</em> <em>integrations</em> <em>SDK</em> to <em>create</em> a custom on-host integration, make sure your system meets these requirements. <em>Infrastructure</em> version To use the <em>infrastructure</em> <em>Integrations</em> <em>SDK</em> v4, you must have the <em>Infrastructure</em> agent version 1.13.0 or higher. To use <em>SDK</em> v3"
      },
      "id": "60440de564441fa14f378ecc"
    },
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-05-05T00:27:50Z",
      "updated_at": "2021-03-16T08:29:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent One or more configuration files For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.19624,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    },
    {
      "sections": [
        "On-host integrations: Newer configuration format",
        "Configuration structure",
        "Tip",
        "Important",
        "List of configuration properties",
        "Select an integration to run",
        "name",
        "exec",
        "cli_args",
        "when",
        "Pass configuration to the integration executable",
        "env",
        "config",
        "config_template_path",
        "Configure how the agent executes your integrations",
        "integration_user",
        "interval",
        "inventory_source",
        "labels",
        "timeout",
        "working_dir",
        "Update older integration configuration"
      ],
      "title": "On-host integrations: Newer configuration format ",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bc5bee12812cff6fabc728961e1b342b83f3e471",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-newer-configuration-format/",
      "published_at": "2021-05-05T00:27:53Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. In December 2019, Infrastructure agent version 1.8.0 began supporting a newer configuration format that makes use of a single configuration file (instead of two separate files), and provides other improvements. This document will explain how this newer format works. The older standard configuration format is supported by current Infrastructure agents. For an introduction to configuration, see Config overview. Configuration structure An on-host integration's configuration YAML must have an integrations top-level section containing a YAML array, where each entry represents an integration and its configuration. For each integration entry, only the name property is mandatory. The other properties are optional. Here's an example configuration featuring two integrations: our built-in Docker integration, which requires no configuration, and our MySQL integration: integrations: # New Relic integration that does not require any configuration - name: nri-docker # New Relic integration that gets its configuration from the environment - name: nri-mysql env: PORT: 3306 USERNAME: newrelic PASSWORD: 123456789 # to hide this field, read the secrets management documentation # Any free-form integration executed via a user-provided command - name: my-own-integration exec: python /opt/integrations/my-script.py --host=127.0.0.1 Copy You can have as many configuration YAML files as you want and can group your integration instances. Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Each configuration YAML file can also contain discovery and variables top-level sections. Important This configuration format does not require an agent restart. When saved, changes are detected and implemented immediately. This means that saving intermediate configuration changes may cause the integration to stop working. List of configuration properties This is a list of the general properties used to configure an integration. For more details about using these properties, including example values, see the documentation following the table. Config Description name Name of the integration. This is the only mandatory configuration property across all on-host integrations. If the exec field is not set it will also be the name of the integration executable. cli_args Optional list of command line arguments when name is used to provide the integration executable. Available since agent version 1.13.0. exec Full path to the integration executable, plus arguments. It may be a single-line string or a string array. If left unspecified, the exec field defaults to the name field. env YAML map containing the environment variables to be passed to the integration, where key is the environment variable name, and value is the variable value. config Configuration that is written as an external file and the path that is passed to the integration with the CONFIG_PATH environment variable or the ${config.path} variable placeholder. config_template_path Any external file whose path is passed to the integration with the CONFIG_PATH environment variable or the ${config.path} variable placeholder. Its usage allows applying discovery and secrets binding to any external configuration. integration_user Name of the user who runs the integration. interval Time between consecutive executions of the integration. It must be a number followed by a time unit (s, m or h), without spaces. inventory_source Allows overriding the category and term of the inventory source. labels Map with labels that decorate the data (metrics, events, inventory) reported by the integration. timeout A number followed by a time unit (ms, s, m or h). An integration that hasn't responded in this time period is killed and restarted. working_dir Working directory for the integration binary. when Integration is only executed if the clause evaluates to true. Conditions are defined below. The remainder of this document describes config properties grouped by their functionality: Select an integration to run Pass configuration to the integration command Configure how the agent executes integrations Select an integration to run There are two properties to select which integration will run: name and exec. The only mandatory property across all on-host integrations is name. The remaining properties specified in this document are optional. Example: integrations: - name: nri-docker - name: my-integration exec: /usr/local/bin/my-integration --metrics --inventory Copy name The mandatory name property can work in two ways: If the exec property is set: The name property only provides an identifier for the integration instance. This identifier is used in log messages and to provide a default inventory category/source in the form integration/<name> (for example, integration/nri-redis). This inventory path can be overridden with the inventory_source configuration option. If the exec property is not set: The agent looks for (and executes) an executable with the name value in any of the following folders: Linux: /var/db/newrelic-infra/newrelic-integrations/bin /var/db/newrelic-infra/newrelic-integrations /var/db/newrelic-infra/custom-integrations/bin /var/db/newrelic-infra/custom-integrations Windows C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations\\bin C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations If there is no executable with this name in the above folders the agent logs an error and the integration is not executed. Important In Windows, do not append the .exe extension to the name. The agent does this for you (for example, name: nri-mysql would look for nri-mysql.exe in the above folders). exec The exec optional property specifies the path, command, and command-line arguments of the integration to execute. When none of the path folders or arguments have spaces, it can be written in a single-line string: - name: my-integration exec: /usr/bin/python /opt/integrations/my-script.py --host=127.0.0.1 Copy If any of the path/arguments have spaces that are part of a single element, you can use a YAML array notation: - name: my-integration exec: - C:\\Program Files\\My Integration\\integration.exe - --host - 127.0.0.1 - --port - 8080 Copy The default working directory is the root directory of the agent configuration. It can be overridden with the working_dir property. cli_args The cli_args optional property specifies command line arguments that should be passed to the integration. It is useful when using name as it only provides the integration name identifier (not compatible with exec). - name: my-integration cli_args: [ -interval 10s ] Copy Usual YAML multi-line list format can be used as well: - name: my-integration cli_args: - -interval - 10s Copy when The when property allows to execute the integration only when all the evaluated conditions are successful. Available conditions are: env_exists: Environment variables exist and match value. file_exists: Given file path exists. feature: Provided feature-flag is enabled. Example: integrations: - name: ssh-integration when: file_exists: /var/run/sshd.pid Copy Pass configuration to the integration executable Often integration executables need to receive a configuration to work properly (for example, hostname and port of the monitored system, user credentials, etc.). The Infrastructure agent allows you to configure the integration commands in three ways (which you can combine): Command-line arguments, passed in the exec property. Environment variables, using the env property. Configuration files, whose path needs to be passed through environment variables or command-line arguments (see the config) property. Example: integrations: - name: my-integration exec: /opt/path/bin/script --host 127.0.0.1 --port 8081 - name: nri-mysql env: STATUS_URL: http://10.0.0.3/server-status?auto REMOTE_MONITORING: true Copy env The env property allows you to set environment variables that are passed to the executable. It is a key-value map with the required variables. Example: integrations: - name: nri-postgresql env: DATABASE: postgres PORT: 6432 COLLECTION_LIST: '[\"postgres\"]' COLLECT_DB_LOCK_METRICS: false VERBOSE: 1 Copy If you expect your integration to receive the configuration from the host's environment rather than specifying it explicitly in the configuration file, you need to set the required variables in the Infrastructure agent passthrough_environment global configuration property config This section describes various ways to pass configuration information to an integration. Pass configuration file directly Some integration commands may get their configuration from an external file. If your integration requires a configuration file, nothing prevents you from directly passing its path directly as a command-line argument or an environment variable. Here's an example using configuration of our Flex integration: integrations: - name: nri-flex env: CONFIG_FILE: /etc/nri-flex/configs/http-service.yaml - name: other-integration exec: /opt/integration/integration -f /opt/integration/config.properties Copy The above example assumes that the http-service.yaml and config.properties files exist. We can see that the nri-flex integration is expecting the http-service.yaml complete path via the CONFIG_FILE environment variable and the other-integration expects the complete config.properties path after the -f command-line flag. In the above example, it's necessary for the integration installer/configurator that the configuration files exist in the provided path and that the agent and integrations have read permissions on them. Pass configuration through config section If you prefer to keep your configuration file with the rest of the integration configuration, you can use the config section in the integration entry, which can contain a valid YAML object or just a multi-line string: integrations: - name: nri-flex env: CONFIG_FILE: ${config.path} config: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv - name: other-integration exec: /opt/integration/integration -f ${config.path} config: | example.cfg.verbose=true example.cfg.logger=/var/logs/integration.log example.cfg.hostname=localhost example.cfg.port=9025 Copy In the above examples, every time the nri-flex integration is executed, the agent creates a temporary file with the following contents: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv Copy The above YAML is only a configuration example for the nri-flex integration. The agent ignores its contents; instead, it creates a temporary file and replaces the ${config.path} variable placeholder with its path. When the integration completes execution the temporary file is removed. Also, the agent creates another temporary file before executing the other-integration integration: example.cfg.verbose=true example.cfg.logger=/var/logs/integration.log example.cfg.hostid=localhost example.cfg.port=9025 Copy It replaces the -f ${config.path} command-line placeholder with the temporary path of the written file. By convention, if you do not place the ${config.path} variable in any command-line argument or environment variable value, the agent passes the path of the configuration file via the CONFIG_PATH environment variable: # assuming that nri-example command is prepared to receive the configuration # file via the CONFIG_PATH environment variable integrations: - name: nri-example config: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv Copy Pass secrets and discovery through config section The main benefit for using a config section instead of hardcoding the full path of an external file is that you can insert ${variable} placeholders to apply our auto-discovery feature and secrets management. Here's an example followed by some explanations: variables: my_credentials: vault: http: url: http://my.vault.host/v1/newengine/data/secret headers: X-Vault-Token: my-vault-token discovery: docker: match: label.service: foo integrations: - name: foo-monitor exec: /opt/foo/bin/monitor --config=${config.path} config: | foo.host=${discovery.ip} foo.port=${discovery.port} foo.user=${my_credentials.user} foo.password=${my_credentials.password} Copy Tip (For more details about the variables and discovery sections, please visit the discovery and secrets management documentation). The above example relies on the following premises: There is a Vault service that allows retrieving a JSON object formed by the user and password fields. There may be a variable number of Docker containers labeled with service=foo, which are accessible from the agent host via a discoverable public IP and Port. The user has configured the foo-monitor integration to monitor all the service=foo labeled containers, which share a common user and password. Each instance of the foo-monitor integration requires executing the /opt/foo/bin/monitor executable, passing the text configuration inside the config section via the --config=<path> command-line argument. As example of workflow, imagine that the Vault invocation returns the following JSON: {\"user\":\"monitorer\",\"password\":\"5up3r53cr3t!\"} Copy At the moment of executing the foo-monitor integration, there are three running containers labeled with service=foo: ip: 10.0.0.3, port: 8080 ip: 10.0.0.3, port: 8081 ip: 10.0.0.3, port: 8082 The agent then creates the following three temporary files, using the contents of the config property as a template, but replacing the ${placeholders} by the acquired variables and discovered items (files' path is invented for the sake of simplicity): First match (/tmp/123_discovered): foo.host=10.0.0.3 foo.port=8080 foo.user=monitorer foo.password=5up3r53cr3t! Copy Second match (/tmp/456_discovered): foo.host=10.0.0.3 foo.port=8081 foo.user=monitorer foo.password=5up3r53cr3t! Copy Third match (/tmp/789_discovered) foo.host=10.0.0.3 foo.port=8082 foo.user=monitorer foo.password=5up3r53cr3t! Copy After the config variable placeholders have been replaced and the temporary files have been created, the /opt/foo/bin/monitor executable is executed three times (one per matched container), replacing the ${config.path} command-line placeholder with the temporary file corresponding to each discovered configuration: First match: /opt/foo/bin/monitor --config=/tmp/123_discovered Second match: /opt/foo/bin/monitor --config=/tmp/456_discovered Third match: /opt/foo/bin/monitor --config=/tmp/789_discovered To ensure security and to minimize the chance of leaking secrets to disk, the agent: Creates the files owned by the agent user, for example, root or nri-agent, depending on the user you have configured to run the agent. Sets read permissions only for the owner. Removes the created files when the integration instance finishes its execution. config_template_path If you want to use the secrets management and discovery in the configuration files that you're passing to the integration executable, but you prefer to keep them as an individual file, you can use the config_template_path: <path> option. It works exactly as in the config section: The agent applies secrets management and discovery to the file contents. The agent creates different temporary files that are passed to the integration via the ${config.path} placeholder (or the CONFIG_PATH environment variable). Example: discovery: docker: match: name: /^redis/ integrations: - name: nri-flex env: CONFIG_FILE: ${config.path} config_template_path: /etc/flex-configs/redis.yml Copy In the above example, the redis.yml external file can contain container discovery variable placeholders, like ${discovery.ip} or ${discovery.port}. Configure how the agent executes your integrations The properties of this section modify the way the Infrastructure agent executes and interacts with the integrations, or the way the agent decorates the integrations' data. integration_user The integration commands run as the same user as the agent (usually root or nri-agent). If due to permission restrictions an integration needs to run as another user, its name must be specified in the integration_user property. Example: integrations: - name: dbus-inventory exec: python my-dbus-script.py integration_user: dbus Copy interval The interval option sets the time between consecutive executions of an integration. The accepted format is an integer immediately followed by a time unit (s for seconds, m for minutes, h for hours). The default is 30s, and the minimum accepted value is 15s. Any value lower than 15s is automatically set to 15s. Example: integrations: - name: nri-nginx env: STATUS_URL: http://127.0.0.1/status STATUS_MODULE: discover interval: 20s Copy inventory_source Any inventory item must be catalogued under a category/source taxonomy. By default, each integration inventory is stored as integration/ + name value (for example, integration/nri-apache, integration/nri-mysql). The inventory_source property allows you to override the default taxonomy of inventory data. Example: integrations: - name: nri-nginx - name: nri-apache exec: - /var/db/newrelic-infra/newrelic-integrations/bin/nri-apache - --inventory inventory_source: config/apache Copy In the above example, the nri-nginx inventory, if any, would be visible in the New Relic UI under the integration/nri-nginx source. The nri-apache inventory would be visible under config/apache. labels labels is a key-value map that allows extra metadata to be provided for the integration. The agent uses those labels to decorate the metrics, events, and inventory that it receives from a given integration instance. Example: integrations: - name: nri-apache inventory_source: config/apache labels: env: production role: load_balancer Copy In the above example, the agent decorates all the metrics and events from the nri-apache instance with the following fields: label.env: production label.role: load_balancer Also, the following entries are added to the integration inventory: config/apache/labels/env: production config/apache/labels/role: load_balancer timeout If an integration has not returned any metric (or a heartbeat message as described below) before the time specified in the timeout value, the agent kills the integration process and restarts it after the corresponding interval. The accepted format is an integer number immediately followed (without spaces) by a time unit (ms for milliseconds, s for seconds, m for minutes, h for hours). If a zero (or negative) timeout value is provided, the integration can run forever without being killed by a timeout expiration. For long-running integrations (integrations that keep running, periodically returning metrics/events/inventory), each time the integration submits a metrics/events/inventory payload, the timeout deadline is restarted. That means that long-running integrations must return a valid JSON payload in an interval that is lower than timeout. Returning an empty JSON ({}) is interpreted as a heart-beat message that restarts the timeout, preventing the long-running integration from being killed, even if they don't have information to report. The default is 120s, and the minimum accepted value is 100ms. Any value lower than 100ms is automatically set to 100ms. Example: integrations: - name: nri-jmx arguments: JMX_HOST: jmx-host.localnet JMX_PORT: 7096 COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml\" timeout: 30s Copy working_dir working_dir sets the working directory of the command. If empty or unspecified, the agent runs the command in the Infrastructure agent's current directory. The default is the Infrastructure agent root directory. Example: integrations: - name: my-integration exec: /opt/integration/bin/integration working_dir: /opt/integration/scratch-zone Copy Update older integration configuration In December 2019, the Infrastructure agent version 1.8.0 began using a different configuration format. For details, see Config format differences. The main difference between these formats is that the older configuration format uses two separate configuration files (a INTEGRATION_NAME-definition.yml file and a INTEGRATION_NAME-config.yml file) and the newer version uses a single configuration file. Here are some of the features added by the newer configuration functionality: Flexible configuration via command-line arguments, environment variables, or external files. Ability to group different integrations in the same file. Hot reload: adding a new integration or changing its configuration does not require restarting the agent. Timeouts: if an integration doesn't respond before a user-specified time, the integration process is killed and restarted. Not all on-host integrations come with the newer configuration format, but you can update the configuration to the new format for all on-host integrations to take advantage of the new features. The following YAML shows an example Apache integration configuration using the older configuration format. Note that this configuration will still work with newer agents, but we recommend updating your integrations to take full advantage of features. integration_name: com.newrelic.apache instances: - name: apache-server-metrics command: metrics arguments: status_url: http://127.0.0.1/server-status?auto remote_monitoring: true labels: env: production role: load_balancer - name: apache-server-inventory command: inventory arguments: remote_monitoring: true labels: env: production role: load_balancer Copy To update an older integration configuration to the new format, you must perform two steps: Rename the instances top-level section to integrations. Remove the integration_name top-level section and add it to each integration entry. You are no longer required to keep a separate file for each integration type and you can group your legacy integration entries in the same file as other integrations. The new version of the Apache integration config: integrations: - name: apache-server-metrics integration_name: com.newrelic.apache command: metrics arguments: status_url: http://127.0.0.1/server-status?auto remote_monitoring: true labels: env: production role: load_balancer - name: apache-server-inventory integration_name: com.newrelic.apache command: inventory arguments: remote_monitoring: true labels: env: production role: load_balancer Copy Please note that because the older configuration format doesn't support hot reloading. You will need to restart the Infrastructure agent to remove the old integrations configuration (otherwise the old instances will coexist with the new ones).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.1747,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Newer configuration format ",
        "sections": "On-host <em>integrations</em>: Newer configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. In December 2019, <em>Infrastructure</em> agent version 1.8.0 began supporting a newer configuration format that makes use of a single configuration file (instead of two separate files), and provides other"
      },
      "id": "603e923928ccbcb8dfeba751"
    }
  ],
  "/docs/create-integrations/infrastructure-integrations-sdk/specifications/integration-logging-recommendations": [
    {
      "sections": [
        "Compatibility and requirements for Infrastructure Integrations SDK",
        "Infrastructure version",
        "Operating systems",
        "Data and file specifications",
        "SDK version changes"
      ],
      "title": "Compatibility and requirements for Infrastructure Integrations SDK",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Get started"
      ],
      "external_id": "ea949057459c7c9648a9215928c1fd54c9d6a703",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/get-started/compatibility-requirements-infrastructure-integrations-sdk/",
      "published_at": "2021-05-04T18:45:04Z",
      "updated_at": "2021-03-13T02:32:09Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before using the New Relic Infrastructure integrations SDK to create a custom on-host integration, make sure your system meets these requirements. Infrastructure version To use the infrastructure Integrations SDK v4, you must have the Infrastructure agent version 1.13.0 or higher. To use SDK v3, you must have agent version 1.0.888 or higher. For Infrastructure agent version information and options for installation and updates, see the Infrastructure release notes. For notes on SDK versions and changes, see the change log. Operating systems Integrations built with the SDK can be compiled for either Linux or Windows operating systems. Data and file specifications Infrastructure on-host integrations can be created with any programming language, as long as they adhere to the data and file specifications. SDK version changes Infrastructure agent version Details 1.13.0 or higher Changes to integration protocol (v4), including support to dynamically register entities in NR1 and send dimensional metrics. New metric types. See SDK v4 release notes. 1.0.888 or higher Changes to JSON format, including support for multiple entities. The new JSON format is referred to as protocol 2 (described in JSON output documentation and also used in the definition file). Uses newer set of Go language build tools (referenced as GoSDK v3). 1.0.726 (for Linux); 1.0.775 (for Windows) Uses JSON protocol 1 (described in JSON output documentation and also used in the definition file). Uses older set of Go language build tools (referenced as GoSDK v2). If you've built an integration using the older Go language build tools and wish to update, see Upgrade from GoSDK v2 to v3 and Upgrade from v3 to v4. For updating the Infrastructure agent, see Update the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.16663,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "sections": "Compatibility and requirements for <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Before using the New Relic <em>Infrastructure</em> <em>integrations</em> <em>SDK</em> to <em>create</em> a custom on-host integration, make sure your system meets these requirements. <em>Infrastructure</em> version To use the <em>infrastructure</em> <em>Integrations</em> <em>SDK</em> v4, you must have the <em>Infrastructure</em> agent version 1.13.0 or higher. To use <em>SDK</em> v3"
      },
      "id": "60440de564441fa14f378ecc"
    },
    {
      "sections": [
        "On-host integration files",
        "Integration files"
      ],
      "title": "On-host integration files",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bad9028bde5eb2b92ad7971e0ca42517530f3796",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integration-files/",
      "published_at": "2021-05-05T00:27:50Z",
      "updated_at": "2021-03-16T08:29:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The New Relic Infrastructure Integrations SDK lets you create an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before you build an on-host integration, verify you meet the compatibility and requirements. An integration requires at least these files: An executable file, written in any language, that export JSON data in a format expected by the Infrastructure agent One or more configuration files For Go language build tools and a tutorial for creating these files, see Build resources.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.19624,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integration</em> files",
        "sections": "On-host <em>integration</em> files",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "The New Relic <em>Infrastructure</em> <em>Integrations</em> <em>SDK</em> lets you <em>create</em> an on-host integration for reporting custom host and service data. This document explains what files an on-host integration requires. New Relic also provides Go language integration building tools and a tutorial. Integration files Before"
      },
      "id": "603ed7e264441ff57a4e883b"
    },
    {
      "sections": [
        "On-host integrations: Newer configuration format",
        "Configuration structure",
        "Tip",
        "Important",
        "List of configuration properties",
        "Select an integration to run",
        "name",
        "exec",
        "cli_args",
        "when",
        "Pass configuration to the integration executable",
        "env",
        "config",
        "config_template_path",
        "Configure how the agent executes your integrations",
        "integration_user",
        "interval",
        "inventory_source",
        "labels",
        "timeout",
        "working_dir",
        "Update older integration configuration"
      ],
      "title": "On-host integrations: Newer configuration format ",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "bc5bee12812cff6fabc728961e1b342b83f3e471",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-newer-configuration-format/",
      "published_at": "2021-05-05T00:27:53Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. In December 2019, Infrastructure agent version 1.8.0 began supporting a newer configuration format that makes use of a single configuration file (instead of two separate files), and provides other improvements. This document will explain how this newer format works. The older standard configuration format is supported by current Infrastructure agents. For an introduction to configuration, see Config overview. Configuration structure An on-host integration's configuration YAML must have an integrations top-level section containing a YAML array, where each entry represents an integration and its configuration. For each integration entry, only the name property is mandatory. The other properties are optional. Here's an example configuration featuring two integrations: our built-in Docker integration, which requires no configuration, and our MySQL integration: integrations: # New Relic integration that does not require any configuration - name: nri-docker # New Relic integration that gets its configuration from the environment - name: nri-mysql env: PORT: 3306 USERNAME: newrelic PASSWORD: 123456789 # to hide this field, read the secrets management documentation # Any free-form integration executed via a user-provided command - name: my-own-integration exec: python /opt/integrations/my-script.py --host=127.0.0.1 Copy You can have as many configuration YAML files as you want and can group your integration instances. Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Each configuration YAML file can also contain discovery and variables top-level sections. Important This configuration format does not require an agent restart. When saved, changes are detected and implemented immediately. This means that saving intermediate configuration changes may cause the integration to stop working. List of configuration properties This is a list of the general properties used to configure an integration. For more details about using these properties, including example values, see the documentation following the table. Config Description name Name of the integration. This is the only mandatory configuration property across all on-host integrations. If the exec field is not set it will also be the name of the integration executable. cli_args Optional list of command line arguments when name is used to provide the integration executable. Available since agent version 1.13.0. exec Full path to the integration executable, plus arguments. It may be a single-line string or a string array. If left unspecified, the exec field defaults to the name field. env YAML map containing the environment variables to be passed to the integration, where key is the environment variable name, and value is the variable value. config Configuration that is written as an external file and the path that is passed to the integration with the CONFIG_PATH environment variable or the ${config.path} variable placeholder. config_template_path Any external file whose path is passed to the integration with the CONFIG_PATH environment variable or the ${config.path} variable placeholder. Its usage allows applying discovery and secrets binding to any external configuration. integration_user Name of the user who runs the integration. interval Time between consecutive executions of the integration. It must be a number followed by a time unit (s, m or h), without spaces. inventory_source Allows overriding the category and term of the inventory source. labels Map with labels that decorate the data (metrics, events, inventory) reported by the integration. timeout A number followed by a time unit (ms, s, m or h). An integration that hasn't responded in this time period is killed and restarted. working_dir Working directory for the integration binary. when Integration is only executed if the clause evaluates to true. Conditions are defined below. The remainder of this document describes config properties grouped by their functionality: Select an integration to run Pass configuration to the integration command Configure how the agent executes integrations Select an integration to run There are two properties to select which integration will run: name and exec. The only mandatory property across all on-host integrations is name. The remaining properties specified in this document are optional. Example: integrations: - name: nri-docker - name: my-integration exec: /usr/local/bin/my-integration --metrics --inventory Copy name The mandatory name property can work in two ways: If the exec property is set: The name property only provides an identifier for the integration instance. This identifier is used in log messages and to provide a default inventory category/source in the form integration/<name> (for example, integration/nri-redis). This inventory path can be overridden with the inventory_source configuration option. If the exec property is not set: The agent looks for (and executes) an executable with the name value in any of the following folders: Linux: /var/db/newrelic-infra/newrelic-integrations/bin /var/db/newrelic-infra/newrelic-integrations /var/db/newrelic-infra/custom-integrations/bin /var/db/newrelic-infra/custom-integrations Windows C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations\\bin C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations If there is no executable with this name in the above folders the agent logs an error and the integration is not executed. Important In Windows, do not append the .exe extension to the name. The agent does this for you (for example, name: nri-mysql would look for nri-mysql.exe in the above folders). exec The exec optional property specifies the path, command, and command-line arguments of the integration to execute. When none of the path folders or arguments have spaces, it can be written in a single-line string: - name: my-integration exec: /usr/bin/python /opt/integrations/my-script.py --host=127.0.0.1 Copy If any of the path/arguments have spaces that are part of a single element, you can use a YAML array notation: - name: my-integration exec: - C:\\Program Files\\My Integration\\integration.exe - --host - 127.0.0.1 - --port - 8080 Copy The default working directory is the root directory of the agent configuration. It can be overridden with the working_dir property. cli_args The cli_args optional property specifies command line arguments that should be passed to the integration. It is useful when using name as it only provides the integration name identifier (not compatible with exec). - name: my-integration cli_args: [ -interval 10s ] Copy Usual YAML multi-line list format can be used as well: - name: my-integration cli_args: - -interval - 10s Copy when The when property allows to execute the integration only when all the evaluated conditions are successful. Available conditions are: env_exists: Environment variables exist and match value. file_exists: Given file path exists. feature: Provided feature-flag is enabled. Example: integrations: - name: ssh-integration when: file_exists: /var/run/sshd.pid Copy Pass configuration to the integration executable Often integration executables need to receive a configuration to work properly (for example, hostname and port of the monitored system, user credentials, etc.). The Infrastructure agent allows you to configure the integration commands in three ways (which you can combine): Command-line arguments, passed in the exec property. Environment variables, using the env property. Configuration files, whose path needs to be passed through environment variables or command-line arguments (see the config) property. Example: integrations: - name: my-integration exec: /opt/path/bin/script --host 127.0.0.1 --port 8081 - name: nri-mysql env: STATUS_URL: http://10.0.0.3/server-status?auto REMOTE_MONITORING: true Copy env The env property allows you to set environment variables that are passed to the executable. It is a key-value map with the required variables. Example: integrations: - name: nri-postgresql env: DATABASE: postgres PORT: 6432 COLLECTION_LIST: '[\"postgres\"]' COLLECT_DB_LOCK_METRICS: false VERBOSE: 1 Copy If you expect your integration to receive the configuration from the host's environment rather than specifying it explicitly in the configuration file, you need to set the required variables in the Infrastructure agent passthrough_environment global configuration property config This section describes various ways to pass configuration information to an integration. Pass configuration file directly Some integration commands may get their configuration from an external file. If your integration requires a configuration file, nothing prevents you from directly passing its path directly as a command-line argument or an environment variable. Here's an example using configuration of our Flex integration: integrations: - name: nri-flex env: CONFIG_FILE: /etc/nri-flex/configs/http-service.yaml - name: other-integration exec: /opt/integration/integration -f /opt/integration/config.properties Copy The above example assumes that the http-service.yaml and config.properties files exist. We can see that the nri-flex integration is expecting the http-service.yaml complete path via the CONFIG_FILE environment variable and the other-integration expects the complete config.properties path after the -f command-line flag. In the above example, it's necessary for the integration installer/configurator that the configuration files exist in the provided path and that the agent and integrations have read permissions on them. Pass configuration through config section If you prefer to keep your configuration file with the rest of the integration configuration, you can use the config section in the integration entry, which can contain a valid YAML object or just a multi-line string: integrations: - name: nri-flex env: CONFIG_FILE: ${config.path} config: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv - name: other-integration exec: /opt/integration/integration -f ${config.path} config: | example.cfg.verbose=true example.cfg.logger=/var/logs/integration.log example.cfg.hostname=localhost example.cfg.port=9025 Copy In the above examples, every time the nri-flex integration is executed, the agent creates a temporary file with the following contents: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv Copy The above YAML is only a configuration example for the nri-flex integration. The agent ignores its contents; instead, it creates a temporary file and replaces the ${config.path} variable placeholder with its path. When the integration completes execution the temporary file is removed. Also, the agent creates another temporary file before executing the other-integration integration: example.cfg.verbose=true example.cfg.logger=/var/logs/integration.log example.cfg.hostid=localhost example.cfg.port=9025 Copy It replaces the -f ${config.path} command-line placeholder with the temporary path of the written file. By convention, if you do not place the ${config.path} variable in any command-line argument or environment variable value, the agent passes the path of the configuration file via the CONFIG_PATH environment variable: # assuming that nri-example command is prepared to receive the configuration # file via the CONFIG_PATH environment variable integrations: - name: nri-example config: name: csvFileExample apis: - name: csvFile file: /Users/hello/test.csv Copy Pass secrets and discovery through config section The main benefit for using a config section instead of hardcoding the full path of an external file is that you can insert ${variable} placeholders to apply our auto-discovery feature and secrets management. Here's an example followed by some explanations: variables: my_credentials: vault: http: url: http://my.vault.host/v1/newengine/data/secret headers: X-Vault-Token: my-vault-token discovery: docker: match: label.service: foo integrations: - name: foo-monitor exec: /opt/foo/bin/monitor --config=${config.path} config: | foo.host=${discovery.ip} foo.port=${discovery.port} foo.user=${my_credentials.user} foo.password=${my_credentials.password} Copy Tip (For more details about the variables and discovery sections, please visit the discovery and secrets management documentation). The above example relies on the following premises: There is a Vault service that allows retrieving a JSON object formed by the user and password fields. There may be a variable number of Docker containers labeled with service=foo, which are accessible from the agent host via a discoverable public IP and Port. The user has configured the foo-monitor integration to monitor all the service=foo labeled containers, which share a common user and password. Each instance of the foo-monitor integration requires executing the /opt/foo/bin/monitor executable, passing the text configuration inside the config section via the --config=<path> command-line argument. As example of workflow, imagine that the Vault invocation returns the following JSON: {\"user\":\"monitorer\",\"password\":\"5up3r53cr3t!\"} Copy At the moment of executing the foo-monitor integration, there are three running containers labeled with service=foo: ip: 10.0.0.3, port: 8080 ip: 10.0.0.3, port: 8081 ip: 10.0.0.3, port: 8082 The agent then creates the following three temporary files, using the contents of the config property as a template, but replacing the ${placeholders} by the acquired variables and discovered items (files' path is invented for the sake of simplicity): First match (/tmp/123_discovered): foo.host=10.0.0.3 foo.port=8080 foo.user=monitorer foo.password=5up3r53cr3t! Copy Second match (/tmp/456_discovered): foo.host=10.0.0.3 foo.port=8081 foo.user=monitorer foo.password=5up3r53cr3t! Copy Third match (/tmp/789_discovered) foo.host=10.0.0.3 foo.port=8082 foo.user=monitorer foo.password=5up3r53cr3t! Copy After the config variable placeholders have been replaced and the temporary files have been created, the /opt/foo/bin/monitor executable is executed three times (one per matched container), replacing the ${config.path} command-line placeholder with the temporary file corresponding to each discovered configuration: First match: /opt/foo/bin/monitor --config=/tmp/123_discovered Second match: /opt/foo/bin/monitor --config=/tmp/456_discovered Third match: /opt/foo/bin/monitor --config=/tmp/789_discovered To ensure security and to minimize the chance of leaking secrets to disk, the agent: Creates the files owned by the agent user, for example, root or nri-agent, depending on the user you have configured to run the agent. Sets read permissions only for the owner. Removes the created files when the integration instance finishes its execution. config_template_path If you want to use the secrets management and discovery in the configuration files that you're passing to the integration executable, but you prefer to keep them as an individual file, you can use the config_template_path: <path> option. It works exactly as in the config section: The agent applies secrets management and discovery to the file contents. The agent creates different temporary files that are passed to the integration via the ${config.path} placeholder (or the CONFIG_PATH environment variable). Example: discovery: docker: match: name: /^redis/ integrations: - name: nri-flex env: CONFIG_FILE: ${config.path} config_template_path: /etc/flex-configs/redis.yml Copy In the above example, the redis.yml external file can contain container discovery variable placeholders, like ${discovery.ip} or ${discovery.port}. Configure how the agent executes your integrations The properties of this section modify the way the Infrastructure agent executes and interacts with the integrations, or the way the agent decorates the integrations' data. integration_user The integration commands run as the same user as the agent (usually root or nri-agent). If due to permission restrictions an integration needs to run as another user, its name must be specified in the integration_user property. Example: integrations: - name: dbus-inventory exec: python my-dbus-script.py integration_user: dbus Copy interval The interval option sets the time between consecutive executions of an integration. The accepted format is an integer immediately followed by a time unit (s for seconds, m for minutes, h for hours). The default is 30s, and the minimum accepted value is 15s. Any value lower than 15s is automatically set to 15s. Example: integrations: - name: nri-nginx env: STATUS_URL: http://127.0.0.1/status STATUS_MODULE: discover interval: 20s Copy inventory_source Any inventory item must be catalogued under a category/source taxonomy. By default, each integration inventory is stored as integration/ + name value (for example, integration/nri-apache, integration/nri-mysql). The inventory_source property allows you to override the default taxonomy of inventory data. Example: integrations: - name: nri-nginx - name: nri-apache exec: - /var/db/newrelic-infra/newrelic-integrations/bin/nri-apache - --inventory inventory_source: config/apache Copy In the above example, the nri-nginx inventory, if any, would be visible in the New Relic UI under the integration/nri-nginx source. The nri-apache inventory would be visible under config/apache. labels labels is a key-value map that allows extra metadata to be provided for the integration. The agent uses those labels to decorate the metrics, events, and inventory that it receives from a given integration instance. Example: integrations: - name: nri-apache inventory_source: config/apache labels: env: production role: load_balancer Copy In the above example, the agent decorates all the metrics and events from the nri-apache instance with the following fields: label.env: production label.role: load_balancer Also, the following entries are added to the integration inventory: config/apache/labels/env: production config/apache/labels/role: load_balancer timeout If an integration has not returned any metric (or a heartbeat message as described below) before the time specified in the timeout value, the agent kills the integration process and restarts it after the corresponding interval. The accepted format is an integer number immediately followed (without spaces) by a time unit (ms for milliseconds, s for seconds, m for minutes, h for hours). If a zero (or negative) timeout value is provided, the integration can run forever without being killed by a timeout expiration. For long-running integrations (integrations that keep running, periodically returning metrics/events/inventory), each time the integration submits a metrics/events/inventory payload, the timeout deadline is restarted. That means that long-running integrations must return a valid JSON payload in an interval that is lower than timeout. Returning an empty JSON ({}) is interpreted as a heart-beat message that restarts the timeout, preventing the long-running integration from being killed, even if they don't have information to report. The default is 120s, and the minimum accepted value is 100ms. Any value lower than 100ms is automatically set to 100ms. Example: integrations: - name: nri-jmx arguments: JMX_HOST: jmx-host.localnet JMX_PORT: 7096 COLLECTION_FILES: \"/etc/newrelic-infra/integrations.d/jvm-metrics.yml\" timeout: 30s Copy working_dir working_dir sets the working directory of the command. If empty or unspecified, the agent runs the command in the Infrastructure agent's current directory. The default is the Infrastructure agent root directory. Example: integrations: - name: my-integration exec: /opt/integration/bin/integration working_dir: /opt/integration/scratch-zone Copy Update older integration configuration In December 2019, the Infrastructure agent version 1.8.0 began using a different configuration format. For details, see Config format differences. The main difference between these formats is that the older configuration format uses two separate configuration files (a INTEGRATION_NAME-definition.yml file and a INTEGRATION_NAME-config.yml file) and the newer version uses a single configuration file. Here are some of the features added by the newer configuration functionality: Flexible configuration via command-line arguments, environment variables, or external files. Ability to group different integrations in the same file. Hot reload: adding a new integration or changing its configuration does not require restarting the agent. Timeouts: if an integration doesn't respond before a user-specified time, the integration process is killed and restarted. Not all on-host integrations come with the newer configuration format, but you can update the configuration to the new format for all on-host integrations to take advantage of the new features. The following YAML shows an example Apache integration configuration using the older configuration format. Note that this configuration will still work with newer agents, but we recommend updating your integrations to take full advantage of features. integration_name: com.newrelic.apache instances: - name: apache-server-metrics command: metrics arguments: status_url: http://127.0.0.1/server-status?auto remote_monitoring: true labels: env: production role: load_balancer - name: apache-server-inventory command: inventory arguments: remote_monitoring: true labels: env: production role: load_balancer Copy To update an older integration configuration to the new format, you must perform two steps: Rename the instances top-level section to integrations. Remove the integration_name top-level section and add it to each integration entry. You are no longer required to keep a separate file for each integration type and you can group your legacy integration entries in the same file as other integrations. The new version of the Apache integration config: integrations: - name: apache-server-metrics integration_name: com.newrelic.apache command: metrics arguments: status_url: http://127.0.0.1/server-status?auto remote_monitoring: true labels: env: production role: load_balancer - name: apache-server-inventory integration_name: com.newrelic.apache command: inventory arguments: remote_monitoring: true labels: env: production role: load_balancer Copy Please note that because the older configuration format doesn't support hot reloading. You will need to restart the Infrastructure agent to remove the old integrations configuration (otherwise the old instances will coexist with the new ones).",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 203.17468,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Newer configuration format ",
        "sections": "On-host <em>integrations</em>: Newer configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. In December 2019, <em>Infrastructure</em> agent version 1.8.0 began supporting a newer configuration format that makes use of a single configuration file (instead of two separate files), and provides other"
      },
      "id": "603e923928ccbcb8dfeba751"
    }
  ],
  "/docs/create-integrations/infrastructure-integrations-sdk/troubleshooting/not-seeing-attributes": [
    {
      "sections": [
        "Not seeing Infrastructure integration data",
        "Problem",
        "Solution",
        "Troubleshoot integration requirements",
        "Check the integration log file for error messages",
        "Check that the integration is loading correctly"
      ],
      "title": "Not seeing Infrastructure integration data",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Troubleshooting"
      ],
      "external_id": "6b918ff0c14a1df232f58fb6cba063d6ab85114e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/troubleshooting/not-seeing-infrastructure-integration-data/",
      "published_at": "2021-05-05T03:33:03Z",
      "updated_at": "2021-03-13T02:49:24Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You created a custom New Relic Infrastructure on-host integration using the Integrations SDK, but you're not seeing data in the Infrastructure UI. Solution To troubleshoot and resolve the problem: Verify that your integration meets New Relic Infrastructure's integration requirements. After ruling out common problems with integration requirements, follow the more in-depth troubleshooting procedures for error logs and integration loading. Troubleshoot integration requirements If you are not receiving data from your custom integration, verify that your integration follows these requirements. Integration requirements Comments Environment Make sure your environment meets the Integrations SDK requirements. Configuration file Use a validator such as yamllint to verify that the configuration file is a valid YAML file. Verify that the file is in the correct location. Definition file Use a validator such as yamllint to verify that the definition file is a valid YAML file. Verify that the header fields have the required format. Verify that the prefix for inventory set in the definition file is no more than two levels deep. Verify that the file is in the correct location. Metric sets Verify that the integration: Does not generate metric sets with more than 120 key-value pairs. Does not generate more than 1000 metric sets. JSON payload Verify that the integration does not generate a single JSON payload of more than 5 MB. Check the integration log file for error messages After ruling out common problems with integration requirements, follow these more in-depth troubleshooting procedures. Recommendation: Configure a log file in the Infrastructure agent configuration. This helps separate the types of errors so you can spot integration errors more easily. When there is an error loading or running your integration, the Infrastructure agent adds an error message to the log file. Errors are logged even if verbose mode is disabled. Check the log file for lines that include \"level=error\". If there are no error messages, check whether the Infrastructure agent is loading the integration correctly. Check that the integration is loading correctly To verify whether the Infrastructure agent is loading the integration correctly: Enable verbose mode in the Infrastructure agent configuration. Restart the Infrastructure agent. Verify that the first lines of the log file contain two messages: \"loaded plugin\" plugin=<your_integration_name> \"found plugin config file\" plugin-path=\"<your_integration_config_path>\" Copy Check for errors just before or after these lines that indicate a problem with loading. The agent schedules data to be sent based on the schedule set in the definition file. Look in the log file for the integration's JSON. JSON payload example nginx-server-metrics: {\\\"name\\\":\\\"com.newrelic.nginx\\\",\\\"protocol_version\\\":\\\"1\\\",\\\"integration_version\\\":\\\"0.5.0\\\",\\\"metrics\\\": [{\\\"event_type\\\":\\\"NginxSample\\\",\\\"net.connectionsAcceptedPerSecond\\\":0.10344827586206896,\\\"net.connectionsActive\\\":1, \\\"net.connectionsDroppedPerSecond\\\":0,\\\"net.connectionsReading\\\":0,\\\"net.connectionsWaiting\\\":0,\\\"net.connectionsWriting\\\":1, \\\"net.requestsPerSecond\\\":0.10344827586206896,\\\"software.edition\\\":\\\"open source\\\",\\\"software.version\\\":\\\"1.4.6 (Ubuntu)\\\"}],\\\"inventory\\\":{},\\\"events\\\":[]} Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 173.70302,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Not seeing <em>Infrastructure</em> <em>integration</em> data",
        "sections": "Not seeing <em>Infrastructure</em> <em>integration</em> data",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Problem You created a custom New Relic <em>Infrastructure</em> on-host integration using the <em>Integrations</em> <em>SDK</em>, but you&#x27;re not seeing data in the <em>Infrastructure</em> UI. Solution To <em>troubleshoot</em> and resolve the problem: Verify that your integration meets New Relic <em>Infrastructure</em>&#x27;s integration requirements. After"
      },
      "id": "6043efdf28ccbc6d182c6081"
    },
    {
      "sections": [
        "Integration logging recommendations",
        "Logging requirements",
        "Recommendations and best practices",
        "For more help"
      ],
      "title": "Integration logging recommendations",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "5cd5cc6af7ef854ed9f5aabb3c8c6ddbc3e123fc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/integration-logging-recommendations/",
      "published_at": "2021-05-05T03:41:54Z",
      "updated_at": "2021-03-13T02:51:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure provides an SDK for creating an on-host integration. This document explains requirements and best practices for generating integration logs. Logging requirements It's up to the integration creator to decide what kind of log messages to create, and what kind of information will be useful for debugging issues. There is only one requirement for how an integration must generate logs: The integration executable must write logs to standard error (stderr). The Infrastructure agent will capture lines written to standard error and merge them into the logging stream written by the Infrastructure agent itself. To avoid depending on third-party logging solutions, the Go integration building library provides a simple log package with the common log-levels. Recommendations and best practices Here are the recommended practices for generating integration logs: By default, an integration should be \"quiet.\" Aside from the data emitted to standard output, there should be very few logging or diagnostic messages generated. It's recommended you include a verbose logging mode similar to the verbose setting in the Infrastructure agent. Include a command line switch to enable and disable verbose logging (for example, -verbose). To debug your integration while the integration is running, include the verbose switch in the definition file as part of the command line to be run. This will send the verbose logs to the Infrastructure agent's own log file. For general debugging purposes, New Relic recommends you use a flag that writes the standard out JSON data in human-readable \"pretty-printed\" form (for example, --pretty). Note that output written in a \"pretty-printed\" form is only for your debugging purposes and is not compatible with the Infrastructure agent. Your integration should be created so that it can run on its own. If in doubt whether the integration is communicating with the Infrastructure agent, you can run the integration from the command line and see if it's producing the correct output or log messages you expect. For information about the Go language logging package, see Logging package. For more help For logging for all New Relic agents, see New Relic agent logs and troubleshooting.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.83849,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integration</em> logging recommendations",
        "sections": "<em>Integration</em> logging recommendations",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> provides an <em>SDK</em> for creating an on-host integration. This document explains requirements and best practices for generating integration logs. Logging requirements It&#x27;s up to the integration creator to decide what kind of log messages to <em>create</em>, and what kind of information"
      },
      "id": "6043f02664441fcb4e378efc"
    },
    {
      "sections": [
        "On-host integrations: Standard configuration format",
        "Configuration file structure",
        "Definition file",
        "Definition file header",
        "Definition file commands",
        "Configuration file",
        "Tip",
        "Config file field definitions"
      ],
      "title": "On-host integrations: Standard configuration format",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "3b4c5ab77b2b8025ca8de375403f24bc75b3dca7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-standard-configuration-format/",
      "published_at": "2021-05-05T00:23:27Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. This document explains the older, standard configuration format. This format is used by most on-host integrations and is supported by newer Infrastructure agents (which also support a newer, improved configuration format). For an introduction to configuration, see Config overview. Configuration file structure An on-host integration that uses the standard configuration format requires two configuration files: A definition file A configuration file Definition file The definition file has a naming format like INTEGRATION_NAME-definition.yml. This file provides descriptive information about the integration, such as: the version of the JSON protocol it supports, a list of commands it can execute, and arguments that it accepts. It lives in this directory: Linux: /var/db/newrelic-infra/newrelic-integrations Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations Copy Here's an example of an NGINX integration definition file with two command sections on a Linux system: name: com.myorg.nginx protocol_version: 2 description: Collect metric and configuration data from NGINX os: linux commands: metrics: command: - myorg-nginx - --metrics interval: 15 inventory: command: - myorg-nginx - --inventory interval: 120 prefix: integration/myorg-nginx Copy A definition file can be broken down into two parts: The header The commands section Definition file header Here are explanations of a definition file's header elements: Definition header field Description name Required. A unique name name to identify the integration for logging, internal metrics, etc. When the agent loads the config file, New Relic uses the name to look up the integration in the agent's registry. protocol_version Required. The version number of the protocol. New Relic uses this to ensure compatibility between the integration and the agent. If the agent does not recognize an integration's version, it will filter out that integration and create a log message. The current version of the JSON protocol is 2. For more on protocol changes, see SDK changes. description Optional. Human-friendly explanation of what the integration does. os Optional. The operating system where the integration runs. New Relic uses this to filter integrations that you intend to run only on specific operating systems. Default: Run the integration regardless of the os value. To restrict the integration to a specific operating system, use either of these options: linux windows Definition file commands After the header is a list of commands. The commands section defines: One or more independent operating modes for the executable The runtime data required for it to be executed The commands section is a YAML map of command definitions, where each key is the unique alias name of the command in the integration's config file that specifies the executable to run. Definition commands Description command Required. The actual command line to be executed as a YAML array of command parts. These are assembled to run the actual command. For simple commands, the array might be only a single element. Additional command rules include: command arguments: The command and any command line arguments that are shared for all instances of the integration configuration. command execution: The command will be executed in the same directory as the definition file. command path: Any commands available on the host's $PATH can be used. Executables located in the same directory as the definition file, or in a subdirectory of it, can be executed using a relative path. For example: Linux: To run an executable called myorg-nginx in the same directory as the definition file, you could use myorg-nginx or ./myorg-nginx. Linux systems will execute myorg-nginx as if the user used ./myorg-nginx. Windows: To run an executable called myorg-nginx.exe in the same directory as the definition file, you could use \\myorg-nginx.exe or .\\myorg-nginx.exe. Windows systems writing myorg-nginx.exe will be executed as if indicating the current path: .\\myorg-nginx.exe. To use a command installed inside a directory on the host's $PATH, simply use the command name. Example: python. To run any other executable which is neither on the host's $PATH nor within the integration's directory, use an absolute path to the executable. Example: /opt/jdk/bin/java. If the given executable name exists within the integration's directory but also exists elsewhere on the system $PATH, the version in the integration's directory takes precedence. interval Optional. The number of seconds between two consecutive executions of the command, in particular between the end of the previous execution and the start of the next execution. Default for metric polling: 30 seconds. Minimum (floor): 15 seconds. Alerts: For metrics being used for alerts, use an interval of 30 seconds or less. prefix Optional. The categorization of the inventory in the form of category/short_integration_name. Example: integration/myorg-nginx. The prefix is not a platform-specific path. The forward slash is the correct separator between the category and short_integration_name. The prefix can have a maximum of two levels. Otherwise inventory will not be reported. Default value if not set: integration/integration_name. Configuration file The configuration file has a naming format like INTEGRATION_NAME-config.yml. This file specifies which executables to run and the parameters required to run them. It lives in this directory: Linux: /etc/newrelic-infra/integrations.d/ Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d Copy Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Here's an example of a config file with one instance defined. Explanations of these fields are explained below the example. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://127.0.0.1/status labels: environment: production role: load_balancer Copy Another example of a config file with two instances defined. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://one.url/status labels: environment: production role: load_balancer - name: nginx2.myorg.com-metrics command: metrics arguments: status_url: http://another.url/status labels: environment: production role: load_balancer Copy Config file field definitions Config file field Description integration_name Required. This is the header and is used to identify which executables to run. This name must exactly match the name specified in the integration's definition file. Recommendation: To ensure unique names, use reverse domain name notation. name Required. This is the name for the specific invocation (instance) of the integration. This is used to help identify any log messages generated by this integration and is also useful for troubleshooting. command Required. This is the command to be executed. This must exactly match one of the unique alias names specified in the integration's definition file. arguments Optional. A YAML object where: Keys: The argument name. Transformed to upper case when set as environment variable. Values: The argument values. Passed through as is. The arguments are made available to an integration as a set of environment variables. Arguments in the config file cannot be capitalised and should use underscores to separate words. labels Optional. A YAML object where: Keys: The label name. Values: The defined label value. integration_user Optional. String with the name the agent will use for executing the integration binary. Default: depends on the usermode. By default, integrations are executed with the same user that's running the integration agent, nri-agent for privileged and unprivileged mode and root user for root mode. When present, the Infrastructure agent will execute the integration binary as the specified user. For example, to run the integration binary as the root user when running the agent in a usermode different than root, just add integration_user: root to the configuration file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.09348,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Standard configuration format",
        "sections": "On-host <em>integrations</em>: Standard configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. This document explains the older, standard configuration format. This format is used by most on-host <em>integrations</em> and is supported by newer <em>Infrastructure</em> agents (which also support a newer, improved"
      },
      "id": "603e923a196a67581ca83db3"
    }
  ],
  "/docs/create-integrations/infrastructure-integrations-sdk/troubleshooting/not-seeing-infrastructure-integration-data": [
    {
      "sections": [
        "Not seeing attributes data",
        "Problem",
        "Solution"
      ],
      "title": "Not seeing attributes data",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Troubleshooting"
      ],
      "external_id": "25a763fd32bebfa1cb33b77caf260df9a4d9fe53",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/troubleshooting/not-seeing-attributes/",
      "published_at": "2021-05-05T00:25:33Z",
      "updated_at": "2021-03-16T08:30:13Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Your integration is reporting data, but a few inventory attributes are missing. Solution The following inventory key names are used internally by our platform. If customer-provided integrations send inventory items with the same key name, they will remain invisible to the user: pid installed_epoch epoch time size boot_id product_uuid installed_time",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 172.77458,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "Problem Your integration is reporting data, but a few inventory attributes are missing. Solution The following inventory key names are used internally by our platform. If customer-provided <em>integrations</em> send inventory items with the same key name, they will remain invisible to the user: pid installed_epoch epoch time size boot_id product_uuid installed_time"
      },
      "id": "60506c9564441f0e645321ab"
    },
    {
      "sections": [
        "Integration logging recommendations",
        "Logging requirements",
        "Recommendations and best practices",
        "For more help"
      ],
      "title": "Integration logging recommendations",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "5cd5cc6af7ef854ed9f5aabb3c8c6ddbc3e123fc",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/integration-logging-recommendations/",
      "published_at": "2021-05-05T03:41:54Z",
      "updated_at": "2021-03-13T02:51:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure provides an SDK for creating an on-host integration. This document explains requirements and best practices for generating integration logs. Logging requirements It's up to the integration creator to decide what kind of log messages to create, and what kind of information will be useful for debugging issues. There is only one requirement for how an integration must generate logs: The integration executable must write logs to standard error (stderr). The Infrastructure agent will capture lines written to standard error and merge them into the logging stream written by the Infrastructure agent itself. To avoid depending on third-party logging solutions, the Go integration building library provides a simple log package with the common log-levels. Recommendations and best practices Here are the recommended practices for generating integration logs: By default, an integration should be \"quiet.\" Aside from the data emitted to standard output, there should be very few logging or diagnostic messages generated. It's recommended you include a verbose logging mode similar to the verbose setting in the Infrastructure agent. Include a command line switch to enable and disable verbose logging (for example, -verbose). To debug your integration while the integration is running, include the verbose switch in the definition file as part of the command line to be run. This will send the verbose logs to the Infrastructure agent's own log file. For general debugging purposes, New Relic recommends you use a flag that writes the standard out JSON data in human-readable \"pretty-printed\" form (for example, --pretty). Note that output written in a \"pretty-printed\" form is only for your debugging purposes and is not compatible with the Infrastructure agent. Your integration should be created so that it can run on its own. If in doubt whether the integration is communicating with the Infrastructure agent, you can run the integration from the command line and see if it's producing the correct output or log messages you expect. For information about the Go language logging package, see Logging package. For more help For logging for all New Relic agents, see New Relic agent logs and troubleshooting.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 170.83849,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Integration</em> logging recommendations",
        "sections": "<em>Integration</em> logging recommendations",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> provides an <em>SDK</em> for creating an on-host integration. This document explains requirements and best practices for generating integration logs. Logging requirements It&#x27;s up to the integration creator to decide what kind of log messages to <em>create</em>, and what kind of information"
      },
      "id": "6043f02664441fcb4e378efc"
    },
    {
      "sections": [
        "On-host integrations: Standard configuration format",
        "Configuration file structure",
        "Definition file",
        "Definition file header",
        "Definition file commands",
        "Configuration file",
        "Tip",
        "Config file field definitions"
      ],
      "title": "On-host integrations: Standard configuration format",
      "type": "docs",
      "tags": [
        "Create integrations",
        "Infrastructure Integrations SDK",
        "Specifications"
      ],
      "external_id": "3b4c5ab77b2b8025ca8de375403f24bc75b3dca7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/create-integrations/infrastructure-integrations-sdk/specifications/host-integrations-standard-configuration-format/",
      "published_at": "2021-05-05T00:23:27Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Infrastructure on-host integrations can use one of two types of configuration formats. This document explains the older, standard configuration format. This format is used by most on-host integrations and is supported by newer Infrastructure agents (which also support a newer, improved configuration format). For an introduction to configuration, see Config overview. Configuration file structure An on-host integration that uses the standard configuration format requires two configuration files: A definition file A configuration file Definition file The definition file has a naming format like INTEGRATION_NAME-definition.yml. This file provides descriptive information about the integration, such as: the version of the JSON protocol it supports, a list of commands it can execute, and arguments that it accepts. It lives in this directory: Linux: /var/db/newrelic-infra/newrelic-integrations Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-integrations Copy Here's an example of an NGINX integration definition file with two command sections on a Linux system: name: com.myorg.nginx protocol_version: 2 description: Collect metric and configuration data from NGINX os: linux commands: metrics: command: - myorg-nginx - --metrics interval: 15 inventory: command: - myorg-nginx - --inventory interval: 120 prefix: integration/myorg-nginx Copy A definition file can be broken down into two parts: The header The commands section Definition file header Here are explanations of a definition file's header elements: Definition header field Description name Required. A unique name name to identify the integration for logging, internal metrics, etc. When the agent loads the config file, New Relic uses the name to look up the integration in the agent's registry. protocol_version Required. The version number of the protocol. New Relic uses this to ensure compatibility between the integration and the agent. If the agent does not recognize an integration's version, it will filter out that integration and create a log message. The current version of the JSON protocol is 2. For more on protocol changes, see SDK changes. description Optional. Human-friendly explanation of what the integration does. os Optional. The operating system where the integration runs. New Relic uses this to filter integrations that you intend to run only on specific operating systems. Default: Run the integration regardless of the os value. To restrict the integration to a specific operating system, use either of these options: linux windows Definition file commands After the header is a list of commands. The commands section defines: One or more independent operating modes for the executable The runtime data required for it to be executed The commands section is a YAML map of command definitions, where each key is the unique alias name of the command in the integration's config file that specifies the executable to run. Definition commands Description command Required. The actual command line to be executed as a YAML array of command parts. These are assembled to run the actual command. For simple commands, the array might be only a single element. Additional command rules include: command arguments: The command and any command line arguments that are shared for all instances of the integration configuration. command execution: The command will be executed in the same directory as the definition file. command path: Any commands available on the host's $PATH can be used. Executables located in the same directory as the definition file, or in a subdirectory of it, can be executed using a relative path. For example: Linux: To run an executable called myorg-nginx in the same directory as the definition file, you could use myorg-nginx or ./myorg-nginx. Linux systems will execute myorg-nginx as if the user used ./myorg-nginx. Windows: To run an executable called myorg-nginx.exe in the same directory as the definition file, you could use \\myorg-nginx.exe or .\\myorg-nginx.exe. Windows systems writing myorg-nginx.exe will be executed as if indicating the current path: .\\myorg-nginx.exe. To use a command installed inside a directory on the host's $PATH, simply use the command name. Example: python. To run any other executable which is neither on the host's $PATH nor within the integration's directory, use an absolute path to the executable. Example: /opt/jdk/bin/java. If the given executable name exists within the integration's directory but also exists elsewhere on the system $PATH, the version in the integration's directory takes precedence. interval Optional. The number of seconds between two consecutive executions of the command, in particular between the end of the previous execution and the start of the next execution. Default for metric polling: 30 seconds. Minimum (floor): 15 seconds. Alerts: For metrics being used for alerts, use an interval of 30 seconds or less. prefix Optional. The categorization of the inventory in the form of category/short_integration_name. Example: integration/myorg-nginx. The prefix is not a platform-specific path. The forward slash is the correct separator between the category and short_integration_name. The prefix can have a maximum of two levels. Otherwise inventory will not be reported. Default value if not set: integration/integration_name. Configuration file The configuration file has a naming format like INTEGRATION_NAME-config.yml. This file specifies which executables to run and the parameters required to run them. It lives in this directory: Linux: /etc/newrelic-infra/integrations.d/ Copy Windows: C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d Copy Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. Here's an example of a config file with one instance defined. Explanations of these fields are explained below the example. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://127.0.0.1/status labels: environment: production role: load_balancer Copy Another example of a config file with two instances defined. integration_name: com.myorg.nginx instances: - name: nginx1.myorg.com-metrics command: metrics arguments: status_url: http://one.url/status labels: environment: production role: load_balancer - name: nginx2.myorg.com-metrics command: metrics arguments: status_url: http://another.url/status labels: environment: production role: load_balancer Copy Config file field definitions Config file field Description integration_name Required. This is the header and is used to identify which executables to run. This name must exactly match the name specified in the integration's definition file. Recommendation: To ensure unique names, use reverse domain name notation. name Required. This is the name for the specific invocation (instance) of the integration. This is used to help identify any log messages generated by this integration and is also useful for troubleshooting. command Required. This is the command to be executed. This must exactly match one of the unique alias names specified in the integration's definition file. arguments Optional. A YAML object where: Keys: The argument name. Transformed to upper case when set as environment variable. Values: The argument values. Passed through as is. The arguments are made available to an integration as a set of environment variables. Arguments in the config file cannot be capitalised and should use underscores to separate words. labels Optional. A YAML object where: Keys: The label name. Values: The defined label value. integration_user Optional. String with the name the agent will use for executing the integration binary. Default: depends on the usermode. By default, integrations are executed with the same user that's running the integration agent, nri-agent for privileged and unprivileged mode and root user for root mode. When present, the Infrastructure agent will execute the integration binary as the specified user. For example, to run the integration binary as the root user when running the agent in a usermode different than root, just add integration_user: root to the configuration file.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 162.09348,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "On-host <em>integrations</em>: Standard configuration format",
        "sections": "On-host <em>integrations</em>: Standard configuration format",
        "tags": "<em>Infrastructure</em> <em>Integrations</em> <em>SDK</em>",
        "body": "New Relic <em>Infrastructure</em> on-host <em>integrations</em> can use one of two types of configuration formats. This document explains the older, standard configuration format. This format is used by most on-host <em>integrations</em> and is supported by newer <em>Infrastructure</em> agents (which also support a newer, improved"
      },
      "id": "603e923a196a67581ca83db3"
    }
  ],
  "/docs/csp-v2-error-inline-javascript-not-allowed": [
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 205.91167,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Response <em>validation</em>",
        "body": " the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON <em>v2</em>. newrelic: For reporting all other trace data. Data"
      },
      "id": "6071cf7628ccbcf8b851c158"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/csp-v2-host-install-browser-agent/",
      "sections": [
        "CSP v2: Host and install the Browser agent",
        "Contents",
        "Requirements",
        "1. Whitelist New Relic Browser domains",
        "2. Copy Browser app's JS snippet",
        "3. Create .js file for page"
      ],
      "published_at": "2021-05-03T06:35:20Z",
      "title": "CSP v2: Host and install the Browser agent",
      "updated_at": "2021-03-13T07:28:47Z",
      "type": "docs",
      "external_id": "75530e0b53071dd92148ca07d572bc3a032e9c4e",
      "document_type": "page",
      "popularity": 1,
      "body": "If your organization has strict requirements about Content Security Policy (CSP) v2, you may need to host the New Relic Browser agent in order to properly install and run it. For example, if the New Relic domain hosting the Browser agent is being blocked by your website, you may see an error similar to this: Browser agent.js is unable to run because it violates the following Content Security Protocol directive: script-src = *.google.com other URLs Copy In this situation, you must copy and paste the agent's JavaScript code and self-host it as a .js file. This allows the page to reference the .js file during page load. Contents Requirements In order to host the Browser agent with your app, make sure you follow these requirements: CSP and Browser agent Requirements Application or platform Your app or platform where your site is hosted must meet New Relic Browser's standard compatibility requirements. Exception: CSP restrictions prevent you from linking an APM app for Browser monitoring. Subscription You must have a Pro+SPA Browser subscription for your app. Host location for domains The .js file for New Relic Browser must be hosted on a highly available or replicated location, such as a CDN or distributed network. This helps ensure performance is not affected. If you see error messages, add New Relic Browser domains to your CSP whitelist. If you want New Relic Browser to monitor Salesforce Lightning pages, follow the Salesforce procedures to add New Relic Browser domains to your CSP whitelist. By hosting the agent, you are responsible for any performance impact on the location where the agent is hosted. Browser agent version Your selected app must use the latest Browser agent version. 1. Whitelist New Relic Browser domains Follow your organization's standard procedures to request a CSP exception. Then add both of these New Relic Browser domains to your CSP whitelist: https://js-agent.newrelic.com: This is where the New Relic Browser agent is hosted. The Browser agent download requires three files to run. Two are downloaded after the initial agent is run to continue capturing performance data. https://bam.nr-data.net: This is the endpoint where the New Relic Browser agent receives data from your site. Here is example HTML to add to the head of your site: Content-Security-Policy: default-src 'self' https://js-agent.newrelic.com https://bam.nr-data.net Copy 2. Copy Browser app's JS snippet To create and host a .js file, you must first copy the Browser agent's JavaScript snippet from the hosting app. Go to rpm.newrelic.com/browser. From the list of Browser apps, select the app for which you want to self-host the Browser agent. Select Settings > Application settings > Copy/paste JavaScript code. Copy the Browser agent's JavaScript snippet. Save and exit the Browser app's Application settings page. Continue with the procedures to create a .js file for your page. 3. Create .js file for page After you copy the Browser agent's JavaScript snippet from the selected Browser app: Open a text editor and paste the Browser agent's JavaScript snippet. Delete the first line (<script type=\"text/javascript\">) and the last line of the JavaScript snippet (</script>). Save the text file as nr-spa-VERSION_NUMBER.min.js, where VERSION_NUMBER is the latest Browser agent version. Reference the .js file in the <head> of your webpage. After you install the Browser agent's JavaScript snippet, you can use the New Relic Browser UI to monitor website performance.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.52527,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CSP</em> <em>v2</em>: Host and install the Browser agent",
        "sections": "<em>CSP</em> <em>v2</em>: Host and install the Browser agent",
        "body": "If your organization has strict requirements about Content Security Policy (<em>CSP</em>) <em>v2</em>, you may need to host the New Relic Browser agent in order to properly install and run it. For example, if the New Relic domain hosting the Browser agent is being blocked by your website, you may see an <em>error</em>"
      },
      "id": "6043d37364441f42cf378ee8"
    },
    {
      "sections": [
        "Record and monitor deployments",
        "Options for tracking deployments",
        "Tip",
        "Record deployments with the REST API",
        "Record a deployment with POST",
        "Record a deployment with PowerShell",
        "View a list of deployments with GET",
        "Sample output from GET",
        "Delete a deployment with DELETE",
        "Character limits and JSON parameters",
        "Important",
        "Record deployments using the New Relic agent",
        "Notify your team of deployments",
        "End of life notification: Hipchat, Campfire"
      ],
      "title": "Record and monitor deployments",
      "type": "docs",
      "tags": [
        "APM",
        "New Relic APM",
        "Maintenance"
      ],
      "external_id": "00b24338386fd261daea0733fe03c01639cce083",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apm/new-relic-apm/maintenance/record-monitor-deployments/",
      "published_at": "2021-05-04T23:57:03Z",
      "updated_at": "2021-04-22T09:32:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Deploying an app can be a risky event—when your app breaks, a bad deployment is often the cause. New Relic allows you to track deployments so you can correlate deploy to your app's performance. Tracking deployments creates deployment markers that appear in APM charts. Options for tracking deployments You can use the New Relic REST API v2 to record new deployments, retrieve a list of past deployments, and delete past deployments on your APM application. In addition, some APM agents have agent-specific methods to record deployments automatically. You can use your Slack integration with New Relic, or a simple webhook, to notify your team in real time of deployments for applications monitored by APM. Slack provides a webhook URL that allows you to post generic JSON that will appear formatted in a chosen Slack channel. There are a few places where you can view deployments in the New Relic UI after they have been recorded. You can view deployments in the UI: In the activity feed of the APM Summary, New Relic One Service summary, and entity summary pages. On APM performance charts as a chart marker. On the Deployments page for summary performance. Tip Deployment markers are not available for Browser applications, but see Browser releases for a way to tag errors with release versions. Record deployments with the REST API You can use the New Relic REST API v2 to record deployments, get a list of past deployments, and delete deployments. The examples in this document use curl as a command line tool. However, you can use any method to make your REST requests. You can also create, view, and delete deployments with the API Explorer. JSON uses double quotes \" for element names and content. Using single quotes ' will cause errors. Record a deployment with POST To record a new deployment, send a POST request that includes your API key to the deployments endpoint. Attach the payload in JSON format (see Character limits and JSON parameters). All payload parameters are optional except revision. For example: curl -X POST \"https://api.newrelic.com/v2/applications/$APP_ID/deployments.json\" \\ -H \"Api-Key:$API_KEY\" \\ -i \\ -H \"Content-Type: application/json\" \\ -d \\ '{ \"deployment\": { \"revision\": \"REVISION\", \"changelog\": \"Added: /v2/deployments.rb, Removed: None\", \"description\": \"Added a deployments resource to the v2 API\", \"user\": \"datanerd@example.com\", \"timestamp\": \"2019-10-08T00:15:36Z\" } }' Copy Record a deployment with PowerShell To record a deployment with PowerShell, send a POST request that includes your API key to the deployments endpoint. Attach the payload in JSON format (see Character limits and JSON parameters). All payload parameters are optional except revision. This example uses PowerShell version 3 or higher: Invoke-WebRequest -Uri https://api.newrelic.com/v2/applications/YOUR_APP_ID/deployments.json -Method POST -Headers @{'Api-Key'='$API_KEY'} -ContentType 'application/json' -Body '{ \"deployment\": { \"revision\": \"REVISION\", \"changelog\": \"Added: /v2/deployments.rb, Removed: None\", \"description\": \"Added a deployments resource to the v2 API\", \"user\": \"datanerd@example.com\", \"timestamp\": \"2019-10-08T00:15:36Z\" } }' Copy This example uses PowerShell version 2 (requires .NET framework 3.5 or higher): $encoding = [System.Text.Encoding]::GetEncoding(\"ASCII\") $data ='{ \"deployment\": { \"revision\": \"REVISION\", \"changelog\": \"Added: /v2/deployments.rb, Removed: None\", \"description\": \"Added a deployments resource to the v2 API\", \"user\": \"datanerd@example.com\", \"timestamp\": \"2019-10-08T00:15:36Z\" } }' $postData = $encoding.GetBytes($data) $request = [System.Net.WebRequest]::Create('https://api.newrelic.com/v2/applications/$APP_ID/deployments.json') $request.Method = 'POST' $request.Headers.add('Api-Key','$API_KEY') $request.ContentType='application/json' $stream = $request.GetRequestStream() $stream.Write($postData,0,$postData.Length) $request.GetResponse() Copy View a list of deployments with GET To retrieve a list of all past deployments for your app, send a GET request that includes your API key to the deployments endpoint. GET requests do not use a JSON payload. For example: curl -X GET \"https://api.newrelic.com/v2/applications/$APP_ID/deployments.json\" \\ -H \"Api-Key:$API_KEY\" \\ -i Copy Sample output from GET This example requests a list of deployments for app ID 9999999: curl -X GET \"https://api.newrelic.com/v2/applications/9999999/deployments.json\" \\ -H \"Api-Key:ABCDEFGHIJKLMNOPQRSTUVWXabcdefghijklmnopqrstuvwx\" \\ -i Copy The API returns this list of deployments: HTTP/1.1 200 OK ETag: \"ABCDEFGHIJKabcdefghijk0123456789\" Cache-Control: max-age=0, private, must-revalidate Content-Type: application/json { \"deployments\": [ { \"id\": 1234567, \"revision\": \"1234123412341234123412341234123412341234\", \"changelog\": \"Fixed the bugs for real this time\", \"description\": \"Example description two\", \"user\": \"Data Nerd\", \"timestamp\": \"2016-02-24T10:09:27-08:00\", \"links\": { \"application\": 9999999 } }, { \"id\": 2345678, \"revision\": \"7890789078907890789078907890789078907890\", \"changelog\": \"Think I fixed all the bugs\", \"description\": null, \"user\": \"Dren Atad\", \"timestamp\": \"2014-10-22T12:23:47-07:00\", \"links\": { \"application\": 9999999 } } ], \"links\": { \"deployment.agent\": \"/v2/applications/{application_id}\" } } Copy Delete a deployment with DELETE To delete a deployment, send a DELETE request that includes your API key to the deployments endpoint. DELETE requests do not use a JSON payload, but you must specify the ID for the deployment you want to delete. To retrieve the ID for a deployment, use the GET request. For example: curl -X DELETE \"https://api.newrelic.com/v2/applications/$APP_ID/deployments/$DEPLOYMENT_ID.json\" \\ -H \"Api-Key:$API_KEY\" \\ -i Copy Character limits and JSON parameters The JSON payload can include the following parameters. Important UTF-8 4 byte characters, such as Emojis and some non-Latin language glyphs, cannot be used in the deployment text. Parameter Data type Description revision String, 127 character maximum Required. A unique ID for this deployment, visible in the Summary page and on the Deployments page. Can be any string, but is usually a version number or a Git checksum. changelog String, 65535 character maximum Optional. A summary of what changed in this deployment, visible in the Deployments page when you select (selected deployment) > Change log. description String, 65535 character maximum Optional. A high-level description of this deployment, visible in the Summary page and on the Deployments page when you select an individual deployment. user String, 31 character maximum Optional. A username to associate with the deployment, visible in the Summary page and on the Deployments page. timestamp ISO 8601 Optional. When the deployment occurred, down to the second. If not specified, the deployment will be recorded at the time when the API call was received. Timestamp requirements: Must be in UTC time. Must be after the most recent deployment timestamp. Cannot be in the future. Must be in ISO8601 format; for example, \"2019-10-08T00:15:36Z\". Record deployments using the New Relic agent Some agents have additional methods to record deployments: All agents: Use the New Relic REST API v2. C: No SDK-specific methods. Use the REST API. Go: No agent-specific methods. Use the REST API. Java: Call the Java agent jar. .NET: Use PowerShell and the REST API. Node.js: No agent-specific methods. Use the REST API. PHP: Use a PHP script. Python: Use the record-deploy subcommand of the newrelic-admin script. Ruby: Use a Capistrano recipe. Notify your team of deployments After a deployment is recorded using the REST API, you can optionally notify a webhook endpoint of the deployment. The destination of the webhook can be your Slack instance. To use webhooks to set up a deployment notification for a Slack channel: Log in to your Slack account as an admin, then go to App directory > Manage > Apps. Search for your New Relic app, then select Add configuration. From Post to channel, select an existing Slack channel or add a new channel, then Add configuration. From the list of options, copy the webhook URL. Go to one.newrelic.com > (account dropdown) > Account settings > Integrations > Deploy notifications > Webhook. Paste the Slack webhook URL, then save. Optional: Send a test message. Tip You can also use webhooks, Slack channels, and other options for alert notifications with New Relic Alerts. End of life notification: Hipchat, Campfire As of September 9, 2019, integrations with Hipchat and Campfire for APM deployment notifications will no longer be available. Recommendation: If you are still using Hipchat or Campfire, use webhooks with Slack channels instead. For more information, see the New Relic Explorers Hub post.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 175.28004,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>View</em> a list of deployments with GET",
        "body": " REST API <em>v2</em>. C: No SDK-specific methods. Use the REST API. Go: No agent-specific methods. Use the REST API. <em>Java</em>: Call the <em>Java</em> agent jar. .NET: Use PowerShell and the REST API. Node.js: No agent-specific methods. Use the REST API. PHP: Use a PHP <em>script</em>. Python: Use the record-deploy subcommand"
      },
      "id": "603eb1c364441fd58b4e888b"
    }
  ],
  "/docs/csp-v2-host-install-browser-agent": [
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/csp-v2-error-inline-javascript-not-allowed/",
      "sections": [
        "CSP v2 error: Inline JavaScript not allowed",
        "Problem",
        "Solution",
        "Cause"
      ],
      "published_at": "2021-05-03T06:35:23Z",
      "title": "CSP v2 error: Inline JavaScript not allowed",
      "updated_at": "2021-03-13T08:26:21Z",
      "type": "docs",
      "external_id": "872dffcae42631a9216ad28d8d2568847dcee2f9",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You are attempting to install New Relic's Browser agent using the auto-injection or copy/paste method. You see an error message indicating that inline scripts cannot run on the page. Solution In order to install the New Relic Browser agent when your website does not allow any third-party hosted JavaScript to run, you must request a CSP exception: Follow your organization's standard procedures to contact your web or security team and request an exception. Follow New Relic's procedures to install the Browser agent for CSP. If your organization has a strict process that prohibits CSP exceptions, get support at support.newrelic.com to discuss other options. Cause Content Security Policy v2 adds restrictions to the types of JavaScript allowed (such as inline scripts). It also limits which domains can load scripts during page load. CSP has multiple levels of complexity. Depending on how it is applied to your pages, it will impact how the Browser agent runs.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 301.31714,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CSP</em> <em>v2</em> error: Inline JavaScript not allowed",
        "sections": "<em>CSP</em> <em>v2</em> error: Inline JavaScript not allowed",
        "body": " exceptions, get support at support.newrelic.com to discuss other options. Cause Content Security Policy <em>v2</em> adds restrictions to the types of JavaScript allowed (such as inline scripts). It also limits which domains can load scripts during page load. <em>CSP</em> has multiple levels of complexity. Depending on how it is applied to your pages, it will impact how the <em>Browser</em> <em>agent</em> runs."
      },
      "id": "6043d191e7b9d228d95799c6"
    },
    {
      "sections": [
        ".NET agent: compatibility and requirements for .NET Framework",
        "Requirements",
        ".NET Framework version",
        "App/web servers",
        "CLRs",
        "Operating system",
        "Use of other monitoring software",
        "Microsoft Azure",
        "Processor architectures",
        "Permissions",
        "Security requirements",
        "Code Access Security",
        "Network requirements",
        "Application lifespan",
        "Automatic instrumentation",
        "App frameworks",
        "Datastores",
        "Messaging",
        "CMS",
        "External call libraries",
        "Connect the agent to other New Relic products"
      ],
      "title": ".NET agent: compatibility and requirements for .NET Framework",
      "type": "docs",
      "tags": [
        "Agents",
        "NET agent",
        "Getting started"
      ],
      "external_id": "9fada40fe709392d7080cc1b5bc873039501d455",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/net-agent/getting-started/net-agent-compatibility-requirements-net-framework/",
      "published_at": "2021-05-05T09:24:09Z",
      "updated_at": "2021-04-21T18:18:18Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our .NET agent supports both .NET Framework and .NET Core. Here we describe the compatibility and support for .NET Framework applications. For .NET Core, see Compatibility and requirements for .NET Core. The agent includes built-in instrumentation for some of the most popular parts of the .NET Framework ecosystem, including frameworks, databases, and message queuing systems. After you download and install the agent, it runs within the monitored process. The agent does not create a separate process or service. For frameworks and libraries that are not automatically instrumented, you can extend the agent with .NET custom instrumentation. Requirements Before you install New Relic's .NET agent, make sure your system meets these requirements: .NET Framework version .NET Framework 4.5 or higher .NET Framework 4.5 or higher requires the New Relic .NET agent version 7.0 or higher. .NET Framework 4.0 or lower To instrument applications running on .NET Framework version 4.0 and lower, you must run a version of the New Relic .NET agent earlier than 7.0. For more information and download procedures, see Technical support for .NET frameworks 4.0 or earlier. Here are additional details about .NET Framework versions 4.0 and lower: .NET Framework version Notes .NET Framework 2.0 Unless you have installed only .NET Framework 4.0 or higher, you must have .NET Framework version 3.5 installed on the target host, in addition to any other .NET Framework versions you have installed. .NET Framework 3.0 Unless you have installed only .NET Framework 4.0 or higher, you must have .NET Framework version 3.5 installed on the target host, in addition to any other .NET Framework versions you have installed. .NET Framework 3.5 Fully supported by .NET agent versions below 7.0. .NET Framework 4.0 Fully supported by .NET agent versions below 7.0. If you use .NET Framework 1.1 for any app on your target system, you must disable instrumentation of that app with applicationPool set to instrument=\"false\". The .NET agent can cause crashes in .NET 1.1 apps. App/web servers You must use one of these app/web servers: IIS Self-hosted OWIN Self-hosted WCF Kestrel Kestrel with IIS reverse proxy via AspNetCoreModule Kestrel with IIS reverse proxy via AspNetCoreModuleV2 The agent automatically creates transactions for apps hosted in IIS. If you self-host with WCF and OWIN version 3 or higher, the agent also automatically creates transactions. For other self-hosted services, you will need to create transactions via custom instrumentation. CLRs The agent requires CLR version 4.0. Legacy applications running on CLR 2.0 can be instrumented with agent versions earlier than 7.0. Operating system The agent requires one of these operating systems: Windows Server 2008 Windows Server 2008 R2 Windows Server 2012 Windows Server 2012 R2 Windows Server 2016 Windows Server 2019 Windows 10 Windows Azure (OS Family 1, 2, and 3) Windows containers running on Windows 2016 (NanoServer based images are not supported) Use of other monitoring software The .NET Common Language Runtime (CLR) only allows one profiler to access the profiling API of a process at any given time. Running our .NET agent alongside other monitoring software will result in a profiler conflict. For more information, see Errors while using other APM software. Microsoft Azure For Azure-specific installation instructions, see: Install on Azure Cloud Services Install on Azure Service Fabric Install on Azure Web Apps Processor architectures The agent is available in both a 32-bit and 64-bit version. On 64-bit systems, the 64-bit agent can instrument both 32-bit and 64-bit applications. Permissions Installation requires elevated privileges (Administrator). For example, you can: Be logged in as an administrator user. Be a member of the Administrators group. On newer operating systems, provide elevation credentials when prompted. The monitored process must have read/write access to the directory in which the agent is installed. The agent runs as a part of the monitored process and relies on those permissions to function. Recommendation: Restrict permissions for the newrelic.config file and give read/write access only to the owner of the monitored process. Review permissions for the logs created by the agent to minimize the number of users with access and their privileges. Security requirements As a standard security measure for data collection, your app server must support SHA-2 (256-bit). SHA-1 is not supported. Code Access Security The use of Code Access Security is compatible with the .NET agent only when Full Trust is provided. The agent is not compatible with more restrictive trust levels. Network requirements The agent requires your firewall to allow outgoing connections to specific networks and ports. Application lifespan The .NET agent uploads data at the end of each harvest cycle (once per minute). If your .NET app doesn't run that long, you can set the service element's sendDataOnExit attribute to true in the newrelic.config file. Automatic instrumentation If your application is hosted in ASP.NET or another fully supported framework, the .NET agent will automatically instrument your application after install. If your app is not automatically instrumented, or if you want to add instrumentation, use custom instrumentation. The .NET agent does not directly monitor datastore processes. Also, by default the .NET SQL parameter capture in a query trace does not list parameters for a parameterized query or a stored procedure. Collection of the SQL query parameters can be enabled in the agent configuration. App frameworks The agent automatically instruments some application frameworks; we refer to these frameworks as fully supported. Compatibility status .NET app frameworks Fully supported These frameworks are fully supported: ASP.NET MVC 2 ASP.NET MVC 3 ASP.NET MVC 4 ASP.NET MVC 5 ASP.NET Web API v2 ASP.NET Core MVC 2.0 ASP.NET Core 2.1 ASP.NET Core 2.2 ASP.NET Web Forms Castle MonoRail v2 (not the same as Mono) OWIN-hosted web API applications on .NET framework using: Microsoft.Owin.Host.HttpListener v2.x Microsoft.Owin.Host.HttpListener v3.x Microsoft.Owin.Host.HttpListener v4.x SOAP-based web services WCF Hosting Models IIS Hosted (with and without ASP Compatibility) Self Hosted Binding Types (both client and service) WCF Instrumentation has been tested for the following common binding types. Varying levels of support are available for distributed tracing (DT) and cross application tracing (CAT): Binding Distributed Tracing (DT) Support Cross Application Tracing (CAT) Support BasicHTTP supported supported WebHTTP supported supported WSHTTP supported supported NetTCP supported supported NetNamedPipe not supported * not supported * NetMSMQ not supported * not supported * * Distributed tracing may be manually implemented using these Distributed Tracing API methods. Invocation Methods (both client and service) Synchronous Asynchronous Processing Model (APM) Begin/End Task Asynchronous Processing (TAP) Event-based Asynchronous Processing (EAP) Client support only. Not fully supported These frameworks are not fully supported: ASP.NET Web API v1: See the troubleshooting information about using ASP.NET Web API v1 with New Relic's .NET agent 5.0 or higher for apps targeting .NET Framework 4.0. (This does not affect .NET Framework 4.5 or higher.) Mono: New Relic does not support Mono, an open-source .NET framework that runs on Linux. This is because there is no Profiler API to inject into the .NET agent as a profiler into Mono-based .NET applications. The Profiler API is a Component Object Model (COM)-based interface and is not supported on Linux. Not supported Classic ASP is not supported, because the agent can only instrument .NET Framework-based apps. Sharepoint is not supported. Datastores Collecting instance details for supported datastores requires .NET agent version 6.5.29.0 or higher and is enabled by default. To request instance-level information from datastores not currently listed, get support at support.newrelic.com. In order to automatically instrument the performance of .NET Framework application calls to these datastores, make sure you have the .NET agent version 8.14 or higher: Datastore Instance details Other notes Couchbase SDK version 2.x.x (2.4.0 or higher is not supported). With Couchbase, the following are not instrumented by default in favor of their multi-document counterparts: Get(string key) GetDocument(string key) Remove(string key) Remove(string key, ulong cas) Upsert<T>(string key, T value) IBM DB2 Microsoft SQL Server MongoDB We support both the modern and legacy MongoDB drivers. Legacy - mongocsharpdriver Driver versions 1.10.0 or earlier: Instance details are not available in these driver versions. Modern - MongoDB.Driver Driver version 2.3.x, 2.4.x, 2.5.x, or 2.6.x Driver version 2.7.x: The .NET agent will support pre-existing API methods, but not new methods introduced in 2.7.x. MySQL Use MySql.Data version 6.10.4. or the MySQL Connector Oracle PostgreSQL Use Npgsql 4.0. Prior versions of Npgsql may also be instrumented, but duplicate and/or missing metrics are possible. ServiceStack.Redis StackExchange.Redis Messaging The agent automatically instruments these message systems: MSMQ: Puts and takes on messages NServiceBus 5.0 (6.0 or higher not supported): Puts and takes on messages and cross application tracing RabbitMQ 3.5 or higher: Puts and takes on messages and queue purge. When receiving messages using an IBasicConsumer, the EventingBasicConsumer is the only implementation that is instrumented by the .NET agent. BasicGet is instrumented, but the agent does not support distributed tracing for BasicGet. CMS The agent automatically instruments the EPiServer content management system. External call libraries The agent automatically instruments these external call libraries : Libraries Supported methods HttpClient The agent instruments these HttpClient methods: SendAsync GetAsync PostAsync PutAsync DeleteAsync GetStringAsync GetStreamAsync GetByteArrayAsync RestSharp The agent instruments these RestSharp methods: ExecuteTaskAsync ExecuteGetTaskAsync ExecutePostTaskAsync Execute ExecuteAsGet ExecuteAsPost DownloadData HttpWebRequest The agent instruments these HttpWebRequest methods: GetResponse Connect the agent to other New Relic products In addition to APM, the agent integrates with other New Relic products to give you end-to-end visibility: Product Integration Browser monitoring For ASP.NET applications, the .NET agent automatically injects the Browser JavaScript agent when you enable auto-instrumentation. After enabling Browser injection, you can view Browser data in the APM Summary page and quickly switch between the APM and Browser data for a particular app. For configuration options and manual instrumentation, see Browser monitoring and the .NET agent. Automatic Browser injection is not supported for ASP.NET core applications. Infrastructure monitoring When you install the Infrastructure and APM agents on the same host, they automatically detect one another. You can then view a list of hosts in the APM UI, and filter your Infrastructure hosts by APM app in the Infrastructure UI. For more information, see APM data in Infrastructure. Dashboards The .NET agent sends default events and attributes for NRQL queries. You can also record custom events for advanced analysis. Synthetic monitoring Synthetic transaction traces connect requests from Synthetics monitors to the underlying APM transaction.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 301.13687,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": ".NET <em>agent</em>: compatibility <em>and</em> requirements for .NET Framework",
        "sections": ".NET <em>agent</em>: compatibility <em>and</em> requirements for .NET Framework",
        "tags": "<em>Agents</em>",
        "body": "&#x2F;web servers: IIS Self-hosted OWIN Self-hosted WCF Kestrel Kestrel with IIS reverse proxy via AspNetCoreModule Kestrel with IIS reverse proxy via AspNetCoreModule<em>V2</em> The <em>agent</em> automatically creates transactions for apps hosted in IIS. If you self-<em>host</em> with WCF and OWIN version 3 or higher, the <em>agent</em>"
      },
      "id": "603ea02e196a671b92a83ddc"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/csp-v2-monitor-salesforce/",
      "sections": [
        "CSP v2: Monitor Salesforce",
        "Contents",
        "Requirements",
        "1. Whitelist New Relic Browser domains",
        "2. Copy Browser app's JS snippet",
        "3. Create .js file for page",
        "4. Keep Browser agent up to date"
      ],
      "published_at": "2021-05-03T06:35:19Z",
      "title": "CSP v2: Monitor Salesforce",
      "updated_at": "2021-03-13T01:16:21Z",
      "type": "docs",
      "external_id": "c1086099119c1b6a2972d0b3380cd11439f48c48",
      "document_type": "page",
      "popularity": 1,
      "body": "Salesforce has strict requirements about Content Security Policy (CSP) v2. In this situation, you must copy and paste the New Relic Browser agent's JavaScript code and self-host it as a .js file. This allows the page to reference the .js file during page load. Contents Requirements In order to use New Relic Browser to monitor Salesforce Lightning pages, make sure you follow these requirements: CSP and Salesforce Lightning Browser agent requirements Application or platform Your app or platform where your site is hosted must meet New Relic Browser's standard compatibility requirements. Exception: CSP restrictions prevent you from linking an APM app for Browser monitoring. Subscription You must have a Pro+SPA Browser subscription for your app. Host location for domains The .js file for New Relic Browser must be hosted on a highly available or replicated location, such as a CDN or distributed network. This helps ensure performance is not affected. Follow the Salesforce procedures to add New Relic Browser URLs to your CSP whitelist. Browser agent version Your selected app must use the latest Browser agent version. You must manually update the .js file whenever the New Relic Browser agent version is updated. New Relic Browser cannot make this change automatically for Salesforce Lightning apps. 1. Whitelist New Relic Browser domains Follow the Salesforce procedures to add New Relic Browser URLs to your CSP whitelist. Add both of these New Relic Browser domains to your CSP whitelist: https://js-agent.newrelic.com: This is where the New Relic Browser agent is hosted. The Browser agent download requires three files to run. Two are downloaded after the initial agent is run to continue capturing performance data. https://bam.nr-data.net: This is the endpoint where the New Relic Browser agent receives data from your site. 2. Copy Browser app's JS snippet To create and host a .js file, you must first copy the Browser agent's JavaScript snippet from the hosting app. Go to rpm.newrelic.com/browser. From the list of Browser apps, select the app for which you want to self-host the Browser agent. Select Settings > Application settings > Copy/paste JavaScript code. Copy the Browser agent's JavaScript snippet. Save and exit the Browser app's Application settings page. Continue with the procedures to create a .js file for your page. 3. Create .js file for page After you copy the Browser agent's JavaScript snippet from the selected Browser app: Open a text editor and paste the Browser agent's JavaScript snippet. Delete the first line (<script type=\"text/javascript\">) and the last line of the JavaScript snippet (</script>). Save the text file as nr-spa-AGENT_VERSION.min.js, where AGENT_VERSION is the latest Browser agent version. Upload the .js file to your Salesforce external JavaScript libs directory, and reference the .js file in the <head> of your webpage. After you install the Browser agent's JavaScript snippet, you can use the New Relic Browser UI to monitor website performance. 4. Keep Browser agent up to date You must manually update the .js file whenever New Relic updates the Browser agent version. New Relic Browser cannot make this change automatically for Salesforce Lightning apps.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 294.09595,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CSP</em> <em>v2</em>: Monitor Salesforce",
        "sections": "<em>CSP</em> <em>v2</em>: Monitor Salesforce",
        "body": "Salesforce has strict requirements about Content Security Policy (<em>CSP</em>) <em>v2</em>. In this situation, you must copy and paste the New Relic <em>Browser</em> <em>agent</em>&#x27;s JavaScript code and self-<em>host</em> it as a .js file. This allows the page to reference the .js file during page load. Contents Requirements In order to use"
      },
      "id": "6043b986e7b9d2d0025799c0"
    }
  ],
  "/docs/csp-v2-monitor-salesforce": [
    {
      "sections": [
        "Listing users for your account",
        "Tip",
        "Listing all account users",
        "Listing by user email",
        "Listing by user id"
      ],
      "title": "Listing users for your account",
      "type": "docs",
      "tags": [
        "APIs",
        "REST API v2",
        "Account admin and usage (v2)"
      ],
      "external_id": "3ca4032c2bf5377abae0fa9bc8c1d93ac0235b95",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/rest-api-v2/account-examples-v2/listing-users-your-account/",
      "published_at": "2021-05-04T17:33:25Z",
      "updated_at": "2021-04-12T04:30:25Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic stores the list of individuals who can use your account in a database by their email address, assigned role, and their first and last name if provided. You can view this data from New Relic's user interface and from the API Explorer (v2). Tip To obtain the same information from the New Relic API Explorer (v2), select Users > GET List. Listing all account users To obtain a list of all users for your account, use this command: curl -X GET 'https://api.newrelic.com/v2/users.json' \\ -H \"x-api-key:$API_KEY\" -i Copy The output will appear similar to this: HTTP/1.1 200 OK Content-Type: application/json { \"users\": [ { \"id\": 123456, \"first_name\": \"My\", \"last_name\": \"Name\", \"email\": \"my.name@mywebsite.com\", \"role\": \"owner\" }, { \"id\": 654321, \"first_name\": \"Adam\", \"last_name\": \"Admin\", \"email\": \"adam.admin@mywebsite.com\", \"role\": \"admin\" }, { \"id\": 345123, \"first_name\": \"Any\", \"last_name\": \"User\", \"email\": \"any.user@mywebsite.com\", \"role\": \"user\" }, ... Copy Listing by user email If you know all or part of the user's email, you can use this command to return the role, name, and user id. The filter[email]= clause specifies the known part of the email (for example, \"my.name\"). Note: Character matching is a simple string. No regular expression capability is available, so multiple matches may occur if the selected string is not unique. curl -X GET 'https://api.newrelic.com/v2/users.json' \\ -H \"x-api-key:$API_KEY\" -i \\ -d 'filter[email]=my.name' Copy The output for this command will be the same as the first entry in the Listing all account users example. Listing by user id If you know the user id, you can use this command to return the role, name, and email. The filter[ids]= clause specifies the user id. curl -X GET 'https://api.newrelic.com/v2/users.json' \\ -H \"x-api-key:$API_KEY\" -i \\ -d 'filter[ids]=123456' Copy You can also use this command, which embeds the user id in the URL and omits the filter. curl -X GET 'https://api.newrelic.com/v2/users/123456.json' \\ -H \"x-api-key:$API_KEY\" -i Copy The output for this command will be the same as the first entry in the Listing all account users example.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 280.62402,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "REST API <em>v2</em>",
        "body": "New Relic stores the list of individuals who can use your account in a database by their email address, assigned role, and their first and last name if provided. You can view this data from New Relic&#x27;s user interface and from the API Explorer (<em>v2</em>). Tip To obtain the same information from the New"
      },
      "id": "603ecc4c28ccbcd60eeba77b"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/docs/csp-v2-host-install-browser-agent/",
      "sections": [
        "CSP v2: Host and install the Browser agent",
        "Contents",
        "Requirements",
        "1. Whitelist New Relic Browser domains",
        "2. Copy Browser app's JS snippet",
        "3. Create .js file for page"
      ],
      "published_at": "2021-05-03T06:35:20Z",
      "title": "CSP v2: Host and install the Browser agent",
      "updated_at": "2021-03-13T07:28:47Z",
      "type": "docs",
      "external_id": "75530e0b53071dd92148ca07d572bc3a032e9c4e",
      "document_type": "page",
      "popularity": 1,
      "body": "If your organization has strict requirements about Content Security Policy (CSP) v2, you may need to host the New Relic Browser agent in order to properly install and run it. For example, if the New Relic domain hosting the Browser agent is being blocked by your website, you may see an error similar to this: Browser agent.js is unable to run because it violates the following Content Security Protocol directive: script-src = *.google.com other URLs Copy In this situation, you must copy and paste the agent's JavaScript code and self-host it as a .js file. This allows the page to reference the .js file during page load. Contents Requirements In order to host the Browser agent with your app, make sure you follow these requirements: CSP and Browser agent Requirements Application or platform Your app or platform where your site is hosted must meet New Relic Browser's standard compatibility requirements. Exception: CSP restrictions prevent you from linking an APM app for Browser monitoring. Subscription You must have a Pro+SPA Browser subscription for your app. Host location for domains The .js file for New Relic Browser must be hosted on a highly available or replicated location, such as a CDN or distributed network. This helps ensure performance is not affected. If you see error messages, add New Relic Browser domains to your CSP whitelist. If you want New Relic Browser to monitor Salesforce Lightning pages, follow the Salesforce procedures to add New Relic Browser domains to your CSP whitelist. By hosting the agent, you are responsible for any performance impact on the location where the agent is hosted. Browser agent version Your selected app must use the latest Browser agent version. 1. Whitelist New Relic Browser domains Follow your organization's standard procedures to request a CSP exception. Then add both of these New Relic Browser domains to your CSP whitelist: https://js-agent.newrelic.com: This is where the New Relic Browser agent is hosted. The Browser agent download requires three files to run. Two are downloaded after the initial agent is run to continue capturing performance data. https://bam.nr-data.net: This is the endpoint where the New Relic Browser agent receives data from your site. Here is example HTML to add to the head of your site: Content-Security-Policy: default-src 'self' https://js-agent.newrelic.com https://bam.nr-data.net Copy 2. Copy Browser app's JS snippet To create and host a .js file, you must first copy the Browser agent's JavaScript snippet from the hosting app. Go to rpm.newrelic.com/browser. From the list of Browser apps, select the app for which you want to self-host the Browser agent. Select Settings > Application settings > Copy/paste JavaScript code. Copy the Browser agent's JavaScript snippet. Save and exit the Browser app's Application settings page. Continue with the procedures to create a .js file for your page. 3. Create .js file for page After you copy the Browser agent's JavaScript snippet from the selected Browser app: Open a text editor and paste the Browser agent's JavaScript snippet. Delete the first line (<script type=\"text/javascript\">) and the last line of the JavaScript snippet (</script>). Save the text file as nr-spa-VERSION_NUMBER.min.js, where VERSION_NUMBER is the latest Browser agent version. Reference the .js file in the <head> of your webpage. After you install the Browser agent's JavaScript snippet, you can use the New Relic Browser UI to monitor website performance.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 273.6445,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>CSP</em> <em>v2</em>: Host and install the Browser agent",
        "sections": "<em>CSP</em> <em>v2</em>: Host and install the Browser agent",
        "body": "If your organization has strict requirements about Content Security Policy (<em>CSP</em>) <em>v2</em>, you may need to host the New Relic Browser agent in order to properly install and run it. For example, if the New Relic domain hosting the Browser agent is being blocked by your website, you may see an error"
      },
      "id": "6043d37364441f42cf378ee8"
    },
    {
      "sections": [
        "Get individual plugin components (v2)",
        "Tip",
        "List by component name",
        "List by component ID",
        "View the component's output"
      ],
      "title": "Get individual plugin components (v2)",
      "type": "docs",
      "tags": [
        "APIs",
        "REST API v2",
        "Plugin examples (v2)"
      ],
      "external_id": "8e5d5b5961b55a021ba90b40d6a352743a86a607",
      "image": "",
      "url": "https://docs.newrelic.com/docs/apis/rest-api-v2/plugin-examples-v2/get-individual-plugin-components-v2/",
      "published_at": "2021-05-06T02:30:35Z",
      "updated_at": "2021-03-16T08:15:46Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use New Relic's REST API and your API key to obtain information about a single component for your plugin, including: The summary metric values in raw and formatted form as set by the plugin publisher Threshold data indicating where Caution and Critical alerts will be triggered The COMPONENT_ID and COMPONENT_NAME for the plugin Summary information, including SUMMARY_METRIC_ID and SUMMARY_METRIC_NAME The associated summary metric information, including METRIC_NAME and METRIC_VALUE You can use the API Explorer to list components. Tip When using these examples, be sure to replace each placeholder (for example, ${PLUGIN_ID}) with its specific ID. List by component name To obtain a list of the summary data values for a single component by name, use: curl -X GET 'https://api.newrelic.com/v2/components.json' \\ -H \"X-Api-Key:${APIKEY}\" -i \\ -d \"filter[name]=${COMPONENT_NAME}\" Copy List by component ID To obtain a list of the summary data for a single component by its component ID, use one of the following commands. curl -X GET 'https://api.newrelic.com/v2/components.json' \\ -H \"X-Api-Key:${APIKEY}\" -i \\ -d \"filter[id]=${COMPONENT_ID}\" Copy OR: curl -X GET \"https://api.newrelic.com/v2/components/${COMPONENT_ID}.json\" \\ -H \"X-Api-Key:${APIKEY}\" -i Copy View the component's output The output for each component ID will be similar to the following example (annotated and formatted for JSON). Due to the potential length of the output, you may want to paginate it. { \"components\": [ { \"id\": 3947935, <---<<< COMPONENT_ID \"name\": \"F5-01-prod\", <---<<< COMPONENT_NAME \"summary_metrics\": [ { \"id\": 116635, <---<<< SUMMARY_METRIC_ID \"name\": \"CPU\", <---<<< SUMMARY_METRIC_NAME \"metric\": \"Component/CPU/Total/Global[%]\", <---<<< METRIC_NAME \"value_function\": \"average_value\", <---<<< METRIC_VALUE \"thresholds\": { \"caution\": 75, <---<<< SUMMARY_METRIC_CAUTION_THRESHOLD \"critical\": 85 <---<<< SUMMARY_METRIC_CRITICAL_THRESHOLD }, \"values\": { \"raw\": 78.79225, <---<<< SUMMARY_METRIC_VALUE \"formatted\": \"78.8 %\" <---<<< Formatted SUMMARY_METRIC_VALUE } }, { \"id\": 116636, \"name\": \"Throughput\", \"metric\": \"Component/Throughput/Total[bits/sec]\", \"value_functions\": ... . . . } . . . ] } }, } Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 249.30296,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Get individual plugin components (<em>v2</em>)",
        "sections": "Get individual plugin components (<em>v2</em>)",
        "tags": "REST API <em>v2</em>",
        "body": " placeholder (for example, ${PLUGIN_ID}) with its specific ID. List by component name To obtain a list of the summary data values for a single component by name, use: curl -X GET &#x27;https:&#x2F;&#x2F;api.newrelic.com&#x2F;<em>v2</em>&#x2F;components.json&#x27; \\ -H &quot;X-Api-Key:${APIKEY}&quot; -i \\ -d &quot;filter[name]=${COMPONENT_NAME}&quot; Copy List"
      },
      "id": "603eb58f196a6724b2a83dbf"
    }
  ],
  "/docs/distributed-tracing/concepts/distributed-tracing-planning-guide": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 254.29105,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, Infinite <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about Infinite <em>Tracing</em>: Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.81158,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> API general requirements and limits ",
        "sections": "<em>Trace</em> API general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Tracing</em>: when you complete the <em>Trace</em> observer setup, you <em>get</em> a custom YOUR_<em>TRACE</em>_OBSERVER_URL value to use as an endpoint. If you&#x27;re using an integration that uses the <em>Trace</em> API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust"
      },
      "id": "6071cf7628ccbcf8b851c158"
    },
    {
      "sections": [
        "Introduction to distributed tracing",
        "Why it matters",
        "Instrumentation: The key to distributed tracing",
        "What you can see in the New Relic UI",
        "Next steps"
      ],
      "title": "Introduction to distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "ac173988a6503674b4411c9c2efe6713912c37f2",
      "image": "https://docs.newrelic.com/static/8a63e4d34a1272fa33ad7b3564db6bf8/b5bda/span-diagram.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/introduction-distributed-tracing/",
      "published_at": "2021-05-04T18:32:24Z",
      "updated_at": "2021-04-11T07:38:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Distributed tracing tracks and observes service requests as they flow through distributed systems. With distributed tracing data, you can quickly pinpoint failures or performance issues and fix them. Distributed tracing systems collect data as the requests go from one service to another, recording each segment of the journey as a span. These spans contain important details about each segment of the request and are combined into one trace. The completed trace gives you a picture of the entire request. Here is an example of three spans captured from an HTTP request. These spans are sent to New Relic where they are combined into one distributed trace. Why it matters A request might pass through various microservices to reach completion. The microservices or functions could be located in multiple containers, serverless environments, virtual machines, different cloud providers, on-premises, or any combination of these. For example, let's say that you're in a situation where a slow-running request affects the experience of a set of customers: The request is distributed across multiple microservices and serverless functions. Several different teams own and monitor the various services that are involved in the request. None of the teams have reported any performance issues with their microservices. Without a way to view the performance of the entire request across the different services, it’s nearly impossible to pinpoint where and why the high latency is occurring and which team should address the issue. Instrumentation: The key to distributed tracing Distributed tracing starts with the instrumentation of your services to enable data collection and correlation across the entire distributed system. Instrumention means either manually adding code to services or installing agents that automatically track trace data. Many of our New Relic solutions automatically instrument your services for a large number of programming languages and frameworks. You can also use open source tools and open instrumentation standards to instrument your environment. OpenTelemetry, part of the Cloud Native Computing Foundation (CNCF), is becoming the one standard for open source instrumentation and telemetry collection. What you can see in the New Relic UI After the data is collected, you can visualize it to see service dependencies, performance, and any anomalous events such as errors or unusual latency. Here are some examples of what you can do with your data: What you can do Description Detect anomalous spans Spans that are slow in comparison to typical behavior are marked as anomalous, with charts comparing them to typical performance. See your errors and logs Frontend and backend errors appear right in the context of your traces. Everything you need to troubleshoot is in one place. Filter results You can filter charts using many data points, so you can analyze trace data in different ways. Customize queries and dashboards You can create custom queries of your trace data and create custom data dashboards. See data across accounts See a global view of traces from across all your accounts and applications in New Relic One. Query traces programmatically Query distributed trace data by using GraphQL in our NerdGraph API explorer. Next steps Here are some tasks to consider: To instrument your services, check out our Quick start. To learn more about what's happening under the hood, see How distributed tracing works.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 207.10315,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>distributed</em> <em>tracing</em>",
        "sections": "Introduction to <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " and create custom data dashboards. See data across accounts See a global view of traces from across all your accounts and applications in New Relic One. Query traces programmatically Query <em>distributed</em> <em>trace</em> data by using GraphQL in our NerdGraph API explorer. Next steps Here are some tasks to consider: To instrument your services, check out our Quick <em>start</em>. To learn more about what&#x27;s happening under the hood, see How <em>distributed</em> <em>tracing</em> works."
      },
      "id": "6072a767e7b9d231f1a5c64c"
    }
  ],
  "/docs/distributed-tracing/concepts/how-new-relic-distributed-tracing-works": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 254.29105,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, Infinite <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about Infinite <em>Tracing</em>: Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.81158,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> API general requirements and limits ",
        "sections": "<em>Trace</em> API general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Tracing</em>: when you complete the <em>Trace</em> observer setup, you <em>get</em> a custom YOUR_<em>TRACE</em>_OBSERVER_URL value to use as an endpoint. If you&#x27;re using an integration that uses the <em>Trace</em> API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust"
      },
      "id": "6071cf7628ccbcf8b851c158"
    },
    {
      "sections": [
        "Distributed tracing: Planning guide",
        "Impact to APM features",
        "External services page has less detail",
        "Transaction trace UI displays service URLs, not transaction links",
        "Cross-application tracing will be disabled",
        "Impacts related to mobile monitoring",
        "Plan your rollout",
        "Enable distributed tracing"
      ],
      "title": "Distributed tracing: Planning guide",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "10263f2b6ec929f082153e28cbce07fe3a1f106a",
      "image": "https://docs.newrelic.com/static/7072dfa9e494767baabba420d78e7094/c1b63/distributed-trace-txn-trace-affects.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/distributed-tracing-planning-guide/",
      "published_at": "2021-05-04T18:34:48Z",
      "updated_at": "2021-04-11T07:32:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are new to New Relic distributed tracing, we recommend you read the following before you enable distributed tracing. Impact to APM features Our distributed tracing improves on APM's previous cross application tracing feature. Here are some key benefits: See more cross-service activity details and more complete end-to-end traces. Filter and query traces, as well as make custom charts. See the complete trace even when calls cross account boundaries (for accounts with the same master account or in the same customer partnership). See Introduction to distributed tracing for other features. Enabling distributed tracing may affect some APM features you currently use. These changes affect only applications monitored by agents that have distributed tracing enabled—they don't apply on an account-level. We may provide backward compatibility with some or all of the affected features in future releases. For now, you should understand the following changes before enabling distributed tracing: External services page has less detail When distributed tracing is enabled for an application, external calls do not have internal transaction details at one.newrelic.com > APM > (select an app) > Monitor > External services > (select external service). To find that information, you would instead go to the Distributed tracing UI page, find the external call URLs, and see what their child spans are. Transaction trace UI displays service URLs, not transaction links When distributed tracing is enabled for an application, the transaction trace UI will no longer have the transaction name and link for the called service (see screenshot below). This will be replaced with the called service's URL. one.newrelic.com > APM > (select an app) > Monitor > Transactions > (select a transaction trace): This shows the transaction trace UI before distributed tracing is enabled, with a link to the associated transaction. With distributed tracing enabled, it will display the service's URL. If you wanted to get more detail about trace activity, you would go to the Distributed tracing UI page and examine that trace. Cross-application tracing will be disabled Enabling distributed tracing will disable the cross application tracing feature. Distributed tracing is an improved version of cross-application tracing and only one can be enabled at a time. Impacts related to mobile monitoring Distributed tracing is not yet available for mobile monitoring. APM-related impacts include: When distributed tracing is enabled for an APM-monitored entity, service maps will not show applications monitored by mobile. The App server drill-down feature of the Mobile HTTP requests UI page is not available. Plan your rollout If you're enabling distributed tracing for a large, distributed system, here are some tips: If you are a current APM user, see Impact to APM features. Determine the requests that are the most important for your business, or the most likely to require analysis and troubleshooting, and enable distributed tracing for those services. Enable tracing for services at roughly the same time so you can more easily gauge how complete your end-to-end traces are. When you look at traces in the distributed tracing UI, you'll see spans in the trace for external calls to other services. Then, you can enable distributed tracing for any of those services you want. If a service is fairly standalone and not often used in context with other services, you may not want to enable distributed tracing for it. Here's a visual representation of such a phased roll-out: If you are using APM for a large, monolithic service, there may be many sub-process spans per trace and APM limits may result in fewer traces than expected. You can solve this by using APM agent instrumentation to disable the reporting of unimportant data. Distributed tracing works by propagating header information from service to service in a request path. Some services may communicate through a proxy or other intermediary service that does not automatically propagate the header. In that case, you will need to configure that proxy so that it allows the newrelic header value to be propagated from source to destination. Enable distributed tracing If you are aware of the impact to APM features and have thought about your rollout, you are ready to set up distributed tracing. See Overview: Enable distributed tracing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 217.86028,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Distributed</em> <em>tracing</em>: Planning guide",
        "sections": "<em>Distributed</em> <em>tracing</em>: Planning guide",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "): This shows the transaction <em>trace</em> UI before <em>distributed</em> <em>tracing</em> is enabled, with a link to the associated transaction. With <em>distributed</em> <em>tracing</em> enabled, it will display the service&#x27;s URL. If you wanted to <em>get</em> more detail about <em>trace</em> activity, you would go to the <em>Distributed</em> <em>tracing</em> UI page and examine"
      },
      "id": "6072a60564441f3a629d8535"
    }
  ],
  "/docs/distributed-tracing/concepts/introduction-distributed-tracing": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 254.29088,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, Infinite <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about Infinite <em>Tracing</em>: Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.81142,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> API general requirements and limits ",
        "sections": "<em>Trace</em> API general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Tracing</em>: when you complete the <em>Trace</em> observer setup, you <em>get</em> a custom YOUR_<em>TRACE</em>_OBSERVER_URL value to use as an endpoint. If you&#x27;re using an integration that uses the <em>Trace</em> API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust"
      },
      "id": "6071cf7628ccbcf8b851c158"
    },
    {
      "sections": [
        "Distributed tracing: Planning guide",
        "Impact to APM features",
        "External services page has less detail",
        "Transaction trace UI displays service URLs, not transaction links",
        "Cross-application tracing will be disabled",
        "Impacts related to mobile monitoring",
        "Plan your rollout",
        "Enable distributed tracing"
      ],
      "title": "Distributed tracing: Planning guide",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "10263f2b6ec929f082153e28cbce07fe3a1f106a",
      "image": "https://docs.newrelic.com/static/7072dfa9e494767baabba420d78e7094/c1b63/distributed-trace-txn-trace-affects.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/distributed-tracing-planning-guide/",
      "published_at": "2021-05-04T18:34:48Z",
      "updated_at": "2021-04-11T07:32:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you are new to New Relic distributed tracing, we recommend you read the following before you enable distributed tracing. Impact to APM features Our distributed tracing improves on APM's previous cross application tracing feature. Here are some key benefits: See more cross-service activity details and more complete end-to-end traces. Filter and query traces, as well as make custom charts. See the complete trace even when calls cross account boundaries (for accounts with the same master account or in the same customer partnership). See Introduction to distributed tracing for other features. Enabling distributed tracing may affect some APM features you currently use. These changes affect only applications monitored by agents that have distributed tracing enabled—they don't apply on an account-level. We may provide backward compatibility with some or all of the affected features in future releases. For now, you should understand the following changes before enabling distributed tracing: External services page has less detail When distributed tracing is enabled for an application, external calls do not have internal transaction details at one.newrelic.com > APM > (select an app) > Monitor > External services > (select external service). To find that information, you would instead go to the Distributed tracing UI page, find the external call URLs, and see what their child spans are. Transaction trace UI displays service URLs, not transaction links When distributed tracing is enabled for an application, the transaction trace UI will no longer have the transaction name and link for the called service (see screenshot below). This will be replaced with the called service's URL. one.newrelic.com > APM > (select an app) > Monitor > Transactions > (select a transaction trace): This shows the transaction trace UI before distributed tracing is enabled, with a link to the associated transaction. With distributed tracing enabled, it will display the service's URL. If you wanted to get more detail about trace activity, you would go to the Distributed tracing UI page and examine that trace. Cross-application tracing will be disabled Enabling distributed tracing will disable the cross application tracing feature. Distributed tracing is an improved version of cross-application tracing and only one can be enabled at a time. Impacts related to mobile monitoring Distributed tracing is not yet available for mobile monitoring. APM-related impacts include: When distributed tracing is enabled for an APM-monitored entity, service maps will not show applications monitored by mobile. The App server drill-down feature of the Mobile HTTP requests UI page is not available. Plan your rollout If you're enabling distributed tracing for a large, distributed system, here are some tips: If you are a current APM user, see Impact to APM features. Determine the requests that are the most important for your business, or the most likely to require analysis and troubleshooting, and enable distributed tracing for those services. Enable tracing for services at roughly the same time so you can more easily gauge how complete your end-to-end traces are. When you look at traces in the distributed tracing UI, you'll see spans in the trace for external calls to other services. Then, you can enable distributed tracing for any of those services you want. If a service is fairly standalone and not often used in context with other services, you may not want to enable distributed tracing for it. Here's a visual representation of such a phased roll-out: If you are using APM for a large, monolithic service, there may be many sub-process spans per trace and APM limits may result in fewer traces than expected. You can solve this by using APM agent instrumentation to disable the reporting of unimportant data. Distributed tracing works by propagating header information from service to service in a request path. Some services may communicate through a proxy or other intermediary service that does not automatically propagate the header. In that case, you will need to configure that proxy so that it allows the newrelic header value to be propagated from source to destination. Enable distributed tracing If you are aware of the impact to APM features and have thought about your rollout, you are ready to set up distributed tracing. See Overview: Enable distributed tracing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 217.86026,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Distributed</em> <em>tracing</em>: Planning guide",
        "sections": "<em>Distributed</em> <em>tracing</em>: Planning guide",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "): This shows the transaction <em>trace</em> UI before <em>distributed</em> <em>tracing</em> is enabled, with a link to the associated transaction. With <em>distributed</em> <em>tracing</em> enabled, it will display the service&#x27;s URL. If you wanted to <em>get</em> more detail about <em>trace</em> activity, you would go to the <em>Distributed</em> <em>tracing</em> UI page and examine"
      },
      "id": "6072a60564441f3a629d8535"
    }
  ],
  "/docs/distributed-tracing/enable-configure/integrations-enable-distributed-tracing": [
    {
      "sections": [
        "Language agents and distributed tracing",
        "Tip",
        "Quick start for standard distributed tracing (recommended):",
        "Step 1. Identify services",
        "Step 2. Instrument each service with an APM agent",
        "Step 3. View traces",
        "View traces that include a specific service",
        "View traces across accounts",
        "Set up Infinite Tracing (advanced option)",
        "Step 1. Complete the instrumentation for standard distributed tracing in the quick start above",
        "Step 2. Set up the trace observer",
        "Step 3: Configure the agent for Infinite Tracing",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Step 4. (Optional) Customize Infinite Tracing",
        "Options for older APM agents",
        "Compatibility guide",
        "Important",
        "Configure standard distributed tracing for your older agents",
        "Manual instrumentation (If automatic instrumentation doesn't work)",
        "Instrument the calling service",
        "Instrument the called service"
      ],
      "title": "Language agents and distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "b87eacf981bfae09990c95604ba3b7fc19741a40",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/language-agents-enable-distributed-tracing/",
      "published_at": "2021-05-05T19:59:35Z",
      "updated_at": "2021-04-22T06:39:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic has APM language agents for C, Go, Java, Node.js, .NET, PHP, Python, and Ruby. Each of these offers several ways to leverage the power of distributed tracing: Quick start for standard distributed tracing (recommended): A fast way to get started Infinite Tracing: An advanced alternative to standard distributed tracing Older APM agents: Tracing options if you have older APM agents Manual instrumentation: Tips if automatic instrumentation doesn't work Tip If you want to get more background before getting started, check out these topics: How span sampling works explains distributed tracing options. Impacts to APM tells you what to expect if you are a current APM user but haven't set up distributed tracing. Quick start for standard distributed tracing (recommended): This is the best approach to set up standard distributed tracing if you haven't installed any New Relic APM agents for your services yet, or if you want to instrument additional services. Tip You'll need a New Relic account to set up distributed tracing. If you don't already have one, you can quickly create a free account. Step 1. Identify services Figure out which services you want to instrument so they each send trace data to New Relic. Step 2. Instrument each service with an APM agent We have installation assistants for a variety of languages to help you instrument each service. You should run the installation assistant for each service you want to instrument to ensure that each installation has a unique application name. To start the assistant, click the link for your language: New Relic APM: C New Relic APM: Golang New Relic APM: Java New Relic APM: .NET New Relic APM: Node.js New Relic APM: PHP New Relic APM: Python New Relic APM: Ruby Tip This quick-start approach with the installation assistant automatically enables distributed tracing for each service you run it on, but if you already have a New Relic APM agent that you want to participate in distributed tracing, you'll need to manually enable distributed tracing. See Options for older APM agents. Step 3. View traces After you instrument each of your services with APM agents, generate some traffic in your application so we can capture some traces. Here are two ways to view your traces in the UI: View traces that include a specific service Here's one way you can see traces for a particular service: Go to one.newrelic.com. Click APM in the top menu bar. Click your service. In the left navigation's Monitor section, click Distributed tracing. If you don't see the traces you want, you can filter by the trace.id. View traces across accounts This option allows you to search all traces across all New Relic accounts in your organization that you have access to. Go to one.newrelic.com. Click Browse data in the top menu bar, and then click Traces. Select your entity in the left pane. If you don't see the traces you want, you can filter by the trace.id. For more help finding your traces in the UI: Understand and use the distributed tracing UI Query distributed trace data Set up Infinite Tracing (advanced option) Standard distributed tracing for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up Infinite Tracing. This alternative to standard distributed tracing is available for all APM language agents except C SDK. Tip To learn more about this feature, see Infinite Tracing. Before beginning, first ensure you meet the requirements. Step 1. Complete the instrumentation for standard distributed tracing in the quick start above The Infinite Tracing setup builds on the instrumentation step from the Quick start for standard distributed tracing. Step 2. Set up the trace observer The trace observer is a New Relic AWS-based service that collects and analyzes all your traces. Follow the instructions in Set up trace observer. When you're done, return here with your trace observer information and continue with the next step to configure the agent. Step 3: Configure the agent for Infinite Tracing Infinite Tracing configuration settings include the standard distributed tracing plus information about the trace observer. Find the settings for your language agent below: C SDK Infinite tracing is not available for C SDK. Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Infinite Tracing Configuration options: newrelic.Config structure: app, err := newrelic.NewApplication( newrelic.ConfigAppName(YOUR_APP_NAME), newrelic.ConfigLicense(YOUR_LICENSE_KEY), func(cfg *newrelic.Config) { cfg.DistributedTracer.Enabled = true cfg.InfiniteTracing.TraceObserver.Host = YOUR_TRACE_OBSERVER_HOST }, ) Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=YOUR_TRACE_OBSERVER_HOST Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> <infiniteTracing> <trace_observer host=\"YOUR_TRACE_OBSERVER_HOST\" /> </infiniteTracing> </configuration> Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } infinite_tracing: { trace_observer: { host: 'YOUR_TRACE_OBSERVER_HOST' } } Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true newrelic.span_events_enabled = true newrelic.infinite_tracing.trace_observer.host= \"YOUR_TRACE_OBSERVER_HOST\" Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Infinite Tracing Pull down the libraries with this installation command, and then set up the configuration file or environment variables: pip install newrelic[infinite-tracing] Copy Configuration options: Configuration file (newrelic.ini): distributed_tracing.enabled = true infinite_tracing.trace_observer_host= YOUR_TRACE_OBSERVER_HOST Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: 'YOUR_TRACE_OBSERVER_HOST' Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Step 4. (Optional) Customize Infinite Tracing After you add the agent configuration settings, you should start seeing data in the New Relic UI. After you spend some time analyzing your data, you may want to adjust some of the features of Infinite Tracing: Configure trace observer monitoring Configure span attribute trace filter Configure random trace filter Options for older APM agents If you have older APM agents, use this section to figure out if the distributed tracing features you want are supported. Following the compatibility information is a section showing the basic configuration settings to turn on standard distributed tracing. If your older agent supports Infinite Tracing and you want to set it up, see the steps above. Compatibility guide Find your language agents below to confirm if you can use your existing agents with distributed tracing: C SDK Install (compile) or update to the required C SDK version. For best results, update to the latest C SDK version. Option C SDK version Standard distributed tracing 1.1.0 or higher (W3C Trace Context not available) Infinite Tracing Not available Go Install or update to the required Go agent version. For best results, update to the latest Go agent version. Option Go agent version Standard distributed tracing 2.1.0 or higher With W3C Trace Context: 3.1.0 or higher Infinite Tracing v3.5.0 (includes W3C Trace Context) Supported environments: Go 1.9 or higher Java Install or update to the required Java agent version. For best results, update to the latest Java agent version. Important Your JVM's networkaddress.cache.ttl security setting must not be set to forever (i.e. -1). For more information about this networking property, please visit the Oracle Network Properties docs. Type Java agent version Standard distributed tracing 4.3.0 or higher With W3C Trace Context: 5.10 or higher Infinite Tracing 5.12.1 or higher (includes W3C Trace Context) Supported environments: Java 8: Update 252 or higher All versions of Java 9 or higher Tip For special considerations, see Infinite Tracing: Configuring SSL for Java 7 and 8. .NET Install or update to the required .NET agent version. For best results, update to the latest .NET agent version. Option .NET agent version Standard distributed tracing 8.6.45.0 or higher With W3C Trace Context: 8.27.139.0 or higher Infinite Tracing 8.30.0 (includes W3C Trace Context) Supported environments: .NET Framework 4.5 or higher .NET Core 2.0 or higher Node.js Install or update to the required Node.js agent version. For best results, update to the latest Node.js agent version. Option Node.js agent version Standard distributed tracing 4.7.0 or higher With W3C Trace Context: 6.4 or higher Infinite Tracing 7.3.0 (includes W3C Trace Context) Supported environments: Node version 10.10.0 or higher PHP Install or update to the required PHP agent version. For best results, update to the latest PHP agent version. Option PHP agent version Standard distributed tracing 8.4 or higher With W3C Trace Context: 9.8 or higher Infinite Tracing 9.12.0.268 or higher Python Install or update to the required Python agent version. For best results, update to the latest Python agent version. Option Python agent version Standard distributed tracing 4.2.0.100 or higher With W3C Trace Context: 5.6 or higher Infinite Tracing 5.12.0.140 (includes W3C Trace Context) Supported environments: CPython only (pypy is unsupported) Ruby Install or update to the required Ruby agent version. For best results, update to the latest Ruby agent version. Also install the additional Ruby agent gem for Infinite Tracing. Option Ruby agent version Standard distributed tracing newrelic_rpm 5.3.0.346 or higher With W3C Trace Context: newrelic_rpm 6.9 or higher Infinite Tracing newrelic_rpm 6.11.0.365 or higher (includes W3C Trace Context) newrelic-infinite_tracing 6.11.0.375 or higher Configure standard distributed tracing for your older agents Distributed tracing is enabled through configuration settings. Review the following agent-specific sections. For general help with agent configurations, see Configure the agent. Important Server-side configuration is not available for Infinite Tracing. C SDK Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your C applications. Type Required configuration Standard distributed tracing Configuration options: newrelic_app_config_t structure: newrelic_app_config_t* config; config = newrelic_create_app_config(app_name, license_key); config->distributed_tracing.enabled = true; Copy Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Standard distributed tracing Configuration options: ConfigOption structure: newrelic.NewApplication( newrelic.ConfigAppName(\"Example App\"), newrelic.ConfigLicense(os.Getenv(\"NEW_RELIC_LICENSE_KEY\")), newrelic.ConfigDistributedTracerEnabled(true), ) Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> </configuration> Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Standard distributed tracing Configuration file (newrelic.ini): distributed_tracing.enabled = true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Tip If you need help with proxy configuration, see Proxy support. Manual instrumentation (If automatic instrumentation doesn't work) Recommendation: Before performing any custom instrumentation, read: How distributed tracing works Troubleshoot missing data If a service is not passing the trace header to other services, you can use the distributed tracing payload APIs to instrument the calling service and the called service. The calling service uses an API call to generate a payload, which is accepted by the called service. Instrument the calling service To instrument the calling service: Ensure the version of the APM agent that monitors the calling service supports distributed tracing. Invoke the agent API call for generating a distributed trace payload: C SDK | Go | Java | .NET | Node.js | PHP | Python | Ruby. Important To maintain proper ordering of spans in a trace, ensure you generate the payload in the context of the span that sends it. Add that payload to the call made to the destination service (for example, in a header). (Optional) Identify the call as an external call: C SDK Go Java .NET: n/a Node.js PHP: n/a Python Ruby Instrument the called service To instrument the called service: Ensure the version of the APM agent that monitors the called service supports distributed tracing. If the New Relic agent on the called service does not identify a New Relic transaction, use the agent API to declare a transaction: C SDK One way to tell that a transaction is not in progress: when newrelic_create_distributed_trace_payload() is called, a NULL pointer is returned. To solve this problem, follow the procedures to create a transaction with the C SDK. Go One way to tell that a transaction is not in progress: when Transaction.InsertDistributedTraceHeaders(h http.Header) is called, no headers are inserted. To create a transaction, see Instrument Go transactions. Java One way to tell that a transaction is not in progress: when Transaction.insertDistributedTraceHeaders(Headers) is called, no headers are inserted (this API requires agent 6.4.0+). To create a transaction, see Java agent transaction-related APIs. .NET One way to tell that a transaction is not in progress: CreateDistributedTracePayload() returns an empty payload. To create a transaction, see Introduction to .NET custom instrumentation. Node.js One way to tell that a transaction is not in progress: the Node.js agent logs will report an error similar to this: No transaction found when calling Transaction.acceptDistributedTracePayload. Copy Use startWebTransaction to create a web transaction or startBackgroundTransaction to capture a non-web transaction. PHP One way to tell that a transaction is not in progress: newrelic_insert_distributed_trace_headers() returns false. To create a transaction, see newrelic_start_transaction. Python To tell that a transaction is not in progress: when transaction = current_transaction() is run, transaction is None. Or, if result = accept_distributed_trace_payload(payload) is run, then the result is False. Use background_task to report a non-web transaction. For more on Python instrumentation, see Monitor transactions and segments. Ruby If you are using a Rack-based web framework and have enabled New Relic's Rack instrumentation, the Ruby agent will handle starting a transaction for you. For other use cases, see the add_transaction_tracer API method. Extract the payload from the call that you received (for example, in a header). Invoke the call for accepting the payload: C SDK | Go | Java | .NET | PHP | Node.js | Python | Ruby.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 312.04312,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Language agents <em>and</em> <em>distributed</em> <em>tracing</em>",
        "sections": "<em>Configure</em> standard <em>distributed</em> <em>tracing</em> for your older agents",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " you&#x27;re done, return here with your <em>trace</em> observer information and continue with the next step to <em>configure</em> the agent. Step 3: <em>Configure</em> the agent for Infinite <em>Tracing</em> Infinite <em>Tracing</em> configuration settings include the standard <em>distributed</em> <em>tracing</em> plus information about the <em>trace</em> observer. Find"
      },
      "id": "6072a66564441fb28e9d8595"
    },
    {
      "sections": [
        "Overview: Set up distributed tracing",
        "New Relic integrations",
        "New Relic integrations for third-party telemetry tools",
        "Set up your own solution with our Trace API",
        "Tip"
      ],
      "title": "Overview: Set up distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "cd81f363a9ee07640029b514cafe1f84ac04ef99",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/overview-enable-distributed-tracing/",
      "published_at": "2021-05-05T19:59:35Z",
      "updated_at": "2021-04-11T07:33:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We offer a variety of ways to capture distributed tracing data, including: New Relic integrations (including APM, AWS Lambda, browser, and mobile monitoring) New Relic integrations for third-party telemetry tools New Relic Trace API New Relic integrations We offer a range of agents to capture trace data from your applications: Product Description Language agents See telemetry data from applications instrumented with our language-specific agents: C, Go, Java, .NET, Node.js, PHP, Python, and Ruby. Monitoring for AWS Lambda See Lambda function activity, and examine the functions in your traces. Browser See users' browser-side traces. Mobile See users' mobile traces. New Relic integrations for third-party telemetry tools If you are collecting data with these telemetry tools, you can send your data to New Relic: OpenTelemetry Istio Kamon OpenCensus AWS X-Ray Set up your own solution with our Trace API Send data from your telemetry tool directly to New Relic without using an integration that exports the data. Tip Note that this may require more manual configuration than using an integration. Data format Description Trace API: New Relic format Convert your trace data to the New Relic format and send it to New Relic for viewing. Trace API: Data in Zipkin format Send your Zipkin trace data to New Relic for viewing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 260.17322,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview: Set up <em>distributed</em> <em>tracing</em>",
        "sections": "Overview: Set up <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "We offer a variety of ways to capture <em>distributed</em> <em>tracing</em> data, including: New Relic integrations (including APM, AWS Lambda, browser, and mobile monitoring) New Relic integrations for third-party telemetry tools New Relic <em>Trace</em> API New Relic integrations We offer a range of agents to capture <em>trace</em>"
      },
      "id": "6072a666196a67a2bf64a758"
    },
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.32883,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "<em>Enable</em> Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Tracing</em> feature. For an overview of all <em>distributed</em> <em>tracing</em> options, see <em>Enable</em> <em>distributed</em> <em>tracing</em>. Tip To use Infinite <em>Tracing</em> and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free"
      },
      "id": "6072a6a4196a67faa964a788"
    }
  ],
  "/docs/distributed-tracing/enable-configure/language-agents-enable-distributed-tracing": [
    {
      "sections": [
        "Enable distributed tracing for our telemetry tool integrations",
        "Sampling considerations",
        "Set up integrations"
      ],
      "title": "Enable distributed tracing for our telemetry tool integrations",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "ca05c9c79d80af7bc4f16230459e9811a23a94b6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/integrations-enable-distributed-tracing/",
      "published_at": "2021-05-05T14:59:53Z",
      "updated_at": "2021-04-15T22:20:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you use the telemetry tools Istio, Kamon, OpenCensus, OpenTelemetry, or AWS X-Ray, you can get that data into New Relic with our telemetry integrations. Sampling considerations Because distributed systems can generate a lot of trace data, telemetry tools rely on data sampling (filtering). When you install a telemetry integration that reports trace data, you'll have an option to enable Infinite Tracing. Choosing Infinite Tracing has implications for how you configure sampling in your telemetry tool: Standard installation without Infinite Tracing: A standard installation assumes you want your telemetry tool to sample trace data before it's sent to us. (If your trace data exceeds our Trace API limits, we may also do additional sampling.) Install with Infinite Tracing: If you choose Infinite Tracing (read requirements), we assume your telemetry tool's sampling is set to 100%, so that all of that tool's trace data is sent to us. The trace observer selects the most important and actionable traces using tail-based sampling, and then that data is ingested via our Trace API. Set up integrations To set up your telemetry tool for sending distributed traces to New Relic, follow the instructions for your tool: OpenTelemetry Istio Kamon OpenCensus AWS X-Ray",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.17007,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Enable</em> <em>distributed</em> <em>tracing</em> for our telemetry tool integrations",
        "sections": "<em>Enable</em> <em>distributed</em> <em>tracing</em> for our telemetry tool integrations",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " you install a telemetry integration that reports <em>trace</em> data, you&#x27;ll have an option to <em>enable</em> Infinite <em>Tracing</em>. Choosing Infinite <em>Tracing</em> has implications for how you <em>configure</em> sampling in your telemetry tool: Standard installation without Infinite <em>Tracing</em>: A standard installation assumes you want"
      },
      "id": "6072a66664441f271c9d8557"
    },
    {
      "sections": [
        "Overview: Set up distributed tracing",
        "New Relic integrations",
        "New Relic integrations for third-party telemetry tools",
        "Set up your own solution with our Trace API",
        "Tip"
      ],
      "title": "Overview: Set up distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "cd81f363a9ee07640029b514cafe1f84ac04ef99",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/overview-enable-distributed-tracing/",
      "published_at": "2021-05-05T19:59:35Z",
      "updated_at": "2021-04-11T07:33:58Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We offer a variety of ways to capture distributed tracing data, including: New Relic integrations (including APM, AWS Lambda, browser, and mobile monitoring) New Relic integrations for third-party telemetry tools New Relic Trace API New Relic integrations We offer a range of agents to capture trace data from your applications: Product Description Language agents See telemetry data from applications instrumented with our language-specific agents: C, Go, Java, .NET, Node.js, PHP, Python, and Ruby. Monitoring for AWS Lambda See Lambda function activity, and examine the functions in your traces. Browser See users' browser-side traces. Mobile See users' mobile traces. New Relic integrations for third-party telemetry tools If you are collecting data with these telemetry tools, you can send your data to New Relic: OpenTelemetry Istio Kamon OpenCensus AWS X-Ray Set up your own solution with our Trace API Send data from your telemetry tool directly to New Relic without using an integration that exports the data. Tip Note that this may require more manual configuration than using an integration. Data format Description Trace API: New Relic format Convert your trace data to the New Relic format and send it to New Relic for viewing. Trace API: Data in Zipkin format Send your Zipkin trace data to New Relic for viewing.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 260.1732,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Overview: Set up <em>distributed</em> <em>tracing</em>",
        "sections": "Overview: Set up <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "We offer a variety of ways to capture <em>distributed</em> <em>tracing</em> data, including: New Relic integrations (including APM, AWS Lambda, browser, and mobile monitoring) New Relic integrations for third-party telemetry tools New Relic <em>Trace</em> API New Relic integrations We offer a range of agents to capture <em>trace</em>"
      },
      "id": "6072a666196a67a2bf64a758"
    },
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.32867,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "<em>Enable</em> Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Tracing</em> feature. For an overview of all <em>distributed</em> <em>tracing</em> options, see <em>Enable</em> <em>distributed</em> <em>tracing</em>. Tip To use Infinite <em>Tracing</em> and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free"
      },
      "id": "6072a6a4196a67faa964a788"
    }
  ],
  "/docs/distributed-tracing/enable-configure/overview-enable-distributed-tracing": [
    {
      "sections": [
        "Language agents and distributed tracing",
        "Tip",
        "Quick start for standard distributed tracing (recommended):",
        "Step 1. Identify services",
        "Step 2. Instrument each service with an APM agent",
        "Step 3. View traces",
        "View traces that include a specific service",
        "View traces across accounts",
        "Set up Infinite Tracing (advanced option)",
        "Step 1. Complete the instrumentation for standard distributed tracing in the quick start above",
        "Step 2. Set up the trace observer",
        "Step 3: Configure the agent for Infinite Tracing",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Step 4. (Optional) Customize Infinite Tracing",
        "Options for older APM agents",
        "Compatibility guide",
        "Important",
        "Configure standard distributed tracing for your older agents",
        "Manual instrumentation (If automatic instrumentation doesn't work)",
        "Instrument the calling service",
        "Instrument the called service"
      ],
      "title": "Language agents and distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "b87eacf981bfae09990c95604ba3b7fc19741a40",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/language-agents-enable-distributed-tracing/",
      "published_at": "2021-05-05T19:59:35Z",
      "updated_at": "2021-04-22T06:39:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic has APM language agents for C, Go, Java, Node.js, .NET, PHP, Python, and Ruby. Each of these offers several ways to leverage the power of distributed tracing: Quick start for standard distributed tracing (recommended): A fast way to get started Infinite Tracing: An advanced alternative to standard distributed tracing Older APM agents: Tracing options if you have older APM agents Manual instrumentation: Tips if automatic instrumentation doesn't work Tip If you want to get more background before getting started, check out these topics: How span sampling works explains distributed tracing options. Impacts to APM tells you what to expect if you are a current APM user but haven't set up distributed tracing. Quick start for standard distributed tracing (recommended): This is the best approach to set up standard distributed tracing if you haven't installed any New Relic APM agents for your services yet, or if you want to instrument additional services. Tip You'll need a New Relic account to set up distributed tracing. If you don't already have one, you can quickly create a free account. Step 1. Identify services Figure out which services you want to instrument so they each send trace data to New Relic. Step 2. Instrument each service with an APM agent We have installation assistants for a variety of languages to help you instrument each service. You should run the installation assistant for each service you want to instrument to ensure that each installation has a unique application name. To start the assistant, click the link for your language: New Relic APM: C New Relic APM: Golang New Relic APM: Java New Relic APM: .NET New Relic APM: Node.js New Relic APM: PHP New Relic APM: Python New Relic APM: Ruby Tip This quick-start approach with the installation assistant automatically enables distributed tracing for each service you run it on, but if you already have a New Relic APM agent that you want to participate in distributed tracing, you'll need to manually enable distributed tracing. See Options for older APM agents. Step 3. View traces After you instrument each of your services with APM agents, generate some traffic in your application so we can capture some traces. Here are two ways to view your traces in the UI: View traces that include a specific service Here's one way you can see traces for a particular service: Go to one.newrelic.com. Click APM in the top menu bar. Click your service. In the left navigation's Monitor section, click Distributed tracing. If you don't see the traces you want, you can filter by the trace.id. View traces across accounts This option allows you to search all traces across all New Relic accounts in your organization that you have access to. Go to one.newrelic.com. Click Browse data in the top menu bar, and then click Traces. Select your entity in the left pane. If you don't see the traces you want, you can filter by the trace.id. For more help finding your traces in the UI: Understand and use the distributed tracing UI Query distributed trace data Set up Infinite Tracing (advanced option) Standard distributed tracing for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up Infinite Tracing. This alternative to standard distributed tracing is available for all APM language agents except C SDK. Tip To learn more about this feature, see Infinite Tracing. Before beginning, first ensure you meet the requirements. Step 1. Complete the instrumentation for standard distributed tracing in the quick start above The Infinite Tracing setup builds on the instrumentation step from the Quick start for standard distributed tracing. Step 2. Set up the trace observer The trace observer is a New Relic AWS-based service that collects and analyzes all your traces. Follow the instructions in Set up trace observer. When you're done, return here with your trace observer information and continue with the next step to configure the agent. Step 3: Configure the agent for Infinite Tracing Infinite Tracing configuration settings include the standard distributed tracing plus information about the trace observer. Find the settings for your language agent below: C SDK Infinite tracing is not available for C SDK. Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Infinite Tracing Configuration options: newrelic.Config structure: app, err := newrelic.NewApplication( newrelic.ConfigAppName(YOUR_APP_NAME), newrelic.ConfigLicense(YOUR_LICENSE_KEY), func(cfg *newrelic.Config) { cfg.DistributedTracer.Enabled = true cfg.InfiniteTracing.TraceObserver.Host = YOUR_TRACE_OBSERVER_HOST }, ) Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=YOUR_TRACE_OBSERVER_HOST Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> <infiniteTracing> <trace_observer host=\"YOUR_TRACE_OBSERVER_HOST\" /> </infiniteTracing> </configuration> Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } infinite_tracing: { trace_observer: { host: 'YOUR_TRACE_OBSERVER_HOST' } } Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true newrelic.span_events_enabled = true newrelic.infinite_tracing.trace_observer.host= \"YOUR_TRACE_OBSERVER_HOST\" Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Infinite Tracing Pull down the libraries with this installation command, and then set up the configuration file or environment variables: pip install newrelic[infinite-tracing] Copy Configuration options: Configuration file (newrelic.ini): distributed_tracing.enabled = true infinite_tracing.trace_observer_host= YOUR_TRACE_OBSERVER_HOST Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: 'YOUR_TRACE_OBSERVER_HOST' Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Step 4. (Optional) Customize Infinite Tracing After you add the agent configuration settings, you should start seeing data in the New Relic UI. After you spend some time analyzing your data, you may want to adjust some of the features of Infinite Tracing: Configure trace observer monitoring Configure span attribute trace filter Configure random trace filter Options for older APM agents If you have older APM agents, use this section to figure out if the distributed tracing features you want are supported. Following the compatibility information is a section showing the basic configuration settings to turn on standard distributed tracing. If your older agent supports Infinite Tracing and you want to set it up, see the steps above. Compatibility guide Find your language agents below to confirm if you can use your existing agents with distributed tracing: C SDK Install (compile) or update to the required C SDK version. For best results, update to the latest C SDK version. Option C SDK version Standard distributed tracing 1.1.0 or higher (W3C Trace Context not available) Infinite Tracing Not available Go Install or update to the required Go agent version. For best results, update to the latest Go agent version. Option Go agent version Standard distributed tracing 2.1.0 or higher With W3C Trace Context: 3.1.0 or higher Infinite Tracing v3.5.0 (includes W3C Trace Context) Supported environments: Go 1.9 or higher Java Install or update to the required Java agent version. For best results, update to the latest Java agent version. Important Your JVM's networkaddress.cache.ttl security setting must not be set to forever (i.e. -1). For more information about this networking property, please visit the Oracle Network Properties docs. Type Java agent version Standard distributed tracing 4.3.0 or higher With W3C Trace Context: 5.10 or higher Infinite Tracing 5.12.1 or higher (includes W3C Trace Context) Supported environments: Java 8: Update 252 or higher All versions of Java 9 or higher Tip For special considerations, see Infinite Tracing: Configuring SSL for Java 7 and 8. .NET Install or update to the required .NET agent version. For best results, update to the latest .NET agent version. Option .NET agent version Standard distributed tracing 8.6.45.0 or higher With W3C Trace Context: 8.27.139.0 or higher Infinite Tracing 8.30.0 (includes W3C Trace Context) Supported environments: .NET Framework 4.5 or higher .NET Core 2.0 or higher Node.js Install or update to the required Node.js agent version. For best results, update to the latest Node.js agent version. Option Node.js agent version Standard distributed tracing 4.7.0 or higher With W3C Trace Context: 6.4 or higher Infinite Tracing 7.3.0 (includes W3C Trace Context) Supported environments: Node version 10.10.0 or higher PHP Install or update to the required PHP agent version. For best results, update to the latest PHP agent version. Option PHP agent version Standard distributed tracing 8.4 or higher With W3C Trace Context: 9.8 or higher Infinite Tracing 9.12.0.268 or higher Python Install or update to the required Python agent version. For best results, update to the latest Python agent version. Option Python agent version Standard distributed tracing 4.2.0.100 or higher With W3C Trace Context: 5.6 or higher Infinite Tracing 5.12.0.140 (includes W3C Trace Context) Supported environments: CPython only (pypy is unsupported) Ruby Install or update to the required Ruby agent version. For best results, update to the latest Ruby agent version. Also install the additional Ruby agent gem for Infinite Tracing. Option Ruby agent version Standard distributed tracing newrelic_rpm 5.3.0.346 or higher With W3C Trace Context: newrelic_rpm 6.9 or higher Infinite Tracing newrelic_rpm 6.11.0.365 or higher (includes W3C Trace Context) newrelic-infinite_tracing 6.11.0.375 or higher Configure standard distributed tracing for your older agents Distributed tracing is enabled through configuration settings. Review the following agent-specific sections. For general help with agent configurations, see Configure the agent. Important Server-side configuration is not available for Infinite Tracing. C SDK Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your C applications. Type Required configuration Standard distributed tracing Configuration options: newrelic_app_config_t structure: newrelic_app_config_t* config; config = newrelic_create_app_config(app_name, license_key); config->distributed_tracing.enabled = true; Copy Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Standard distributed tracing Configuration options: ConfigOption structure: newrelic.NewApplication( newrelic.ConfigAppName(\"Example App\"), newrelic.ConfigLicense(os.Getenv(\"NEW_RELIC_LICENSE_KEY\")), newrelic.ConfigDistributedTracerEnabled(true), ) Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> </configuration> Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Standard distributed tracing Configuration file (newrelic.ini): distributed_tracing.enabled = true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Tip If you need help with proxy configuration, see Proxy support. Manual instrumentation (If automatic instrumentation doesn't work) Recommendation: Before performing any custom instrumentation, read: How distributed tracing works Troubleshoot missing data If a service is not passing the trace header to other services, you can use the distributed tracing payload APIs to instrument the calling service and the called service. The calling service uses an API call to generate a payload, which is accepted by the called service. Instrument the calling service To instrument the calling service: Ensure the version of the APM agent that monitors the calling service supports distributed tracing. Invoke the agent API call for generating a distributed trace payload: C SDK | Go | Java | .NET | Node.js | PHP | Python | Ruby. Important To maintain proper ordering of spans in a trace, ensure you generate the payload in the context of the span that sends it. Add that payload to the call made to the destination service (for example, in a header). (Optional) Identify the call as an external call: C SDK Go Java .NET: n/a Node.js PHP: n/a Python Ruby Instrument the called service To instrument the called service: Ensure the version of the APM agent that monitors the called service supports distributed tracing. If the New Relic agent on the called service does not identify a New Relic transaction, use the agent API to declare a transaction: C SDK One way to tell that a transaction is not in progress: when newrelic_create_distributed_trace_payload() is called, a NULL pointer is returned. To solve this problem, follow the procedures to create a transaction with the C SDK. Go One way to tell that a transaction is not in progress: when Transaction.InsertDistributedTraceHeaders(h http.Header) is called, no headers are inserted. To create a transaction, see Instrument Go transactions. Java One way to tell that a transaction is not in progress: when Transaction.insertDistributedTraceHeaders(Headers) is called, no headers are inserted (this API requires agent 6.4.0+). To create a transaction, see Java agent transaction-related APIs. .NET One way to tell that a transaction is not in progress: CreateDistributedTracePayload() returns an empty payload. To create a transaction, see Introduction to .NET custom instrumentation. Node.js One way to tell that a transaction is not in progress: the Node.js agent logs will report an error similar to this: No transaction found when calling Transaction.acceptDistributedTracePayload. Copy Use startWebTransaction to create a web transaction or startBackgroundTransaction to capture a non-web transaction. PHP One way to tell that a transaction is not in progress: newrelic_insert_distributed_trace_headers() returns false. To create a transaction, see newrelic_start_transaction. Python To tell that a transaction is not in progress: when transaction = current_transaction() is run, transaction is None. Or, if result = accept_distributed_trace_payload(payload) is run, then the result is False. Use background_task to report a non-web transaction. For more on Python instrumentation, see Monitor transactions and segments. Ruby If you are using a Rack-based web framework and have enabled New Relic's Rack instrumentation, the Ruby agent will handle starting a transaction for you. For other use cases, see the add_transaction_tracer API method. Extract the payload from the call that you received (for example, in a header). Invoke the call for accepting the payload: C SDK | Go | Java | .NET | PHP | Node.js | Python | Ruby.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 312.0431,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Language agents <em>and</em> <em>distributed</em> <em>tracing</em>",
        "sections": "<em>Configure</em> standard <em>distributed</em> <em>tracing</em> for your older agents",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " you&#x27;re done, return here with your <em>trace</em> observer information and continue with the next step to <em>configure</em> the agent. Step 3: <em>Configure</em> the agent for Infinite <em>Tracing</em> Infinite <em>Tracing</em> configuration settings include the standard <em>distributed</em> <em>tracing</em> plus information about the <em>trace</em> observer. Find"
      },
      "id": "6072a66564441fb28e9d8595"
    },
    {
      "sections": [
        "Enable distributed tracing for our telemetry tool integrations",
        "Sampling considerations",
        "Set up integrations"
      ],
      "title": "Enable distributed tracing for our telemetry tool integrations",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "ca05c9c79d80af7bc4f16230459e9811a23a94b6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/integrations-enable-distributed-tracing/",
      "published_at": "2021-05-05T14:59:53Z",
      "updated_at": "2021-04-15T22:20:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you use the telemetry tools Istio, Kamon, OpenCensus, OpenTelemetry, or AWS X-Ray, you can get that data into New Relic with our telemetry integrations. Sampling considerations Because distributed systems can generate a lot of trace data, telemetry tools rely on data sampling (filtering). When you install a telemetry integration that reports trace data, you'll have an option to enable Infinite Tracing. Choosing Infinite Tracing has implications for how you configure sampling in your telemetry tool: Standard installation without Infinite Tracing: A standard installation assumes you want your telemetry tool to sample trace data before it's sent to us. (If your trace data exceeds our Trace API limits, we may also do additional sampling.) Install with Infinite Tracing: If you choose Infinite Tracing (read requirements), we assume your telemetry tool's sampling is set to 100%, so that all of that tool's trace data is sent to us. The trace observer selects the most important and actionable traces using tail-based sampling, and then that data is ingested via our Trace API. Set up integrations To set up your telemetry tool for sending distributed traces to New Relic, follow the instructions for your tool: OpenTelemetry Istio Kamon OpenCensus AWS X-Ray",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 270.17007,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Enable</em> <em>distributed</em> <em>tracing</em> for our telemetry tool integrations",
        "sections": "<em>Enable</em> <em>distributed</em> <em>tracing</em> for our telemetry tool integrations",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " you install a telemetry integration that reports <em>trace</em> data, you&#x27;ll have an option to <em>enable</em> Infinite <em>Tracing</em>. Choosing Infinite <em>Tracing</em> has implications for how you <em>configure</em> sampling in your telemetry tool: Standard installation without Infinite <em>Tracing</em>: A standard installation assumes you want"
      },
      "id": "6072a66664441f271c9d8557"
    },
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.32867,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "<em>Enable</em> Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Tracing</em> feature. For an overview of all <em>distributed</em> <em>tracing</em> options, see <em>Enable</em> <em>distributed</em> <em>tracing</em>. Tip To use Infinite <em>Tracing</em> and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free"
      },
      "id": "6072a6a4196a67faa964a788"
    }
  ],
  "/docs/distributed-tracing/enable-configure/quick-start": [
    {
      "sections": [
        "Set up your development environment",
        "Before you begin",
        "A note on support",
        "Tip",
        "Prepare to build or modify apps",
        "Start building",
        "Contribute to developer.newrelic.com"
      ],
      "title": "Set up your development environment",
      "type": "developer",
      "tags": [
        "developer account",
        "API key",
        "New Relic One CLI"
      ],
      "external_id": "c45638a9cd548d1ffffc9f1c7708f115a92ae04a",
      "image": "",
      "url": "https://developer.newrelic.com/build-apps/set-up-dev-env/",
      "published_at": "2021-05-07T01:43:31Z",
      "updated_at": "2021-05-05T01:51:53Z",
      "document_type": "page",
      "popularity": 1,
      "info": "Prepare to build apps and contribute to this site",
      "body": "If you've decided to build a custom app or modify one of our open source apps, you need a few essential tools: The New Relic One command line interface (CLI) An API key, which you get when you download the CLI Depending on what you want to do with your app, you might have some additional setup and configuration. This guide covers: Downloading the New Relic One CLI to build or modify apps Contribute content to this website Before you begin Before you begin, we recommend first reading about permissions. To start building, you must have: A github account account - While not strictly necessary for building apps, a GitHub account enables you to download and customize our open source apps, and contribute an open source project. A New Relic developer account - if you don't already have one, you can get a free trial account for developing New Relic applications. npm - If you've installed Node.js, then you already have npm, which is used to share, reuse, and update JavaScript code, and is necessary for working with React components that are the framework for New Relic apps and this website. A note on support Building a New Relic One application is the same as building any JavaScript/React application. We offer support to help with our building tools (our CLI and SDK library). However, we don't offer support for basic JavaScript or React coding questions or issues. For common questions and answers about building, see the Explorers Hub page on building on New Relic One. Tip Use the New Relic One VSCode extension or the New Relic VSCode extension pack to build your apps. Prepare to build or modify apps Step 1 of 1 Download the CLI and API key. On the Build New Relic One applications page, complete the Quick start steps. These six Quick start steps get you an API key for use with developing apps, and the New Relic One CLI, for building and deploying apps. At the end of the Quick start, you have a project consisting of the following: A Nerdpack - The package containing all the files required by your application. It contains two types of files that you customize to build your app: Nerdlets, and the launcher. One or more Nerdlet files - A specific UI view or window. A Nerdlet is a React JavaScript package that includes an index.js file, a stylesheet, and a JSON-format config file. It can contain any JS functionality (charts, interactive fields, tooltips, etc.). A launcher file: This is the basis for the launcher, which is used to open your application from New Relic One after you publish your app. Start building Step 1 of 1 If you're ready to code, cd to your Nerdpack and get started. If you want to learn more about building applications, try these step-by-step guides: Build a \"Hello, World!\" application shows how to create a little application, publish it to New Relic One, and share it with others by subscribing accounts to it. Map pageviews by region takes you through the steps to create one of our popular open source apps. You learn to add a custom query to an app and view it in a table, then add that data to a map. Contribute to developer.newrelic.com This site is open source, and we want your input. Create a pull request if you see a mistake you know how to fix. Drop us a GitHub issue if you see some content gaps you want us to work on. Or write up a whole new guide if you have one you'd like to share. Read on to learn how. Step 1 of 3 Fork the developer-website GithHub repo. Forking the repo enables you to work on your own copy of the developer.newrelic.com files, and build the site locally. It also enables us to more easily manage incomimg pull requests. On the developer-website page in GitHub, select the Fork button on the top right of the page, choose the account you want to fork to, and wait a few seconds while the fork is created. Sync regularly to keep your fork up to date with changes and additions to the main branch upstream. Step 2 of 3 Make a feature or documentation request. On any page, select the GitHub button at the top of the page, and then select the kind of change you want, and fill out the GitHub form. Step 3 of 3 Contribute a new guide. Check out our contributors guidelines, which will walk you through the process.",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 621.3511,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Start</em> building",
        "body": " Relic One VSCode extension or the New Relic VSCode extension pack to build your apps. Prepare to build or modify apps Step 1 of 1 Download the CLI and API key. On the Build New Relic One applications page, complete the <em>Quick</em> <em>start</em> steps. These six <em>Quick</em> <em>start</em> steps get you an API key for use"
      },
      "id": "6091fa3ae7b9d2e0595068b1"
    },
    {
      "sections": [
        "New Relic One CLI reference",
        "Installing the New Relic One CLI",
        "Tip",
        "New Relic One CLI Commands",
        "Get started",
        "Configure your CLI preferences",
        "Set up your Nerdpacks",
        "Manage your Nerdpack subscriptions",
        "Install and manage plugins",
        "Manage catalog information"
      ],
      "title": "New Relic One CLI reference",
      "type": "developer",
      "tags": [
        "New Relic One app",
        "nerdpack commands"
      ],
      "external_id": "858339a44ead21c83257778ce60b4c352cd30d3b",
      "image": "https://developer.newrelic.com/static/2c6d337608b38a3312b4fc740afe6167/7272b/developercenter.png",
      "url": "https://developer.newrelic.com/explore-docs/nr1-cli/",
      "published_at": "2021-05-07T01:42:20Z",
      "updated_at": "2021-05-05T01:53:28Z",
      "document_type": "page",
      "popularity": 1,
      "info": "An overview of the CLI to help you build, deploy, and manage New Relic apps.",
      "body": "To build a New Relic One app, you must install the New Relic One CLI. The CLI helps you build, publish, and manage your New Relic app. We provide a variety of tools for building apps, including the New Relic One CLI (command line interface). This page explains how to use CLI commands to: Generate Nerdpack/Nerdlet templates Locally serve Nerdpacks (when developing) Publish and deploy Subscribe to Nerdpacks Add screenshots and metadata to the catalog Installing the New Relic One CLI In New Relic, click Apps and then in the New Relic One catalog area, click the Build your own application launcher and follow the quick start instructions. The quick start automatically generates an API key for the account you select, and gives you the pre-populated commands to create a profile, generate your first \"Hello World\" app, and serve it locally. Tip Use the NR1 VS Code extension to build your apps. New Relic One CLI Commands This table provides descriptions for the New Relic One commands. For more context, including usage and option details, click any individual command or the command category. For details on user permissions, see Permissions. For more on how to serve and publish your application, see our guide on Deploying your New Relic One app. Get started nr1 help Shows all nr1 commands or details about each command. nr1 update Updates to the latest version of the CLI. nr1 create Creates a new component from a template (Nerdpack, Nerdlet, launcher, or catalog). nr1 profiles Manages the profiles you use to run CLI commands. nr1 autocomplete Displays autocomplete installation instructions. nr1 nrql Fetches data using NRQL (New Relic query language). Configure your CLI preferences nr1 config:set Sets a specific configuration value. nr1 config:get Shows a specific configuration. nr1 config:list Lists your configuration choices. nr1 config:delete Removes the value of a specific configuration. Set up your Nerdpacks nr1 nerdpack:build Assembles your Nerdpack into bundles. nr1 nerdpack:clone Clones an open source Nerdpack from our GitHub repository. nr1 nerdpack:serve Serves your Nerdpack for testing and development purposes. nr1 nerdpack:uuid Shows or regenerates the UUID of a Nerdpack. nr1 nerdpack:publish Publishes your Nerdpack to New Relic. nr1 nerdpack:deploy Deploys a Nerdpack version to a specific channel. nr1 nerdpack:undeploy Undeploys a Nerdpack version from a specific channel. nr1 nerdpack:clean Cleans your developtment folders. nr1 nerdpack:validate Validates the contents of your Nerdpack. nr1 nerdpack:info Shows the state of your Nerdpack in the New Relic's registry. Manage your Nerdpack subscriptions nr1 subscription:set Subscribes your account to a Nerdpack and channel. nr1 subscription:list Lists all the Nerdpacks your account is subscribed to. nr1 subscription:unset Unsubscribes your account from a Nerdpack. Install and manage plugins nr1 plugins:install Installs a plugin into the CLI. nr1 plugins:link Links a plugin into the CLI for development. nr1 plugins:update Updates your installed plugins. nr1 plugins:uninstall Removes a plugin from the CLI. Manage catalog information nr1 catalog:info Shows the Nerdpack info stored in the catalog. nr1 catalog:submit Gathers and submits the catalog info on the current folder.",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 589.9952,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Get <em>started</em>",
        "body": " launcher and follow the <em>quick</em> <em>start</em> instructions. The <em>quick</em> <em>start</em> automatically generates an API key for the account you select, and gives you the pre-populated commands to create a profile, generate your first &quot;Hello World&quot; app, and serve it locally. Tip Use the NR1 VS Code extension to build your apps"
      },
      "id": "6091fa9864441feb412f36d4"
    },
    {
      "sections": [
        "Create a \"Hello, World!\" application",
        "Before you begin",
        "Tip",
        "Create a local version of the \"Hello, World!\" application",
        "Publish your application to New Relic",
        "Add details to describe your project",
        "Subscribe accounts to your application",
        "Proxy browser setup",
        "Summary"
      ],
      "title": "Create a \"Hello, World!\" application",
      "type": "developer",
      "tags": [
        "nr1 cli",
        "Nerdpack file structure",
        "NR One Catalog",
        "Subscribe applications"
      ],
      "external_id": "aa427030169067481fb69a3560798265b6b52b7c",
      "image": "https://developer.newrelic.com/static/cb65a35ad6fa52f5245359ecd24158ff/9466d/hello-world-output-local.png",
      "url": "https://developer.newrelic.com/build-apps/build-hello-world-app/",
      "published_at": "2021-05-07T01:40:10Z",
      "updated_at": "2021-05-05T01:50:00Z",
      "document_type": "page",
      "popularity": 1,
      "info": "Build a \"Hello, World!\" app and publish it to New Relic One",
      "body": "Here's how you can quickly build a \"Hello, World!\" application in New Relic One. In these steps, you create a local version of the New Relic One site where you can prototype your application. Then, when you're ready to share the application with others, you can publish it to New Relic One. See the video, which demonstrates the steps in this guide in five minutes. Before you begin To get started, make sure you have accounts in GitHub and New Relic. To develop projects, you need the New Relic One CLI (command line interface). If you haven't already installed it, do the following: Install Node.js. Complete all the steps in the CLI quick start. For additional details about setting up your environment, see Set up your development environment and Enable advanced configurations for your Nerdpack. Tip Use the New Relic One VSCode extension or the New Relic VSCode extension pack to build your apps. Create a local version of the \"Hello, World!\" application The CLI allows you to run a local version of New Relic One. You can develop your application locally before you publish it in New Relic One. If you followed all the steps in the CLI quick start, you now have files under a new directory named after your nerdpack project. Here's how you edit those files to create a \"Hello, World!\" project: Step 1 of 9 Open a code editor and point it to the new directory named after your nerdpack project (for example, my-awesome-nerdpack). Your code editor displays two artifacts: launchers containing the homepage tile nerdlets containing your application code Step 2 of 9 Expand nerdlets in your code editor, and open index.js. Step 3 of 9 Change the default return message to \"Hello, World!\": import React from 'react'; // https://docs.newrelic.com/docs/new-relic-programmable-platform-introduction export default class MyAwesomeNerdpackNerdletNerdlet extends React.Component { render() { return <h1>\"Hello, World!\"</h1>; } } Copy Step 4 of 9 As an optional step, you can add a custom launcher icon using any image file named icon.png. Replace the default icon.png file under launcher by dragging in your new image file: Tip You will need an icon.png in 2 places for it to be representing properly in the New Relic One Catalog, To set a launcher icon place an icon.png file in the the launcher directory. To set the icon for the app details page place an icon in the root directory of the project. Step 5 of 9 To change the name of the launcher to something meaningful, in your code editor under launchers, open nr1.json. Step 6 of 9 Change the value for displayName to anything you want as the launcher label, and save the file: { \"schemaType\": \"LAUNCHER\", \"id\": \"my-awesome-nerdpack-launcher\", \"description\": \"Describe me\", \"displayName\": \"INSERT_YOUR_TILE_LABEL_HERE\", \"rootNerdletId\": \"my-awesome-nerdpack-nerdlet\" } Copy Step 7 of 9 To see your new changes locally, start the Node server with this command in your terminal: bash Copy $ npm start Step 8 of 9 Open a browser and go to https://one.newrelic.com/?nerdpacks=local (this url is also shown in the terminal). Step 9 of 9 When the browser opens, click Apps, and then in the Other apps section, click the new launcher for your application. Here's an example where we inserted a leaf icon: After you click the new launcher, your \"Hello, World!\" appears: Publish your application to New Relic Your colleagues can't see your local application, so when you are ready to share it, publish it to the New Relic One catalog. The catalog is where you can find any pre-existing custom applications, as well as any applications you create in your own organization. Step 1 of 4 Execute the following in your terminal: bash Copy $ nr1 nerdpack:publish Step 2 of 4 Close your local New Relic One development tab, and open New Relic One. Step 3 of 4 Click the Apps launcher. Step 4 of 4 Under New Relic One catalog, click the launcher for your new application. When your new application opens, notice that it doesn't display any helpful descriptive information. The next section shows you how to add descriptive metadata. Add details to describe your project Now that your new application is in the New Relic One catalog, you can add details that help users understand what your application does and how to use it. Step 1 of 5 Go to your project in the terminal and execute the following: bash Copy $ nr1 create Step 2 of 5 Select catalog, which creates a stub in your project under the catalog directory. Here's how the results might look in your code editor: Step 3 of 5 In the catalog directory of your project, add screenshots or various types of metadata to describe your project. For details about what you can add, see Add catalog metadata and screenshots. Step 4 of 5 After you add the screenshots and descriptions you want, execute the following to save your metadata to the catalog: bash Copy $ nr1 catalog:submit Step 5 of 5 Return to the catalog and refresh the page to see your new screenshots and metadata describing your project. Subscribe accounts to your application To make sure other users see your application in the catalog, you need to subscribe accounts to the application. Any user with the NerdPack manager or admin role can subscribe to an application from accounts that they have permission to manage. Step 1 of 3 If you're not already displaying your application's description page in the browser, click the launcher for the application in the catalog under Your company applications. Step 2 of 3 On your application's description page, click Add this app. Step 3 of 3 Select the accounts you want to subscribe to the application, and then click Update accounts to save your selections. When you return to the Apps page, you'll see the launcher for your new application. Proxy browser setup Generally, you don't need to know how your application code is loaded in New Relic One. But understanding how this works can help you if you encounter issues (for example, with a proxy or browser setup). When developing, your code is served from your local laptop by using an nr-local.net subdomain. This domain points to 127.0.0.1, and will use a variety of ports to connect, including 9973. In order to load third-party code into the platform, (both in development and production), New Relic One uses an <IFRAME>, as well as a separate domain (a subdomain of nr-ext.net) to perform the load, relying on same-domain policy to sandbox the code. For this reason, your network administrator may need to enable access to: Any subdomain of nr-local.net, or, at the very least, to any domain of the shape of <UserId>.nr-local.net, where UserId is a 32-hexadecimal character identifier that is unique to the user logged into the platform (you can use [0-9a-f]{32} to discriminate it). Any subdomain of nr-ext.net, or, at the very least, to any domain in the shape of <NerdpackUuid>.g <NerdpackGid>.nr-ext.net, where NerdpackUuid is the UUID assigned to your package, and NerdpackGid is zero, or a positive integer (you can use 0|[1-9]\\d+ to discriminate it). Summary Now that you've completed the steps in this example, you learned the basic steps to: Create a local application. Publish the application to the New Relic One catalog so you can share it with your colleagues. Add details to the project in the catalog so users understand how to use it. Subscribe accounts to your application so other users can use it.",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 450.6128,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " Node.js. Complete all the steps in the CLI <em>quick</em> <em>start</em>. For additional details about setting up your environment, see Set up your development environment and Enable advanced configurations for your Nerdpack. Tip Use the New Relic One VSCode extension or the New Relic VSCode extension pack to build your"
      },
      "id": "6091f9c864441f70d82f36c4"
    }
  ],
  "/docs/distributed-tracing/index": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1003.24133,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Distributed</em> <em>tracing</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, Infinite <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about Infinite <em>Tracing</em>: Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Introduction to distributed tracing",
        "Why it matters",
        "Instrumentation: The key to distributed tracing",
        "What you can see in the New Relic UI",
        "Next steps"
      ],
      "title": "Introduction to distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Get started"
      ],
      "external_id": "ac173988a6503674b4411c9c2efe6713912c37f2",
      "image": "https://docs.newrelic.com/static/8a63e4d34a1272fa33ad7b3564db6bf8/b5bda/span-diagram.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/concepts/introduction-distributed-tracing/",
      "published_at": "2021-05-04T18:32:24Z",
      "updated_at": "2021-04-11T07:38:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Distributed tracing tracks and observes service requests as they flow through distributed systems. With distributed tracing data, you can quickly pinpoint failures or performance issues and fix them. Distributed tracing systems collect data as the requests go from one service to another, recording each segment of the journey as a span. These spans contain important details about each segment of the request and are combined into one trace. The completed trace gives you a picture of the entire request. Here is an example of three spans captured from an HTTP request. These spans are sent to New Relic where they are combined into one distributed trace. Why it matters A request might pass through various microservices to reach completion. The microservices or functions could be located in multiple containers, serverless environments, virtual machines, different cloud providers, on-premises, or any combination of these. For example, let's say that you're in a situation where a slow-running request affects the experience of a set of customers: The request is distributed across multiple microservices and serverless functions. Several different teams own and monitor the various services that are involved in the request. None of the teams have reported any performance issues with their microservices. Without a way to view the performance of the entire request across the different services, it’s nearly impossible to pinpoint where and why the high latency is occurring and which team should address the issue. Instrumentation: The key to distributed tracing Distributed tracing starts with the instrumentation of your services to enable data collection and correlation across the entire distributed system. Instrumention means either manually adding code to services or installing agents that automatically track trace data. Many of our New Relic solutions automatically instrument your services for a large number of programming languages and frameworks. You can also use open source tools and open instrumentation standards to instrument your environment. OpenTelemetry, part of the Cloud Native Computing Foundation (CNCF), is becoming the one standard for open source instrumentation and telemetry collection. What you can see in the New Relic UI After the data is collected, you can visualize it to see service dependencies, performance, and any anomalous events such as errors or unusual latency. Here are some examples of what you can do with your data: What you can do Description Detect anomalous spans Spans that are slow in comparison to typical behavior are marked as anomalous, with charts comparing them to typical performance. See your errors and logs Frontend and backend errors appear right in the context of your traces. Everything you need to troubleshoot is in one place. Filter results You can filter charts using many data points, so you can analyze trace data in different ways. Customize queries and dashboards You can create custom queries of your trace data and create custom data dashboards. See data across accounts See a global view of traces from across all your accounts and applications in New Relic One. Query traces programmatically Query distributed trace data by using GraphQL in our NerdGraph API explorer. Next steps Here are some tasks to consider: To instrument your services, check out our Quick start. To learn more about what's happening under the hood, see How distributed tracing works.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>distributed</em> <em>tracing</em>",
        "sections": "Introduction to <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Distributed</em> <em>tracing</em>",
        "body": " and create custom data dashboards. See data across accounts See a global view of traces from across all your accounts and applications in New Relic One. Query traces programmatically Query <em>distributed</em> <em>trace</em> data by using GraphQL in our NerdGraph API explorer. Next steps Here are some tasks to consider: To instrument your services, check out our Quick start. To learn more about what&#x27;s happening under the hood, see How <em>distributed</em> <em>tracing</em> works."
      },
      "id": "6072a767e7b9d231f1a5c64c"
    },
    {
      "image": "https://docs.newrelic.com/static/f487e8c287d614c494f56bd35fd38bb5/c1b63/arrow-step-diagram-trans.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/quick-start/",
      "sections": [
        "Quick start"
      ],
      "published_at": "2021-05-04T18:34:48Z",
      "title": "Quick start",
      "updated_at": "2021-04-11T07:32:21Z",
      "type": "docs",
      "external_id": "f9f4aa287602eee82a0eb7d15775d033ada26d63",
      "document_type": "page",
      "popularity": 1,
      "body": "To set up distributed tracing, you'll complete these three general steps: Identify services: Identify and write down the endpoints, services, languages, and systems that are used to complete this request (you'll need this information in the next step). If you have an environment diagram like the following, you could use it to create a list of services handling requests: Instrument services: Instrument each service you identify so it can send your trace data. Some tools, such as New Relic APM agents, instrument services automatically, while other tools require you to insert some code in the services. Click the icon below for instrumentation steps: New Relic APM: C New Relic APM: Golang New Relic APM: Java New Relic APM: .NET New Relic APM: Node.js New Relic APM: PHP New Relic APM: Python New Relic APM: Ruby New Relic Browser New Relic Mobile AWS Lambda Functions Istio Kamon OpenTelemetry X-Ray Zipkin format: custom integration New Relic format: custom integration View traces: After you instrument the services, generate some traffic in your application, and then go to the New Relic UI to see your trace data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 1,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": "To set up <em>distributed</em> <em>tracing</em>, you&#x27;ll complete these three general steps: Identify services: Identify and write down the endpoints, services, languages, and systems that are used to complete this request (you&#x27;ll need this information in the next step). If you have an environment diagram like"
      },
      "id": "6072a60564441f2f6f9d8541"
    }
  ],
  "/docs/distributed-tracing/infinite-tracing/infinite-tracing-configure-proxy-support": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 454.55457,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "sections": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, <em>Infinite</em> <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about <em>Infinite</em> <em>Tracing</em>: <em>Infinite</em>"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Language agents and distributed tracing",
        "Tip",
        "Quick start for standard distributed tracing (recommended):",
        "Step 1. Identify services",
        "Step 2. Instrument each service with an APM agent",
        "Step 3. View traces",
        "View traces that include a specific service",
        "View traces across accounts",
        "Set up Infinite Tracing (advanced option)",
        "Step 1. Complete the instrumentation for standard distributed tracing in the quick start above",
        "Step 2. Set up the trace observer",
        "Step 3: Configure the agent for Infinite Tracing",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Step 4. (Optional) Customize Infinite Tracing",
        "Options for older APM agents",
        "Compatibility guide",
        "Important",
        "Configure standard distributed tracing for your older agents",
        "Manual instrumentation (If automatic instrumentation doesn't work)",
        "Instrument the calling service",
        "Instrument the called service"
      ],
      "title": "Language agents and distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "b87eacf981bfae09990c95604ba3b7fc19741a40",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/language-agents-enable-distributed-tracing/",
      "published_at": "2021-05-05T19:59:35Z",
      "updated_at": "2021-04-22T06:39:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic has APM language agents for C, Go, Java, Node.js, .NET, PHP, Python, and Ruby. Each of these offers several ways to leverage the power of distributed tracing: Quick start for standard distributed tracing (recommended): A fast way to get started Infinite Tracing: An advanced alternative to standard distributed tracing Older APM agents: Tracing options if you have older APM agents Manual instrumentation: Tips if automatic instrumentation doesn't work Tip If you want to get more background before getting started, check out these topics: How span sampling works explains distributed tracing options. Impacts to APM tells you what to expect if you are a current APM user but haven't set up distributed tracing. Quick start for standard distributed tracing (recommended): This is the best approach to set up standard distributed tracing if you haven't installed any New Relic APM agents for your services yet, or if you want to instrument additional services. Tip You'll need a New Relic account to set up distributed tracing. If you don't already have one, you can quickly create a free account. Step 1. Identify services Figure out which services you want to instrument so they each send trace data to New Relic. Step 2. Instrument each service with an APM agent We have installation assistants for a variety of languages to help you instrument each service. You should run the installation assistant for each service you want to instrument to ensure that each installation has a unique application name. To start the assistant, click the link for your language: New Relic APM: C New Relic APM: Golang New Relic APM: Java New Relic APM: .NET New Relic APM: Node.js New Relic APM: PHP New Relic APM: Python New Relic APM: Ruby Tip This quick-start approach with the installation assistant automatically enables distributed tracing for each service you run it on, but if you already have a New Relic APM agent that you want to participate in distributed tracing, you'll need to manually enable distributed tracing. See Options for older APM agents. Step 3. View traces After you instrument each of your services with APM agents, generate some traffic in your application so we can capture some traces. Here are two ways to view your traces in the UI: View traces that include a specific service Here's one way you can see traces for a particular service: Go to one.newrelic.com. Click APM in the top menu bar. Click your service. In the left navigation's Monitor section, click Distributed tracing. If you don't see the traces you want, you can filter by the trace.id. View traces across accounts This option allows you to search all traces across all New Relic accounts in your organization that you have access to. Go to one.newrelic.com. Click Browse data in the top menu bar, and then click Traces. Select your entity in the left pane. If you don't see the traces you want, you can filter by the trace.id. For more help finding your traces in the UI: Understand and use the distributed tracing UI Query distributed trace data Set up Infinite Tracing (advanced option) Standard distributed tracing for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up Infinite Tracing. This alternative to standard distributed tracing is available for all APM language agents except C SDK. Tip To learn more about this feature, see Infinite Tracing. Before beginning, first ensure you meet the requirements. Step 1. Complete the instrumentation for standard distributed tracing in the quick start above The Infinite Tracing setup builds on the instrumentation step from the Quick start for standard distributed tracing. Step 2. Set up the trace observer The trace observer is a New Relic AWS-based service that collects and analyzes all your traces. Follow the instructions in Set up trace observer. When you're done, return here with your trace observer information and continue with the next step to configure the agent. Step 3: Configure the agent for Infinite Tracing Infinite Tracing configuration settings include the standard distributed tracing plus information about the trace observer. Find the settings for your language agent below: C SDK Infinite tracing is not available for C SDK. Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Infinite Tracing Configuration options: newrelic.Config structure: app, err := newrelic.NewApplication( newrelic.ConfigAppName(YOUR_APP_NAME), newrelic.ConfigLicense(YOUR_LICENSE_KEY), func(cfg *newrelic.Config) { cfg.DistributedTracer.Enabled = true cfg.InfiniteTracing.TraceObserver.Host = YOUR_TRACE_OBSERVER_HOST }, ) Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=YOUR_TRACE_OBSERVER_HOST Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> <infiniteTracing> <trace_observer host=\"YOUR_TRACE_OBSERVER_HOST\" /> </infiniteTracing> </configuration> Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } infinite_tracing: { trace_observer: { host: 'YOUR_TRACE_OBSERVER_HOST' } } Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true newrelic.span_events_enabled = true newrelic.infinite_tracing.trace_observer.host= \"YOUR_TRACE_OBSERVER_HOST\" Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Infinite Tracing Pull down the libraries with this installation command, and then set up the configuration file or environment variables: pip install newrelic[infinite-tracing] Copy Configuration options: Configuration file (newrelic.ini): distributed_tracing.enabled = true infinite_tracing.trace_observer_host= YOUR_TRACE_OBSERVER_HOST Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: 'YOUR_TRACE_OBSERVER_HOST' Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Step 4. (Optional) Customize Infinite Tracing After you add the agent configuration settings, you should start seeing data in the New Relic UI. After you spend some time analyzing your data, you may want to adjust some of the features of Infinite Tracing: Configure trace observer monitoring Configure span attribute trace filter Configure random trace filter Options for older APM agents If you have older APM agents, use this section to figure out if the distributed tracing features you want are supported. Following the compatibility information is a section showing the basic configuration settings to turn on standard distributed tracing. If your older agent supports Infinite Tracing and you want to set it up, see the steps above. Compatibility guide Find your language agents below to confirm if you can use your existing agents with distributed tracing: C SDK Install (compile) or update to the required C SDK version. For best results, update to the latest C SDK version. Option C SDK version Standard distributed tracing 1.1.0 or higher (W3C Trace Context not available) Infinite Tracing Not available Go Install or update to the required Go agent version. For best results, update to the latest Go agent version. Option Go agent version Standard distributed tracing 2.1.0 or higher With W3C Trace Context: 3.1.0 or higher Infinite Tracing v3.5.0 (includes W3C Trace Context) Supported environments: Go 1.9 or higher Java Install or update to the required Java agent version. For best results, update to the latest Java agent version. Important Your JVM's networkaddress.cache.ttl security setting must not be set to forever (i.e. -1). For more information about this networking property, please visit the Oracle Network Properties docs. Type Java agent version Standard distributed tracing 4.3.0 or higher With W3C Trace Context: 5.10 or higher Infinite Tracing 5.12.1 or higher (includes W3C Trace Context) Supported environments: Java 8: Update 252 or higher All versions of Java 9 or higher Tip For special considerations, see Infinite Tracing: Configuring SSL for Java 7 and 8. .NET Install or update to the required .NET agent version. For best results, update to the latest .NET agent version. Option .NET agent version Standard distributed tracing 8.6.45.0 or higher With W3C Trace Context: 8.27.139.0 or higher Infinite Tracing 8.30.0 (includes W3C Trace Context) Supported environments: .NET Framework 4.5 or higher .NET Core 2.0 or higher Node.js Install or update to the required Node.js agent version. For best results, update to the latest Node.js agent version. Option Node.js agent version Standard distributed tracing 4.7.0 or higher With W3C Trace Context: 6.4 or higher Infinite Tracing 7.3.0 (includes W3C Trace Context) Supported environments: Node version 10.10.0 or higher PHP Install or update to the required PHP agent version. For best results, update to the latest PHP agent version. Option PHP agent version Standard distributed tracing 8.4 or higher With W3C Trace Context: 9.8 or higher Infinite Tracing 9.12.0.268 or higher Python Install or update to the required Python agent version. For best results, update to the latest Python agent version. Option Python agent version Standard distributed tracing 4.2.0.100 or higher With W3C Trace Context: 5.6 or higher Infinite Tracing 5.12.0.140 (includes W3C Trace Context) Supported environments: CPython only (pypy is unsupported) Ruby Install or update to the required Ruby agent version. For best results, update to the latest Ruby agent version. Also install the additional Ruby agent gem for Infinite Tracing. Option Ruby agent version Standard distributed tracing newrelic_rpm 5.3.0.346 or higher With W3C Trace Context: newrelic_rpm 6.9 or higher Infinite Tracing newrelic_rpm 6.11.0.365 or higher (includes W3C Trace Context) newrelic-infinite_tracing 6.11.0.375 or higher Configure standard distributed tracing for your older agents Distributed tracing is enabled through configuration settings. Review the following agent-specific sections. For general help with agent configurations, see Configure the agent. Important Server-side configuration is not available for Infinite Tracing. C SDK Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your C applications. Type Required configuration Standard distributed tracing Configuration options: newrelic_app_config_t structure: newrelic_app_config_t* config; config = newrelic_create_app_config(app_name, license_key); config->distributed_tracing.enabled = true; Copy Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Standard distributed tracing Configuration options: ConfigOption structure: newrelic.NewApplication( newrelic.ConfigAppName(\"Example App\"), newrelic.ConfigLicense(os.Getenv(\"NEW_RELIC_LICENSE_KEY\")), newrelic.ConfigDistributedTracerEnabled(true), ) Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> </configuration> Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Standard distributed tracing Configuration file (newrelic.ini): distributed_tracing.enabled = true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Tip If you need help with proxy configuration, see Proxy support. Manual instrumentation (If automatic instrumentation doesn't work) Recommendation: Before performing any custom instrumentation, read: How distributed tracing works Troubleshoot missing data If a service is not passing the trace header to other services, you can use the distributed tracing payload APIs to instrument the calling service and the called service. The calling service uses an API call to generate a payload, which is accepted by the called service. Instrument the calling service To instrument the calling service: Ensure the version of the APM agent that monitors the calling service supports distributed tracing. Invoke the agent API call for generating a distributed trace payload: C SDK | Go | Java | .NET | Node.js | PHP | Python | Ruby. Important To maintain proper ordering of spans in a trace, ensure you generate the payload in the context of the span that sends it. Add that payload to the call made to the destination service (for example, in a header). (Optional) Identify the call as an external call: C SDK Go Java .NET: n/a Node.js PHP: n/a Python Ruby Instrument the called service To instrument the called service: Ensure the version of the APM agent that monitors the called service supports distributed tracing. If the New Relic agent on the called service does not identify a New Relic transaction, use the agent API to declare a transaction: C SDK One way to tell that a transaction is not in progress: when newrelic_create_distributed_trace_payload() is called, a NULL pointer is returned. To solve this problem, follow the procedures to create a transaction with the C SDK. Go One way to tell that a transaction is not in progress: when Transaction.InsertDistributedTraceHeaders(h http.Header) is called, no headers are inserted. To create a transaction, see Instrument Go transactions. Java One way to tell that a transaction is not in progress: when Transaction.insertDistributedTraceHeaders(Headers) is called, no headers are inserted (this API requires agent 6.4.0+). To create a transaction, see Java agent transaction-related APIs. .NET One way to tell that a transaction is not in progress: CreateDistributedTracePayload() returns an empty payload. To create a transaction, see Introduction to .NET custom instrumentation. Node.js One way to tell that a transaction is not in progress: the Node.js agent logs will report an error similar to this: No transaction found when calling Transaction.acceptDistributedTracePayload. Copy Use startWebTransaction to create a web transaction or startBackgroundTransaction to capture a non-web transaction. PHP One way to tell that a transaction is not in progress: newrelic_insert_distributed_trace_headers() returns false. To create a transaction, see newrelic_start_transaction. Python To tell that a transaction is not in progress: when transaction = current_transaction() is run, transaction is None. Or, if result = accept_distributed_trace_payload(payload) is run, then the result is False. Use background_task to report a non-web transaction. For more on Python instrumentation, see Monitor transactions and segments. Ruby If you are using a Rack-based web framework and have enabled New Relic's Rack instrumentation, the Ruby agent will handle starting a transaction for you. For other use cases, see the add_transaction_tracer API method. Extract the payload from the call that you received (for example, in a header). Invoke the call for accepting the payload: C SDK | Go | Java | .NET | PHP | Node.js | Python | Ruby.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 324.79193,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Language agents and <em>distributed</em> <em>tracing</em>",
        "sections": "Language agents and <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>tracing</em> UI Query <em>distributed</em> <em>trace</em> data Set up <em>Infinite</em> <em>Tracing</em> (advanced option) Standard <em>distributed</em> <em>tracing</em> for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up <em>Infinite</em> <em>Tracing</em>. This alternative"
      },
      "id": "6072a66564441fb28e9d8595"
    },
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.10175,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> API general requirements and limits ",
        "sections": "<em>Trace</em> API general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "-format <em>trace</em> data Endpoints All <em>trace</em> data is sent via a POST to a <em>Trace</em> API endpoint. We have a few endpoints, depending on your setup: Default <em>Trace</em> API endpoint: https:&#x2F;&#x2F;<em>trace</em>-api.newrelic.com&#x2F;<em>trace</em>&#x2F;v1 EU data centers: https:&#x2F;&#x2F;<em>trace</em>-api.eu.newrelic.com&#x2F;<em>trace</em>&#x2F;v1 (see other EU endpoints). <em>Infinite</em>"
      },
      "id": "6071cf7628ccbcf8b851c158"
    }
  ],
  "/docs/distributed-tracing/infinite-tracing/infinite-tracing-configure-random-trace-filter": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 454.55457,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "sections": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, <em>Infinite</em> <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about <em>Infinite</em> <em>Tracing</em>: <em>Infinite</em>"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Language agents and distributed tracing",
        "Tip",
        "Quick start for standard distributed tracing (recommended):",
        "Step 1. Identify services",
        "Step 2. Instrument each service with an APM agent",
        "Step 3. View traces",
        "View traces that include a specific service",
        "View traces across accounts",
        "Set up Infinite Tracing (advanced option)",
        "Step 1. Complete the instrumentation for standard distributed tracing in the quick start above",
        "Step 2. Set up the trace observer",
        "Step 3: Configure the agent for Infinite Tracing",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Step 4. (Optional) Customize Infinite Tracing",
        "Options for older APM agents",
        "Compatibility guide",
        "Important",
        "Configure standard distributed tracing for your older agents",
        "Manual instrumentation (If automatic instrumentation doesn't work)",
        "Instrument the calling service",
        "Instrument the called service"
      ],
      "title": "Language agents and distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "b87eacf981bfae09990c95604ba3b7fc19741a40",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/language-agents-enable-distributed-tracing/",
      "published_at": "2021-05-05T19:59:35Z",
      "updated_at": "2021-04-22T06:39:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic has APM language agents for C, Go, Java, Node.js, .NET, PHP, Python, and Ruby. Each of these offers several ways to leverage the power of distributed tracing: Quick start for standard distributed tracing (recommended): A fast way to get started Infinite Tracing: An advanced alternative to standard distributed tracing Older APM agents: Tracing options if you have older APM agents Manual instrumentation: Tips if automatic instrumentation doesn't work Tip If you want to get more background before getting started, check out these topics: How span sampling works explains distributed tracing options. Impacts to APM tells you what to expect if you are a current APM user but haven't set up distributed tracing. Quick start for standard distributed tracing (recommended): This is the best approach to set up standard distributed tracing if you haven't installed any New Relic APM agents for your services yet, or if you want to instrument additional services. Tip You'll need a New Relic account to set up distributed tracing. If you don't already have one, you can quickly create a free account. Step 1. Identify services Figure out which services you want to instrument so they each send trace data to New Relic. Step 2. Instrument each service with an APM agent We have installation assistants for a variety of languages to help you instrument each service. You should run the installation assistant for each service you want to instrument to ensure that each installation has a unique application name. To start the assistant, click the link for your language: New Relic APM: C New Relic APM: Golang New Relic APM: Java New Relic APM: .NET New Relic APM: Node.js New Relic APM: PHP New Relic APM: Python New Relic APM: Ruby Tip This quick-start approach with the installation assistant automatically enables distributed tracing for each service you run it on, but if you already have a New Relic APM agent that you want to participate in distributed tracing, you'll need to manually enable distributed tracing. See Options for older APM agents. Step 3. View traces After you instrument each of your services with APM agents, generate some traffic in your application so we can capture some traces. Here are two ways to view your traces in the UI: View traces that include a specific service Here's one way you can see traces for a particular service: Go to one.newrelic.com. Click APM in the top menu bar. Click your service. In the left navigation's Monitor section, click Distributed tracing. If you don't see the traces you want, you can filter by the trace.id. View traces across accounts This option allows you to search all traces across all New Relic accounts in your organization that you have access to. Go to one.newrelic.com. Click Browse data in the top menu bar, and then click Traces. Select your entity in the left pane. If you don't see the traces you want, you can filter by the trace.id. For more help finding your traces in the UI: Understand and use the distributed tracing UI Query distributed trace data Set up Infinite Tracing (advanced option) Standard distributed tracing for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up Infinite Tracing. This alternative to standard distributed tracing is available for all APM language agents except C SDK. Tip To learn more about this feature, see Infinite Tracing. Before beginning, first ensure you meet the requirements. Step 1. Complete the instrumentation for standard distributed tracing in the quick start above The Infinite Tracing setup builds on the instrumentation step from the Quick start for standard distributed tracing. Step 2. Set up the trace observer The trace observer is a New Relic AWS-based service that collects and analyzes all your traces. Follow the instructions in Set up trace observer. When you're done, return here with your trace observer information and continue with the next step to configure the agent. Step 3: Configure the agent for Infinite Tracing Infinite Tracing configuration settings include the standard distributed tracing plus information about the trace observer. Find the settings for your language agent below: C SDK Infinite tracing is not available for C SDK. Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Infinite Tracing Configuration options: newrelic.Config structure: app, err := newrelic.NewApplication( newrelic.ConfigAppName(YOUR_APP_NAME), newrelic.ConfigLicense(YOUR_LICENSE_KEY), func(cfg *newrelic.Config) { cfg.DistributedTracer.Enabled = true cfg.InfiniteTracing.TraceObserver.Host = YOUR_TRACE_OBSERVER_HOST }, ) Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=YOUR_TRACE_OBSERVER_HOST Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> <infiniteTracing> <trace_observer host=\"YOUR_TRACE_OBSERVER_HOST\" /> </infiniteTracing> </configuration> Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } infinite_tracing: { trace_observer: { host: 'YOUR_TRACE_OBSERVER_HOST' } } Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true newrelic.span_events_enabled = true newrelic.infinite_tracing.trace_observer.host= \"YOUR_TRACE_OBSERVER_HOST\" Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Infinite Tracing Pull down the libraries with this installation command, and then set up the configuration file or environment variables: pip install newrelic[infinite-tracing] Copy Configuration options: Configuration file (newrelic.ini): distributed_tracing.enabled = true infinite_tracing.trace_observer_host= YOUR_TRACE_OBSERVER_HOST Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: 'YOUR_TRACE_OBSERVER_HOST' Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Step 4. (Optional) Customize Infinite Tracing After you add the agent configuration settings, you should start seeing data in the New Relic UI. After you spend some time analyzing your data, you may want to adjust some of the features of Infinite Tracing: Configure trace observer monitoring Configure span attribute trace filter Configure random trace filter Options for older APM agents If you have older APM agents, use this section to figure out if the distributed tracing features you want are supported. Following the compatibility information is a section showing the basic configuration settings to turn on standard distributed tracing. If your older agent supports Infinite Tracing and you want to set it up, see the steps above. Compatibility guide Find your language agents below to confirm if you can use your existing agents with distributed tracing: C SDK Install (compile) or update to the required C SDK version. For best results, update to the latest C SDK version. Option C SDK version Standard distributed tracing 1.1.0 or higher (W3C Trace Context not available) Infinite Tracing Not available Go Install or update to the required Go agent version. For best results, update to the latest Go agent version. Option Go agent version Standard distributed tracing 2.1.0 or higher With W3C Trace Context: 3.1.0 or higher Infinite Tracing v3.5.0 (includes W3C Trace Context) Supported environments: Go 1.9 or higher Java Install or update to the required Java agent version. For best results, update to the latest Java agent version. Important Your JVM's networkaddress.cache.ttl security setting must not be set to forever (i.e. -1). For more information about this networking property, please visit the Oracle Network Properties docs. Type Java agent version Standard distributed tracing 4.3.0 or higher With W3C Trace Context: 5.10 or higher Infinite Tracing 5.12.1 or higher (includes W3C Trace Context) Supported environments: Java 8: Update 252 or higher All versions of Java 9 or higher Tip For special considerations, see Infinite Tracing: Configuring SSL for Java 7 and 8. .NET Install or update to the required .NET agent version. For best results, update to the latest .NET agent version. Option .NET agent version Standard distributed tracing 8.6.45.0 or higher With W3C Trace Context: 8.27.139.0 or higher Infinite Tracing 8.30.0 (includes W3C Trace Context) Supported environments: .NET Framework 4.5 or higher .NET Core 2.0 or higher Node.js Install or update to the required Node.js agent version. For best results, update to the latest Node.js agent version. Option Node.js agent version Standard distributed tracing 4.7.0 or higher With W3C Trace Context: 6.4 or higher Infinite Tracing 7.3.0 (includes W3C Trace Context) Supported environments: Node version 10.10.0 or higher PHP Install or update to the required PHP agent version. For best results, update to the latest PHP agent version. Option PHP agent version Standard distributed tracing 8.4 or higher With W3C Trace Context: 9.8 or higher Infinite Tracing 9.12.0.268 or higher Python Install or update to the required Python agent version. For best results, update to the latest Python agent version. Option Python agent version Standard distributed tracing 4.2.0.100 or higher With W3C Trace Context: 5.6 or higher Infinite Tracing 5.12.0.140 (includes W3C Trace Context) Supported environments: CPython only (pypy is unsupported) Ruby Install or update to the required Ruby agent version. For best results, update to the latest Ruby agent version. Also install the additional Ruby agent gem for Infinite Tracing. Option Ruby agent version Standard distributed tracing newrelic_rpm 5.3.0.346 or higher With W3C Trace Context: newrelic_rpm 6.9 or higher Infinite Tracing newrelic_rpm 6.11.0.365 or higher (includes W3C Trace Context) newrelic-infinite_tracing 6.11.0.375 or higher Configure standard distributed tracing for your older agents Distributed tracing is enabled through configuration settings. Review the following agent-specific sections. For general help with agent configurations, see Configure the agent. Important Server-side configuration is not available for Infinite Tracing. C SDK Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your C applications. Type Required configuration Standard distributed tracing Configuration options: newrelic_app_config_t structure: newrelic_app_config_t* config; config = newrelic_create_app_config(app_name, license_key); config->distributed_tracing.enabled = true; Copy Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Standard distributed tracing Configuration options: ConfigOption structure: newrelic.NewApplication( newrelic.ConfigAppName(\"Example App\"), newrelic.ConfigLicense(os.Getenv(\"NEW_RELIC_LICENSE_KEY\")), newrelic.ConfigDistributedTracerEnabled(true), ) Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> </configuration> Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Standard distributed tracing Configuration file (newrelic.ini): distributed_tracing.enabled = true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Tip If you need help with proxy configuration, see Proxy support. Manual instrumentation (If automatic instrumentation doesn't work) Recommendation: Before performing any custom instrumentation, read: How distributed tracing works Troubleshoot missing data If a service is not passing the trace header to other services, you can use the distributed tracing payload APIs to instrument the calling service and the called service. The calling service uses an API call to generate a payload, which is accepted by the called service. Instrument the calling service To instrument the calling service: Ensure the version of the APM agent that monitors the calling service supports distributed tracing. Invoke the agent API call for generating a distributed trace payload: C SDK | Go | Java | .NET | Node.js | PHP | Python | Ruby. Important To maintain proper ordering of spans in a trace, ensure you generate the payload in the context of the span that sends it. Add that payload to the call made to the destination service (for example, in a header). (Optional) Identify the call as an external call: C SDK Go Java .NET: n/a Node.js PHP: n/a Python Ruby Instrument the called service To instrument the called service: Ensure the version of the APM agent that monitors the called service supports distributed tracing. If the New Relic agent on the called service does not identify a New Relic transaction, use the agent API to declare a transaction: C SDK One way to tell that a transaction is not in progress: when newrelic_create_distributed_trace_payload() is called, a NULL pointer is returned. To solve this problem, follow the procedures to create a transaction with the C SDK. Go One way to tell that a transaction is not in progress: when Transaction.InsertDistributedTraceHeaders(h http.Header) is called, no headers are inserted. To create a transaction, see Instrument Go transactions. Java One way to tell that a transaction is not in progress: when Transaction.insertDistributedTraceHeaders(Headers) is called, no headers are inserted (this API requires agent 6.4.0+). To create a transaction, see Java agent transaction-related APIs. .NET One way to tell that a transaction is not in progress: CreateDistributedTracePayload() returns an empty payload. To create a transaction, see Introduction to .NET custom instrumentation. Node.js One way to tell that a transaction is not in progress: the Node.js agent logs will report an error similar to this: No transaction found when calling Transaction.acceptDistributedTracePayload. Copy Use startWebTransaction to create a web transaction or startBackgroundTransaction to capture a non-web transaction. PHP One way to tell that a transaction is not in progress: newrelic_insert_distributed_trace_headers() returns false. To create a transaction, see newrelic_start_transaction. Python To tell that a transaction is not in progress: when transaction = current_transaction() is run, transaction is None. Or, if result = accept_distributed_trace_payload(payload) is run, then the result is False. Use background_task to report a non-web transaction. For more on Python instrumentation, see Monitor transactions and segments. Ruby If you are using a Rack-based web framework and have enabled New Relic's Rack instrumentation, the Ruby agent will handle starting a transaction for you. For other use cases, see the add_transaction_tracer API method. Extract the payload from the call that you received (for example, in a header). Invoke the call for accepting the payload: C SDK | Go | Java | .NET | PHP | Node.js | Python | Ruby.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 324.79193,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Language agents and <em>distributed</em> <em>tracing</em>",
        "sections": "Language agents and <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>tracing</em> UI Query <em>distributed</em> <em>trace</em> data Set up <em>Infinite</em> <em>Tracing</em> (advanced option) Standard <em>distributed</em> <em>tracing</em> for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up <em>Infinite</em> <em>Tracing</em>. This alternative"
      },
      "id": "6072a66564441fb28e9d8595"
    },
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.10175,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> API general requirements and limits ",
        "sections": "<em>Trace</em> API general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "-format <em>trace</em> data Endpoints All <em>trace</em> data is sent via a POST to a <em>Trace</em> API endpoint. We have a few endpoints, depending on your setup: Default <em>Trace</em> API endpoint: https:&#x2F;&#x2F;<em>trace</em>-api.newrelic.com&#x2F;<em>trace</em>&#x2F;v1 EU data centers: https:&#x2F;&#x2F;<em>trace</em>-api.eu.newrelic.com&#x2F;<em>trace</em>&#x2F;v1 (see other EU endpoints). <em>Infinite</em>"
      },
      "id": "6071cf7628ccbcf8b851c158"
    }
  ],
  "/docs/distributed-tracing/infinite-tracing/infinite-tracing-configure-span-attribute-trace-filter": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 454.55426,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "sections": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, <em>Infinite</em> <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about <em>Infinite</em> <em>Tracing</em>: <em>Infinite</em>"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Language agents and distributed tracing",
        "Tip",
        "Quick start for standard distributed tracing (recommended):",
        "Step 1. Identify services",
        "Step 2. Instrument each service with an APM agent",
        "Step 3. View traces",
        "View traces that include a specific service",
        "View traces across accounts",
        "Set up Infinite Tracing (advanced option)",
        "Step 1. Complete the instrumentation for standard distributed tracing in the quick start above",
        "Step 2. Set up the trace observer",
        "Step 3: Configure the agent for Infinite Tracing",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Step 4. (Optional) Customize Infinite Tracing",
        "Options for older APM agents",
        "Compatibility guide",
        "Important",
        "Configure standard distributed tracing for your older agents",
        "Manual instrumentation (If automatic instrumentation doesn't work)",
        "Instrument the calling service",
        "Instrument the called service"
      ],
      "title": "Language agents and distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "b87eacf981bfae09990c95604ba3b7fc19741a40",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/language-agents-enable-distributed-tracing/",
      "published_at": "2021-05-05T19:59:35Z",
      "updated_at": "2021-04-22T06:39:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic has APM language agents for C, Go, Java, Node.js, .NET, PHP, Python, and Ruby. Each of these offers several ways to leverage the power of distributed tracing: Quick start for standard distributed tracing (recommended): A fast way to get started Infinite Tracing: An advanced alternative to standard distributed tracing Older APM agents: Tracing options if you have older APM agents Manual instrumentation: Tips if automatic instrumentation doesn't work Tip If you want to get more background before getting started, check out these topics: How span sampling works explains distributed tracing options. Impacts to APM tells you what to expect if you are a current APM user but haven't set up distributed tracing. Quick start for standard distributed tracing (recommended): This is the best approach to set up standard distributed tracing if you haven't installed any New Relic APM agents for your services yet, or if you want to instrument additional services. Tip You'll need a New Relic account to set up distributed tracing. If you don't already have one, you can quickly create a free account. Step 1. Identify services Figure out which services you want to instrument so they each send trace data to New Relic. Step 2. Instrument each service with an APM agent We have installation assistants for a variety of languages to help you instrument each service. You should run the installation assistant for each service you want to instrument to ensure that each installation has a unique application name. To start the assistant, click the link for your language: New Relic APM: C New Relic APM: Golang New Relic APM: Java New Relic APM: .NET New Relic APM: Node.js New Relic APM: PHP New Relic APM: Python New Relic APM: Ruby Tip This quick-start approach with the installation assistant automatically enables distributed tracing for each service you run it on, but if you already have a New Relic APM agent that you want to participate in distributed tracing, you'll need to manually enable distributed tracing. See Options for older APM agents. Step 3. View traces After you instrument each of your services with APM agents, generate some traffic in your application so we can capture some traces. Here are two ways to view your traces in the UI: View traces that include a specific service Here's one way you can see traces for a particular service: Go to one.newrelic.com. Click APM in the top menu bar. Click your service. In the left navigation's Monitor section, click Distributed tracing. If you don't see the traces you want, you can filter by the trace.id. View traces across accounts This option allows you to search all traces across all New Relic accounts in your organization that you have access to. Go to one.newrelic.com. Click Browse data in the top menu bar, and then click Traces. Select your entity in the left pane. If you don't see the traces you want, you can filter by the trace.id. For more help finding your traces in the UI: Understand and use the distributed tracing UI Query distributed trace data Set up Infinite Tracing (advanced option) Standard distributed tracing for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up Infinite Tracing. This alternative to standard distributed tracing is available for all APM language agents except C SDK. Tip To learn more about this feature, see Infinite Tracing. Before beginning, first ensure you meet the requirements. Step 1. Complete the instrumentation for standard distributed tracing in the quick start above The Infinite Tracing setup builds on the instrumentation step from the Quick start for standard distributed tracing. Step 2. Set up the trace observer The trace observer is a New Relic AWS-based service that collects and analyzes all your traces. Follow the instructions in Set up trace observer. When you're done, return here with your trace observer information and continue with the next step to configure the agent. Step 3: Configure the agent for Infinite Tracing Infinite Tracing configuration settings include the standard distributed tracing plus information about the trace observer. Find the settings for your language agent below: C SDK Infinite tracing is not available for C SDK. Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Infinite Tracing Configuration options: newrelic.Config structure: app, err := newrelic.NewApplication( newrelic.ConfigAppName(YOUR_APP_NAME), newrelic.ConfigLicense(YOUR_LICENSE_KEY), func(cfg *newrelic.Config) { cfg.DistributedTracer.Enabled = true cfg.InfiniteTracing.TraceObserver.Host = YOUR_TRACE_OBSERVER_HOST }, ) Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=YOUR_TRACE_OBSERVER_HOST Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> <infiniteTracing> <trace_observer host=\"YOUR_TRACE_OBSERVER_HOST\" /> </infiniteTracing> </configuration> Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } infinite_tracing: { trace_observer: { host: 'YOUR_TRACE_OBSERVER_HOST' } } Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true newrelic.span_events_enabled = true newrelic.infinite_tracing.trace_observer.host= \"YOUR_TRACE_OBSERVER_HOST\" Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Infinite Tracing Pull down the libraries with this installation command, and then set up the configuration file or environment variables: pip install newrelic[infinite-tracing] Copy Configuration options: Configuration file (newrelic.ini): distributed_tracing.enabled = true infinite_tracing.trace_observer_host= YOUR_TRACE_OBSERVER_HOST Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: 'YOUR_TRACE_OBSERVER_HOST' Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Step 4. (Optional) Customize Infinite Tracing After you add the agent configuration settings, you should start seeing data in the New Relic UI. After you spend some time analyzing your data, you may want to adjust some of the features of Infinite Tracing: Configure trace observer monitoring Configure span attribute trace filter Configure random trace filter Options for older APM agents If you have older APM agents, use this section to figure out if the distributed tracing features you want are supported. Following the compatibility information is a section showing the basic configuration settings to turn on standard distributed tracing. If your older agent supports Infinite Tracing and you want to set it up, see the steps above. Compatibility guide Find your language agents below to confirm if you can use your existing agents with distributed tracing: C SDK Install (compile) or update to the required C SDK version. For best results, update to the latest C SDK version. Option C SDK version Standard distributed tracing 1.1.0 or higher (W3C Trace Context not available) Infinite Tracing Not available Go Install or update to the required Go agent version. For best results, update to the latest Go agent version. Option Go agent version Standard distributed tracing 2.1.0 or higher With W3C Trace Context: 3.1.0 or higher Infinite Tracing v3.5.0 (includes W3C Trace Context) Supported environments: Go 1.9 or higher Java Install or update to the required Java agent version. For best results, update to the latest Java agent version. Important Your JVM's networkaddress.cache.ttl security setting must not be set to forever (i.e. -1). For more information about this networking property, please visit the Oracle Network Properties docs. Type Java agent version Standard distributed tracing 4.3.0 or higher With W3C Trace Context: 5.10 or higher Infinite Tracing 5.12.1 or higher (includes W3C Trace Context) Supported environments: Java 8: Update 252 or higher All versions of Java 9 or higher Tip For special considerations, see Infinite Tracing: Configuring SSL for Java 7 and 8. .NET Install or update to the required .NET agent version. For best results, update to the latest .NET agent version. Option .NET agent version Standard distributed tracing 8.6.45.0 or higher With W3C Trace Context: 8.27.139.0 or higher Infinite Tracing 8.30.0 (includes W3C Trace Context) Supported environments: .NET Framework 4.5 or higher .NET Core 2.0 or higher Node.js Install or update to the required Node.js agent version. For best results, update to the latest Node.js agent version. Option Node.js agent version Standard distributed tracing 4.7.0 or higher With W3C Trace Context: 6.4 or higher Infinite Tracing 7.3.0 (includes W3C Trace Context) Supported environments: Node version 10.10.0 or higher PHP Install or update to the required PHP agent version. For best results, update to the latest PHP agent version. Option PHP agent version Standard distributed tracing 8.4 or higher With W3C Trace Context: 9.8 or higher Infinite Tracing 9.12.0.268 or higher Python Install or update to the required Python agent version. For best results, update to the latest Python agent version. Option Python agent version Standard distributed tracing 4.2.0.100 or higher With W3C Trace Context: 5.6 or higher Infinite Tracing 5.12.0.140 (includes W3C Trace Context) Supported environments: CPython only (pypy is unsupported) Ruby Install or update to the required Ruby agent version. For best results, update to the latest Ruby agent version. Also install the additional Ruby agent gem for Infinite Tracing. Option Ruby agent version Standard distributed tracing newrelic_rpm 5.3.0.346 or higher With W3C Trace Context: newrelic_rpm 6.9 or higher Infinite Tracing newrelic_rpm 6.11.0.365 or higher (includes W3C Trace Context) newrelic-infinite_tracing 6.11.0.375 or higher Configure standard distributed tracing for your older agents Distributed tracing is enabled through configuration settings. Review the following agent-specific sections. For general help with agent configurations, see Configure the agent. Important Server-side configuration is not available for Infinite Tracing. C SDK Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your C applications. Type Required configuration Standard distributed tracing Configuration options: newrelic_app_config_t structure: newrelic_app_config_t* config; config = newrelic_create_app_config(app_name, license_key); config->distributed_tracing.enabled = true; Copy Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Standard distributed tracing Configuration options: ConfigOption structure: newrelic.NewApplication( newrelic.ConfigAppName(\"Example App\"), newrelic.ConfigLicense(os.Getenv(\"NEW_RELIC_LICENSE_KEY\")), newrelic.ConfigDistributedTracerEnabled(true), ) Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> </configuration> Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Standard distributed tracing Configuration file (newrelic.ini): distributed_tracing.enabled = true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Tip If you need help with proxy configuration, see Proxy support. Manual instrumentation (If automatic instrumentation doesn't work) Recommendation: Before performing any custom instrumentation, read: How distributed tracing works Troubleshoot missing data If a service is not passing the trace header to other services, you can use the distributed tracing payload APIs to instrument the calling service and the called service. The calling service uses an API call to generate a payload, which is accepted by the called service. Instrument the calling service To instrument the calling service: Ensure the version of the APM agent that monitors the calling service supports distributed tracing. Invoke the agent API call for generating a distributed trace payload: C SDK | Go | Java | .NET | Node.js | PHP | Python | Ruby. Important To maintain proper ordering of spans in a trace, ensure you generate the payload in the context of the span that sends it. Add that payload to the call made to the destination service (for example, in a header). (Optional) Identify the call as an external call: C SDK Go Java .NET: n/a Node.js PHP: n/a Python Ruby Instrument the called service To instrument the called service: Ensure the version of the APM agent that monitors the called service supports distributed tracing. If the New Relic agent on the called service does not identify a New Relic transaction, use the agent API to declare a transaction: C SDK One way to tell that a transaction is not in progress: when newrelic_create_distributed_trace_payload() is called, a NULL pointer is returned. To solve this problem, follow the procedures to create a transaction with the C SDK. Go One way to tell that a transaction is not in progress: when Transaction.InsertDistributedTraceHeaders(h http.Header) is called, no headers are inserted. To create a transaction, see Instrument Go transactions. Java One way to tell that a transaction is not in progress: when Transaction.insertDistributedTraceHeaders(Headers) is called, no headers are inserted (this API requires agent 6.4.0+). To create a transaction, see Java agent transaction-related APIs. .NET One way to tell that a transaction is not in progress: CreateDistributedTracePayload() returns an empty payload. To create a transaction, see Introduction to .NET custom instrumentation. Node.js One way to tell that a transaction is not in progress: the Node.js agent logs will report an error similar to this: No transaction found when calling Transaction.acceptDistributedTracePayload. Copy Use startWebTransaction to create a web transaction or startBackgroundTransaction to capture a non-web transaction. PHP One way to tell that a transaction is not in progress: newrelic_insert_distributed_trace_headers() returns false. To create a transaction, see newrelic_start_transaction. Python To tell that a transaction is not in progress: when transaction = current_transaction() is run, transaction is None. Or, if result = accept_distributed_trace_payload(payload) is run, then the result is False. Use background_task to report a non-web transaction. For more on Python instrumentation, see Monitor transactions and segments. Ruby If you are using a Rack-based web framework and have enabled New Relic's Rack instrumentation, the Ruby agent will handle starting a transaction for you. For other use cases, see the add_transaction_tracer API method. Extract the payload from the call that you received (for example, in a header). Invoke the call for accepting the payload: C SDK | Go | Java | .NET | PHP | Node.js | Python | Ruby.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 324.79187,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Language agents and <em>distributed</em> <em>tracing</em>",
        "sections": "Language agents and <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>tracing</em> UI Query <em>distributed</em> <em>trace</em> data Set up <em>Infinite</em> <em>Tracing</em> (advanced option) Standard <em>distributed</em> <em>tracing</em> for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up <em>Infinite</em> <em>Tracing</em>. This alternative"
      },
      "id": "6072a66564441fb28e9d8595"
    },
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.10156,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> API general requirements and limits ",
        "sections": "<em>Trace</em> API general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "-format <em>trace</em> data Endpoints All <em>trace</em> data is sent via a POST to a <em>Trace</em> API endpoint. We have a few endpoints, depending on your setup: Default <em>Trace</em> API endpoint: https:&#x2F;&#x2F;<em>trace</em>-api.newrelic.com&#x2F;<em>trace</em>&#x2F;v1 EU data centers: https:&#x2F;&#x2F;<em>trace</em>-api.eu.newrelic.com&#x2F;<em>trace</em>&#x2F;v1 (see other EU endpoints). <em>Infinite</em>"
      },
      "id": "6071cf7628ccbcf8b851c158"
    }
  ],
  "/docs/distributed-tracing/infinite-tracing/infinite-tracing-configure-trace-observer-monitoring": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 454.55426,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "sections": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, <em>Infinite</em> <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about <em>Infinite</em> <em>Tracing</em>: <em>Infinite</em>"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Language agents and distributed tracing",
        "Tip",
        "Quick start for standard distributed tracing (recommended):",
        "Step 1. Identify services",
        "Step 2. Instrument each service with an APM agent",
        "Step 3. View traces",
        "View traces that include a specific service",
        "View traces across accounts",
        "Set up Infinite Tracing (advanced option)",
        "Step 1. Complete the instrumentation for standard distributed tracing in the quick start above",
        "Step 2. Set up the trace observer",
        "Step 3: Configure the agent for Infinite Tracing",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Step 4. (Optional) Customize Infinite Tracing",
        "Options for older APM agents",
        "Compatibility guide",
        "Important",
        "Configure standard distributed tracing for your older agents",
        "Manual instrumentation (If automatic instrumentation doesn't work)",
        "Instrument the calling service",
        "Instrument the called service"
      ],
      "title": "Language agents and distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "b87eacf981bfae09990c95604ba3b7fc19741a40",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/language-agents-enable-distributed-tracing/",
      "published_at": "2021-05-05T19:59:35Z",
      "updated_at": "2021-04-22T06:39:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic has APM language agents for C, Go, Java, Node.js, .NET, PHP, Python, and Ruby. Each of these offers several ways to leverage the power of distributed tracing: Quick start for standard distributed tracing (recommended): A fast way to get started Infinite Tracing: An advanced alternative to standard distributed tracing Older APM agents: Tracing options if you have older APM agents Manual instrumentation: Tips if automatic instrumentation doesn't work Tip If you want to get more background before getting started, check out these topics: How span sampling works explains distributed tracing options. Impacts to APM tells you what to expect if you are a current APM user but haven't set up distributed tracing. Quick start for standard distributed tracing (recommended): This is the best approach to set up standard distributed tracing if you haven't installed any New Relic APM agents for your services yet, or if you want to instrument additional services. Tip You'll need a New Relic account to set up distributed tracing. If you don't already have one, you can quickly create a free account. Step 1. Identify services Figure out which services you want to instrument so they each send trace data to New Relic. Step 2. Instrument each service with an APM agent We have installation assistants for a variety of languages to help you instrument each service. You should run the installation assistant for each service you want to instrument to ensure that each installation has a unique application name. To start the assistant, click the link for your language: New Relic APM: C New Relic APM: Golang New Relic APM: Java New Relic APM: .NET New Relic APM: Node.js New Relic APM: PHP New Relic APM: Python New Relic APM: Ruby Tip This quick-start approach with the installation assistant automatically enables distributed tracing for each service you run it on, but if you already have a New Relic APM agent that you want to participate in distributed tracing, you'll need to manually enable distributed tracing. See Options for older APM agents. Step 3. View traces After you instrument each of your services with APM agents, generate some traffic in your application so we can capture some traces. Here are two ways to view your traces in the UI: View traces that include a specific service Here's one way you can see traces for a particular service: Go to one.newrelic.com. Click APM in the top menu bar. Click your service. In the left navigation's Monitor section, click Distributed tracing. If you don't see the traces you want, you can filter by the trace.id. View traces across accounts This option allows you to search all traces across all New Relic accounts in your organization that you have access to. Go to one.newrelic.com. Click Browse data in the top menu bar, and then click Traces. Select your entity in the left pane. If you don't see the traces you want, you can filter by the trace.id. For more help finding your traces in the UI: Understand and use the distributed tracing UI Query distributed trace data Set up Infinite Tracing (advanced option) Standard distributed tracing for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up Infinite Tracing. This alternative to standard distributed tracing is available for all APM language agents except C SDK. Tip To learn more about this feature, see Infinite Tracing. Before beginning, first ensure you meet the requirements. Step 1. Complete the instrumentation for standard distributed tracing in the quick start above The Infinite Tracing setup builds on the instrumentation step from the Quick start for standard distributed tracing. Step 2. Set up the trace observer The trace observer is a New Relic AWS-based service that collects and analyzes all your traces. Follow the instructions in Set up trace observer. When you're done, return here with your trace observer information and continue with the next step to configure the agent. Step 3: Configure the agent for Infinite Tracing Infinite Tracing configuration settings include the standard distributed tracing plus information about the trace observer. Find the settings for your language agent below: C SDK Infinite tracing is not available for C SDK. Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Infinite Tracing Configuration options: newrelic.Config structure: app, err := newrelic.NewApplication( newrelic.ConfigAppName(YOUR_APP_NAME), newrelic.ConfigLicense(YOUR_LICENSE_KEY), func(cfg *newrelic.Config) { cfg.DistributedTracer.Enabled = true cfg.InfiniteTracing.TraceObserver.Host = YOUR_TRACE_OBSERVER_HOST }, ) Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=YOUR_TRACE_OBSERVER_HOST Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> <infiniteTracing> <trace_observer host=\"YOUR_TRACE_OBSERVER_HOST\" /> </infiniteTracing> </configuration> Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } infinite_tracing: { trace_observer: { host: 'YOUR_TRACE_OBSERVER_HOST' } } Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true newrelic.span_events_enabled = true newrelic.infinite_tracing.trace_observer.host= \"YOUR_TRACE_OBSERVER_HOST\" Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Infinite Tracing Pull down the libraries with this installation command, and then set up the configuration file or environment variables: pip install newrelic[infinite-tracing] Copy Configuration options: Configuration file (newrelic.ini): distributed_tracing.enabled = true infinite_tracing.trace_observer_host= YOUR_TRACE_OBSERVER_HOST Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: 'YOUR_TRACE_OBSERVER_HOST' Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Step 4. (Optional) Customize Infinite Tracing After you add the agent configuration settings, you should start seeing data in the New Relic UI. After you spend some time analyzing your data, you may want to adjust some of the features of Infinite Tracing: Configure trace observer monitoring Configure span attribute trace filter Configure random trace filter Options for older APM agents If you have older APM agents, use this section to figure out if the distributed tracing features you want are supported. Following the compatibility information is a section showing the basic configuration settings to turn on standard distributed tracing. If your older agent supports Infinite Tracing and you want to set it up, see the steps above. Compatibility guide Find your language agents below to confirm if you can use your existing agents with distributed tracing: C SDK Install (compile) or update to the required C SDK version. For best results, update to the latest C SDK version. Option C SDK version Standard distributed tracing 1.1.0 or higher (W3C Trace Context not available) Infinite Tracing Not available Go Install or update to the required Go agent version. For best results, update to the latest Go agent version. Option Go agent version Standard distributed tracing 2.1.0 or higher With W3C Trace Context: 3.1.0 or higher Infinite Tracing v3.5.0 (includes W3C Trace Context) Supported environments: Go 1.9 or higher Java Install or update to the required Java agent version. For best results, update to the latest Java agent version. Important Your JVM's networkaddress.cache.ttl security setting must not be set to forever (i.e. -1). For more information about this networking property, please visit the Oracle Network Properties docs. Type Java agent version Standard distributed tracing 4.3.0 or higher With W3C Trace Context: 5.10 or higher Infinite Tracing 5.12.1 or higher (includes W3C Trace Context) Supported environments: Java 8: Update 252 or higher All versions of Java 9 or higher Tip For special considerations, see Infinite Tracing: Configuring SSL for Java 7 and 8. .NET Install or update to the required .NET agent version. For best results, update to the latest .NET agent version. Option .NET agent version Standard distributed tracing 8.6.45.0 or higher With W3C Trace Context: 8.27.139.0 or higher Infinite Tracing 8.30.0 (includes W3C Trace Context) Supported environments: .NET Framework 4.5 or higher .NET Core 2.0 or higher Node.js Install or update to the required Node.js agent version. For best results, update to the latest Node.js agent version. Option Node.js agent version Standard distributed tracing 4.7.0 or higher With W3C Trace Context: 6.4 or higher Infinite Tracing 7.3.0 (includes W3C Trace Context) Supported environments: Node version 10.10.0 or higher PHP Install or update to the required PHP agent version. For best results, update to the latest PHP agent version. Option PHP agent version Standard distributed tracing 8.4 or higher With W3C Trace Context: 9.8 or higher Infinite Tracing 9.12.0.268 or higher Python Install or update to the required Python agent version. For best results, update to the latest Python agent version. Option Python agent version Standard distributed tracing 4.2.0.100 or higher With W3C Trace Context: 5.6 or higher Infinite Tracing 5.12.0.140 (includes W3C Trace Context) Supported environments: CPython only (pypy is unsupported) Ruby Install or update to the required Ruby agent version. For best results, update to the latest Ruby agent version. Also install the additional Ruby agent gem for Infinite Tracing. Option Ruby agent version Standard distributed tracing newrelic_rpm 5.3.0.346 or higher With W3C Trace Context: newrelic_rpm 6.9 or higher Infinite Tracing newrelic_rpm 6.11.0.365 or higher (includes W3C Trace Context) newrelic-infinite_tracing 6.11.0.375 or higher Configure standard distributed tracing for your older agents Distributed tracing is enabled through configuration settings. Review the following agent-specific sections. For general help with agent configurations, see Configure the agent. Important Server-side configuration is not available for Infinite Tracing. C SDK Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your C applications. Type Required configuration Standard distributed tracing Configuration options: newrelic_app_config_t structure: newrelic_app_config_t* config; config = newrelic_create_app_config(app_name, license_key); config->distributed_tracing.enabled = true; Copy Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Standard distributed tracing Configuration options: ConfigOption structure: newrelic.NewApplication( newrelic.ConfigAppName(\"Example App\"), newrelic.ConfigLicense(os.Getenv(\"NEW_RELIC_LICENSE_KEY\")), newrelic.ConfigDistributedTracerEnabled(true), ) Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> </configuration> Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Standard distributed tracing Configuration file (newrelic.ini): distributed_tracing.enabled = true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Tip If you need help with proxy configuration, see Proxy support. Manual instrumentation (If automatic instrumentation doesn't work) Recommendation: Before performing any custom instrumentation, read: How distributed tracing works Troubleshoot missing data If a service is not passing the trace header to other services, you can use the distributed tracing payload APIs to instrument the calling service and the called service. The calling service uses an API call to generate a payload, which is accepted by the called service. Instrument the calling service To instrument the calling service: Ensure the version of the APM agent that monitors the calling service supports distributed tracing. Invoke the agent API call for generating a distributed trace payload: C SDK | Go | Java | .NET | Node.js | PHP | Python | Ruby. Important To maintain proper ordering of spans in a trace, ensure you generate the payload in the context of the span that sends it. Add that payload to the call made to the destination service (for example, in a header). (Optional) Identify the call as an external call: C SDK Go Java .NET: n/a Node.js PHP: n/a Python Ruby Instrument the called service To instrument the called service: Ensure the version of the APM agent that monitors the called service supports distributed tracing. If the New Relic agent on the called service does not identify a New Relic transaction, use the agent API to declare a transaction: C SDK One way to tell that a transaction is not in progress: when newrelic_create_distributed_trace_payload() is called, a NULL pointer is returned. To solve this problem, follow the procedures to create a transaction with the C SDK. Go One way to tell that a transaction is not in progress: when Transaction.InsertDistributedTraceHeaders(h http.Header) is called, no headers are inserted. To create a transaction, see Instrument Go transactions. Java One way to tell that a transaction is not in progress: when Transaction.insertDistributedTraceHeaders(Headers) is called, no headers are inserted (this API requires agent 6.4.0+). To create a transaction, see Java agent transaction-related APIs. .NET One way to tell that a transaction is not in progress: CreateDistributedTracePayload() returns an empty payload. To create a transaction, see Introduction to .NET custom instrumentation. Node.js One way to tell that a transaction is not in progress: the Node.js agent logs will report an error similar to this: No transaction found when calling Transaction.acceptDistributedTracePayload. Copy Use startWebTransaction to create a web transaction or startBackgroundTransaction to capture a non-web transaction. PHP One way to tell that a transaction is not in progress: newrelic_insert_distributed_trace_headers() returns false. To create a transaction, see newrelic_start_transaction. Python To tell that a transaction is not in progress: when transaction = current_transaction() is run, transaction is None. Or, if result = accept_distributed_trace_payload(payload) is run, then the result is False. Use background_task to report a non-web transaction. For more on Python instrumentation, see Monitor transactions and segments. Ruby If you are using a Rack-based web framework and have enabled New Relic's Rack instrumentation, the Ruby agent will handle starting a transaction for you. For other use cases, see the add_transaction_tracer API method. Extract the payload from the call that you received (for example, in a header). Invoke the call for accepting the payload: C SDK | Go | Java | .NET | PHP | Node.js | Python | Ruby.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 324.79187,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Language agents and <em>distributed</em> <em>tracing</em>",
        "sections": "Language agents and <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>tracing</em> UI Query <em>distributed</em> <em>trace</em> data Set up <em>Infinite</em> <em>Tracing</em> (advanced option) Standard <em>distributed</em> <em>tracing</em> for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up <em>Infinite</em> <em>Tracing</em>. This alternative"
      },
      "id": "6072a66564441fb28e9d8595"
    },
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.10156,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> API general requirements and limits ",
        "sections": "<em>Trace</em> API general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "-format <em>trace</em> data Endpoints All <em>trace</em> data is sent via a POST to a <em>Trace</em> API endpoint. We have a few endpoints, depending on your setup: Default <em>Trace</em> API endpoint: https:&#x2F;&#x2F;<em>trace</em>-api.newrelic.com&#x2F;<em>trace</em>&#x2F;v1 EU data centers: https:&#x2F;&#x2F;<em>trace</em>-api.eu.newrelic.com&#x2F;<em>trace</em>&#x2F;v1 (see other EU endpoints). <em>Infinite</em>"
      },
      "id": "6071cf7628ccbcf8b851c158"
    }
  ],
  "/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing": [
    {
      "sections": [
        "Language agents and distributed tracing",
        "Tip",
        "Quick start for standard distributed tracing (recommended):",
        "Step 1. Identify services",
        "Step 2. Instrument each service with an APM agent",
        "Step 3. View traces",
        "View traces that include a specific service",
        "View traces across accounts",
        "Set up Infinite Tracing (advanced option)",
        "Step 1. Complete the instrumentation for standard distributed tracing in the quick start above",
        "Step 2. Set up the trace observer",
        "Step 3: Configure the agent for Infinite Tracing",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Step 4. (Optional) Customize Infinite Tracing",
        "Options for older APM agents",
        "Compatibility guide",
        "Important",
        "Configure standard distributed tracing for your older agents",
        "Manual instrumentation (If automatic instrumentation doesn't work)",
        "Instrument the calling service",
        "Instrument the called service"
      ],
      "title": "Language agents and distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "b87eacf981bfae09990c95604ba3b7fc19741a40",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/language-agents-enable-distributed-tracing/",
      "published_at": "2021-05-05T19:59:35Z",
      "updated_at": "2021-04-22T06:39:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic has APM language agents for C, Go, Java, Node.js, .NET, PHP, Python, and Ruby. Each of these offers several ways to leverage the power of distributed tracing: Quick start for standard distributed tracing (recommended): A fast way to get started Infinite Tracing: An advanced alternative to standard distributed tracing Older APM agents: Tracing options if you have older APM agents Manual instrumentation: Tips if automatic instrumentation doesn't work Tip If you want to get more background before getting started, check out these topics: How span sampling works explains distributed tracing options. Impacts to APM tells you what to expect if you are a current APM user but haven't set up distributed tracing. Quick start for standard distributed tracing (recommended): This is the best approach to set up standard distributed tracing if you haven't installed any New Relic APM agents for your services yet, or if you want to instrument additional services. Tip You'll need a New Relic account to set up distributed tracing. If you don't already have one, you can quickly create a free account. Step 1. Identify services Figure out which services you want to instrument so they each send trace data to New Relic. Step 2. Instrument each service with an APM agent We have installation assistants for a variety of languages to help you instrument each service. You should run the installation assistant for each service you want to instrument to ensure that each installation has a unique application name. To start the assistant, click the link for your language: New Relic APM: C New Relic APM: Golang New Relic APM: Java New Relic APM: .NET New Relic APM: Node.js New Relic APM: PHP New Relic APM: Python New Relic APM: Ruby Tip This quick-start approach with the installation assistant automatically enables distributed tracing for each service you run it on, but if you already have a New Relic APM agent that you want to participate in distributed tracing, you'll need to manually enable distributed tracing. See Options for older APM agents. Step 3. View traces After you instrument each of your services with APM agents, generate some traffic in your application so we can capture some traces. Here are two ways to view your traces in the UI: View traces that include a specific service Here's one way you can see traces for a particular service: Go to one.newrelic.com. Click APM in the top menu bar. Click your service. In the left navigation's Monitor section, click Distributed tracing. If you don't see the traces you want, you can filter by the trace.id. View traces across accounts This option allows you to search all traces across all New Relic accounts in your organization that you have access to. Go to one.newrelic.com. Click Browse data in the top menu bar, and then click Traces. Select your entity in the left pane. If you don't see the traces you want, you can filter by the trace.id. For more help finding your traces in the UI: Understand and use the distributed tracing UI Query distributed trace data Set up Infinite Tracing (advanced option) Standard distributed tracing for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up Infinite Tracing. This alternative to standard distributed tracing is available for all APM language agents except C SDK. Tip To learn more about this feature, see Infinite Tracing. Before beginning, first ensure you meet the requirements. Step 1. Complete the instrumentation for standard distributed tracing in the quick start above The Infinite Tracing setup builds on the instrumentation step from the Quick start for standard distributed tracing. Step 2. Set up the trace observer The trace observer is a New Relic AWS-based service that collects and analyzes all your traces. Follow the instructions in Set up trace observer. When you're done, return here with your trace observer information and continue with the next step to configure the agent. Step 3: Configure the agent for Infinite Tracing Infinite Tracing configuration settings include the standard distributed tracing plus information about the trace observer. Find the settings for your language agent below: C SDK Infinite tracing is not available for C SDK. Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Infinite Tracing Configuration options: newrelic.Config structure: app, err := newrelic.NewApplication( newrelic.ConfigAppName(YOUR_APP_NAME), newrelic.ConfigLicense(YOUR_LICENSE_KEY), func(cfg *newrelic.Config) { cfg.DistributedTracer.Enabled = true cfg.InfiniteTracing.TraceObserver.Host = YOUR_TRACE_OBSERVER_HOST }, ) Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=YOUR_TRACE_OBSERVER_HOST Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> <infiniteTracing> <trace_observer host=\"YOUR_TRACE_OBSERVER_HOST\" /> </infiniteTracing> </configuration> Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } infinite_tracing: { trace_observer: { host: 'YOUR_TRACE_OBSERVER_HOST' } } Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true newrelic.span_events_enabled = true newrelic.infinite_tracing.trace_observer.host= \"YOUR_TRACE_OBSERVER_HOST\" Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Infinite Tracing Pull down the libraries with this installation command, and then set up the configuration file or environment variables: pip install newrelic[infinite-tracing] Copy Configuration options: Configuration file (newrelic.ini): distributed_tracing.enabled = true infinite_tracing.trace_observer_host= YOUR_TRACE_OBSERVER_HOST Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: 'YOUR_TRACE_OBSERVER_HOST' Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Step 4. (Optional) Customize Infinite Tracing After you add the agent configuration settings, you should start seeing data in the New Relic UI. After you spend some time analyzing your data, you may want to adjust some of the features of Infinite Tracing: Configure trace observer monitoring Configure span attribute trace filter Configure random trace filter Options for older APM agents If you have older APM agents, use this section to figure out if the distributed tracing features you want are supported. Following the compatibility information is a section showing the basic configuration settings to turn on standard distributed tracing. If your older agent supports Infinite Tracing and you want to set it up, see the steps above. Compatibility guide Find your language agents below to confirm if you can use your existing agents with distributed tracing: C SDK Install (compile) or update to the required C SDK version. For best results, update to the latest C SDK version. Option C SDK version Standard distributed tracing 1.1.0 or higher (W3C Trace Context not available) Infinite Tracing Not available Go Install or update to the required Go agent version. For best results, update to the latest Go agent version. Option Go agent version Standard distributed tracing 2.1.0 or higher With W3C Trace Context: 3.1.0 or higher Infinite Tracing v3.5.0 (includes W3C Trace Context) Supported environments: Go 1.9 or higher Java Install or update to the required Java agent version. For best results, update to the latest Java agent version. Important Your JVM's networkaddress.cache.ttl security setting must not be set to forever (i.e. -1). For more information about this networking property, please visit the Oracle Network Properties docs. Type Java agent version Standard distributed tracing 4.3.0 or higher With W3C Trace Context: 5.10 or higher Infinite Tracing 5.12.1 or higher (includes W3C Trace Context) Supported environments: Java 8: Update 252 or higher All versions of Java 9 or higher Tip For special considerations, see Infinite Tracing: Configuring SSL for Java 7 and 8. .NET Install or update to the required .NET agent version. For best results, update to the latest .NET agent version. Option .NET agent version Standard distributed tracing 8.6.45.0 or higher With W3C Trace Context: 8.27.139.0 or higher Infinite Tracing 8.30.0 (includes W3C Trace Context) Supported environments: .NET Framework 4.5 or higher .NET Core 2.0 or higher Node.js Install or update to the required Node.js agent version. For best results, update to the latest Node.js agent version. Option Node.js agent version Standard distributed tracing 4.7.0 or higher With W3C Trace Context: 6.4 or higher Infinite Tracing 7.3.0 (includes W3C Trace Context) Supported environments: Node version 10.10.0 or higher PHP Install or update to the required PHP agent version. For best results, update to the latest PHP agent version. Option PHP agent version Standard distributed tracing 8.4 or higher With W3C Trace Context: 9.8 or higher Infinite Tracing 9.12.0.268 or higher Python Install or update to the required Python agent version. For best results, update to the latest Python agent version. Option Python agent version Standard distributed tracing 4.2.0.100 or higher With W3C Trace Context: 5.6 or higher Infinite Tracing 5.12.0.140 (includes W3C Trace Context) Supported environments: CPython only (pypy is unsupported) Ruby Install or update to the required Ruby agent version. For best results, update to the latest Ruby agent version. Also install the additional Ruby agent gem for Infinite Tracing. Option Ruby agent version Standard distributed tracing newrelic_rpm 5.3.0.346 or higher With W3C Trace Context: newrelic_rpm 6.9 or higher Infinite Tracing newrelic_rpm 6.11.0.365 or higher (includes W3C Trace Context) newrelic-infinite_tracing 6.11.0.375 or higher Configure standard distributed tracing for your older agents Distributed tracing is enabled through configuration settings. Review the following agent-specific sections. For general help with agent configurations, see Configure the agent. Important Server-side configuration is not available for Infinite Tracing. C SDK Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your C applications. Type Required configuration Standard distributed tracing Configuration options: newrelic_app_config_t structure: newrelic_app_config_t* config; config = newrelic_create_app_config(app_name, license_key); config->distributed_tracing.enabled = true; Copy Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Standard distributed tracing Configuration options: ConfigOption structure: newrelic.NewApplication( newrelic.ConfigAppName(\"Example App\"), newrelic.ConfigLicense(os.Getenv(\"NEW_RELIC_LICENSE_KEY\")), newrelic.ConfigDistributedTracerEnabled(true), ) Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> </configuration> Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Standard distributed tracing Configuration file (newrelic.ini): distributed_tracing.enabled = true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Tip If you need help with proxy configuration, see Proxy support. Manual instrumentation (If automatic instrumentation doesn't work) Recommendation: Before performing any custom instrumentation, read: How distributed tracing works Troubleshoot missing data If a service is not passing the trace header to other services, you can use the distributed tracing payload APIs to instrument the calling service and the called service. The calling service uses an API call to generate a payload, which is accepted by the called service. Instrument the calling service To instrument the calling service: Ensure the version of the APM agent that monitors the calling service supports distributed tracing. Invoke the agent API call for generating a distributed trace payload: C SDK | Go | Java | .NET | Node.js | PHP | Python | Ruby. Important To maintain proper ordering of spans in a trace, ensure you generate the payload in the context of the span that sends it. Add that payload to the call made to the destination service (for example, in a header). (Optional) Identify the call as an external call: C SDK Go Java .NET: n/a Node.js PHP: n/a Python Ruby Instrument the called service To instrument the called service: Ensure the version of the APM agent that monitors the called service supports distributed tracing. If the New Relic agent on the called service does not identify a New Relic transaction, use the agent API to declare a transaction: C SDK One way to tell that a transaction is not in progress: when newrelic_create_distributed_trace_payload() is called, a NULL pointer is returned. To solve this problem, follow the procedures to create a transaction with the C SDK. Go One way to tell that a transaction is not in progress: when Transaction.InsertDistributedTraceHeaders(h http.Header) is called, no headers are inserted. To create a transaction, see Instrument Go transactions. Java One way to tell that a transaction is not in progress: when Transaction.insertDistributedTraceHeaders(Headers) is called, no headers are inserted (this API requires agent 6.4.0+). To create a transaction, see Java agent transaction-related APIs. .NET One way to tell that a transaction is not in progress: CreateDistributedTracePayload() returns an empty payload. To create a transaction, see Introduction to .NET custom instrumentation. Node.js One way to tell that a transaction is not in progress: the Node.js agent logs will report an error similar to this: No transaction found when calling Transaction.acceptDistributedTracePayload. Copy Use startWebTransaction to create a web transaction or startBackgroundTransaction to capture a non-web transaction. PHP One way to tell that a transaction is not in progress: newrelic_insert_distributed_trace_headers() returns false. To create a transaction, see newrelic_start_transaction. Python To tell that a transaction is not in progress: when transaction = current_transaction() is run, transaction is None. Or, if result = accept_distributed_trace_payload(payload) is run, then the result is False. Use background_task to report a non-web transaction. For more on Python instrumentation, see Monitor transactions and segments. Ruby If you are using a Rack-based web framework and have enabled New Relic's Rack instrumentation, the Ruby agent will handle starting a transaction for you. For other use cases, see the add_transaction_tracer API method. Extract the payload from the call that you received (for example, in a header). Invoke the call for accepting the payload: C SDK | Go | Java | .NET | PHP | Node.js | Python | Ruby.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 324.7918,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Language agents and <em>distributed</em> <em>tracing</em>",
        "sections": "Language agents and <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>tracing</em> UI Query <em>distributed</em> <em>trace</em> data Set up <em>Infinite</em> <em>Tracing</em> (advanced option) Standard <em>distributed</em> <em>tracing</em> for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up <em>Infinite</em> <em>Tracing</em>. This alternative"
      },
      "id": "6072a66564441fb28e9d8595"
    },
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.10135,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> API general requirements and limits ",
        "sections": "<em>Trace</em> API general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "-format <em>trace</em> data Endpoints All <em>trace</em> data is sent via a POST to a <em>Trace</em> API endpoint. We have a few endpoints, depending on your setup: Default <em>Trace</em> API endpoint: https:&#x2F;&#x2F;<em>trace</em>-api.newrelic.com&#x2F;<em>trace</em>&#x2F;v1 EU data centers: https:&#x2F;&#x2F;<em>trace</em>-api.eu.newrelic.com&#x2F;<em>trace</em>&#x2F;v1 (see other EU endpoints). <em>Infinite</em>"
      },
      "id": "6071cf7628ccbcf8b851c158"
    },
    {
      "sections": [
        "Infinite Tracing: trace configuration conflicts",
        "Problem"
      ],
      "title": "Infinite Tracing: trace configuration conflicts",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Troubleshooting"
      ],
      "external_id": "b5f9f72d5adf410a972ef089484d53ffa355aa65",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/troubleshooting/infinite-tracing-trace-configuration-conflicts/",
      "published_at": "2021-05-05T18:31:59Z",
      "updated_at": "2021-04-11T07:38:14Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Traces with configuration conflicts are caused when you've enabled Infinite Tracing for one or more services in a trace, but other services in that trace have our standard (non-Infinite Tracing) distributed tracing solutions enabled. You can solve this problem by enabling Infinite Tracing for all services in a trace. A tracing configuration conflict can cause issues like: Fragmented traces and orphaned spans. Incomplete Infinite Tracing metrics due to not taking standard trace sampling into account. Discrepancies for metrics like span count, service count, durations, and error count. Confusing search results. For example, for standard tracing spans that send headers to Infinite Tracing-instrumented services, those spans may show in the trace list but not in the trace waterfall view. Missing traces for monitored browser apps and mobile apps. Because Infinite Tracing isn’t yet available for browser or mobile monitoring, spans from these services won’t show up in the trace waterfall when they make requests to Infinite Tracing-enabled services. For traces with a configuration conflict, we’ll display only the Infinite Tracing data in the UI because that data is of higher quality. (All tracing data is queryable via NRQL.)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 296.06784,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infinite</em> <em>Tracing</em>: <em>trace</em> configuration conflicts",
        "sections": "<em>Infinite</em> <em>Tracing</em>: <em>trace</em> configuration conflicts",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "Problem Traces with configuration conflicts are caused when you&#x27;ve enabled <em>Infinite</em> <em>Tracing</em> for one or more services in a <em>trace</em>, but other services in that <em>trace</em> have our standard (non-<em>Infinite</em> <em>Tracing</em>) <em>distributed</em> <em>tracing</em> solutions enabled. You can solve this problem by enabling <em>Infinite</em> <em>Tracing</em>"
      },
      "id": "6072a76664441fe9c09d859a"
    }
  ],
  "/docs/distributed-tracing/infinite-tracing/set-trace-observer": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 454.554,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "sections": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, <em>Infinite</em> <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about <em>Infinite</em> <em>Tracing</em>: <em>Infinite</em>"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Language agents and distributed tracing",
        "Tip",
        "Quick start for standard distributed tracing (recommended):",
        "Step 1. Identify services",
        "Step 2. Instrument each service with an APM agent",
        "Step 3. View traces",
        "View traces that include a specific service",
        "View traces across accounts",
        "Set up Infinite Tracing (advanced option)",
        "Step 1. Complete the instrumentation for standard distributed tracing in the quick start above",
        "Step 2. Set up the trace observer",
        "Step 3: Configure the agent for Infinite Tracing",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Step 4. (Optional) Customize Infinite Tracing",
        "Options for older APM agents",
        "Compatibility guide",
        "Important",
        "Configure standard distributed tracing for your older agents",
        "Manual instrumentation (If automatic instrumentation doesn't work)",
        "Instrument the calling service",
        "Instrument the called service"
      ],
      "title": "Language agents and distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "b87eacf981bfae09990c95604ba3b7fc19741a40",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/language-agents-enable-distributed-tracing/",
      "published_at": "2021-05-05T19:59:35Z",
      "updated_at": "2021-04-22T06:39:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic has APM language agents for C, Go, Java, Node.js, .NET, PHP, Python, and Ruby. Each of these offers several ways to leverage the power of distributed tracing: Quick start for standard distributed tracing (recommended): A fast way to get started Infinite Tracing: An advanced alternative to standard distributed tracing Older APM agents: Tracing options if you have older APM agents Manual instrumentation: Tips if automatic instrumentation doesn't work Tip If you want to get more background before getting started, check out these topics: How span sampling works explains distributed tracing options. Impacts to APM tells you what to expect if you are a current APM user but haven't set up distributed tracing. Quick start for standard distributed tracing (recommended): This is the best approach to set up standard distributed tracing if you haven't installed any New Relic APM agents for your services yet, or if you want to instrument additional services. Tip You'll need a New Relic account to set up distributed tracing. If you don't already have one, you can quickly create a free account. Step 1. Identify services Figure out which services you want to instrument so they each send trace data to New Relic. Step 2. Instrument each service with an APM agent We have installation assistants for a variety of languages to help you instrument each service. You should run the installation assistant for each service you want to instrument to ensure that each installation has a unique application name. To start the assistant, click the link for your language: New Relic APM: C New Relic APM: Golang New Relic APM: Java New Relic APM: .NET New Relic APM: Node.js New Relic APM: PHP New Relic APM: Python New Relic APM: Ruby Tip This quick-start approach with the installation assistant automatically enables distributed tracing for each service you run it on, but if you already have a New Relic APM agent that you want to participate in distributed tracing, you'll need to manually enable distributed tracing. See Options for older APM agents. Step 3. View traces After you instrument each of your services with APM agents, generate some traffic in your application so we can capture some traces. Here are two ways to view your traces in the UI: View traces that include a specific service Here's one way you can see traces for a particular service: Go to one.newrelic.com. Click APM in the top menu bar. Click your service. In the left navigation's Monitor section, click Distributed tracing. If you don't see the traces you want, you can filter by the trace.id. View traces across accounts This option allows you to search all traces across all New Relic accounts in your organization that you have access to. Go to one.newrelic.com. Click Browse data in the top menu bar, and then click Traces. Select your entity in the left pane. If you don't see the traces you want, you can filter by the trace.id. For more help finding your traces in the UI: Understand and use the distributed tracing UI Query distributed trace data Set up Infinite Tracing (advanced option) Standard distributed tracing for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up Infinite Tracing. This alternative to standard distributed tracing is available for all APM language agents except C SDK. Tip To learn more about this feature, see Infinite Tracing. Before beginning, first ensure you meet the requirements. Step 1. Complete the instrumentation for standard distributed tracing in the quick start above The Infinite Tracing setup builds on the instrumentation step from the Quick start for standard distributed tracing. Step 2. Set up the trace observer The trace observer is a New Relic AWS-based service that collects and analyzes all your traces. Follow the instructions in Set up trace observer. When you're done, return here with your trace observer information and continue with the next step to configure the agent. Step 3: Configure the agent for Infinite Tracing Infinite Tracing configuration settings include the standard distributed tracing plus information about the trace observer. Find the settings for your language agent below: C SDK Infinite tracing is not available for C SDK. Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Infinite Tracing Configuration options: newrelic.Config structure: app, err := newrelic.NewApplication( newrelic.ConfigAppName(YOUR_APP_NAME), newrelic.ConfigLicense(YOUR_LICENSE_KEY), func(cfg *newrelic.Config) { cfg.DistributedTracer.Enabled = true cfg.InfiniteTracing.TraceObserver.Host = YOUR_TRACE_OBSERVER_HOST }, ) Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=YOUR_TRACE_OBSERVER_HOST Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> <infiniteTracing> <trace_observer host=\"YOUR_TRACE_OBSERVER_HOST\" /> </infiniteTracing> </configuration> Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } infinite_tracing: { trace_observer: { host: 'YOUR_TRACE_OBSERVER_HOST' } } Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true newrelic.span_events_enabled = true newrelic.infinite_tracing.trace_observer.host= \"YOUR_TRACE_OBSERVER_HOST\" Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Infinite Tracing Pull down the libraries with this installation command, and then set up the configuration file or environment variables: pip install newrelic[infinite-tracing] Copy Configuration options: Configuration file (newrelic.ini): distributed_tracing.enabled = true infinite_tracing.trace_observer_host= YOUR_TRACE_OBSERVER_HOST Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: 'YOUR_TRACE_OBSERVER_HOST' Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Step 4. (Optional) Customize Infinite Tracing After you add the agent configuration settings, you should start seeing data in the New Relic UI. After you spend some time analyzing your data, you may want to adjust some of the features of Infinite Tracing: Configure trace observer monitoring Configure span attribute trace filter Configure random trace filter Options for older APM agents If you have older APM agents, use this section to figure out if the distributed tracing features you want are supported. Following the compatibility information is a section showing the basic configuration settings to turn on standard distributed tracing. If your older agent supports Infinite Tracing and you want to set it up, see the steps above. Compatibility guide Find your language agents below to confirm if you can use your existing agents with distributed tracing: C SDK Install (compile) or update to the required C SDK version. For best results, update to the latest C SDK version. Option C SDK version Standard distributed tracing 1.1.0 or higher (W3C Trace Context not available) Infinite Tracing Not available Go Install or update to the required Go agent version. For best results, update to the latest Go agent version. Option Go agent version Standard distributed tracing 2.1.0 or higher With W3C Trace Context: 3.1.0 or higher Infinite Tracing v3.5.0 (includes W3C Trace Context) Supported environments: Go 1.9 or higher Java Install or update to the required Java agent version. For best results, update to the latest Java agent version. Important Your JVM's networkaddress.cache.ttl security setting must not be set to forever (i.e. -1). For more information about this networking property, please visit the Oracle Network Properties docs. Type Java agent version Standard distributed tracing 4.3.0 or higher With W3C Trace Context: 5.10 or higher Infinite Tracing 5.12.1 or higher (includes W3C Trace Context) Supported environments: Java 8: Update 252 or higher All versions of Java 9 or higher Tip For special considerations, see Infinite Tracing: Configuring SSL for Java 7 and 8. .NET Install or update to the required .NET agent version. For best results, update to the latest .NET agent version. Option .NET agent version Standard distributed tracing 8.6.45.0 or higher With W3C Trace Context: 8.27.139.0 or higher Infinite Tracing 8.30.0 (includes W3C Trace Context) Supported environments: .NET Framework 4.5 or higher .NET Core 2.0 or higher Node.js Install or update to the required Node.js agent version. For best results, update to the latest Node.js agent version. Option Node.js agent version Standard distributed tracing 4.7.0 or higher With W3C Trace Context: 6.4 or higher Infinite Tracing 7.3.0 (includes W3C Trace Context) Supported environments: Node version 10.10.0 or higher PHP Install or update to the required PHP agent version. For best results, update to the latest PHP agent version. Option PHP agent version Standard distributed tracing 8.4 or higher With W3C Trace Context: 9.8 or higher Infinite Tracing 9.12.0.268 or higher Python Install or update to the required Python agent version. For best results, update to the latest Python agent version. Option Python agent version Standard distributed tracing 4.2.0.100 or higher With W3C Trace Context: 5.6 or higher Infinite Tracing 5.12.0.140 (includes W3C Trace Context) Supported environments: CPython only (pypy is unsupported) Ruby Install or update to the required Ruby agent version. For best results, update to the latest Ruby agent version. Also install the additional Ruby agent gem for Infinite Tracing. Option Ruby agent version Standard distributed tracing newrelic_rpm 5.3.0.346 or higher With W3C Trace Context: newrelic_rpm 6.9 or higher Infinite Tracing newrelic_rpm 6.11.0.365 or higher (includes W3C Trace Context) newrelic-infinite_tracing 6.11.0.375 or higher Configure standard distributed tracing for your older agents Distributed tracing is enabled through configuration settings. Review the following agent-specific sections. For general help with agent configurations, see Configure the agent. Important Server-side configuration is not available for Infinite Tracing. C SDK Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your C applications. Type Required configuration Standard distributed tracing Configuration options: newrelic_app_config_t structure: newrelic_app_config_t* config; config = newrelic_create_app_config(app_name, license_key); config->distributed_tracing.enabled = true; Copy Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Standard distributed tracing Configuration options: ConfigOption structure: newrelic.NewApplication( newrelic.ConfigAppName(\"Example App\"), newrelic.ConfigLicense(os.Getenv(\"NEW_RELIC_LICENSE_KEY\")), newrelic.ConfigDistributedTracerEnabled(true), ) Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> </configuration> Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Standard distributed tracing Configuration file (newrelic.ini): distributed_tracing.enabled = true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Tip If you need help with proxy configuration, see Proxy support. Manual instrumentation (If automatic instrumentation doesn't work) Recommendation: Before performing any custom instrumentation, read: How distributed tracing works Troubleshoot missing data If a service is not passing the trace header to other services, you can use the distributed tracing payload APIs to instrument the calling service and the called service. The calling service uses an API call to generate a payload, which is accepted by the called service. Instrument the calling service To instrument the calling service: Ensure the version of the APM agent that monitors the calling service supports distributed tracing. Invoke the agent API call for generating a distributed trace payload: C SDK | Go | Java | .NET | Node.js | PHP | Python | Ruby. Important To maintain proper ordering of spans in a trace, ensure you generate the payload in the context of the span that sends it. Add that payload to the call made to the destination service (for example, in a header). (Optional) Identify the call as an external call: C SDK Go Java .NET: n/a Node.js PHP: n/a Python Ruby Instrument the called service To instrument the called service: Ensure the version of the APM agent that monitors the called service supports distributed tracing. If the New Relic agent on the called service does not identify a New Relic transaction, use the agent API to declare a transaction: C SDK One way to tell that a transaction is not in progress: when newrelic_create_distributed_trace_payload() is called, a NULL pointer is returned. To solve this problem, follow the procedures to create a transaction with the C SDK. Go One way to tell that a transaction is not in progress: when Transaction.InsertDistributedTraceHeaders(h http.Header) is called, no headers are inserted. To create a transaction, see Instrument Go transactions. Java One way to tell that a transaction is not in progress: when Transaction.insertDistributedTraceHeaders(Headers) is called, no headers are inserted (this API requires agent 6.4.0+). To create a transaction, see Java agent transaction-related APIs. .NET One way to tell that a transaction is not in progress: CreateDistributedTracePayload() returns an empty payload. To create a transaction, see Introduction to .NET custom instrumentation. Node.js One way to tell that a transaction is not in progress: the Node.js agent logs will report an error similar to this: No transaction found when calling Transaction.acceptDistributedTracePayload. Copy Use startWebTransaction to create a web transaction or startBackgroundTransaction to capture a non-web transaction. PHP One way to tell that a transaction is not in progress: newrelic_insert_distributed_trace_headers() returns false. To create a transaction, see newrelic_start_transaction. Python To tell that a transaction is not in progress: when transaction = current_transaction() is run, transaction is None. Or, if result = accept_distributed_trace_payload(payload) is run, then the result is False. Use background_task to report a non-web transaction. For more on Python instrumentation, see Monitor transactions and segments. Ruby If you are using a Rack-based web framework and have enabled New Relic's Rack instrumentation, the Ruby agent will handle starting a transaction for you. For other use cases, see the add_transaction_tracer API method. Extract the payload from the call that you received (for example, in a header). Invoke the call for accepting the payload: C SDK | Go | Java | .NET | PHP | Node.js | Python | Ruby.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 324.7918,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Language agents and <em>distributed</em> <em>tracing</em>",
        "sections": "Language agents and <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>tracing</em> UI Query <em>distributed</em> <em>trace</em> data Set up <em>Infinite</em> <em>Tracing</em> (advanced option) Standard <em>distributed</em> <em>tracing</em> for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up <em>Infinite</em> <em>Tracing</em>. This alternative"
      },
      "id": "6072a66564441fb28e9d8595"
    },
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.10135,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> API general requirements and limits ",
        "sections": "<em>Trace</em> API general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "-format <em>trace</em> data Endpoints All <em>trace</em> data is sent via a POST to a <em>Trace</em> API endpoint. We have a few endpoints, depending on your setup: Default <em>Trace</em> API endpoint: https:&#x2F;&#x2F;<em>trace</em>-api.newrelic.com&#x2F;<em>trace</em>&#x2F;v1 EU data centers: https:&#x2F;&#x2F;<em>trace</em>-api.eu.newrelic.com&#x2F;<em>trace</em>&#x2F;v1 (see other EU endpoints). <em>Infinite</em>"
      },
      "id": "6071cf7628ccbcf8b851c158"
    }
  ],
  "/docs/distributed-tracing/other-requirements/infinite-tracing-configuring-ssl-java-7-8": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 454.55368,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "sections": "Introduction to <em>Infinite</em> <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, <em>Infinite</em> <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about <em>Infinite</em> <em>Tracing</em>: <em>Infinite</em>"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Language agents and distributed tracing",
        "Tip",
        "Quick start for standard distributed tracing (recommended):",
        "Step 1. Identify services",
        "Step 2. Instrument each service with an APM agent",
        "Step 3. View traces",
        "View traces that include a specific service",
        "View traces across accounts",
        "Set up Infinite Tracing (advanced option)",
        "Step 1. Complete the instrumentation for standard distributed tracing in the quick start above",
        "Step 2. Set up the trace observer",
        "Step 3: Configure the agent for Infinite Tracing",
        "C SDK",
        "Go",
        "Java",
        ".NET",
        "Node.js",
        "PHP",
        "Python",
        "Ruby",
        "Step 4. (Optional) Customize Infinite Tracing",
        "Options for older APM agents",
        "Compatibility guide",
        "Important",
        "Configure standard distributed tracing for your older agents",
        "Manual instrumentation (If automatic instrumentation doesn't work)",
        "Instrument the calling service",
        "Instrument the called service"
      ],
      "title": "Language agents and distributed tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Enable and configure"
      ],
      "external_id": "b87eacf981bfae09990c95604ba3b7fc19741a40",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/enable-configure/language-agents-enable-distributed-tracing/",
      "published_at": "2021-05-05T19:59:35Z",
      "updated_at": "2021-04-22T06:39:53Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic has APM language agents for C, Go, Java, Node.js, .NET, PHP, Python, and Ruby. Each of these offers several ways to leverage the power of distributed tracing: Quick start for standard distributed tracing (recommended): A fast way to get started Infinite Tracing: An advanced alternative to standard distributed tracing Older APM agents: Tracing options if you have older APM agents Manual instrumentation: Tips if automatic instrumentation doesn't work Tip If you want to get more background before getting started, check out these topics: How span sampling works explains distributed tracing options. Impacts to APM tells you what to expect if you are a current APM user but haven't set up distributed tracing. Quick start for standard distributed tracing (recommended): This is the best approach to set up standard distributed tracing if you haven't installed any New Relic APM agents for your services yet, or if you want to instrument additional services. Tip You'll need a New Relic account to set up distributed tracing. If you don't already have one, you can quickly create a free account. Step 1. Identify services Figure out which services you want to instrument so they each send trace data to New Relic. Step 2. Instrument each service with an APM agent We have installation assistants for a variety of languages to help you instrument each service. You should run the installation assistant for each service you want to instrument to ensure that each installation has a unique application name. To start the assistant, click the link for your language: New Relic APM: C New Relic APM: Golang New Relic APM: Java New Relic APM: .NET New Relic APM: Node.js New Relic APM: PHP New Relic APM: Python New Relic APM: Ruby Tip This quick-start approach with the installation assistant automatically enables distributed tracing for each service you run it on, but if you already have a New Relic APM agent that you want to participate in distributed tracing, you'll need to manually enable distributed tracing. See Options for older APM agents. Step 3. View traces After you instrument each of your services with APM agents, generate some traffic in your application so we can capture some traces. Here are two ways to view your traces in the UI: View traces that include a specific service Here's one way you can see traces for a particular service: Go to one.newrelic.com. Click APM in the top menu bar. Click your service. In the left navigation's Monitor section, click Distributed tracing. If you don't see the traces you want, you can filter by the trace.id. View traces across accounts This option allows you to search all traces across all New Relic accounts in your organization that you have access to. Go to one.newrelic.com. Click Browse data in the top menu bar, and then click Traces. Select your entity in the left pane. If you don't see the traces you want, you can filter by the trace.id. For more help finding your traces in the UI: Understand and use the distributed tracing UI Query distributed trace data Set up Infinite Tracing (advanced option) Standard distributed tracing for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up Infinite Tracing. This alternative to standard distributed tracing is available for all APM language agents except C SDK. Tip To learn more about this feature, see Infinite Tracing. Before beginning, first ensure you meet the requirements. Step 1. Complete the instrumentation for standard distributed tracing in the quick start above The Infinite Tracing setup builds on the instrumentation step from the Quick start for standard distributed tracing. Step 2. Set up the trace observer The trace observer is a New Relic AWS-based service that collects and analyzes all your traces. Follow the instructions in Set up trace observer. When you're done, return here with your trace observer information and continue with the next step to configure the agent. Step 3: Configure the agent for Infinite Tracing Infinite Tracing configuration settings include the standard distributed tracing plus information about the trace observer. Find the settings for your language agent below: C SDK Infinite tracing is not available for C SDK. Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Infinite Tracing Configuration options: newrelic.Config structure: app, err := newrelic.NewApplication( newrelic.ConfigAppName(YOUR_APP_NAME), newrelic.ConfigLicense(YOUR_LICENSE_KEY), func(cfg *newrelic.Config) { cfg.DistributedTracer.Enabled = true cfg.InfiniteTracing.TraceObserver.Host = YOUR_TRACE_OBSERVER_HOST }, ) Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=YOUR_TRACE_OBSERVER_HOST Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> <infiniteTracing> <trace_observer host=\"YOUR_TRACE_OBSERVER_HOST\" /> </infiniteTracing> </configuration> Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } infinite_tracing: { trace_observer: { host: 'YOUR_TRACE_OBSERVER_HOST' } } Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true newrelic.span_events_enabled = true newrelic.infinite_tracing.trace_observer.host= \"YOUR_TRACE_OBSERVER_HOST\" Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Infinite Tracing Pull down the libraries with this installation command, and then set up the configuration file or environment variables: pip install newrelic[infinite-tracing] Copy Configuration options: Configuration file (newrelic.ini): distributed_tracing.enabled = true infinite_tracing.trace_observer_host= YOUR_TRACE_OBSERVER_HOST Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Infinite Tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: 'YOUR_TRACE_OBSERVER_HOST' Copy Environment variables: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy Step 4. (Optional) Customize Infinite Tracing After you add the agent configuration settings, you should start seeing data in the New Relic UI. After you spend some time analyzing your data, you may want to adjust some of the features of Infinite Tracing: Configure trace observer monitoring Configure span attribute trace filter Configure random trace filter Options for older APM agents If you have older APM agents, use this section to figure out if the distributed tracing features you want are supported. Following the compatibility information is a section showing the basic configuration settings to turn on standard distributed tracing. If your older agent supports Infinite Tracing and you want to set it up, see the steps above. Compatibility guide Find your language agents below to confirm if you can use your existing agents with distributed tracing: C SDK Install (compile) or update to the required C SDK version. For best results, update to the latest C SDK version. Option C SDK version Standard distributed tracing 1.1.0 or higher (W3C Trace Context not available) Infinite Tracing Not available Go Install or update to the required Go agent version. For best results, update to the latest Go agent version. Option Go agent version Standard distributed tracing 2.1.0 or higher With W3C Trace Context: 3.1.0 or higher Infinite Tracing v3.5.0 (includes W3C Trace Context) Supported environments: Go 1.9 or higher Java Install or update to the required Java agent version. For best results, update to the latest Java agent version. Important Your JVM's networkaddress.cache.ttl security setting must not be set to forever (i.e. -1). For more information about this networking property, please visit the Oracle Network Properties docs. Type Java agent version Standard distributed tracing 4.3.0 or higher With W3C Trace Context: 5.10 or higher Infinite Tracing 5.12.1 or higher (includes W3C Trace Context) Supported environments: Java 8: Update 252 or higher All versions of Java 9 or higher Tip For special considerations, see Infinite Tracing: Configuring SSL for Java 7 and 8. .NET Install or update to the required .NET agent version. For best results, update to the latest .NET agent version. Option .NET agent version Standard distributed tracing 8.6.45.0 or higher With W3C Trace Context: 8.27.139.0 or higher Infinite Tracing 8.30.0 (includes W3C Trace Context) Supported environments: .NET Framework 4.5 or higher .NET Core 2.0 or higher Node.js Install or update to the required Node.js agent version. For best results, update to the latest Node.js agent version. Option Node.js agent version Standard distributed tracing 4.7.0 or higher With W3C Trace Context: 6.4 or higher Infinite Tracing 7.3.0 (includes W3C Trace Context) Supported environments: Node version 10.10.0 or higher PHP Install or update to the required PHP agent version. For best results, update to the latest PHP agent version. Option PHP agent version Standard distributed tracing 8.4 or higher With W3C Trace Context: 9.8 or higher Infinite Tracing 9.12.0.268 or higher Python Install or update to the required Python agent version. For best results, update to the latest Python agent version. Option Python agent version Standard distributed tracing 4.2.0.100 or higher With W3C Trace Context: 5.6 or higher Infinite Tracing 5.12.0.140 (includes W3C Trace Context) Supported environments: CPython only (pypy is unsupported) Ruby Install or update to the required Ruby agent version. For best results, update to the latest Ruby agent version. Also install the additional Ruby agent gem for Infinite Tracing. Option Ruby agent version Standard distributed tracing newrelic_rpm 5.3.0.346 or higher With W3C Trace Context: newrelic_rpm 6.9 or higher Infinite Tracing newrelic_rpm 6.11.0.365 or higher (includes W3C Trace Context) newrelic-infinite_tracing 6.11.0.375 or higher Configure standard distributed tracing for your older agents Distributed tracing is enabled through configuration settings. Review the following agent-specific sections. For general help with agent configurations, see Configure the agent. Important Server-side configuration is not available for Infinite Tracing. C SDK Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your C applications. Type Required configuration Standard distributed tracing Configuration options: newrelic_app_config_t structure: newrelic_app_config_t* config; config = newrelic_create_app_config(app_name, license_key); config->distributed_tracing.enabled = true; Copy Go Here's an overview of the settings. For more help with configuration, see Enable distributed tracing for your Go applications. Type Required configuration Standard distributed tracing Configuration options: ConfigOption structure: newrelic.NewApplication( newrelic.ConfigAppName(\"Example App\"), newrelic.ConfigLicense(os.Getenv(\"NEW_RELIC_LICENSE_KEY\")), newrelic.ConfigDistributedTracerEnabled(true), ) Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Java Here's an overview of the settings. For more help with configuration, see Java agent configuration: Config file. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Infinite Tracing Configuration options: Configuration file (newrelic.yml) (indented 2 spaces under the common stanza): distributed_tracing: enabled: true infinite_tracing: trace_observer: host: \"YOUR_TRACE_OBSERVER_HOST\" Copy Java system property: -Dnewrelic.config.distributed_tracing.enabled=true -Dnewrelic.config.infinite_tracing.trace_observer.host=\"YOUR_TRACE_OBSERVER_HOST\" Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true NEW_RELIC_INFINITE_TRACING_TRACE_OBSERVER_HOST=\"YOUR_TRACE_OBSERVER_HOST\" Copy .NET Here's an overview of the settings. For more help with configuration, see .NET agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.config): <configuration . . . > <distributedTracing enabled=\"true\" /> </configuration> Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Node.js Here's an overview of the settings. For more help with configuration, see Node.js agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.js): distributed_tracing: { enabled: true } Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy PHP Here's an overview of the settings. For more help with configuration, see Distributed tracing for the PHP agent Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.ini): newrelic.distributed_tracing_enabled = true Copy Python Here's an overview of the settings. For more help with configuration, see Python agent configuration Type Required configuration Standard distributed tracing Configuration file (newrelic.ini): distributed_tracing.enabled = true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Ruby Here's an overview of the settings. For more help with configuration, see Ruby agent configuration. Type Required configuration Standard distributed tracing Configuration options: Configuration file (newrelic.yml): distributed_tracing: enabled: true Copy Environment variable: NEW_RELIC_DISTRIBUTED_TRACING_ENABLED=true Copy Tip If you need help with proxy configuration, see Proxy support. Manual instrumentation (If automatic instrumentation doesn't work) Recommendation: Before performing any custom instrumentation, read: How distributed tracing works Troubleshoot missing data If a service is not passing the trace header to other services, you can use the distributed tracing payload APIs to instrument the calling service and the called service. The calling service uses an API call to generate a payload, which is accepted by the called service. Instrument the calling service To instrument the calling service: Ensure the version of the APM agent that monitors the calling service supports distributed tracing. Invoke the agent API call for generating a distributed trace payload: C SDK | Go | Java | .NET | Node.js | PHP | Python | Ruby. Important To maintain proper ordering of spans in a trace, ensure you generate the payload in the context of the span that sends it. Add that payload to the call made to the destination service (for example, in a header). (Optional) Identify the call as an external call: C SDK Go Java .NET: n/a Node.js PHP: n/a Python Ruby Instrument the called service To instrument the called service: Ensure the version of the APM agent that monitors the called service supports distributed tracing. If the New Relic agent on the called service does not identify a New Relic transaction, use the agent API to declare a transaction: C SDK One way to tell that a transaction is not in progress: when newrelic_create_distributed_trace_payload() is called, a NULL pointer is returned. To solve this problem, follow the procedures to create a transaction with the C SDK. Go One way to tell that a transaction is not in progress: when Transaction.InsertDistributedTraceHeaders(h http.Header) is called, no headers are inserted. To create a transaction, see Instrument Go transactions. Java One way to tell that a transaction is not in progress: when Transaction.insertDistributedTraceHeaders(Headers) is called, no headers are inserted (this API requires agent 6.4.0+). To create a transaction, see Java agent transaction-related APIs. .NET One way to tell that a transaction is not in progress: CreateDistributedTracePayload() returns an empty payload. To create a transaction, see Introduction to .NET custom instrumentation. Node.js One way to tell that a transaction is not in progress: the Node.js agent logs will report an error similar to this: No transaction found when calling Transaction.acceptDistributedTracePayload. Copy Use startWebTransaction to create a web transaction or startBackgroundTransaction to capture a non-web transaction. PHP One way to tell that a transaction is not in progress: newrelic_insert_distributed_trace_headers() returns false. To create a transaction, see newrelic_start_transaction. Python To tell that a transaction is not in progress: when transaction = current_transaction() is run, transaction is None. Or, if result = accept_distributed_trace_payload(payload) is run, then the result is False. Use background_task to report a non-web transaction. For more on Python instrumentation, see Monitor transactions and segments. Ruby If you are using a Rack-based web framework and have enabled New Relic's Rack instrumentation, the Ruby agent will handle starting a transaction for you. For other use cases, see the add_transaction_tracer API method. Extract the payload from the call that you received (for example, in a header). Invoke the call for accepting the payload: C SDK | Go | Java | .NET | PHP | Node.js | Python | Ruby.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 324.79178,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Language agents and <em>distributed</em> <em>tracing</em>",
        "sections": "Language agents and <em>distributed</em> <em>tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>tracing</em> UI Query <em>distributed</em> <em>trace</em> data Set up <em>Infinite</em> <em>Tracing</em> (advanced option) Standard <em>distributed</em> <em>tracing</em> for APM agents (above) captures up to 10% of your traces, but if you want us to analyze all your data and find the most relevant traces, you can set up <em>Infinite</em> <em>Tracing</em>. This alternative"
      },
      "id": "6072a66564441fb28e9d8595"
    },
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 319.10114,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> API general requirements and limits ",
        "sections": "<em>Trace</em> API general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "-format <em>trace</em> data Endpoints All <em>trace</em> data is sent via a POST to a <em>Trace</em> API endpoint. We have a few endpoints, depending on your setup: Default <em>Trace</em> API endpoint: https:&#x2F;&#x2F;<em>trace</em>-api.newrelic.com&#x2F;<em>trace</em>&#x2F;v1 EU data centers: https:&#x2F;&#x2F;<em>trace</em>-api.eu.newrelic.com&#x2F;<em>trace</em>&#x2F;v1 (see other EU endpoints). <em>Infinite</em>"
      },
      "id": "6071cf7628ccbcf8b851c158"
    }
  ],
  "/docs/distributed-tracing/trace-api/introduction-trace-api": [
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 346.85672,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> <em>API</em> general requirements and limits ",
        "sections": "<em>Trace</em> <em>API</em> general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Tracing</em>: when you complete the <em>Trace</em> observer setup, you get a custom YOUR_<em>TRACE</em>_OBSERVER_URL value to use as an endpoint. If you&#x27;re using an integration that uses the <em>Trace</em> <em>API</em> (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust"
      },
      "id": "6071cf7628ccbcf8b851c158"
    },
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 310.32117,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, Infinite <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about Infinite <em>Tracing</em>: Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Report Zipkin-format traces via Trace API",
        "Zipkin version requirements",
        "Overview of using the Trace API",
        "Send sample Zipkin trace payload",
        "Send Zipkin-format payload",
        "Send data from existing Zipkin instrumentation",
        "Transformation of Zipkin data",
        "Add other tags/attributes"
      ],
      "title": "Report Zipkin-format traces via Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "dba8334d1f068236c741ff04c13ecc2be2c184fc",
      "image": "https://docs.newrelic.com/static/96e69137f0dd86b313ec72d5f0c1ad83/119c7/Screen-Shot-2020-08-13-at-1.26.17-PM.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/report-zipkin-format-traces-trace-api/",
      "published_at": "2021-05-05T20:00:43Z",
      "updated_at": "2021-04-10T16:18:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to create your own tracing implementation, you can use our Trace API. This doc explains how to send Zipkin-format trace data to our Trace API. (For our general data format, see New Relic format.) Zipkin version requirements The Trace API supports data from Zipkin JSON v2 (or higher) without any modification. For details on this version, see Zipkin v2 release details and the Zipkin v2 schema. Overview of using the Trace API Using our Trace API is as simple as: Sending trace data in the expected format (in this case, zipkin format). Sending that data to the appropriate endpoint Our send-data instructions have options for enabling Infinite Tracing. To learn more about this, see Intro to Infinite Tracing and Sampling considerations. To get started using the Trace API, choose an option: Send a sample trace: This shows a curl example of sending a trace to New Relic. This is useful for understanding how the Trace API works, and to verify you're seeing data in New Relic. Report data from existing Zipkin instrumentation: if you have an existing Zipkin implementation, you can simply change the endpoint of where your data gets sent. Send sample Zipkin trace payload This section describes how to send a simple Zipkin-format trace to our Trace API via curl request. You might choose to do this in order to learn how our API works and to verify that data is showing up in New Relic before doing in-depth instrumentation. To get started sending a sample payload: (Optional, to enable Infinite Tracing) First, you must set up a trace observer. That procedure includes instructions for sending a sample trace using our general new-relic format. When you get to that step, return here to instead learn how to send a Zipkin-format trace. Send a Zipkin-format payload following the instructions below. Send Zipkin-format payload To send a sample Zipkin-format trace: Get an Insights insert key: Go to the API keys UI and select Insights insert keys. If you don't already have a key, create a new one by selecting Insert keys +. You'll be executing a curl request, below. Notes on this: Replace the insert key placeholder with your insert key. If you're using Infinite Tracing, use the YOUR_TRACE_OBSERVER_URL value in place of the standard endpoint. If you want to send more than one post, change the trace ID to a different value. Sending the same payload or span id multiple times for the same traceId may result in fragmented traces in the UI. curl -i -H 'Content-Type: application/json' \\ -H 'Api-Key: $NEW_RELIC_INSERT_KEY' \\ -H 'Data-Format: zipkin' \\ -H 'Data-Format-Version: 2' \\ -X POST \\ -d '[ { \"traceId\": \"test-zipkin-trace-id-1\", \"id\": \"3e0f5885710776cd\", \"kind\": \"CLIENT\", \"name\": \"post\", \"duration\": 508068, \"localEndpoint\": { \"serviceName\": \"service-1\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { } }, { \"traceId\": \"test-zipkin-trace-id-1\", \"parentId\": \"3e0f5885710776cd\", \"id\": \"asdf9asdn123lkasdf\", \"kind\": \"CLIENT\", \"name\": \"service 2 span\", \"duration\": 2019, \"localEndpoint\": { \"serviceName\": \"service-2\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { \"error.message\": \"Invalid credentials\" } } ]' 'https://trace-api.newrelic.com/trace/v1' Copy Within a minute, the trace should be available in the our distributed tracing UI. To find it, run a query for the trace.id. In this example, it was test-zipkin-trace-id-1. Note that you search by the transformed attribute of trace.id (not traceId). To learn more: Learn where Trace API data shows up in the UI. Send data from an existing Zipkin instrumentation. Learn how to decorate spans by adding tags. This helps you customize how traces are displayed in our UI for a richer, more helpful experience. Learn about general endpoint information (data limits, required metadata, and response validation). Learn about how Zipkin data is transformed and stored in our format. If you don't see your trace data, see Troubleshooting. Send data from existing Zipkin instrumentation Preliminary notes: If you want to enable Infinite Tracing, you first must set up a trace observer. It can be helpful to first send a sample payload to verify things are working properly. To report data from an existing Zipkin instrumentation, you'll point the Zipkin tracer at the appropriate Trace API endpoint with some required request metadata. You can send the required metadata as headers or query parameters (some Zipkin tracer versions don't allow specifying HTTP headers). Here's an example of what it might look like to create a Zipkin OkHttpSender in Java configured for the Trace API: OkHttpSender.create(\"https://trace-api.newrelic.com/trace/v1?Api-Key=NEW_RELIC_INSERT_API_KEY&Data-Format=zipkin&Data-Format-Version=2\"); Copy Note that if you were using Infinite Tracing, or had an EU-region New Relic account, the endpoint would be different. For an explanation of Api-Key and the other metadata, see Request metadata. Transformation of Zipkin data To create a consistent search/query experience, some Zipkin data will be transformed to match New Relic attribute naming. For more on how we store and structure trace data, see How distributed tracing works. Zipkin tag Stored in New Relic as... Details traceId trace.id Unique identifier for a trace. id id Unique identifier for a span. parentId parent.id Identifier of the upstream span that called the service. kind kind Either Client or Server. name name Name of span. duration duration.ms Zipkin v2 spans must have durations specified in microseconds, and will be converted to milliseconds. localEndpoint: serviceName service.name We use the Zipkin v2 service name to identify the entity that created this span. localEndpoint: port localEndpoint.port All values in the localEndpoint object will be flattened to a span attribute called localEndpoint.key tags reported as attributes Key:value pairs in the tags object in Zipkin v2 will be written as span attributes. annotations not supported We do not currently support annotations in the Trace API. Spans will not be rejected if they contain annotations, but the annotations data will not be written. Add other tags/attributes You can add any tags you want to the tags block, with the exception of the restricted tags. For example, you might want to add attributes like customer.id or user.id to help you analyze your trace data. Tags will be converted to New Relic attributes. To learn how to control how spans appear in New Relic (for example, adding errors or setting a span as a datastore span), see Decorate spans.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.31784,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report Zipkin-format <em>traces</em> via <em>Trace</em> <em>API</em>",
        "sections": "Report Zipkin-format <em>traces</em> via <em>Trace</em> <em>API</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": ", &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;service-2&quot;, &quot;ipv4&quot;: &quot;127.0.0.1&quot;, &quot;port&quot;: 8080 }, &quot;tags&quot;: { &quot;error.message&quot;: &quot;Invalid credentials&quot; } } ]&#x27; &#x27;https:&#x2F;&#x2F;<em>trace</em>-<em>api</em>.newrelic.com&#x2F;<em>trace</em>&#x2F;v1&#x27; Copy Within a minute, the <em>trace</em> should be available in the our <em>distributed</em> <em>tracing</em> UI. To find it, run a query"
      },
      "id": "6071cfc864441fa88f9d8530"
    }
  ],
  "/docs/distributed-tracing/trace-api/report-new-relic-format-traces-trace-api": [
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 346.8565,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> <em>API</em> general requirements and limits ",
        "sections": "<em>Trace</em> <em>API</em> general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Tracing</em>: when you complete the <em>Trace</em> observer setup, you get a custom YOUR_<em>TRACE</em>_OBSERVER_URL value to use as an endpoint. If you&#x27;re using an integration that uses the <em>Trace</em> <em>API</em> (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust"
      },
      "id": "6071cf7628ccbcf8b851c158"
    },
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 310.32098,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, Infinite <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about Infinite <em>Tracing</em>: Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Introduction to the Trace API",
        "Tip",
        "What is the Trace API?",
        "Requirements",
        "Sampling considerations",
        "Start reporting data",
        "Find data",
        "Next steps"
      ],
      "title": "Introduction to the Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "119447fbb33c4469c81877ffaa273bd7b1956e9f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/introduction-trace-api/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-04-10T16:16:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Trace API is used to send distributed tracing data to New Relic: either in our own generic format or the Zipkin data format. This API is also how trace data from some of our integrations and exporters is reported to New Relic. To skip some introductory content and get started quickly, go to Start reporting data. Tip To use APIs and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is the Trace API? The Trace API is one way that New Relic collects distributed tracing data. We have some tracing tools that report data via this API (Infinite Tracing, our open source integrations, and our Telemetry SDKs), or you can use the API directly to create your own tracing implementation. Reasons to use the Trace API: You have your own custom distributed tracing tool and want to see that data in New Relic without changing your instrumentation. You have a tool that emits tracing data but that requires a backend for trace storage. You want to report distributed tracing data to New Relic without the use of our installed solutions. You use Zipkin and want to see that trace data in New Relic without changing your instrumentation. Want to understand how trace data relates to other New Relic data? Read about our data types. Requirements For details about what data you can send and how it is handled, see Rules and limits. Sampling considerations When using the Trace API, you have the option to enable Infinite Tracing. Whether you use Infinite Tracing has implications for how you configure sampling: Use your own sampling (no Infinite Tracing): If you don't use Infinite Tracing, it's assumed you want to use your own sampling implementation to sample traces before they're sent to us. (If your trace data exceeds our data limits, we do enact sampling.) Use Infinite Tracing: If you want to use Infinite Tracing, a typical approach is to send us 100% of your trace data and rely on Infinite Tracing sampling. (When you go through the Start reporting data instructions, below, you'll have an option for enabling Infinite Tracing.) Start reporting data The Trace API accepts trace data in these two JSON formats: zipkin: the Zipkin JSON v2 trace data format (learn what Zipkin is). Get started sending Zipkin data. newrelic: our general format. If you don't have Zipkin-format data, use this. Get started sending New Relic-format data. Having problems getting data to report? See Troubleshooting. Find data You can find data sent via the Trace API, or from integrations that use this API, in these locations: In our distributed tracing UI. By querying: For details about how to query and create custom charts, see Query distributed tracing data. Next steps Here are some additional steps to consider: Learn more about how distributed tracing works and the data structure. Decorate span data with custom attributes. Use our GraphQL-based NerdGraph API to programmatically query your trace data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.0997,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Trace</em> <em>API</em>",
        "sections": "Introduction to the <em>Trace</em> <em>API</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "Our <em>Trace</em> <em>API</em> is used to send <em>distributed</em> <em>tracing</em> data to New Relic: either in our own generic format or the Zipkin data format. This <em>API</em> is also how <em>trace</em> data from some of our integrations and exporters is reported to New Relic. To skip some introductory content and get started quickly, go"
      },
      "id": "6071cf7728ccbcf4bc51c16a"
    }
  ],
  "/docs/distributed-tracing/trace-api/report-zipkin-format-traces-trace-api": [
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 346.8565,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> <em>API</em> general requirements and limits ",
        "sections": "<em>Trace</em> <em>API</em> general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Tracing</em>: when you complete the <em>Trace</em> observer setup, you get a custom YOUR_<em>TRACE</em>_OBSERVER_URL value to use as an endpoint. If you&#x27;re using an integration that uses the <em>Trace</em> <em>API</em> (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust"
      },
      "id": "6071cf7628ccbcf8b851c158"
    },
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 310.32098,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, Infinite <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about Infinite <em>Tracing</em>: Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Introduction to the Trace API",
        "Tip",
        "What is the Trace API?",
        "Requirements",
        "Sampling considerations",
        "Start reporting data",
        "Find data",
        "Next steps"
      ],
      "title": "Introduction to the Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "119447fbb33c4469c81877ffaa273bd7b1956e9f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/introduction-trace-api/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-04-10T16:16:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Trace API is used to send distributed tracing data to New Relic: either in our own generic format or the Zipkin data format. This API is also how trace data from some of our integrations and exporters is reported to New Relic. To skip some introductory content and get started quickly, go to Start reporting data. Tip To use APIs and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is the Trace API? The Trace API is one way that New Relic collects distributed tracing data. We have some tracing tools that report data via this API (Infinite Tracing, our open source integrations, and our Telemetry SDKs), or you can use the API directly to create your own tracing implementation. Reasons to use the Trace API: You have your own custom distributed tracing tool and want to see that data in New Relic without changing your instrumentation. You have a tool that emits tracing data but that requires a backend for trace storage. You want to report distributed tracing data to New Relic without the use of our installed solutions. You use Zipkin and want to see that trace data in New Relic without changing your instrumentation. Want to understand how trace data relates to other New Relic data? Read about our data types. Requirements For details about what data you can send and how it is handled, see Rules and limits. Sampling considerations When using the Trace API, you have the option to enable Infinite Tracing. Whether you use Infinite Tracing has implications for how you configure sampling: Use your own sampling (no Infinite Tracing): If you don't use Infinite Tracing, it's assumed you want to use your own sampling implementation to sample traces before they're sent to us. (If your trace data exceeds our data limits, we do enact sampling.) Use Infinite Tracing: If you want to use Infinite Tracing, a typical approach is to send us 100% of your trace data and rely on Infinite Tracing sampling. (When you go through the Start reporting data instructions, below, you'll have an option for enabling Infinite Tracing.) Start reporting data The Trace API accepts trace data in these two JSON formats: zipkin: the Zipkin JSON v2 trace data format (learn what Zipkin is). Get started sending Zipkin data. newrelic: our general format. If you don't have Zipkin-format data, use this. Get started sending New Relic-format data. Having problems getting data to report? See Troubleshooting. Find data You can find data sent via the Trace API, or from integrations that use this API, in these locations: In our distributed tracing UI. By querying: For details about how to query and create custom charts, see Query distributed tracing data. Next steps Here are some additional steps to consider: Learn more about how distributed tracing works and the data structure. Decorate span data with custom attributes. Use our GraphQL-based NerdGraph API to programmatically query your trace data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.0997,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Trace</em> <em>API</em>",
        "sections": "Introduction to the <em>Trace</em> <em>API</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "Our <em>Trace</em> <em>API</em> is used to send <em>distributed</em> <em>tracing</em> data to New Relic: either in our own generic format or the Zipkin data format. This <em>API</em> is also how <em>trace</em> data from some of our integrations and exporters is reported to New Relic. To skip some introductory content and get started quickly, go"
      },
      "id": "6071cf7728ccbcf4bc51c16a"
    }
  ],
  "/docs/distributed-tracing/trace-api/trace-api-decorate-spans-attributes": [
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 346.85626,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> <em>API</em> general requirements and limits ",
        "sections": "<em>Trace</em> <em>API</em> general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Tracing</em>: when you complete the <em>Trace</em> observer setup, you get a custom YOUR_<em>TRACE</em>_OBSERVER_URL value to use as an endpoint. If you&#x27;re using an integration that uses the <em>Trace</em> <em>API</em> (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust"
      },
      "id": "6071cf7628ccbcf8b851c158"
    },
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 310.3208,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, Infinite <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about Infinite <em>Tracing</em>: Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Introduction to the Trace API",
        "Tip",
        "What is the Trace API?",
        "Requirements",
        "Sampling considerations",
        "Start reporting data",
        "Find data",
        "Next steps"
      ],
      "title": "Introduction to the Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "119447fbb33c4469c81877ffaa273bd7b1956e9f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/introduction-trace-api/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-04-10T16:16:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Trace API is used to send distributed tracing data to New Relic: either in our own generic format or the Zipkin data format. This API is also how trace data from some of our integrations and exporters is reported to New Relic. To skip some introductory content and get started quickly, go to Start reporting data. Tip To use APIs and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is the Trace API? The Trace API is one way that New Relic collects distributed tracing data. We have some tracing tools that report data via this API (Infinite Tracing, our open source integrations, and our Telemetry SDKs), or you can use the API directly to create your own tracing implementation. Reasons to use the Trace API: You have your own custom distributed tracing tool and want to see that data in New Relic without changing your instrumentation. You have a tool that emits tracing data but that requires a backend for trace storage. You want to report distributed tracing data to New Relic without the use of our installed solutions. You use Zipkin and want to see that trace data in New Relic without changing your instrumentation. Want to understand how trace data relates to other New Relic data? Read about our data types. Requirements For details about what data you can send and how it is handled, see Rules and limits. Sampling considerations When using the Trace API, you have the option to enable Infinite Tracing. Whether you use Infinite Tracing has implications for how you configure sampling: Use your own sampling (no Infinite Tracing): If you don't use Infinite Tracing, it's assumed you want to use your own sampling implementation to sample traces before they're sent to us. (If your trace data exceeds our data limits, we do enact sampling.) Use Infinite Tracing: If you want to use Infinite Tracing, a typical approach is to send us 100% of your trace data and rely on Infinite Tracing sampling. (When you go through the Start reporting data instructions, below, you'll have an option for enabling Infinite Tracing.) Start reporting data The Trace API accepts trace data in these two JSON formats: zipkin: the Zipkin JSON v2 trace data format (learn what Zipkin is). Get started sending Zipkin data. newrelic: our general format. If you don't have Zipkin-format data, use this. Get started sending New Relic-format data. Having problems getting data to report? See Troubleshooting. Find data You can find data sent via the Trace API, or from integrations that use this API, in these locations: In our distributed tracing UI. By querying: For details about how to query and create custom charts, see Query distributed tracing data. Next steps Here are some additional steps to consider: Learn more about how distributed tracing works and the data structure. Decorate span data with custom attributes. Use our GraphQL-based NerdGraph API to programmatically query your trace data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.09969,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Trace</em> <em>API</em>",
        "sections": "Introduction to the <em>Trace</em> <em>API</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "Our <em>Trace</em> <em>API</em> is used to send <em>distributed</em> <em>tracing</em> data to New Relic: either in our own generic format or the Zipkin data format. This <em>API</em> is also how <em>trace</em> data from some of our integrations and exporters is reported to New Relic. To skip some introductory content and get started quickly, go"
      },
      "id": "6071cf7728ccbcf4bc51c16a"
    }
  ],
  "/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 310.3208,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, Infinite <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about Infinite <em>Tracing</em>: Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Introduction to the Trace API",
        "Tip",
        "What is the Trace API?",
        "Requirements",
        "Sampling considerations",
        "Start reporting data",
        "Find data",
        "Next steps"
      ],
      "title": "Introduction to the Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "119447fbb33c4469c81877ffaa273bd7b1956e9f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/introduction-trace-api/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-04-10T16:16:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our Trace API is used to send distributed tracing data to New Relic: either in our own generic format or the Zipkin data format. This API is also how trace data from some of our integrations and exporters is reported to New Relic. To skip some introductory content and get started quickly, go to Start reporting data. Tip To use APIs and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is the Trace API? The Trace API is one way that New Relic collects distributed tracing data. We have some tracing tools that report data via this API (Infinite Tracing, our open source integrations, and our Telemetry SDKs), or you can use the API directly to create your own tracing implementation. Reasons to use the Trace API: You have your own custom distributed tracing tool and want to see that data in New Relic without changing your instrumentation. You have a tool that emits tracing data but that requires a backend for trace storage. You want to report distributed tracing data to New Relic without the use of our installed solutions. You use Zipkin and want to see that trace data in New Relic without changing your instrumentation. Want to understand how trace data relates to other New Relic data? Read about our data types. Requirements For details about what data you can send and how it is handled, see Rules and limits. Sampling considerations When using the Trace API, you have the option to enable Infinite Tracing. Whether you use Infinite Tracing has implications for how you configure sampling: Use your own sampling (no Infinite Tracing): If you don't use Infinite Tracing, it's assumed you want to use your own sampling implementation to sample traces before they're sent to us. (If your trace data exceeds our data limits, we do enact sampling.) Use Infinite Tracing: If you want to use Infinite Tracing, a typical approach is to send us 100% of your trace data and rely on Infinite Tracing sampling. (When you go through the Start reporting data instructions, below, you'll have an option for enabling Infinite Tracing.) Start reporting data The Trace API accepts trace data in these two JSON formats: zipkin: the Zipkin JSON v2 trace data format (learn what Zipkin is). Get started sending Zipkin data. newrelic: our general format. If you don't have Zipkin-format data, use this. Get started sending New Relic-format data. Having problems getting data to report? See Troubleshooting. Find data You can find data sent via the Trace API, or from integrations that use this API, in these locations: In our distributed tracing UI. By querying: For details about how to query and create custom charts, see Query distributed tracing data. Next steps Here are some additional steps to consider: Learn more about how distributed tracing works and the data structure. Decorate span data with custom attributes. Use our GraphQL-based NerdGraph API to programmatically query your trace data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 222.09969,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to the <em>Trace</em> <em>API</em>",
        "sections": "Introduction to the <em>Trace</em> <em>API</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "Our <em>Trace</em> <em>API</em> is used to send <em>distributed</em> <em>tracing</em> data to New Relic: either in our own generic format or the Zipkin data format. This <em>API</em> is also how <em>trace</em> data from some of our integrations and exporters is reported to New Relic. To skip some introductory content and get started quickly, go"
      },
      "id": "6071cf7728ccbcf4bc51c16a"
    },
    {
      "sections": [
        "Report Zipkin-format traces via Trace API",
        "Zipkin version requirements",
        "Overview of using the Trace API",
        "Send sample Zipkin trace payload",
        "Send Zipkin-format payload",
        "Send data from existing Zipkin instrumentation",
        "Transformation of Zipkin data",
        "Add other tags/attributes"
      ],
      "title": "Report Zipkin-format traces via Trace API",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "dba8334d1f068236c741ff04c13ecc2be2c184fc",
      "image": "https://docs.newrelic.com/static/96e69137f0dd86b313ec72d5f0c1ad83/119c7/Screen-Shot-2020-08-13-at-1.26.17-PM.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/report-zipkin-format-traces-trace-api/",
      "published_at": "2021-05-05T20:00:43Z",
      "updated_at": "2021-04-10T16:18:16Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you want to create your own tracing implementation, you can use our Trace API. This doc explains how to send Zipkin-format trace data to our Trace API. (For our general data format, see New Relic format.) Zipkin version requirements The Trace API supports data from Zipkin JSON v2 (or higher) without any modification. For details on this version, see Zipkin v2 release details and the Zipkin v2 schema. Overview of using the Trace API Using our Trace API is as simple as: Sending trace data in the expected format (in this case, zipkin format). Sending that data to the appropriate endpoint Our send-data instructions have options for enabling Infinite Tracing. To learn more about this, see Intro to Infinite Tracing and Sampling considerations. To get started using the Trace API, choose an option: Send a sample trace: This shows a curl example of sending a trace to New Relic. This is useful for understanding how the Trace API works, and to verify you're seeing data in New Relic. Report data from existing Zipkin instrumentation: if you have an existing Zipkin implementation, you can simply change the endpoint of where your data gets sent. Send sample Zipkin trace payload This section describes how to send a simple Zipkin-format trace to our Trace API via curl request. You might choose to do this in order to learn how our API works and to verify that data is showing up in New Relic before doing in-depth instrumentation. To get started sending a sample payload: (Optional, to enable Infinite Tracing) First, you must set up a trace observer. That procedure includes instructions for sending a sample trace using our general new-relic format. When you get to that step, return here to instead learn how to send a Zipkin-format trace. Send a Zipkin-format payload following the instructions below. Send Zipkin-format payload To send a sample Zipkin-format trace: Get an Insights insert key: Go to the API keys UI and select Insights insert keys. If you don't already have a key, create a new one by selecting Insert keys +. You'll be executing a curl request, below. Notes on this: Replace the insert key placeholder with your insert key. If you're using Infinite Tracing, use the YOUR_TRACE_OBSERVER_URL value in place of the standard endpoint. If you want to send more than one post, change the trace ID to a different value. Sending the same payload or span id multiple times for the same traceId may result in fragmented traces in the UI. curl -i -H 'Content-Type: application/json' \\ -H 'Api-Key: $NEW_RELIC_INSERT_KEY' \\ -H 'Data-Format: zipkin' \\ -H 'Data-Format-Version: 2' \\ -X POST \\ -d '[ { \"traceId\": \"test-zipkin-trace-id-1\", \"id\": \"3e0f5885710776cd\", \"kind\": \"CLIENT\", \"name\": \"post\", \"duration\": 508068, \"localEndpoint\": { \"serviceName\": \"service-1\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { } }, { \"traceId\": \"test-zipkin-trace-id-1\", \"parentId\": \"3e0f5885710776cd\", \"id\": \"asdf9asdn123lkasdf\", \"kind\": \"CLIENT\", \"name\": \"service 2 span\", \"duration\": 2019, \"localEndpoint\": { \"serviceName\": \"service-2\", \"ipv4\": \"127.0.0.1\", \"port\": 8080 }, \"tags\": { \"error.message\": \"Invalid credentials\" } } ]' 'https://trace-api.newrelic.com/trace/v1' Copy Within a minute, the trace should be available in the our distributed tracing UI. To find it, run a query for the trace.id. In this example, it was test-zipkin-trace-id-1. Note that you search by the transformed attribute of trace.id (not traceId). To learn more: Learn where Trace API data shows up in the UI. Send data from an existing Zipkin instrumentation. Learn how to decorate spans by adding tags. This helps you customize how traces are displayed in our UI for a richer, more helpful experience. Learn about general endpoint information (data limits, required metadata, and response validation). Learn about how Zipkin data is transformed and stored in our format. If you don't see your trace data, see Troubleshooting. Send data from existing Zipkin instrumentation Preliminary notes: If you want to enable Infinite Tracing, you first must set up a trace observer. It can be helpful to first send a sample payload to verify things are working properly. To report data from an existing Zipkin instrumentation, you'll point the Zipkin tracer at the appropriate Trace API endpoint with some required request metadata. You can send the required metadata as headers or query parameters (some Zipkin tracer versions don't allow specifying HTTP headers). Here's an example of what it might look like to create a Zipkin OkHttpSender in Java configured for the Trace API: OkHttpSender.create(\"https://trace-api.newrelic.com/trace/v1?Api-Key=NEW_RELIC_INSERT_API_KEY&Data-Format=zipkin&Data-Format-Version=2\"); Copy Note that if you were using Infinite Tracing, or had an EU-region New Relic account, the endpoint would be different. For an explanation of Api-Key and the other metadata, see Request metadata. Transformation of Zipkin data To create a consistent search/query experience, some Zipkin data will be transformed to match New Relic attribute naming. For more on how we store and structure trace data, see How distributed tracing works. Zipkin tag Stored in New Relic as... Details traceId trace.id Unique identifier for a trace. id id Unique identifier for a span. parentId parent.id Identifier of the upstream span that called the service. kind kind Either Client or Server. name name Name of span. duration duration.ms Zipkin v2 spans must have durations specified in microseconds, and will be converted to milliseconds. localEndpoint: serviceName service.name We use the Zipkin v2 service name to identify the entity that created this span. localEndpoint: port localEndpoint.port All values in the localEndpoint object will be flattened to a span attribute called localEndpoint.key tags reported as attributes Key:value pairs in the tags object in Zipkin v2 will be written as span attributes. annotations not supported We do not currently support annotations in the Trace API. Spans will not be rejected if they contain annotations, but the annotations data will not be written. Add other tags/attributes You can add any tags you want to the tags block, with the exception of the restricted tags. For example, you might want to add attributes like customer.id or user.id to help you analyze your trace data. Tags will be converted to New Relic attributes. To learn how to control how spans appear in New Relic (for example, adding errors or setting a span as a datastore span), see Decorate spans.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 211.3178,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Report Zipkin-format <em>traces</em> via <em>Trace</em> <em>API</em>",
        "sections": "Report Zipkin-format <em>traces</em> via <em>Trace</em> <em>API</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": ", &quot;localEndpoint&quot;: { &quot;serviceName&quot;: &quot;service-2&quot;, &quot;ipv4&quot;: &quot;127.0.0.1&quot;, &quot;port&quot;: 8080 }, &quot;tags&quot;: { &quot;error.message&quot;: &quot;Invalid credentials&quot; } } ]&#x27; &#x27;https:&#x2F;&#x2F;<em>trace</em>-<em>api</em>.newrelic.com&#x2F;<em>trace</em>&#x2F;v1&#x27; Copy Within a minute, the <em>trace</em> should be available in the our <em>distributed</em> <em>tracing</em> UI. To find it, run a query"
      },
      "id": "6071cfc864441fa88f9d8530"
    }
  ],
  "/docs/distributed-tracing/troubleshooting/infinite-tracing-trace-configuration-conflicts": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.32733,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, Infinite <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about Infinite <em>Tracing</em>: Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 250.51807,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> API general requirements and limits ",
        "sections": "<em>Trace</em> API general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Tracing</em>: when you complete the <em>Trace</em> observer setup, you get a custom YOUR_<em>TRACE</em>_OBSERVER_URL value to use as an endpoint. If you&#x27;re using an integration that uses the <em>Trace</em> API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust"
      },
      "id": "6071cf7628ccbcf8b851c158"
    },
    {
      "sections": [
        "Missing trace data",
        "Problem",
        "Solution",
        "Important",
        "Problems with enabling or instrumenting",
        "Missing spans due to service not having distributed tracing enabled",
        "Missing apps/services may require manual instrumentation",
        "Problems with spans",
        "Infinite Tracing: missing spans",
        "Missing span not getting exported",
        "Missing spans due to sampling process",
        "Missing spans due to span limits maxed out",
        "Missing spans due to spans being sent late",
        "Problems with trace details",
        "Middleware doesn't recognize proprietary New Relic header",
        "An intermediary is missing or isn't passing trace context",
        "Tip",
        "Stitching together spans from mixed sources",
        "Trace details are obfuscated",
        "Trace list information and trace details don't match",
        "Long traces with short backend times",
        "Problems with browser applications",
        "Missing spans and transactions after enabling for a browser application",
        "Not seeing browser app end-user spans",
        "Browser spans are not connected to other spans",
        "Other problems",
        "Search by entity.name not finding associated app names",
        "Supporting OpenTelemetry"
      ],
      "title": "Missing trace data",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Troubleshooting"
      ],
      "external_id": "2997172d74563c4fa31d5a9fc05c562d62c1c790",
      "image": "https://docs.newrelic.com/static/ef51359ad9a7999f7fdaf812fab535bc/d7542/missing-exporter.png",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/troubleshooting/missing-trace-data/",
      "published_at": "2021-05-05T18:32:36Z",
      "updated_at": "2021-04-11T07:38:15Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You have enabled distributed tracing but data you expected to see does not appear in New Relic's distributed tracing UI. Solution Important Before performing troubleshooting, we recommend reading How distributed tracing works. Here are some causes and solutions when you have problems finding expected data in the distributed tracing UI: Problems with enabling or instrumenting Missing spans due to service not having distributed tracing enabled In order for distributed tracing to report details for all nodes in a trace, each application must be monitored by a New Relic agent that has had distributed tracing enabled. If an application's New Relic account has not had distributed tracing enabled, it will have these issues: Its distributed tracing UI page won't have data. It won't report data to other accounts' distributed traces. Missing apps/services may require manual instrumentation When you enable distributed tracing for applications and services that New Relic automatically instruments, you'll usually see complete and detailed data for those nodes in the distributed tracing UI. However, you may notice that some services or applications are missing from traces, or that there are some internal spans you expect to see that are missing. If that's the case, you may want to implement custom instrumentation of applications or specific transactions to see more detail in traces. Some examples of when you may need to do this: Transactions not automatically instrumented. To ensure your application is automatically instrumented, read the compatibility and requirements documentation for the New Relic agent you're using. If an application isn't automatically instrumented, or if you'd like to add instrumentation of specific activity, see Custom instrumentation. All Go applications. The Go agent, unlike other agents, requires manual instrumentation of your code. For instructions, see Instrument a Go application. A service doesn't use HTTP. If a service doesn't communicate via HTTP, the New Relic agent won't send distributed tracing headers. This may be the case for some non-web applications or message queues. To remedy this, use the distributed tracing APIs to instrument either the calling or called application. Problems with spans Infinite Tracing: missing spans If your APM agent can’t write data fast enough to the trace observer, queue_size is an additional APM agent configuration to limit the number of spans the agent will hold. See the following examples for your agent: .NET configuration method Example Configuration file <configuration . . . > <infiniteTracing> <trace_observer> <span_events queue_size=\"100000\" /> </trace_observer> </infiniteTracing> </configuration> Copy Environment variable NEW_RELIC_INFINITE_TRACING_SPAN_EVENTS_QUEUE_SIZE=100000 Copy Python configuration method Example Configuration file infinite_tracing.span_queue_size = 100000 Environment Variable NEW_RELIC_INFINITE_TRACING_SPAN_QUEUE_SIZE = 100000 Missing span not getting exported Sometimes header propagation is successful, but the span information isn't getting sent to New Relic. For example, if OpenTelemetry is not instrumented with a New Relic exporter, the span details never make it to New Relic. In this diagram, notice that the header propagation is successful, but no exporter is set up in Service 2 to send the span to New Relic: The following diagram also shows successful header propagation, but it includes an exporter in Service 2 that sends the span details to New Relic (see Trace API): Missing spans due to sampling process Standard distributed tracing for APM uses adaptive sampling. This means that a percentage of calls to a service will be reported as part of a distributed trace. Specific calls to your service might not have been selected to be sampled. Missing spans due to span limits maxed out There are limits on the number of spans that can be collected and displayed. If an application generates a very large number of spans for a single call, it might exceed the APM agent's span-collection limit for that harvest cycle. This could result in missing spans and significantly limit the number of traces the agent can completely sample and report. We currently only show 10,000 spans at a time. Missing spans due to spans being sent late Spans must be sent within the last twenty minutes to be captured in a trace index. If you send any spans older than twenty minutes but newer than a day, the span data will still be written. However, it won't be rolled into the trace index, which controls the trace list in the distributed tracing UI. If a span has a timestamp older than a day, it will be dropped. This often occurs when there is clock skew (timing differences) between systems or long running background jobs. Problems with trace details Middleware doesn't recognize proprietary New Relic header If your transactions are only sending the proprietary New Relic header, some middleware might not recognize the format and then drop the information as shown in this diagram: One solution is to upgrade your New Relic agent to a version that supports W3C trace context. In the diagram below, the W3C-compliant New Relic agent passes the prior header along with two standardized headers: An intermediary is missing or isn't passing trace context Some potential problems with proxies and other intermediaries: Incomplete trace. Some intermediaries won't automatically propagate the distributed tracing header. In that case, you must configure that component to allow the header to be passed from source to destination. For instructions, consult the documentation for that intermediary component. Missing intermediary in trace. If the intermediary is New Relic-monitored, ensure that it propagates the newrelic header that is generated or updated by the New Relic agent running on that intermediary. This may manifest when an intermediary was previously visible in traces, but disappeared after distributed tracing was enabled for an upstream entity (for example, a Browser-monitored application). Tip If some entities report trace data to another tracing system, you can use the trace ID from the New Relic UI to search other tracing systems for missing spans. Stitching together spans from mixed sources If each agent in a chain supports W3C Trace Context, then we can stitch the spans together into a complete trace. If part of the chain is from an agent, such as Zipkin, which doesn't support W3C Trace Context, then spans coming from that agent may not be included in the trace. Trace details are obfuscated If a trace contains data from applications monitored by multiple New Relic accounts, and your user permissions don't allow you to access those accounts, some of the span and service details will be obfuscated in the UI. For example, you may see a series of asterisks ( * * * * * ) instead of the service name in your distributed tracing list if you don't have access to the account linked with the service. Trace list information and trace details don't match The trace list is generated by trace indexes, which are captured in a twenty minute window from when the first spans are received. Usually, this is due to late spans. Long traces with short backend times If you're seeing unusually short backend times for long traces, this is likely an issue with the timestamps being sent. For example, the root span might be reposting microseconds as milliseconds. This can also happen if the root span is a browser application. When using an external client like a web browser, you may experience clock skew (timing differences) more often. Problems with browser applications Missing spans and transactions after enabling for a browser application Older versions of some APM agents are incompatible with distributed tracing for browser applications. If the browser application makes an AJAX request to an APM application running an incompatible agent, then the APM agent may not record transaction and span data for that request. If distributed tracing is enabled for a browser application and you are not seeing transaction or span data for downstream APM requests, review the Browser data in distributed tracing requirements, and confirm that all applications are running supported versions of the APM agent. Not seeing browser app end-user spans If traces seem to be missing end-user spans, be sure you've read and understand the Browser distributed tracing requirements and enable procedures. On the New Relic Browser AJAX UI page, there are links to the distributed tracing UI regardless of whether there are end-user spans present in that trace. For details about what data generates spans, see Requirements. Browser spans are not connected to other spans Older versions of some APM agents are incompatible with distributed tracing for browser applications. If APM spans are missing consistently from traces that include browser applications, please refer to the Browser data in distributed tracing requirements and confirm that all applications are running supported versions of the APM agent. For other causes of orphaned browser spans, see Browser span reporting. Other problems Search by entity.name not finding associated app names Potential cause: For applications that have multiple app names, the entity.name attribute will be associated only with the primary app name. To search by other app names, search using the appName attribute. Supporting OpenTelemetry Questions about integrating with OpenTelemetry should be taken to the Explorers Hub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 204.56128,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Missing <em>trace</em> data",
        "sections": "Missing spans due to service not having <em>distributed</em> <em>tracing</em> enabled",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "Problem You have enabled <em>distributed</em> <em>tracing</em> but data you expected to see does not appear in New Relic&#x27;s <em>distributed</em> <em>tracing</em> UI. Solution Important Before performing <em>troubleshooting</em>, we recommend reading How <em>distributed</em> <em>tracing</em> works. Here are some causes and solutions when you have problems"
      },
      "id": "6072a76764441f109b9d857b"
    }
  ],
  "/docs/distributed-tracing/troubleshooting/missing-trace-data": [
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.32733,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, Infinite <em>Tracing</em> can process more <em>trace</em> data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about Infinite <em>Tracing</em>: Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 250.51807,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> API general requirements and limits ",
        "sections": "<em>Trace</em> API general requirements and limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " <em>Tracing</em>: when you complete the <em>Trace</em> observer setup, you get a custom YOUR_<em>TRACE</em>_OBSERVER_URL value to use as an endpoint. If you&#x27;re using an integration that uses the <em>Trace</em> API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust"
      },
      "id": "6071cf7628ccbcf8b851c158"
    },
    {
      "sections": [
        "Infinite Tracing: trace configuration conflicts",
        "Problem"
      ],
      "title": "Infinite Tracing: trace configuration conflicts",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Troubleshooting"
      ],
      "external_id": "b5f9f72d5adf410a972ef089484d53ffa355aa65",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/troubleshooting/infinite-tracing-trace-configuration-conflicts/",
      "published_at": "2021-05-05T18:31:59Z",
      "updated_at": "2021-04-11T07:38:14Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem Traces with configuration conflicts are caused when you've enabled Infinite Tracing for one or more services in a trace, but other services in that trace have our standard (non-Infinite Tracing) distributed tracing solutions enabled. You can solve this problem by enabling Infinite Tracing for all services in a trace. A tracing configuration conflict can cause issues like: Fragmented traces and orphaned spans. Incomplete Infinite Tracing metrics due to not taking standard trace sampling into account. Discrepancies for metrics like span count, service count, durations, and error count. Confusing search results. For example, for standard tracing spans that send headers to Infinite Tracing-instrumented services, those spans may show in the trace list but not in the trace waterfall view. Missing traces for monitored browser apps and mobile apps. Because Infinite Tracing isn’t yet available for browser or mobile monitoring, spans from these services won’t show up in the trace waterfall when they make requests to Infinite Tracing-enabled services. For traces with a configuration conflict, we’ll display only the Infinite Tracing data in the UI because that data is of higher quality. (All tracing data is queryable via NRQL.)",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 199.52051,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Infinite <em>Tracing</em>: <em>trace</em> configuration conflicts",
        "sections": "Infinite <em>Tracing</em>: <em>trace</em> configuration conflicts",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "Problem Traces with configuration conflicts are caused when you&#x27;ve enabled Infinite <em>Tracing</em> for one or more services in a <em>trace</em>, but other services in that <em>trace</em> have our standard (non-Infinite <em>Tracing</em>) <em>distributed</em> <em>tracing</em> solutions enabled. You can solve this problem by enabling Infinite <em>Tracing</em>"
      },
      "id": "6072a76664441fe9c09d859a"
    }
  ],
  "/docs/distributed-tracing/ui-data/query-distributed-trace-data": [
    {
      "sections": [
        "Understand and use the distributed tracing UI",
        "See distributed tracing UI",
        "View traces for a specific service",
        "View traces across all accounts",
        "Tip",
        "Sort through your traces",
        "Filter using the query bar",
        "Find traces that touch two services",
        "Find error spans using the like operator",
        "Filter using the scatter plot",
        "Filter traces using histograms",
        "Important",
        "Trace details UI page",
        "Span details pane",
        "View related logs",
        "Additional UI details",
        "How to understand span errors",
        "Anomalous spans",
        "Client span duration: time differences between client and server spans",
        "Fragmented traces",
        "Trace details obfuscated based on account access",
        "Span limits and sampling",
        "Incomplete span names in waterfall view",
        "Missing spans and span/service count discrepancies"
      ],
      "title": "Understand and use the distributed tracing UI",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "UI and data"
      ],
      "external_id": "f5b66e03583e42613810f1390b4e5adab4ed2caa",
      "image": "https://docs.newrelic.com/static/ec08996f31e6586bb257c6f89b3c8f99/e5166/new-relic-distributed-tracing-client-span-time.jpg",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/ui-data/understand-use-distributed-tracing-ui/",
      "published_at": "2021-05-05T20:01:43Z",
      "updated_at": "2021-04-11T07:36:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Distributed tracing helps you monitor and analyze the behavior of your distributed system. After you enable distributed tracing, you can use our UI tools to search for traces and analyze them. For example, let's say you are an engineer troubleshooting errors in a complex transaction spanning many services. Here's what you can do in our UI: Open the distributed tracing UI page. Sort through your traces using a filter to find that specific request and show only traces containing errors. On the trace details page, you review the span along the request route that originated the error. Noting the error class and message, navigate to the service from its span in the trace so you can see that the error is occurring at a high rate. Read on to explore the options in the distributed tracing UI. See distributed tracing UI Here's how you can access the distributed tracing UI, depending on the type of search you want to do: View traces for a specific service The New Relic Explorer and APM are two menu options that help you navigate to a specific service so you can see traces that include that service. Go to one.newrelic.com. Click Explorer or APM in the top menu bar. Filter to the service you enabled for distributed tracing by typing the service name, and then press Enter. In the left navigation's Monitor section, click Distributed tracing. View traces across all accounts If you want to view traces from across all accounts you have access to, there are two ways to view that UI: From one.newrelic.com, mouse over Browse data and select Traces. OR From one.newrelic.com, click Apps, and click Distributed tracing. Tip If you don't have access to accounts for some services in a trace, we'll obfuscate some details for those services. Sort through your traces The opening distributed tracing page is populated with a list of traces, and you can quickly refine this list using these tools: Query bar Scatter plot chart Interactive histograms (only available for cross-account searches) Tip In addition to these tools, you can also use other options mentioned in Query distributed trace data. Filter using the query bar The Find traces query bar is a quick way to narrow your search for traces. You can either start typing in the query bar or use the dropdown to create a compound query. Query returns are based on span attributes, not on trace attributes. You define spans that have certain criteria, and the search displays traces that contain those spans. If you use a multi-attribute filter, it is affected by first attribute selected. Distributed tracing reports on two types of data: transaction events and spans. When you select an attribute in the filter, the data type that attribute is attached to dictates the available attributes. For example, if you filter on an attribute that is attached to a transaction event, only transaction event attributes are available when you attempt to add filter on additional attribute values. Queries for traces are similar to NRQL (our query language). Here are the main exceptions: String values don't require quote marks (for example, you can use either appName = MyApp or appName = 'MyApp') The like operator doesn’t require % (for example, you can use either appName like product or appName like %product%). Tip Some queries that return a large number of results may return false positives. The trace list limits these incorrect results to 10% of the returned results. False positives may also result in histogram chart results that are not displayed in the trace list. Here are two query bar examples: Find traces that touch two services The query in the image below finds traces that: Pass through both WebPortal and Inventory Service applications Have an Inventory Service datastore call that takes longer than 500 ms Contains an error in any span. Go to one.newrelic.com > Apps > Distributed tracing Find error spans using the like operator The query in the image below finds traces that: Contain spans that pass through the WebPortal application and where an error occurred on any span in the WebPortal application Contain spans where the customer_user_email attribute contains a value ending with hotmail.com anywhere in the trace. Go to one.newrelic.com > Apps > Distributed tracing Filter using the scatter plot The trace scatter plot on the opening page of distributed tracing is a quick way to search for outlying traces. You can move the cursor across the chart to view trace details and you can click individual points to get details. Go to one.newrelic.com > Apps > Favorites > Distributed tracing or go to one.newrelic.com > APM > (select an app) > Monitor > Distributed tracing. When viewing the trace scatter plot, you can select the duration type in View by, and you can select one of these options in Group traces by: Errors: Group by whether traces contain errors or not. Root service: Group by the name of the first service in traces. In a trace where Service A calls Service B and Service B calls Service C, the root service would be Service A. Root entry span: Group by the root transaction, which is the root service's endpoint. In a trace where Service A calls Service B and Service B calls Service C, the root entry span is Service A's endpoint. For example: \"Service A - GET /user/%\". Service entry span: Group by the span name of the service currently being viewed in APM. For example, for a trace where Service A calls Service B and Service B calls Service C, if you are viewing Service B in APM and select this grouping, the traces will be represented by their Service B span names. If a service has multiple spans in a trace, this grouping option will use that service's first entry point. Filter traces using histograms Tip Trace histogram charts are available only on the global distributed tracing page. The histogram charts on the global distributed tracing page give you a quick understanding of trace distribution for important values, like duration. You can use the sliders under the charts to control what traces are displayed. For example, you can drag the Trace duration chart slider to show only traces over 500 ms, as shown in the histogram example below. Important Some queries that produce many results may result in false positives in histograms. This could manifest as histograms showing trace results that are not in the trace list. Trace details UI page When you select a trace from the trace list, you see that trace's timeline and spans: one.newrelic.com > APM > (select an application) > Monitor > Distributed tracing > (select a trace) > (select a span): See the spans in a trace. Examine individual span details and see notifications for spans with anomalous behavior. The UI indicates some span properties with icons: Span property Indicator Description Service This icon represents a span that's a service's entry point. In-process This icon represents an in-process span, which is a span that takes place within a process (as opposed to a cross-process span). Examples: middleware instrumentation, user-created spans. Datastore This icon represents a span call to a datastore. External This icon represents category representing a call to an external service made via HTTP. Browser app This icon represents a browser application span. Lambda This icon represents a span from a Lambda function. Some spans will have additional indicators: Span property Indicator Description Type of connection Solid lines indicate a direct parent-child relationship; in other words, one process or function directly calling another. A dotted line indicates a non-direct relationship. For more on relationships between spans, see Trace structure. Errors A span with an error. See How to understand span errors. Anomalous This icon represents the detection of an anomalous span. Orphaned spans Some spans may be \"orphaned,\" or separated, from the trace. These spans will appear at the bottom of the trace. For more details, see Fragmented traces. Multiple app names When beside a span name, this represents an entity that has had multiple app names set. Select this to see all app names it reports to. To search trace data by alternate app names, use the appName attribute. Client/server time difference If a span's duration indicator is not completely colored in (like in this example), it means that there is a time discrepancy between the server-side duration and the client-side duration for that activity. For details on this, see Client/server time difference. For more on the trace structure and how span properties are determined, see Trace structure. Span details pane When you select a span, a pane opens up with span details. These details can be helpful for troubleshooting performance issues. Details include: Performance charts Span attributes The span's data source Anomalous span details The span's full name What a span displays is based on its span type. For example, a datastore span's name attribute will contain the datastore query. View related logs If you are using our logs in context feature together with our log management, you can see any logs that are linked to your traces: Go to the trace details page by clicking on a trace. Click See logs in the upper-right corner. For details related to an individual log message, click directly on the message. Additional UI details Here are some additional distributed tracing UI details, rules, and limits: How to understand span errors Span-level errors show you where errors originated in a process, how they bubbled up, and where they were handled. Every span that ends with an exception is shown with an error in the UI and contributes to the total error count for that trace. Here are some general tips about understanding span errors: Spans with errors are highlighted red in the distributed tracing UI. You can see more information on the Error Details pane for each span. All spans that exit with errors are counted in the span error count. When multiple errors occur on the same span, only one is written to the span in this order of precedence: A noticeError The most recent span exception This table describes how different span errors are handled: Error type Description Spans ending in exceptions An exception that leaves the boundary of a span results in an error on that span and on any ancestor spans that also exit with an error, until the exception is caught or exits the transaction. You can see if an exception is caught in an ancestor span. Notice errors Errors noticed by calls to the agent noticeError API or by the automatic agent instrumentation are attached to the currently executing span. Response code errors Response code errors are attached to the associated span, such as: Client span: External transactions prefixed with http or db. Entry span: In the case of a transaction ending in a response code error. The response code for these spans is captured as an attribute httpResponseCode and attached to that span. Anomalous spans If a span is displayed as anomalous in the UI, it means that the following are both true: The span is more than two standard deviations slower than the average of all spans with the same name from the same service over the last six hours. The span's duration is more than 10% of the trace's duration. Client span duration: time differences between client and server spans When a process calls another process, and both processes are instrumented by New Relic, the trace contains both a client-side representation of the call and a server-side representation. The client span (calling process) can have time-related differences when compared to the server span (called process). These differences could be due to: Clock skew, due to system clock time differences Differences in duration, due to things like network latency or DNS resolution delay The UI shows these time-related differences by displaying an outline of the client span in the same space as the server span. This span represents the duration of the client span. It isn't possible to determine every factor contributing to these time-related discrepancies, but here are some common span patterns and tips for understanding them: When a client span is longer than the server span, this could be due to latency in a number of areas, such as: network time, queue time, DNS resolution time, or from a load balancer that we cannot see. When a client span starts and ends before a server span begins, this could be due to clock skew, or due to the server doing asynchronous work that continues after sending the response. When a client span starts after a server span, this is most likely clock skew. Fragmented traces Fragmented traces are traces with missing spans. When a span is missing or has invalid parent span IDs, its children spans become separated from the rest of the trace, which we refer to as \"orphaned.\" Orphaned spans appear at the bottom of the trace, and they will lack connecting lines to the rest of the trace. Types of orphaned span properties indicated in the UI: No root span. Missing the root span, which is the first operation in the request. When this happens, the span with the earliest timestamp is displayed as the root. Orphaned span. A single span with a missing parent span. This could be due to the parent span having an ID that doesn't match its child span. Orphaned trace fragment. A group of connected spans where the first span in the group is an orphan span. This can happen for a number of reasons, including: Collection limits. Some high-throughput applications may exceed collection limits (for example, APM agent collection limits, or API limits). When this happens, it may result in traces having missing spans. One way to remedy this is to turn off some reporting, so that the limit is not reached. Incorrect instrumentation. If an application is instrumented incorrectly, it won't pass trace context correctly and this will result in fragmented traces. To remedy this, examine the data source that is generating orphan spans to ensure instrumentation is done correctly. To discover a span's data source, select it and examine its span details. Spans still arriving. If some parent spans haven't been collected yet, this can result in temporary gaps until the entire trace has reported. UI display limits. Orphaned spans may result if a trace exceeds the 10K span display limit. Trace details obfuscated based on account access If you don’t have access to the New Relic accounts that monitor other services, some of the span and service details will be obfuscated in the UI. Obfuscation can include: Span name concealed by asterisks Service name replaced with New Relic account ID and app ID The two main factors affecting this obfuscation: Account permissions. Master/sub-account relationships will impact access. If you have access to only a sub-account, you’ll be able to see details for only that sub-account. If you have access to a master account, you’ll be able to see details for that account’s sub-accounts. Authentication. You’ll be able to see span details only for New Relic accounts you can access based on your current login. This means that, for example, even the admin of a master account may not be able to see all details if the trace crosses the boundaries of different authentication mechanisms. Span limits and sampling See Sampling. Incomplete span names in waterfall view When viewing the span waterfall, span names may be displayed in an incomplete form that is more human-readable than the complete span name. To find the complete name, select that span and look for the Full span name. Knowing the complete name can be valuable for querying that data with NRQL. Missing spans and span/service count discrepancies A trace may sometimes have (or seem to have) missing spans or services. This can manifest as a discrepancy between the count of a trace's spans or services displayed in the trace list and the count displayed on the trace details page. Reasons for missing spans and count discrepancies include: An APM agent may have hit its 1K span collection limit. A span may be initially counted but not make it into a trace display, for reasons such as network latency or a query issue. The UI may have hit its 10K span display limit. All spans collected, including those not displayed, can be queried with NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 255.06226,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Understand</em> <em>and</em> use the <em>distributed</em> <em>tracing</em> <em>UI</em>",
        "sections": "<em>Understand</em> <em>and</em> use the <em>distributed</em> <em>tracing</em> <em>UI</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " services. Here&#x27;s what you can do in our <em>UI</em>: Open the <em>distributed</em> <em>tracing</em> <em>UI</em> page. Sort through your traces using a filter to find that specific request and show only traces containing errors. On the <em>trace</em> details page, you review the span along the request route that originated the error. Noting"
      },
      "id": "6072a70028ccbc265a51c13d"
    },
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.32718,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, Infinite <em>Tracing</em> can process more <em>trace</em> <em>data</em>. It uses superior tail-based sampling (sampling after <em>data</em> is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about Infinite <em>Tracing</em>: Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 250.51791,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> API general requirements <em>and</em> limits ",
        "sections": "<em>Trace</em> API general requirements <em>and</em> limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": ". For all errors of this type, the attribute newRelicFeature will be <em>Distributed</em> <em>Tracing</em> and requestId will be the requestId from the endpoint response. If you receive a 202 response and don&#x27;t see an NrIntegrationError event, your <em>data</em> should be visible in New Relic One&#x27;s global <em>distributed</em> <em>tracing</em> <em>UI</em>"
      },
      "id": "6071cf7628ccbcf8b851c158"
    }
  ],
  "/docs/distributed-tracing/ui-data/span-attributes": [
    {
      "sections": [
        "Query distributed trace data",
        "Example NRQL queries",
        "Tip",
        "Datastore time percentile for an app",
        "Datastore query time for an app, faceted by host",
        "Average duration for a method of a service, faceted by host",
        "Histogram of external services called by a service, faceted by external URI",
        "Average duration for external calls across all applications",
        "Example NerdGraph queries",
        "Can't find data?"
      ],
      "title": "Query distributed trace data",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "UI and data"
      ],
      "external_id": "7ad60264aa5c46ef3859a886fc5c97471ccfb02f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/ui-data/query-distributed-trace-data/",
      "published_at": "2021-05-05T20:01:43Z",
      "updated_at": "2021-04-11T07:36:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can query your distributed tracing data in several ways: The search bar at top of the distributed tracing UI NRQL query NerdGraph GraphiQL explorer To learn about trace structure, see How distributed tracing works. Example NRQL queries Tip You can also construct complex queries in the search bar at the top of the distributed tracing UI. Some example NRQL queries: Datastore time percentile for an app SELECT percentile(duration, 50, 95) FROM Span WHERE category = 'datastore' and appName = 'YOUR_APP_NAME' SINCE 4 hours ago TIMESERIES 1 minute Copy Datastore query time for an app, faceted by host SELECT average(duration) FROM Span WHERE appName = 'YOUR_APP_NAME' and category = 'datastore' FACET host TIMESERIES 1 minute Copy Average duration for a method of a service, faceted by host SELECT average(duration) FROM Span WHERE appName = 'YOUR_APP_NAME' and name = 'FUNCTION_NAME' FACET host TIMESERIES 1 minute Copy Histogram of external services called by a service, faceted by external URI SELECT histogram(duration, 10, 60) FROM Span WHERE category = 'http' and appName = 'YOUR_APP_NAME' FACET `http.url` SINCE 4 hours ago Copy Average duration for external calls across all applications SELECT average(duration) FROM Span WHERE category = 'http' SINCE 4 hours ago FACET `http.url` TIMESERIES 1 minute Copy Example NerdGraph queries You can also use NerdGraph to query your trace data using the API. For more information, see the NerdGraph distributed tracing data query examples. Can't find data? Having trouble finding data when querying? See Troubleshooting: missing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 261.79718,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Query <em>distributed</em> <em>trace</em> <em>data</em>",
        "sections": "Query <em>distributed</em> <em>trace</em> <em>data</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "You can query your <em>distributed</em> <em>tracing</em> <em>data</em> in several ways: The search bar at top of the <em>distributed</em> <em>tracing</em> <em>UI</em> NRQL query NerdGraph GraphiQL explorer To learn about <em>trace</em> structure, see How <em>distributed</em> <em>tracing</em> works. Example NRQL queries Tip You can also construct complex queries in the search"
      },
      "id": "6072a6ff196a67ddaf64a75a"
    },
    {
      "sections": [
        "Understand and use the distributed tracing UI",
        "See distributed tracing UI",
        "View traces for a specific service",
        "View traces across all accounts",
        "Tip",
        "Sort through your traces",
        "Filter using the query bar",
        "Find traces that touch two services",
        "Find error spans using the like operator",
        "Filter using the scatter plot",
        "Filter traces using histograms",
        "Important",
        "Trace details UI page",
        "Span details pane",
        "View related logs",
        "Additional UI details",
        "How to understand span errors",
        "Anomalous spans",
        "Client span duration: time differences between client and server spans",
        "Fragmented traces",
        "Trace details obfuscated based on account access",
        "Span limits and sampling",
        "Incomplete span names in waterfall view",
        "Missing spans and span/service count discrepancies"
      ],
      "title": "Understand and use the distributed tracing UI",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "UI and data"
      ],
      "external_id": "f5b66e03583e42613810f1390b4e5adab4ed2caa",
      "image": "https://docs.newrelic.com/static/ec08996f31e6586bb257c6f89b3c8f99/e5166/new-relic-distributed-tracing-client-span-time.jpg",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/ui-data/understand-use-distributed-tracing-ui/",
      "published_at": "2021-05-05T20:01:43Z",
      "updated_at": "2021-04-11T07:36:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Distributed tracing helps you monitor and analyze the behavior of your distributed system. After you enable distributed tracing, you can use our UI tools to search for traces and analyze them. For example, let's say you are an engineer troubleshooting errors in a complex transaction spanning many services. Here's what you can do in our UI: Open the distributed tracing UI page. Sort through your traces using a filter to find that specific request and show only traces containing errors. On the trace details page, you review the span along the request route that originated the error. Noting the error class and message, navigate to the service from its span in the trace so you can see that the error is occurring at a high rate. Read on to explore the options in the distributed tracing UI. See distributed tracing UI Here's how you can access the distributed tracing UI, depending on the type of search you want to do: View traces for a specific service The New Relic Explorer and APM are two menu options that help you navigate to a specific service so you can see traces that include that service. Go to one.newrelic.com. Click Explorer or APM in the top menu bar. Filter to the service you enabled for distributed tracing by typing the service name, and then press Enter. In the left navigation's Monitor section, click Distributed tracing. View traces across all accounts If you want to view traces from across all accounts you have access to, there are two ways to view that UI: From one.newrelic.com, mouse over Browse data and select Traces. OR From one.newrelic.com, click Apps, and click Distributed tracing. Tip If you don't have access to accounts for some services in a trace, we'll obfuscate some details for those services. Sort through your traces The opening distributed tracing page is populated with a list of traces, and you can quickly refine this list using these tools: Query bar Scatter plot chart Interactive histograms (only available for cross-account searches) Tip In addition to these tools, you can also use other options mentioned in Query distributed trace data. Filter using the query bar The Find traces query bar is a quick way to narrow your search for traces. You can either start typing in the query bar or use the dropdown to create a compound query. Query returns are based on span attributes, not on trace attributes. You define spans that have certain criteria, and the search displays traces that contain those spans. If you use a multi-attribute filter, it is affected by first attribute selected. Distributed tracing reports on two types of data: transaction events and spans. When you select an attribute in the filter, the data type that attribute is attached to dictates the available attributes. For example, if you filter on an attribute that is attached to a transaction event, only transaction event attributes are available when you attempt to add filter on additional attribute values. Queries for traces are similar to NRQL (our query language). Here are the main exceptions: String values don't require quote marks (for example, you can use either appName = MyApp or appName = 'MyApp') The like operator doesn’t require % (for example, you can use either appName like product or appName like %product%). Tip Some queries that return a large number of results may return false positives. The trace list limits these incorrect results to 10% of the returned results. False positives may also result in histogram chart results that are not displayed in the trace list. Here are two query bar examples: Find traces that touch two services The query in the image below finds traces that: Pass through both WebPortal and Inventory Service applications Have an Inventory Service datastore call that takes longer than 500 ms Contains an error in any span. Go to one.newrelic.com > Apps > Distributed tracing Find error spans using the like operator The query in the image below finds traces that: Contain spans that pass through the WebPortal application and where an error occurred on any span in the WebPortal application Contain spans where the customer_user_email attribute contains a value ending with hotmail.com anywhere in the trace. Go to one.newrelic.com > Apps > Distributed tracing Filter using the scatter plot The trace scatter plot on the opening page of distributed tracing is a quick way to search for outlying traces. You can move the cursor across the chart to view trace details and you can click individual points to get details. Go to one.newrelic.com > Apps > Favorites > Distributed tracing or go to one.newrelic.com > APM > (select an app) > Monitor > Distributed tracing. When viewing the trace scatter plot, you can select the duration type in View by, and you can select one of these options in Group traces by: Errors: Group by whether traces contain errors or not. Root service: Group by the name of the first service in traces. In a trace where Service A calls Service B and Service B calls Service C, the root service would be Service A. Root entry span: Group by the root transaction, which is the root service's endpoint. In a trace where Service A calls Service B and Service B calls Service C, the root entry span is Service A's endpoint. For example: \"Service A - GET /user/%\". Service entry span: Group by the span name of the service currently being viewed in APM. For example, for a trace where Service A calls Service B and Service B calls Service C, if you are viewing Service B in APM and select this grouping, the traces will be represented by their Service B span names. If a service has multiple spans in a trace, this grouping option will use that service's first entry point. Filter traces using histograms Tip Trace histogram charts are available only on the global distributed tracing page. The histogram charts on the global distributed tracing page give you a quick understanding of trace distribution for important values, like duration. You can use the sliders under the charts to control what traces are displayed. For example, you can drag the Trace duration chart slider to show only traces over 500 ms, as shown in the histogram example below. Important Some queries that produce many results may result in false positives in histograms. This could manifest as histograms showing trace results that are not in the trace list. Trace details UI page When you select a trace from the trace list, you see that trace's timeline and spans: one.newrelic.com > APM > (select an application) > Monitor > Distributed tracing > (select a trace) > (select a span): See the spans in a trace. Examine individual span details and see notifications for spans with anomalous behavior. The UI indicates some span properties with icons: Span property Indicator Description Service This icon represents a span that's a service's entry point. In-process This icon represents an in-process span, which is a span that takes place within a process (as opposed to a cross-process span). Examples: middleware instrumentation, user-created spans. Datastore This icon represents a span call to a datastore. External This icon represents category representing a call to an external service made via HTTP. Browser app This icon represents a browser application span. Lambda This icon represents a span from a Lambda function. Some spans will have additional indicators: Span property Indicator Description Type of connection Solid lines indicate a direct parent-child relationship; in other words, one process or function directly calling another. A dotted line indicates a non-direct relationship. For more on relationships between spans, see Trace structure. Errors A span with an error. See How to understand span errors. Anomalous This icon represents the detection of an anomalous span. Orphaned spans Some spans may be \"orphaned,\" or separated, from the trace. These spans will appear at the bottom of the trace. For more details, see Fragmented traces. Multiple app names When beside a span name, this represents an entity that has had multiple app names set. Select this to see all app names it reports to. To search trace data by alternate app names, use the appName attribute. Client/server time difference If a span's duration indicator is not completely colored in (like in this example), it means that there is a time discrepancy between the server-side duration and the client-side duration for that activity. For details on this, see Client/server time difference. For more on the trace structure and how span properties are determined, see Trace structure. Span details pane When you select a span, a pane opens up with span details. These details can be helpful for troubleshooting performance issues. Details include: Performance charts Span attributes The span's data source Anomalous span details The span's full name What a span displays is based on its span type. For example, a datastore span's name attribute will contain the datastore query. View related logs If you are using our logs in context feature together with our log management, you can see any logs that are linked to your traces: Go to the trace details page by clicking on a trace. Click See logs in the upper-right corner. For details related to an individual log message, click directly on the message. Additional UI details Here are some additional distributed tracing UI details, rules, and limits: How to understand span errors Span-level errors show you where errors originated in a process, how they bubbled up, and where they were handled. Every span that ends with an exception is shown with an error in the UI and contributes to the total error count for that trace. Here are some general tips about understanding span errors: Spans with errors are highlighted red in the distributed tracing UI. You can see more information on the Error Details pane for each span. All spans that exit with errors are counted in the span error count. When multiple errors occur on the same span, only one is written to the span in this order of precedence: A noticeError The most recent span exception This table describes how different span errors are handled: Error type Description Spans ending in exceptions An exception that leaves the boundary of a span results in an error on that span and on any ancestor spans that also exit with an error, until the exception is caught or exits the transaction. You can see if an exception is caught in an ancestor span. Notice errors Errors noticed by calls to the agent noticeError API or by the automatic agent instrumentation are attached to the currently executing span. Response code errors Response code errors are attached to the associated span, such as: Client span: External transactions prefixed with http or db. Entry span: In the case of a transaction ending in a response code error. The response code for these spans is captured as an attribute httpResponseCode and attached to that span. Anomalous spans If a span is displayed as anomalous in the UI, it means that the following are both true: The span is more than two standard deviations slower than the average of all spans with the same name from the same service over the last six hours. The span's duration is more than 10% of the trace's duration. Client span duration: time differences between client and server spans When a process calls another process, and both processes are instrumented by New Relic, the trace contains both a client-side representation of the call and a server-side representation. The client span (calling process) can have time-related differences when compared to the server span (called process). These differences could be due to: Clock skew, due to system clock time differences Differences in duration, due to things like network latency or DNS resolution delay The UI shows these time-related differences by displaying an outline of the client span in the same space as the server span. This span represents the duration of the client span. It isn't possible to determine every factor contributing to these time-related discrepancies, but here are some common span patterns and tips for understanding them: When a client span is longer than the server span, this could be due to latency in a number of areas, such as: network time, queue time, DNS resolution time, or from a load balancer that we cannot see. When a client span starts and ends before a server span begins, this could be due to clock skew, or due to the server doing asynchronous work that continues after sending the response. When a client span starts after a server span, this is most likely clock skew. Fragmented traces Fragmented traces are traces with missing spans. When a span is missing or has invalid parent span IDs, its children spans become separated from the rest of the trace, which we refer to as \"orphaned.\" Orphaned spans appear at the bottom of the trace, and they will lack connecting lines to the rest of the trace. Types of orphaned span properties indicated in the UI: No root span. Missing the root span, which is the first operation in the request. When this happens, the span with the earliest timestamp is displayed as the root. Orphaned span. A single span with a missing parent span. This could be due to the parent span having an ID that doesn't match its child span. Orphaned trace fragment. A group of connected spans where the first span in the group is an orphan span. This can happen for a number of reasons, including: Collection limits. Some high-throughput applications may exceed collection limits (for example, APM agent collection limits, or API limits). When this happens, it may result in traces having missing spans. One way to remedy this is to turn off some reporting, so that the limit is not reached. Incorrect instrumentation. If an application is instrumented incorrectly, it won't pass trace context correctly and this will result in fragmented traces. To remedy this, examine the data source that is generating orphan spans to ensure instrumentation is done correctly. To discover a span's data source, select it and examine its span details. Spans still arriving. If some parent spans haven't been collected yet, this can result in temporary gaps until the entire trace has reported. UI display limits. Orphaned spans may result if a trace exceeds the 10K span display limit. Trace details obfuscated based on account access If you don’t have access to the New Relic accounts that monitor other services, some of the span and service details will be obfuscated in the UI. Obfuscation can include: Span name concealed by asterisks Service name replaced with New Relic account ID and app ID The two main factors affecting this obfuscation: Account permissions. Master/sub-account relationships will impact access. If you have access to only a sub-account, you’ll be able to see details for only that sub-account. If you have access to a master account, you’ll be able to see details for that account’s sub-accounts. Authentication. You’ll be able to see span details only for New Relic accounts you can access based on your current login. This means that, for example, even the admin of a master account may not be able to see all details if the trace crosses the boundaries of different authentication mechanisms. Span limits and sampling See Sampling. Incomplete span names in waterfall view When viewing the span waterfall, span names may be displayed in an incomplete form that is more human-readable than the complete span name. To find the complete name, select that span and look for the Full span name. Knowing the complete name can be valuable for querying that data with NRQL. Missing spans and span/service count discrepancies A trace may sometimes have (or seem to have) missing spans or services. This can manifest as a discrepancy between the count of a trace's spans or services displayed in the trace list and the count displayed on the trace details page. Reasons for missing spans and count discrepancies include: An APM agent may have hit its 1K span collection limit. A span may be initially counted but not make it into a trace display, for reasons such as network latency or a query issue. The UI may have hit its 10K span display limit. All spans collected, including those not displayed, can be queried with NRQL.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 255.06226,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Understand</em> <em>and</em> use the <em>distributed</em> <em>tracing</em> <em>UI</em>",
        "sections": "<em>Understand</em> <em>and</em> use the <em>distributed</em> <em>tracing</em> <em>UI</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " services. Here&#x27;s what you can do in our <em>UI</em>: Open the <em>distributed</em> <em>tracing</em> <em>UI</em> page. Sort through your traces using a filter to find that specific request and show only traces containing errors. On the <em>trace</em> details page, you review the span along the request route that originated the error. Noting"
      },
      "id": "6072a70028ccbc265a51c13d"
    },
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.32718,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, Infinite <em>Tracing</em> can process more <em>trace</em> <em>data</em>. It uses superior tail-based sampling (sampling after <em>data</em> is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about Infinite <em>Tracing</em>: Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    }
  ],
  "/docs/distributed-tracing/ui-data/understand-use-distributed-tracing-ui": [
    {
      "sections": [
        "Query distributed trace data",
        "Example NRQL queries",
        "Tip",
        "Datastore time percentile for an app",
        "Datastore query time for an app, faceted by host",
        "Average duration for a method of a service, faceted by host",
        "Histogram of external services called by a service, faceted by external URI",
        "Average duration for external calls across all applications",
        "Example NerdGraph queries",
        "Can't find data?"
      ],
      "title": "Query distributed trace data",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "UI and data"
      ],
      "external_id": "7ad60264aa5c46ef3859a886fc5c97471ccfb02f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/ui-data/query-distributed-trace-data/",
      "published_at": "2021-05-05T20:01:43Z",
      "updated_at": "2021-04-11T07:36:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can query your distributed tracing data in several ways: The search bar at top of the distributed tracing UI NRQL query NerdGraph GraphiQL explorer To learn about trace structure, see How distributed tracing works. Example NRQL queries Tip You can also construct complex queries in the search bar at the top of the distributed tracing UI. Some example NRQL queries: Datastore time percentile for an app SELECT percentile(duration, 50, 95) FROM Span WHERE category = 'datastore' and appName = 'YOUR_APP_NAME' SINCE 4 hours ago TIMESERIES 1 minute Copy Datastore query time for an app, faceted by host SELECT average(duration) FROM Span WHERE appName = 'YOUR_APP_NAME' and category = 'datastore' FACET host TIMESERIES 1 minute Copy Average duration for a method of a service, faceted by host SELECT average(duration) FROM Span WHERE appName = 'YOUR_APP_NAME' and name = 'FUNCTION_NAME' FACET host TIMESERIES 1 minute Copy Histogram of external services called by a service, faceted by external URI SELECT histogram(duration, 10, 60) FROM Span WHERE category = 'http' and appName = 'YOUR_APP_NAME' FACET `http.url` SINCE 4 hours ago Copy Average duration for external calls across all applications SELECT average(duration) FROM Span WHERE category = 'http' SINCE 4 hours ago FACET `http.url` TIMESERIES 1 minute Copy Example NerdGraph queries You can also use NerdGraph to query your trace data using the API. For more information, see the NerdGraph distributed tracing data query examples. Can't find data? Having trouble finding data when querying? See Troubleshooting: missing data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 261.79718,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Query <em>distributed</em> <em>trace</em> <em>data</em>",
        "sections": "Query <em>distributed</em> <em>trace</em> <em>data</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": "You can query your <em>distributed</em> <em>tracing</em> <em>data</em> in several ways: The search bar at top of the <em>distributed</em> <em>tracing</em> <em>UI</em> NRQL query NerdGraph GraphiQL explorer To learn about <em>trace</em> structure, see How <em>distributed</em> <em>tracing</em> works. Example NRQL queries Tip You can also construct complex queries in the search"
      },
      "id": "6072a6ff196a67ddaf64a75a"
    },
    {
      "sections": [
        "Introduction to Infinite Tracing",
        "Tip",
        "What is Infinite Tracing?",
        "Requirements",
        "Enable Infinite Tracing",
        "Configure Infinite Tracing"
      ],
      "title": "Introduction to Infinite Tracing",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Infinite Tracing"
      ],
      "external_id": "836125c2bb783114009b0b4748837b36fefb7a91",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/infinite-tracing/introduction-infinite-tracing/",
      "published_at": "2021-05-05T19:20:12Z",
      "updated_at": "2021-05-05T19:20:12Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Some of our tracing solutions support our Infinite Tracing feature. Infinite Tracing is a fully managed cloud-based solution that can analyze 100% of your trace data and choose the most actionable data, letting you investigate and solve issues quickly. This document only applies to our Infinite Tracing feature. For an overview of all distributed tracing options, see Enable distributed tracing. Tip To use Infinite Tracing and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. What is Infinite Tracing? Infinite Tracing allows you to send all your trace data to our cloud-based service and rely on Infinite Tracing to make sampling decisions. You can configure Infinite Tracing in various ways to ensure it's keeping the trace data you need to see. Unlike our standard distributed tracing options, Infinite Tracing can process more trace data. It uses superior tail-based sampling (sampling after data is collected), as opposed to the head-based sampling that our standard tracing feature uses. Resources for learning more about Infinite Tracing: Infinite Tracing product page Technical details about sampling and architecture Requirements Requirements differ depending on your pricing plan: New Relic One pricing: requires Pro or Enterprise edition. Original pricing: requires New Relic help to enable it for your organization. For questions, contact your New Relic account representative. Enable Infinite Tracing When enabling Infinite Tracing, you should ideally enable it for all associated services. If you have a mix of Infinite Tracing and our standard tracing solutions enabled, traces will have configuration conflict issues. To start setting up Infinite Tracing, and to see specific requirements, see the docs for these tools: Our language agents Our integrations for third-party telemetry tools Our Trace API Configure Infinite Tracing After enabling Infinite Tracing, there are various ways you can configure it to ensure it's keeping the data you want. See Configure.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 252.32703,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Introduction to Infinite <em>Tracing</em>",
        "sections": "Introduction to Infinite <em>Tracing</em>",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": " standard <em>distributed</em> <em>tracing</em> options, Infinite <em>Tracing</em> can process more <em>trace</em> <em>data</em>. It uses superior tail-based sampling (sampling after <em>data</em> is collected), as opposed to the head-based sampling that our standard <em>tracing</em> feature uses. Resources for learning more about Infinite <em>Tracing</em>: Infinite"
      },
      "id": "6072a6a4196a67faa964a788"
    },
    {
      "sections": [
        "Trace API general requirements and limits",
        "Endpoints",
        "Data formats",
        "Data limits",
        "Restricted attributes",
        "Request metadata (headers and query parameters)",
        "Important",
        "Response validation",
        "See HTTP status codes",
        "Exceeding span limits"
      ],
      "title": "Trace API general requirements and limits ",
      "type": "docs",
      "tags": [
        "Understand dependencies",
        "Distributed tracing",
        "Trace API"
      ],
      "external_id": "f77504082dae8374e0c7009a31abebbd4c0123f3",
      "image": "",
      "url": "https://docs.newrelic.com/docs/distributed-tracing/trace-api/trace-api-general-requirements-limits/",
      "published_at": "2021-05-05T20:02:51Z",
      "updated_at": "2021-05-05T20:02:51Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Information about Trace API data requirements, including: Data specifications and max limits Required metadata (headers, query parameters) Response validation details This document applies to the Trace API overall. For rules regarding specific data formats, see: New Relic-format trace data Zipkin-format trace data Endpoints All trace data is sent via a POST to a Trace API endpoint. We have a few endpoints, depending on your setup: Default Trace API endpoint: https://trace-api.newrelic.com/trace/v1 EU data centers: https://trace-api.eu.newrelic.com/trace/v1 (see other EU endpoints). Infinite Tracing: when you complete the Trace observer setup, you get a custom YOUR_TRACE_OBSERVER_URL value to use as an endpoint. If you're using an integration that uses the Trace API (for example, these integrations), you must configure that integration with that endpoint. You will also want to adjust the sampling of your tracing service to send us 100% of spans. For FedRAMP, see FedRAMP endpoints. Data formats Currently, the Trace API accepts two types of data formats: zipkin: For reporting Zipkin trace data. Zipkin data must be Zipkin JSON v2. newrelic: For reporting all other trace data. Data limits Data limits and rules: Condition Limit Max age of span timestamp values 20 minutes. timestamp must be within 20 minutes of current time at ingest, or within 20 minutes from the time the last span with the same trace.id was received by New Relic. Max payload size 1MB (10^6 bytes) (gzip compression supported) Max requests per minute 100K Max spans per minute per account family Dependent on agreement. Max limit: 2M. Max spans per trace 50K Max attributes per span 200 Max span attribute value length 4000 characters Allowed HTTP protocols HTTPS only Cross-account visibility of span details Potential data obfuscation To see an example of how span limits are enforced, see Exceeding limits. Restricted attributes The attributes in the table below are restricted in the newrelic-format JSON (in the attributes block) and in the zipkin-format JSON (in the tags block). Any values with these keys will be omitted: Restricted attribute description entityGuid string Unique identifier for the entity that created this span. Generated from service.name, if available. guid string Used for backwards compatibility with data from APM agents. Request metadata (headers and query parameters) The following table shows the required request metadata for all trace data formats. This metadata can be sent as HTTP headers on an ingest request or, in some cases, provided as query parameters, which may be required for tracing frameworks that don't allow header modification. Important Security note: We suggest using headers because query parameters are present in the URL and may be logged before being encrypted and received by New Relic. All data sent as query parameters must be URL-safe. Header Query param? Details Content-Type No Required. Must be application/json. Content-Length No Required. The length of the request body in octets (8-bit bytes) unless sent with chunked encoding. This header is generally set by default by the underlying HTTP client sending the data and in most cases should not require any additional effort by the end user. Api-Key Yes (case-sensitive) Required. The Trace API requires the Insights insert key. If this is provided as both a header and a query parameter, the values must match. Content-Encoding No Required if compressed payload. The value must be gzip. Data-Format Yes Required for zipkin. Optional for newrelic. If present, Data-Format-Version must also be present. Data-Format-Version Yes Required for zipkin. If present, Data-Format must also be present. There are only two possible pairings for these values: If Data-Format is zipkin, Data-Format-Version must be 2. If Data-Format is newrelic, Data-Format-Version must be 1. x-request-id No Optional - Reserved for future use. The value must be a valid UUID4. The value is expected to be unique for each request. Response validation A response for successfully sending trace data will include a requestId. For example: {\"requestId\":\"c1bb62fc-001a-b000-0000-016bb152e1bb\"} Copy There are two ways success/errors are signaled: HTTP status code (synchronous). Authentication and request errors will be signaled via HTTP status code. See HTTP status codes Code Meaning 202 Data accepted. This means that you've passed preliminary checks, but is not a guarantee that the data has been successfully parsed and indexed as part of a distributed trace. 400 The structure of the request was invalid. Errors with query parameters, etc. 403 Authentication error. May occur with an invalid license key or if you lack necessary entitlement to use the Trace API. 404 The request path is incorrect. 405 For any request method other than POST. 408 The request took too long to reach the endpoint. 411 The Content-Length header wasn’t included. 413 The payload was too big. 414 The request URI was too long. 415 The Content-Type or Content-Encoding was invalid. 429 The request rate quota has been exceeded. 431 The request headers are too long. 5xx There was a server error (please retry). NrIntegrationError events (asynchronous). Errors with the JSON payload or other semantic errors are asynchronously signaled via NrIntegrationError events that are stored in the account whose license key is associated with the request. For all errors of this type, the attribute newRelicFeature will be Distributed Tracing and requestId will be the requestId from the endpoint response. If you receive a 202 response and don't see an NrIntegrationError event, your data should be visible in New Relic One's global distributed tracing UI in about a minute. You should be able to find the trace using a standard trace search like: traceId = TRACE_ID_SENT Copy Exceeding span limits When you exceed your span rate limit, an NrIntegrationError event is generated. You can query rate limit messages with this NRQL: SELECT * FROM NrIntegrationError WHERE newRelicFeature = 'Distributed Tracing' AND category = 'RateLimit' AND rateLimitType = 'SpansPerMinute' Copy To get a notification when you exceed the limit, you can set up a NRQL alert. We calculate a rolling 10-minute average based on your span rate limit. This allows for temporary rate bursts, and lets us prioritize keeping and dropping complete traces instead of indiscriminately dropping spans on a per minute limit basis. In the example below of exceeding the rate, the rate limit is the default 100,000 spans per minute. New Relic allows a burst above 100K for a couple of minutes without downsampling, because the remaining minutes in the 10-minute window averaged under 100K spans/minute. For the previous 10 minutes (8:50 - 9:00) the service received 60,000 spans/minute. Minute Spans sent to API Total for past 10 minutes 8:59 60,000 600,000 9:00 40,000 580,000 9:01 50,000 570,000 9:02 250,000 760,000 9:03 220,000 920,000 9:04 125,000 985,000 9:05 70,000 995,000 9:06 50,000 985,000 9:07 40,000 965,000 9:08 40,000 945,000 9:09 40,000 925,000",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 250.51773,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Trace</em> API general requirements <em>and</em> limits ",
        "sections": "<em>Trace</em> API general requirements <em>and</em> limits",
        "tags": "<em>Understand</em> <em>dependencies</em>",
        "body": ". For all errors of this type, the attribute newRelicFeature will be <em>Distributed</em> <em>Tracing</em> and requestId will be the requestId from the endpoint response. If you receive a 202 response and don&#x27;t see an NrIntegrationError event, your <em>data</em> should be visible in New Relic One&#x27;s global <em>distributed</em> <em>tracing</em> <em>UI</em>"
      },
      "id": "6071cf7628ccbcf8b851c158"
    }
  ],
  "/docs/fedramp-endpoint-logs-metrics": [
    {
      "sections": [
        "FedRAMP-compliant endpoints",
        "Customer FedRAMP obligations",
        "Overview of data sources",
        "Agents",
        "APM agents",
        "Mobile agents",
        "Infrastructure monitoring",
        "Infrastructure agent versions below 1.15.0",
        "Browser agent",
        "Data-ingest APIs",
        "Metric API",
        "Telemetry integrations",
        "Telemetry SDKs",
        "Event API",
        "Log API",
        "Log forwarders",
        "Trace API"
      ],
      "title": "FedRAMP-compliant endpoints",
      "type": "docs",
      "tags": [
        "Security",
        "Security and Privacy",
        "Compliance"
      ],
      "external_id": "ffce8ad6f802717392aca80e0965c9f3fe77ffdf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/security/security-privacy/compliance/fedramp-compliant-endpoints/",
      "published_at": "2021-05-04T18:28:40Z",
      "updated_at": "2021-05-04T18:28:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This document provides information on FedRAMP-compliant endpoints in New Relic. For more information about our security accreditation for the Federal Risk and Authorization Management Program (FedRAMP), see our data encryption documentation. Customer FedRAMP obligations New Relic customers must meet all of the following requirements for New Relic’s FedRAMP environment: New Relic-approved customers: New Relic’s FedRAMP-Moderate authorized environment is only available to New Relic-approved customers. For more information, contact your New Relic account representative. Order form: Customer’s order form with New Relic must include customer’s eligibility for FedRAMP. Subscription level: Customer must have a current and valid subscription to New Relic Full-Stack Observability Enterprise or New Relic-approved subscription. Authorized New Relic endpoints: Customer must send its data only to New Relic’s FedRAMP-designated endpoints. Authorized services and features: Customer must use only FedRAMP audited and authorized New Relic services and features (see below). Overview of data sources There are multiple ways to get data into New Relic. This doc has two sections: Agent settings: for our APM agents, infrastructure agent, browser agent, and mobile agent. Data-ingest APIs: for our Metric API, Event API, Trace API, and Log API, and the integrations that use those APIs. Agents New Relic has several agents for reporting data, like our APM agents, infrastructure agents, mobile agents, and browser agent. Setting these agents to send FedRAMP-compliant data involves setting a configuration setting to use the relevant FedRAMP endpoint. APM agents To ensure FedRAMP compliance, all APM agent configurations must report to gov-collector.newrelic.com rather than the default. Depending on the agent, you can either use code-based configuration or an environment variable. Here are details on enabling this: Language Code or environment variable C SDK In code: strcpy(_newrelic_app_config_t->redirect_collector, \"gov-collector.newrelic.com\"); Copy Environment variable: none Go In code: app, err = newrelic.NewApplication( newrelic.ConfigAppName(\"App Name\"), newrelic.ConfigLicense(os.Getenv(\"NEW_RELIC_LICENSE_KEY\")), func(cfg *newrelic.Config) { cfg.Host = \"gov-collector.newrelic.com\" }, ) Copy Environment variable: NEW_RELIC_HOST Java In newrelic.yml: common: &default_settings host: gov-collector.newrelic.com Copy Or set a system property of: newrelic.config.host Copy Environment variable: NEW_RELIC_HOST .NET In your XML config next to the license key: <service licenseKey=\"YOUR_LICENSE_KEY\" host=\"gov-collector.newrelic.com\"/> Copy Environment variable: NEW_RELIC_HOST Node.js In newrelic.js: host: 'gov-collector.newrelic.com' Copy Environment variable: NEW_RELIC_HOST PHP In newrelic.ini: newrelic.daemon.collector_host = gov-collector.newrelic.com Copy Environment variable: none Python In newrelic.ini: [newrelic] host = gov-collector.newrelic.com Copy Environment variable: NEW_RELIC_HOST Ruby In newrelic.yml: common: &default_settings host: gov-collector.newrelic.com Copy Environment variable: NEW_RELIC_HOST Elixir (open source agent) In config.exs: config :new_relic_agent, host: \"gov-collector.newrelic.com\" Copy Environment variable: NEW_RELIC_HOST For more on configuring APM agents, see APM configuration. Mobile agents To ensure FedRAMP compliance when using our mobile monitoring agents, all agent configurations must report to gov-mobile-collector.newrelic.com rather than the default. You must use code-based configuration. Environment variables are not available. Framework-specific configurations: Agent Code or environment variable Android In code: NewRelic.withApplicationToken({APP_TOKEN}) .usingCollectorAddress(\"gov-mobile-collector.newrelic.com\") .usingCrashCollectorAddress(\"gov-mobile-crash.newrelic.com\") .start(this.getApplication()); Copy Environment variable: none iOS In code: [NewRelic startWithApplicationToken:@\"{APP_TOKEN}\" andCollectorAddress:@\"gov-mobile-collector.newrelic.com\" andCrashCollectorAddress:@\"gov-mobile-crash.newrelic.com\"]; Copy Environment variable: none Infrastructure monitoring If you have infrastructure agent version 1.15.0 or higher, simply enable the FedRAMP configuration option. If you have an older agent version, use the following values to edit your YAML configuration: Infrastructure agent versions below 1.15.0 If you have an infrastructure agent version below 1.15.0, you must change three of the endpoints used for reporting. To set these endpoints, you can change your YAML configuration or use environment variables. YAML config field Endpoint URL collector_url https://gov-infra-api.newrelic.com Copy identity_url https://gov-identity-api.newrelic.com Copy command_channel_url https://gov-infrastructure-command-api.newrelic.com Copy To edit environment variables, use these values: Environment variable Endpoint URL NRIA_COLLECTOR_URL https://gov-infra-api.newrelic.com Copy NRIA_IDENTITY_URL https://gov-identity-api.newrelic.com Copy NRIA_COMMAND_CHANNEL_URL https://gov-infrastructure-command-api.newrelic.com Copy Browser agent To configure the browser agent to use a FedRAMP-compliant endpoint, you must use the copy-paste method method (other browser agent install methods are not supported) and edit the browser code’s script element tag so that the domain is gov-bam.nr-data.net for both beacon and error_beacon, like this: window.NREUM||(NREUM={});NREUM.info={\"beacon\":\"gov-bam.nr-data.net\",\"errorBeacon\":\"gov-bam.nr-data.net\"... Copy Data-ingest APIs Below are details about the FedRAMP endpoint for our ingest APIs: Metric API, the Event API, the Log API, and the Trace API. Metric API To ensure FedRAMP compliance when using the Metric API, instead of sending metric data to the default Metric API endpoint of https://metric-api.newrelic.com/metric/v1, it must be sent to https://gov-metric-api.newrelic.com/metric/v1. The Metric API can be used directly but it's mainly used by various New Relic tools. Below are instructions showing where to edit the configuration for setting the FedRAMP endpoint. Telemetry integrations Here are instructions for our open source telemetry integrations that report metric data: Dropwizard: use the overrideUri configuration. Kamon: use the metric-ingest-url configuration. See Override endpoints. Micrometer: override the public String uri() method on your NewRelicRegistryConfig to return the new endpoint. See an example. Prometheus: Prometheus OpenMetrics: if you are using our nri-prometheus helm chart, you can change the endpoint in your values.yml file, like in this example. If you're using the nri-bundle chart, you need to nest this value under the nri-prometheus key to propagate it to the sub-chart. Remote write integration: not available. Telemetry SDKs Here are instructions for our Telemetry SDKs that report metric data: Go: use the MetricsURLOverride configuration. Java: in the MetricBatchSender section, configure the endpoint. See an example. .NET: use the MetricUrlOverride configuration. Node.js: edit the METRIC_HOST = 'metric-api.newrelic.com' configuration. Python: edit the HOST = \"metric-api.newrelic.com\" configuration. Event API To ensure FedRAMP compliance for the Event API, all traffic reporting to insights-collector.newrelic.com must instead report to gov-insights-collector.newrelic.com. The Event API endpoint is configurable for the following Telemetry SDKs. The Telemetry SDKs are used by our open-source telemetry integrations. Language Solution Java Telemetry SDK In code: SenderConfiguration configuration = SenderConfiguration .builder( \"gov-insights-collector.newrelic.com\", EventBatchSender.EVENTS_PATH) .build(); EventBatchSender eventBatchSender = EventBatchSender.create(configuration); Copy Python Telemetry SDK In code: event_client = EventClient(host=\"gov-insights-collector.newrelic.com\") Copy For more information, see our Telemetry API documentation in GitHub. Log API To ensure FedRAMP compliance for data sent via the Log API, the solution for almost all our logging tools is to replace the https://log-api.newrelic.com/log/v1 endpoint with https://gov-log-api.newrelic.com/log/v1. Here are details for various tools: Log forwarders Here are details on changing the endpoint for our log forwarders: AWS Firelens: Add the endpoint property to the options field of the logConfiguration, similar to to the EU account endpoint change shown in these Firelens endpoint configuration instructions. Fluentbit: Use our Fluentbit endpoint configuration. Fluentd: Use our Fluentd endpoint instructions. Infrastructure agent: See FedRAMP for infrastructure. Kubernetes: Our Kubernetes integration logs are based on fluentbit’s output plugin. Use these endpoint instructions. Logstash: Use our Logstash endpoint configuration. Syslog: For configuring syslog clients, see TCP endpoint configuration. S3: Not available. Vector: Not available. To use the Log API directly, you'd edit the Log API endpoint configuration. Trace API To ensure FedRAMP compliance for data sent via the Trace API (including telemetry integrations that use this API), replace the https://trace-api.newrelic.com/trace/v1 endpoint with https://gov-trace-api.newrelic.com/trace/v1. Notes about FedRAMP compliance for other trace data: Trace data is reported by some of our agents, like our APM agents, browser agent, and mobile agent. To enable FedRAMP compliance for that data, you would enable FedRAMP for the applicable agent. Currently Infinite Tracing is not FedRAMP compliant.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 610.1283,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>FedRAMP</em>-compliant <em>endpoints</em>",
        "sections": "<em>FedRAMP</em>-compliant <em>endpoints</em>",
        "tags": "Security <em>and</em> Privacy",
        "body": " and error_beacon, like this: window.NREUM||(NREUM={});NREUM.info={&quot;beacon&quot;:&quot;gov-bam.nr-data.net&quot;,&quot;errorBeacon&quot;:&quot;gov-bam.nr-data.net&quot;... Copy Data-ingest APIs Below are details about the <em>FedRAMP</em> <em>endpoint</em> for our ingest APIs: <em>Metric</em> API, the Event API, the <em>Log</em> API, and the Trace API. <em>Metric</em> API To ensure"
      },
      "id": "603e945164441f64384e8872"
    },
    {
      "image": "",
      "url": "https://docs.newrelic.com/whats-new/2021/03/fedramp-logs-metrics/",
      "sections": [
        "FedRAMP: Logs and Metrics now certified",
        "Accelerate government IT modernization"
      ],
      "published_at": "2021-05-04T17:11:27Z",
      "title": "FedRAMP: Logs and Metrics now certified",
      "updated_at": "2021-05-04T17:11:27Z",
      "type": "docs",
      "external_id": "683cc62dc2addc69602ccc714dcada9713161d6c",
      "document_type": "nr1_announcement",
      "popularity": 1,
      "body": "Accelerate government IT modernization ​ Protecting your data is our highest priority, which is why we achieved the US Government’s rigorous FedRAMP Moderate certification in 2020. And now we’re adding support for Logs and Metrics to our long list of supported services. ​ New Relic’s FedRAMP authority to operate enables US federal government customers (and those that work with the federal government) to get the same level of real-time insights as commercial organizations, while still ensuring compliance with established security standards ​ Access to New Relic’s FedRAMP environment is based on the following: ​ Your account must be specifically approved by New Relic. You must send your data only to New Relic’s FedRAMP-designated endpoints. Check out the Nerdlog segment below to learn more: Don’t miss the Nerdlog LIVE every Thursday at 12 p.m. PT (8 p.m. UTC) on Twitch to get the latest product updates from the people who built them.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 551.7722,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>FedRAMP</em>: <em>Logs</em> <em>and</em> <em>Metrics</em> now certified",
        "sections": "<em>FedRAMP</em>: <em>Logs</em> <em>and</em> <em>Metrics</em> now certified",
        "body": "Accelerate government IT modernization ​ Protecting your data is our highest priority, which is why we achieved the US Government’s rigorous <em>FedRAMP</em> Moderate certification in 2020. And now we’re adding support for <em>Logs</em> and <em>Metrics</em> to our long list of supported services. ​ New Relic’s <em>FedRAMP</em>"
      },
      "id": "606a22bfe7b9d2706694462a"
    },
    {
      "sections": [
        "Regulatory audits for New Relic services",
        "Customer FedRAMP obligations",
        "Time frames",
        "Audits",
        "Customer risk management"
      ],
      "title": "Regulatory audits for New Relic services",
      "type": "docs",
      "tags": [
        "Security",
        "Security and Privacy",
        "Compliance"
      ],
      "external_id": "30dc1bcd07cce70ead8896f1c2f2a95b5da85120",
      "image": "",
      "url": "https://docs.newrelic.com/docs/security/security-privacy/compliance/regulatory-audits-new-relic-services/",
      "published_at": "2021-05-05T16:00:41Z",
      "updated_at": "2021-05-05T16:00:40Z",
      "document_type": "page",
      "popularity": 1,
      "body": "This list is current. Last updated 23 December, 2020. This document describes New Relic's products and services as they relate to regulatory framework compliance status. Customer FedRAMP obligations New Relic customers must meet all of the following requirements for New Relic’s FedRAMP environment: New Relic-approved customers: New Relic’s FedRAMP-Moderate authorized environment is only available to New Relic-approved customers. For more information, contact your New Relic account representative. Order form: Customer’s order form with New Relic must include customer’s eligibility for FedRAMP. Subscription level: Customer must have a current and valid subscription to New Relic Full-Stack Observability Enterprise or New Relic-approved subscription. Authorized New Relic endpoints: Customer must send its data only to New Relic’s FedRAMP-designated endpoints. Authorized services and features: Customer must use only FedRAMP audited and authorized New Relic services and features. Time frames New Relic's time frames for supported regulatory frameworks and annual audits include: SOC2 Type 2 audit: Reviews New Relic's implementation and maintenance of controls for the previous 12 months. The annual audit spans August 1 of the previous year through July 31 of the current year (for example, August 1, 2019 through July 31, 2020). FedRAMP Agency (Moderate): Reviews New Relic's implementation and maintenance of NIST 800-53 rev. 4 controls for the previous 12 months. The annual audit spans November 28 of the previous year through November 28 of the current year (for example, November 28, 2019 through November 28, 2020). Audits New Relic service SOC2 FedRAMP Moderate (Agency level ATO) Alerts APM AWS Metric Streams Browser monitoring Incident Intelligence (Applied Intelligence) Infrastructure monitoring Insights Logging (with exception of log patterns) Metric API Mobile agents Programmability: New Relic One apps Plugins Proactive Detection (Applied Intelligence) Serverless Synthetic monitoring Trace API Notes on icons used in this table: A check indicates the SOC2 or FedRAMP authorized service was included in the most recent FedRAMP annual audit. An information circle icon indicates the service will be included in upcoming annual audits and assessments. A caution icon indicates the service is on the roadmap for regulatory framework compliance at a time frame to be determined. Customer risk management All New Relic services are intended to be covered by our compliance programs. However, a new service may not be covered by one or more of our compliance programs at any given time throughout the year. This is primarily dependent on the timing when the service achieved General Availability (GA) status and the timing of the specific compliance program's annual authorization, certification, or assessment. You can use any New Relic service regardless of its compliance program status. However, if a service is not yet in scope of our compliance programs, we encourage you to consider your risk appetite in the decision to use the specific New Relic product or service. If you choose to use New Relic services that are not yet in our compliance program scope, you assume the responsibility to review, understand, and risk-manage your security controls as you deem appropriate. You also have the option to wait for New Relic to authorize these services before you use them.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 523.0137,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Regulatory audits <em>for</em> New Relic services",
        "sections": "Customer <em>FedRAMP</em> obligations",
        "tags": "Security <em>and</em> Privacy",
        "body": ") Infrastructure monitoring Insights Logging (with exception of <em>log</em> patterns) <em>Metric</em> API Mobile agents Programmability: New Relic One apps Plugins Proactive Detection (Applied Intelligence) Serverless Synthetic monitoring Trace API Notes on icons used in this table: A check indicates the SOC2 or <em>FedRAMP</em>"
      },
      "id": "603e81e728ccbc68bfeba794"
    }
  ],
  "/docs/full-stack-observability/index": [
    {
      "sections": [
        "New Relic guided install overview",
        "Supported APM agents",
        "Why it matters",
        "Some technical detail",
        "Important",
        "On-host integration (OHI) recipes"
      ],
      "title": "New Relic guided install overview",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Observe everything",
        "Get started"
      ],
      "external_id": "2058522f6cb1e82dbbe111a176c22ec4aa515ae5",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/observe-everything/get-started/new-relic-guided-install-overview/",
      "published_at": "2021-05-04T22:25:26Z",
      "updated_at": "2021-04-12T05:47:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Instrument your systems and send telemetry data to New Relic with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click the Guided install button. If your account reports data through our EU datacenter, click EU Guided install. Guided install EU Guided install Our infrastructure agent discovers the applications and infrastructure and log sources running in your environment, and recommends which ones should be instrumented. The install automates the configuration and deployment of each system you choose to instrument. Supported APM agents If you have a .NET Windows application on IIS, the guided install configures and enables an APM agent. Guided install for .NET EU Guided install for .NET Why it matters With our guided install, you can instrument your applications and infrastructure and start seeing your data in New Relic in minutes. The guided install uses our command line interface (CLI), the infrastructure agent for your host environment, and a library of installation recipes to instrument your applications and infrastructure for you. That means less toil for you. Because our instrumentation recipes are open source, you can modify existing recipes, or build new ones, to suit your needs. Some technical detail The New Relic guided install uses open source installation recipes to instrument on-host integrations. These recipes include installation and setup commands, information about logs, and metadata related to what’s being installed. They're collected in a YAML file for each type of system and have all of the installation details necessary to install the infrastructure agent for a specific integration. Important On Windows, our guided install only supports Microsoft SQL Server, logs, and the infrastructure agent. All other integrations are only supported on Linux. On-host integration (OHI) recipes The guided install automates the discovery, configuration, and installation of OHIs. However, there may be times when you want to instrument them one-by-one using the CLI install command. To install any individual on-host integration, run this command: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=API_KEY NEW_RELIC_ACCOUNT_ID=ACCOUNT_ID /usr/local/bin/newrelic install -n INTEGRATION-FLAG Copy For example: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=<API_KEY> NEW_RELIC_ACCOUNT_ID=<ACCOUNT_ID> /usr/local/bin/newrelic install -n apache-open-source-integration Copy The table lists the integrations supported by the guided install CLI command. The specific on-host integration commands are provided for your reference. Our open source integrations send performance metrics and inventory data from your servers and applications to the New Relic platform. You can view pre-built dashboards of your metric data, create alert policies, and create your own custom queries and charts. Integration Command Apache newrelic install -n apache-open-source-integration Cassandra newrelic install -n cassandra-open-source-integration Couchbase newrelic install -n couchbase-open-source-integration ElasticSearch newrelic install -n elasticsearch-open-source-integration HAProxy newrelic install -n haproxy-open-source-integration HashiCorp Consul newrelic install -n hashicorp-consul-open-source-integration JMX newrelic install -n jmx-open-source-integration Memcached newrelic install -n memcached-open-source-integration Microsoft SQL Server (Windows only) newrelic install -n mssql-server-integration-installer MongoDB newrelic install -n mongodb-open-source-integration MySQL newrelic install -n mysql-open-source-integration Nagios newrelic install -n nagios-open-source-integration Nginx newrelic install -n nginx-open-source-integration PostgreSQL newrelic install -n postgres-open-source-integration RabbitMQ newrelic install -n rabbitmq-open-source-integration Redis newrelic install -n redis-open-source-integration Varnish Cache newrelic install -n varnish-cache-open-source-integration",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 892.2568,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>"
      },
      "id": "604130a7e7b9d299cb2a07c0"
    },
    {
      "sections": [
        "Get started with Full-Stack Observability",
        "Tip",
        "You’re in control because you understand your system",
        "All the answers in one place",
        "Start anywhere"
      ],
      "title": "Get started with Full-Stack Observability",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Observe everything",
        "Get started"
      ],
      "external_id": "e7fc0bf91fa26b38a11933b6570c8b1e483a1ff9",
      "image": "https://docs.newrelic.com/static/44970161aec793f3141cfcdc0fc96a57/c1b63/observability.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/observe-everything/get-started/get-started-full-stack-observability/",
      "published_at": "2021-05-04T22:30:38Z",
      "updated_at": "2021-03-16T08:30:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Full-Stack Observability is the power of knowing what is happening in your digital system and why, at any time, whatever solution you’re using. It’s getting the whole picture of everything that enables your applications and devices to deliver value to your customers, from the container running a microservice in the cloud to a mobile website's shopping cart button. Tip To use Full-Stack Observability and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. You’re in control because you understand your system New Relic helps you cut through the layers of complexity surrounding your systems by bringing together and connecting data from any instrumented source and environment, without having to jump between tools. You can interrogate your data for patterns, discover them using our data platform, or get proactive results from our machine learning tools. New Relic provides answers to essential questions in one place. All the answers in one place As a full user you get access to our entire set of observability tools. All our tools are interconnected and accessible in New Relic One. All the data you bring to New Relic through agents and integrations are metrics, events, logs, and traces that feed our platform's analytics and monitoring capabilities. New Relic links your data in a meaningful way so that you can explore it, build dashboards, and set up alerts. Full-Stack Observability curated experiences allow to visualize, analyze, and troubleshoot your entire software stack in one unified platform. The New Relic Explorer consolidates all the entities in your system, and how they're connected, in a single place, so you can easily detect performance trends and issues. By automatically connecting infrastructure health with application performance and end-user behavior, you can cut through the noise to find the signal. Start anywhere Being fully-connected, New Relic allows you to start your observability journey from any element of your stack. For example, you can get to crucial infrastructure logs from traces of an application running on a problematic Kubernetes pod. The more entities you instrument, the more data you'll bring in. The more data you've brought to New Relic, the more you'll understand your metrics, events, logs, and traces. You want to instrument Start with Keep exploring Front-end applications Mobile applications User behavior and flows New Relic Explorer Browser monitoring Mobile monitoring Synthetic monitoring Single page monitoring Scripted browsers Containerized minions Workloads Backend applications Serverless applications New Relic Explorer Application monitoring Serverless monitoring Learning about Apdex Distributed tracing Logs in context APM data to infrastructure Workloads Infrastructure hosts and services (on-premise, cloud, orchestrated) Container environments and orchestration tools (Kubernetes, ECS, etc.) Infrastructure monitoring Infrastructure integrations Kubernetes integration Docker integration ECS integration Log forwarding APM data to infrastructure Custom integrations Kubernetes cluster explorer Infrastructure alerts Workloads",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 792.37244,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Get started with <em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "sections": "Get started with <em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": "<em>Full</em>-<em>Stack</em> <em>Observability</em> is the power of knowing what is happening in your digital system and why, at any time, whatever solution you’re using. It’s getting the whole picture of everything that enables your applications and devices to deliver value to your customers, from the container running"
      },
      "id": "603e891528ccbce6d9eba765"
    },
    {
      "sections": [
        "Introduction to New Relic integrations",
        "Tip",
        "Choose what's right for you",
        "Create your own solutions"
      ],
      "title": "Introduction to New Relic integrations",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Instrument everything",
        "Get started"
      ],
      "external_id": "03217983a29af22737c1163da9ef0811b29c2bcd",
      "image": "",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/instrument-everything/get-started-new-relic-instrumentation/introduction-new-relic-integrations/",
      "published_at": "2021-05-06T04:28:30Z",
      "updated_at": "2021-03-16T07:30:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We provide hundreds of solutions to get your data into New Relic so you can analyze the data in one place. They give you a steady flow of useful data to fix problems quickly, maintain complex systems, improve your code, and accelerate your digital transformation. You can bring in data from hundreds of applications, frameworks, services, operating systems, and other technologies. Our integrations gather the data, and the agents send it to New Relic. The solution you need may require you to install both an integration and an agent. In some cases, you can just install our agents that contain integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to get you started. Tip To use integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Choose what's right for you We offer a wide range of solutions so you can easily collect data across your environment. You may only need one of our solutions to get the data you need, or you can choose a variety of options to capture a broader range of data types. Go to New Relic Integrations to find solutions that fit your environment. Here is a sample of what you’ll find there: Application performance monitoring (APM): C, Go, Java, Node, .NET, PHP, Python, and Ruby Mobile apps: Android and iOS Browser monitoring: Google Chrome, Mozilla Firefox, Microsoft Internet Explorer, and Apple Safari Host monitoring: Linux and Microsoft Windows Cloud platform monitoring: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry integrations: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you create your own: Use New Relic Flex to create lightweight monitoring solutions using infrastructure monitoring. Use New Relic Telemetry SDKs to build custom solutions for sending metrics, traces, and more. Build your own New Relic One applications that you can share with your colleagues, or edit open source applications in our catalog.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 792.2869,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": " <em>observability</em> platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Choose what&#x27;s right for you We offer a wide range of solutions so you can easily collect data across your environment. You may only need"
      },
      "id": "603e817f28ccbc4857eba798"
    }
  ],
  "/docs/full-stack-observability/instrument-everything/get-started-new-relic-instrumentation/introduction-new-relic-integrations": [
    {
      "sections": [
        "New Relic guided install overview",
        "Supported APM agents",
        "Why it matters",
        "Some technical detail",
        "Important",
        "On-host integration (OHI) recipes"
      ],
      "title": "New Relic guided install overview",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Observe everything",
        "Get started"
      ],
      "external_id": "2058522f6cb1e82dbbe111a176c22ec4aa515ae5",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/observe-everything/get-started/new-relic-guided-install-overview/",
      "published_at": "2021-05-04T22:25:26Z",
      "updated_at": "2021-04-12T05:47:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Instrument your systems and send telemetry data to New Relic with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click the Guided install button. If your account reports data through our EU datacenter, click EU Guided install. Guided install EU Guided install Our infrastructure agent discovers the applications and infrastructure and log sources running in your environment, and recommends which ones should be instrumented. The install automates the configuration and deployment of each system you choose to instrument. Supported APM agents If you have a .NET Windows application on IIS, the guided install configures and enables an APM agent. Guided install for .NET EU Guided install for .NET Why it matters With our guided install, you can instrument your applications and infrastructure and start seeing your data in New Relic in minutes. The guided install uses our command line interface (CLI), the infrastructure agent for your host environment, and a library of installation recipes to instrument your applications and infrastructure for you. That means less toil for you. Because our instrumentation recipes are open source, you can modify existing recipes, or build new ones, to suit your needs. Some technical detail The New Relic guided install uses open source installation recipes to instrument on-host integrations. These recipes include installation and setup commands, information about logs, and metadata related to what’s being installed. They're collected in a YAML file for each type of system and have all of the installation details necessary to install the infrastructure agent for a specific integration. Important On Windows, our guided install only supports Microsoft SQL Server, logs, and the infrastructure agent. All other integrations are only supported on Linux. On-host integration (OHI) recipes The guided install automates the discovery, configuration, and installation of OHIs. However, there may be times when you want to instrument them one-by-one using the CLI install command. To install any individual on-host integration, run this command: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=API_KEY NEW_RELIC_ACCOUNT_ID=ACCOUNT_ID /usr/local/bin/newrelic install -n INTEGRATION-FLAG Copy For example: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=<API_KEY> NEW_RELIC_ACCOUNT_ID=<ACCOUNT_ID> /usr/local/bin/newrelic install -n apache-open-source-integration Copy The table lists the integrations supported by the guided install CLI command. The specific on-host integration commands are provided for your reference. Our open source integrations send performance metrics and inventory data from your servers and applications to the New Relic platform. You can view pre-built dashboards of your metric data, create alert policies, and create your own custom queries and charts. Integration Command Apache newrelic install -n apache-open-source-integration Cassandra newrelic install -n cassandra-open-source-integration Couchbase newrelic install -n couchbase-open-source-integration ElasticSearch newrelic install -n elasticsearch-open-source-integration HAProxy newrelic install -n haproxy-open-source-integration HashiCorp Consul newrelic install -n hashicorp-consul-open-source-integration JMX newrelic install -n jmx-open-source-integration Memcached newrelic install -n memcached-open-source-integration Microsoft SQL Server (Windows only) newrelic install -n mssql-server-integration-installer MongoDB newrelic install -n mongodb-open-source-integration MySQL newrelic install -n mysql-open-source-integration Nagios newrelic install -n nagios-open-source-integration Nginx newrelic install -n nginx-open-source-integration PostgreSQL newrelic install -n postgres-open-source-integration RabbitMQ newrelic install -n rabbitmq-open-source-integration Redis newrelic install -n redis-open-source-integration Varnish Cache newrelic install -n varnish-cache-open-source-integration",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 171.0269,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": "<em>Instrument</em> your systems and send telemetry data to New Relic with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to <em>get</em> <em>started</em>? Click the Guided install button. If your"
      },
      "id": "604130a7e7b9d299cb2a07c0"
    },
    {
      "sections": [
        "Cloud services integrations",
        "AWS integrations",
        "GCP integrations",
        "Azure integrations"
      ],
      "title": "Cloud services integrations",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Instrument everything",
        "Instrument core services and applications"
      ],
      "external_id": "71020c70edfb43072cbf081b3eccd3b18f9e6289",
      "image": "https://docs.newrelic.com/static/78ac85c1fc41f94776fce7235e327f01/69538/img-integration-aws%25402x.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/instrument-everything/instrument-core-services-applications/cloud-services-integrations/",
      "published_at": "2021-05-06T04:29:41Z",
      "updated_at": "2021-03-16T06:35:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic you can easily instrument your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 155.2667,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": "With New Relic you can easily <em>instrument</em> your services in AWS, Google Cloud Platform, and Azure. AWS integrations Introduction to AWS integrations List of AWS integrations GCP integrations Introduction to GCP integrations List of GCP integrations Azure integrations Introduction to Azure integrations List of Azure integrations"
      },
      "id": "603e829ae7b9d20bb12a080c"
    },
    {
      "sections": [
        "New Relic Flex: Build your own integration",
        "What is Flex?",
        "Requirements",
        "How does Flex work?",
        "Example config",
        "Learn more"
      ],
      "title": "New Relic Flex: Build your own integration",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Instrument everything",
        "Develop your own integrations"
      ],
      "external_id": "d9e77fa458eb408a90de1ebdd60891694ea6feb2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/flex-integration-tool-build-your-own-integration/",
      "published_at": "2021-05-04T16:43:29Z",
      "updated_at": "2021-03-11T08:47:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides integrations for many popular services and frameworks. If you have New Relic and want to report data from a service we don't have an integration for, there are several ways New Relic lets you create your own integration: With New Relic infrastructure monitoring, you can use our lightweight Flex tool (recommended, documented below) or, to build a complete on-host integration, see our Integrations SDK. Telemetry (metrics, traces) monitoring solutions: Use our Telemetry SDKs. Build a custom New Relic One application that uses your own JavaScript UI functionality. What is Flex? New Relic Flex is an application-agnostic, all-in-one tool that allows you to collect metric data from a wide variety of services. It comes bundled with our infrastructure agent. You can instrument any app that exposes metrics over a standard protocol (HTTP, file, shell) in a standard format (for example, JSON or plain text): you create a YAML config file, start the Infrastructure agent, and your data is reported to New Relic. Flex can send event and metric data to New Relic from a wide range of sources. Using a simple YAML config file, you can run HTTP/HTTPS requests, run shell commands, and parse file content. You can also use standard regex expressions to customize and control the data gathered from those inputs. See an example config. After collecting and cleaning up the data, you can then query Flex data in New Relic, create custom charts for it, and use that data in your dashboards. Requirements Flex comes bundled with our infrastructure agent. To use Flex, you need: Infrastructure agent version 1.10.7 or higher (update | check version) running on Linux, Windows, or Kubernetes. How does Flex work? Flex uses our infrastructure agent to execute commands that generate the data you want to report. Here's a brief overview of how Flex works to report data: You define the data you want to report in a YAML configuration file, located in the infrastructure agent package. See an example configuration: Example config The following is an example of a Flex configuration for monitoring the uptime of a Linux server. This configuration is placed in a file named flex-uptime.yml. This would be placed in the infrastructure agent's integration configuration section, located at /etc/newrelic-infra/integrations.d/flex-uptime.yml. integrations: - name: nri-flex config: name: linuxUptimeIntegration apis: - name: Uptime commands: - run: 'cat /proc/uptime' split: horizontal split_by: \\s+ set_header: [uptimeSeconds,idletimeSeconds] Copy Some notes on what this configuration does: run defines the command to execute. The name indicated by name: Uptime is appended with Sample to generate an event called UptimeSample. The name should not start with the ESX or PCF prefix. The split_by: \\s+ splits the fields based on the space character. The command generates attributes attached to the UptimeSample event. The attributes are named uptimeSeconds and idletimeSeconds. The infrastructure agent runs Flex at a frequency based on its own configuration (default: every 30 seconds) and sends the data to New Relic. You can then query your data, create custom charts with it, and add it to dashboards. Learn more The Flex integration comes bundled with the infrastructure agent. Learn more about requirements. To learn more, see our complete documentation on GitHub: README Tutorial",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 153.44992,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": "? New Relic Flex is an application-agnostic, all-in-one tool that allows you to collect metric data from a wide variety of services. It comes bundled with our infrastructure agent. You can <em>instrument</em> any app that exposes metrics over a standard protocol (HTTP, file, shell) in a standard format"
      },
      "id": "6044e44f196a678d15960f6e"
    }
  ],
  "/docs/full-stack-observability/instrument-everything/instrument-core-services-applications/cloud-services-integrations": [
    {
      "sections": [
        "Introduction to New Relic integrations",
        "Tip",
        "Choose what's right for you",
        "Create your own solutions"
      ],
      "title": "Introduction to New Relic integrations",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Instrument everything",
        "Get started"
      ],
      "external_id": "03217983a29af22737c1163da9ef0811b29c2bcd",
      "image": "",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/instrument-everything/get-started-new-relic-instrumentation/introduction-new-relic-integrations/",
      "published_at": "2021-05-06T04:28:30Z",
      "updated_at": "2021-03-16T07:30:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We provide hundreds of solutions to get your data into New Relic so you can analyze the data in one place. They give you a steady flow of useful data to fix problems quickly, maintain complex systems, improve your code, and accelerate your digital transformation. You can bring in data from hundreds of applications, frameworks, services, operating systems, and other technologies. Our integrations gather the data, and the agents send it to New Relic. The solution you need may require you to install both an integration and an agent. In some cases, you can just install our agents that contain integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to get you started. Tip To use integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Choose what's right for you We offer a wide range of solutions so you can easily collect data across your environment. You may only need one of our solutions to get the data you need, or you can choose a variety of options to capture a broader range of data types. Go to New Relic Integrations to find solutions that fit your environment. Here is a sample of what you’ll find there: Application performance monitoring (APM): C, Go, Java, Node, .NET, PHP, Python, and Ruby Mobile apps: Android and iOS Browser monitoring: Google Chrome, Mozilla Firefox, Microsoft Internet Explorer, and Apple Safari Host monitoring: Linux and Microsoft Windows Cloud platform monitoring: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry integrations: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you create your own: Use New Relic Flex to create lightweight monitoring solutions using infrastructure monitoring. Use New Relic Telemetry SDKs to build custom solutions for sending metrics, traces, and more. Build your own New Relic One applications that you can share with your colleagues, or edit open source applications in our catalog.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 185.6764,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": " of <em>applications</em>, frameworks, <em>services</em>, operating systems, and other technologies. Our integrations gather the data, and the agents send it to New Relic. The solution you need may require you to install both an integration and an agent. In some cases, you can just install our agents that contain"
      },
      "id": "603e817f28ccbc4857eba798"
    },
    {
      "sections": [
        "New Relic Flex: Build your own integration",
        "What is Flex?",
        "Requirements",
        "How does Flex work?",
        "Example config",
        "Learn more"
      ],
      "title": "New Relic Flex: Build your own integration",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Instrument everything",
        "Develop your own integrations"
      ],
      "external_id": "d9e77fa458eb408a90de1ebdd60891694ea6feb2",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/flex-integration-tool-build-your-own-integration/",
      "published_at": "2021-05-04T16:43:29Z",
      "updated_at": "2021-03-11T08:47:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides integrations for many popular services and frameworks. If you have New Relic and want to report data from a service we don't have an integration for, there are several ways New Relic lets you create your own integration: With New Relic infrastructure monitoring, you can use our lightweight Flex tool (recommended, documented below) or, to build a complete on-host integration, see our Integrations SDK. Telemetry (metrics, traces) monitoring solutions: Use our Telemetry SDKs. Build a custom New Relic One application that uses your own JavaScript UI functionality. What is Flex? New Relic Flex is an application-agnostic, all-in-one tool that allows you to collect metric data from a wide variety of services. It comes bundled with our infrastructure agent. You can instrument any app that exposes metrics over a standard protocol (HTTP, file, shell) in a standard format (for example, JSON or plain text): you create a YAML config file, start the Infrastructure agent, and your data is reported to New Relic. Flex can send event and metric data to New Relic from a wide range of sources. Using a simple YAML config file, you can run HTTP/HTTPS requests, run shell commands, and parse file content. You can also use standard regex expressions to customize and control the data gathered from those inputs. See an example config. After collecting and cleaning up the data, you can then query Flex data in New Relic, create custom charts for it, and use that data in your dashboards. Requirements Flex comes bundled with our infrastructure agent. To use Flex, you need: Infrastructure agent version 1.10.7 or higher (update | check version) running on Linux, Windows, or Kubernetes. How does Flex work? Flex uses our infrastructure agent to execute commands that generate the data you want to report. Here's a brief overview of how Flex works to report data: You define the data you want to report in a YAML configuration file, located in the infrastructure agent package. See an example configuration: Example config The following is an example of a Flex configuration for monitoring the uptime of a Linux server. This configuration is placed in a file named flex-uptime.yml. This would be placed in the infrastructure agent's integration configuration section, located at /etc/newrelic-infra/integrations.d/flex-uptime.yml. integrations: - name: nri-flex config: name: linuxUptimeIntegration apis: - name: Uptime commands: - run: 'cat /proc/uptime' split: horizontal split_by: \\s+ set_header: [uptimeSeconds,idletimeSeconds] Copy Some notes on what this configuration does: run defines the command to execute. The name indicated by name: Uptime is appended with Sample to generate an event called UptimeSample. The name should not start with the ESX or PCF prefix. The split_by: \\s+ splits the fields based on the space character. The command generates attributes attached to the UptimeSample event. The attributes are named uptimeSeconds and idletimeSeconds. The infrastructure agent runs Flex at a frequency based on its own configuration (default: every 30 seconds) and sends the data to New Relic. You can then query your data, create custom charts with it, and add it to dashboards. Learn more The Flex integration comes bundled with the infrastructure agent. Learn more about requirements. To learn more, see our complete documentation on GitHub: README Tutorial",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 153.44992,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": "? New Relic Flex is an application-agnostic, all-in-one tool that allows you to collect metric data from a wide variety of <em>services</em>. It comes bundled with our infrastructure agent. You can <em>instrument</em> any app that exposes metrics over a standard protocol (HTTP, file, shell) in a standard format"
      },
      "id": "6044e44f196a678d15960f6e"
    },
    {
      "sections": [
        "New Relic guided install overview",
        "Supported APM agents",
        "Why it matters",
        "Some technical detail",
        "Important",
        "On-host integration (OHI) recipes"
      ],
      "title": "New Relic guided install overview",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Observe everything",
        "Get started"
      ],
      "external_id": "2058522f6cb1e82dbbe111a176c22ec4aa515ae5",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/observe-everything/get-started/new-relic-guided-install-overview/",
      "published_at": "2021-05-04T22:25:26Z",
      "updated_at": "2021-04-12T05:47:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Instrument your systems and send telemetry data to New Relic with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click the Guided install button. If your account reports data through our EU datacenter, click EU Guided install. Guided install EU Guided install Our infrastructure agent discovers the applications and infrastructure and log sources running in your environment, and recommends which ones should be instrumented. The install automates the configuration and deployment of each system you choose to instrument. Supported APM agents If you have a .NET Windows application on IIS, the guided install configures and enables an APM agent. Guided install for .NET EU Guided install for .NET Why it matters With our guided install, you can instrument your applications and infrastructure and start seeing your data in New Relic in minutes. The guided install uses our command line interface (CLI), the infrastructure agent for your host environment, and a library of installation recipes to instrument your applications and infrastructure for you. That means less toil for you. Because our instrumentation recipes are open source, you can modify existing recipes, or build new ones, to suit your needs. Some technical detail The New Relic guided install uses open source installation recipes to instrument on-host integrations. These recipes include installation and setup commands, information about logs, and metadata related to what’s being installed. They're collected in a YAML file for each type of system and have all of the installation details necessary to install the infrastructure agent for a specific integration. Important On Windows, our guided install only supports Microsoft SQL Server, logs, and the infrastructure agent. All other integrations are only supported on Linux. On-host integration (OHI) recipes The guided install automates the discovery, configuration, and installation of OHIs. However, there may be times when you want to instrument them one-by-one using the CLI install command. To install any individual on-host integration, run this command: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=API_KEY NEW_RELIC_ACCOUNT_ID=ACCOUNT_ID /usr/local/bin/newrelic install -n INTEGRATION-FLAG Copy For example: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=<API_KEY> NEW_RELIC_ACCOUNT_ID=<ACCOUNT_ID> /usr/local/bin/newrelic install -n apache-open-source-integration Copy The table lists the integrations supported by the guided install CLI command. The specific on-host integration commands are provided for your reference. Our open source integrations send performance metrics and inventory data from your servers and applications to the New Relic platform. You can view pre-built dashboards of your metric data, create alert policies, and create your own custom queries and charts. Integration Command Apache newrelic install -n apache-open-source-integration Cassandra newrelic install -n cassandra-open-source-integration Couchbase newrelic install -n couchbase-open-source-integration ElasticSearch newrelic install -n elasticsearch-open-source-integration HAProxy newrelic install -n haproxy-open-source-integration HashiCorp Consul newrelic install -n hashicorp-consul-open-source-integration JMX newrelic install -n jmx-open-source-integration Memcached newrelic install -n memcached-open-source-integration Microsoft SQL Server (Windows only) newrelic install -n mssql-server-integration-installer MongoDB newrelic install -n mongodb-open-source-integration MySQL newrelic install -n mysql-open-source-integration Nagios newrelic install -n nagios-open-source-integration Nginx newrelic install -n nginx-open-source-integration PostgreSQL newrelic install -n postgres-open-source-integration RabbitMQ newrelic install -n rabbitmq-open-source-integration Redis newrelic install -n redis-open-source-integration Varnish Cache newrelic install -n varnish-cache-open-source-integration",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 127.08975,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": " your <em>applications</em> and infrastructure and start seeing your data in New Relic in minutes. The guided install uses our command line interface (CLI), the infrastructure agent for your host environment, and a library of installation recipes to <em>instrument</em> your <em>applications</em> and infrastructure for you"
      },
      "id": "604130a7e7b9d299cb2a07c0"
    }
  ],
  "/docs/full-stack-observability/observe-everything/get-started/get-started-full-stack-observability": [
    {
      "sections": [
        "New Relic guided install overview",
        "Supported APM agents",
        "Why it matters",
        "Some technical detail",
        "Important",
        "On-host integration (OHI) recipes"
      ],
      "title": "New Relic guided install overview",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Observe everything",
        "Get started"
      ],
      "external_id": "2058522f6cb1e82dbbe111a176c22ec4aa515ae5",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/observe-everything/get-started/new-relic-guided-install-overview/",
      "published_at": "2021-05-04T22:25:26Z",
      "updated_at": "2021-04-12T05:47:14Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Instrument your systems and send telemetry data to New Relic with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to get started? Click the Guided install button. If your account reports data through our EU datacenter, click EU Guided install. Guided install EU Guided install Our infrastructure agent discovers the applications and infrastructure and log sources running in your environment, and recommends which ones should be instrumented. The install automates the configuration and deployment of each system you choose to instrument. Supported APM agents If you have a .NET Windows application on IIS, the guided install configures and enables an APM agent. Guided install for .NET EU Guided install for .NET Why it matters With our guided install, you can instrument your applications and infrastructure and start seeing your data in New Relic in minutes. The guided install uses our command line interface (CLI), the infrastructure agent for your host environment, and a library of installation recipes to instrument your applications and infrastructure for you. That means less toil for you. Because our instrumentation recipes are open source, you can modify existing recipes, or build new ones, to suit your needs. Some technical detail The New Relic guided install uses open source installation recipes to instrument on-host integrations. These recipes include installation and setup commands, information about logs, and metadata related to what’s being installed. They're collected in a YAML file for each type of system and have all of the installation details necessary to install the infrastructure agent for a specific integration. Important On Windows, our guided install only supports Microsoft SQL Server, logs, and the infrastructure agent. All other integrations are only supported on Linux. On-host integration (OHI) recipes The guided install automates the discovery, configuration, and installation of OHIs. However, there may be times when you want to instrument them one-by-one using the CLI install command. To install any individual on-host integration, run this command: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=API_KEY NEW_RELIC_ACCOUNT_ID=ACCOUNT_ID /usr/local/bin/newrelic install -n INTEGRATION-FLAG Copy For example: curl -Ls https://raw.githubusercontent.com/newrelic/newrelic-cli/master/scripts/install.sh | bash && sudo NEW_RELIC_API_KEY=<API_KEY> NEW_RELIC_ACCOUNT_ID=<ACCOUNT_ID> /usr/local/bin/newrelic install -n apache-open-source-integration Copy The table lists the integrations supported by the guided install CLI command. The specific on-host integration commands are provided for your reference. Our open source integrations send performance metrics and inventory data from your servers and applications to the New Relic platform. You can view pre-built dashboards of your metric data, create alert policies, and create your own custom queries and charts. Integration Command Apache newrelic install -n apache-open-source-integration Cassandra newrelic install -n cassandra-open-source-integration Couchbase newrelic install -n couchbase-open-source-integration ElasticSearch newrelic install -n elasticsearch-open-source-integration HAProxy newrelic install -n haproxy-open-source-integration HashiCorp Consul newrelic install -n hashicorp-consul-open-source-integration JMX newrelic install -n jmx-open-source-integration Memcached newrelic install -n memcached-open-source-integration Microsoft SQL Server (Windows only) newrelic install -n mssql-server-integration-installer MongoDB newrelic install -n mongodb-open-source-integration MySQL newrelic install -n mysql-open-source-integration Nagios newrelic install -n nagios-open-source-integration Nginx newrelic install -n nginx-open-source-integration PostgreSQL newrelic install -n postgres-open-source-integration RabbitMQ newrelic install -n rabbitmq-open-source-integration Redis newrelic install -n redis-open-source-integration Varnish Cache newrelic install -n varnish-cache-open-source-integration",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 265.86105,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": "Instrument your systems and send telemetry data to New Relic with guided install. Our guided install creates a customized CLI command for your environment that downloads and installs the New Relic CLI and the infrastructure agent. Ready to <em>get</em> <em>started</em>? Click the Guided install button. If your"
      },
      "id": "604130a7e7b9d299cb2a07c0"
    },
    {
      "sections": [
        "Introduction to New Relic integrations",
        "Tip",
        "Choose what's right for you",
        "Create your own solutions"
      ],
      "title": "Introduction to New Relic integrations",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Instrument everything",
        "Get started"
      ],
      "external_id": "03217983a29af22737c1163da9ef0811b29c2bcd",
      "image": "",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/instrument-everything/get-started-new-relic-instrumentation/introduction-new-relic-integrations/",
      "published_at": "2021-05-06T04:28:30Z",
      "updated_at": "2021-03-16T07:30:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We provide hundreds of solutions to get your data into New Relic so you can analyze the data in one place. They give you a steady flow of useful data to fix problems quickly, maintain complex systems, improve your code, and accelerate your digital transformation. You can bring in data from hundreds of applications, frameworks, services, operating systems, and other technologies. Our integrations gather the data, and the agents send it to New Relic. The solution you need may require you to install both an integration and an agent. In some cases, you can just install our agents that contain integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to get you started. Tip To use integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Choose what's right for you We offer a wide range of solutions so you can easily collect data across your environment. You may only need one of our solutions to get the data you need, or you can choose a variety of options to capture a broader range of data types. Go to New Relic Integrations to find solutions that fit your environment. Here is a sample of what you’ll find there: Application performance monitoring (APM): C, Go, Java, Node, .NET, PHP, Python, and Ruby Mobile apps: Android and iOS Browser monitoring: Google Chrome, Mozilla Firefox, Microsoft Internet Explorer, and Apple Safari Host monitoring: Linux and Microsoft Windows Cloud platform monitoring: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry integrations: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you create your own: Use New Relic Flex to create lightweight monitoring solutions using infrastructure monitoring. Use New Relic Telemetry SDKs to build custom solutions for sending metrics, traces, and more. Build your own New Relic One applications that you can share with your colleagues, or edit open source applications in our catalog.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 152.1328,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": " integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to <em>get</em> you <em>started</em>. Tip To use integrations and infrastructure monitoring, as well as the rest of our"
      },
      "id": "603e817f28ccbc4857eba798"
    },
    {
      "sections": [
        "APM best practices guide",
        "Tip",
        "1. Standardize application names",
        "How to do it",
        "2. Add tags to your applications",
        "Caution",
        "3. Create and evaluate alert policies",
        "4. Identify and set up key transactions",
        "5. Track deployment history",
        "6. Review APM reports",
        "7. Review your environment with service maps",
        "8. Keep current",
        "9. Manage user access",
        "Important"
      ],
      "title": "APM best practices guide",
      "type": "docs",
      "tags": [
        "New Relic solutions",
        "Best practices guides",
        "Full-stack observability"
      ],
      "external_id": "368a1a5688384d5bebf128604a9b8f190d335524",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/best-practices-guides/full-stack-observability/apm-best-practices-guide/",
      "published_at": "2021-05-04T23:58:05Z",
      "updated_at": "2021-03-30T13:08:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Application Monitoring Tips You Need To Know It's one thing to know how to use APM, but it's another thing to know how to use New Relic's application performance monitoring software well. Here are some best practices designed to help you become an APM master—and a key asset to your team! Tip To get a high-level overview of all your applications and services, use the New Relic Explorer in New Relic One. 1. Standardize application names Most New Relic agents provide a default application name, such as \"My Application\" or \"PHP Application,\" if you don't specify one in your New Relic configuration file. You don't want to end up with 20 identically named applications, be sure to select a descriptive identifier for your apps as soon you deploy them. To keep things consistent and easy to navigate, New Relic recommends standardizing your application naming (for example, all apps in Staging append [ Staging] or the like at the end of their names). Ideally, you want your new Java applications to be named automatically to reduce the chances of typographical errors and misnaming. How to do it For Java applications, automatic application naming can come from the following sources: Request attribute Servlet init parameter Filter init parameter Web app context parameter Web app context name (display name) Web app context path Choose the method that fits best your needs and follow these steps. For non-Java applications, there are no automatic naming methods, so refer to the documentation for your APM agent. 2. Add tags to your applications When several different applications use the same account, and each application spans multiple environments (for example, development, test, pre-production, production), it can be hard to find a specific application in your overview dashboard. That's why we recommend adding tags to your apps so that you can segment them into logical groups. The two most common tags that mature APM customers use are application name and environment. So, for example, if you wanted to view the billing application in Test, you could simply filter by \"billing app\" (name tag) and \"test\" (environment tag). Caution In the APM agent configuration settings files, use the labels field to add tags to your data. For example, see this description of the Python labels setting. APM is designed so that apps can roll up into an unlimited number of meaningful tag categories. How to do it Read the overview: Use tags to organize and group what you monitor. Follow these instructions for creating tags: NerdGraph tagging API tutorial. 3. Create and evaluate alert policies When key performance indicators spike or drop, individuals and teams in your organization need to be notified. Alerting in New Relic provides a set of tools including dynamic baselines that allow you to detect problems before they impact your end users. Alert policies can be set up in two primary ways: Static threshold alerts are great when you already know the nature of an application and its normal behaviors aren't likely to change anytime soon. Apdex score, response time, error rate, throughput are some of the static thresholds you can create alert policies on. Dynamic baseline alerts make it easy to determine and set dynamic alert thresholds for applications with varying seasonal patterns and growth trends (which make it difficult to set thresholds that define normal behavior). These alerts use baselines modeled from your application’s historical metric data. Each alert policy can contain as many conditions as you need, and each alert condition includes three components: Type of condition (metric, external service, and so on) Entities that the policy targets (for example, apps monitored by New Relic APM or New Relic Browser, hosts monitored by New Relic Infrastructure, and so on) Thresholds that escalate into alerting situations with increasing severity Once you have your alerting set up, you then want to make sure you're taking advantage of all viable notification channels. After all, what good are alerts if no one knows about them? You can manage alerts by creating specific user groups and by leveraging New Relic's integrated alert channels, including Slack, PagerDuty, webhooks, and email. Be sure to evaluate alert policies on a regular basis to ensure that they are always valid. How to do it See the detailed documentation: To set up dynamic baseline alerts and choose an application, follow standard procedures. You will see a preview of the metric with the predicted baseline You can select a metric for that application and see the corresponding baseline. Then, using the threshold sliders, you can set how closely you want your threshold to follow the baseline prediction. To set up static threshold alerts for your Apdex settings, follow standard procedures. To set up your alert notification channels, follow standard procedures. 4. Identify and set up key transactions Depending on the nature of your application, some transactions may be more important to you than others. New Relic's key transactions feature is designed to help you closely monitor what you consider to be your app's most business-critical transactions, whether that's end-user or app response time, call counts, error rates, or something else. You can also set alert threshold levels for notifications when your key transactions are performing poorly. How to do it In the menu bar, select More > Key transactions, and then select Add more. Then select the app and web transaction or, from the selected transaction, select Track as key transaction. Type a name for the key transaction, and select Track key transaction. Optional: If the agent for the selected app supports custom alerting, use the default values that New Relic automatically fills, or select Edit key alert transaction policy to set the Apdex and alert threshold values. To view the key transactions dashboard details, select View new key transaction. 5. Track deployment history When development teams are pushing new code out as frequently as possible, it can be hard to measure the impact that each deployment is having on performance. One way to stay in tune with how these changes are affecting your application is with deployment reports. These reports list recent deployments and their impact on end-users and app servers' Apdex scores, along with response times, throughput, and errors. You can also view and drill down into the details to catch errors related to recent deployments, or file a ticket and share details with your team. How to do it From the New Relic menu bar, select APM > (selected app) > Events > Deployments. To view performance after a deployment, go to the selected app's Overview dashboard in the Recent events section. A blue vertical bar on a chart indicates a deployment. To view summary information about the deployment, point to the blue bar. 6. Review APM reports From SLA, deployment, and capacity to scalability, host usage reports, and more, New Relic APM offers a variety of downloadable reporting tools surfacing historical trends—all great ways to report to senior executive teams or customers. Take a look at the full list of reports and use them to your advantage. How to do it From the New Relic APM menu bar, select Applications > (selected app) > Reports. Select the report you'd like to see. If you want to save or export a report to share, select Download this report as .csv, which will create a report with comma-separated values. 7. Review your environment with service maps Use New Relic service maps, a feature included in APM, to understand how apps and services in your architecture connect and talk to each other. Service maps are visual, customizable representations of your application architecture. Maps automatically show you your app's connections and dependencies, including databases and external services. Health indicators and performance metrics show you the current operational status for every part of your architecture. How to do it Go to one.newrelic.com > More > service maps. To get started, read the instructions in Introduction to service maps. 8. Keep current With New Relic’s SaaS platform, getting new features is as easy as updating your agent. Most likely your organization already has a set of scripts for deploying application upgrades into your environment. In a similar fashion, you can also automate your New Relic agent deployment to ensure that your systems are up to date. Both Puppet and Chef scripts are great examples of deployment frameworks that make life easier by allowing you to automate your entire deployment and management process. How to do it Regularly review which version of the agent you're using so that you know when an update is needed. If the latest agent release contains a needed fix or added functionality, download it. To deploy the agent automatically (preferred as a method to avoid errors): Use existing deployment scripts, provided they can be adapted to handle the deployment. OR Create and maintain a script that specifically deploys and configures the New Relic agent. Ideally, the script would pull the agent files from a repository where the files are versioned (for rollback purposes). Once the script has been created, shut down the application (unless script handles this). Run the deployment script. Start the application (unless script handles this). If problems arise, run the script to roll back to the previous version. To deploy the agent manually: Back up the current agent directory. Deploy the updated agent into the existing agent directory. Modify configuration files by comparing new files with existing files. In particular, make sure things like license key and custom extensions are copied over to the new configuration. Restart the application. If problems arise, restore the old agent using the backup and restart. 9. Manage user access Important This is available only for accounts on our original product-based pricing plan. Enable role-based access control (RBAC) and single sign-on (SSO) New Relic allows authorized individuals to access the broadest possible amount of data, regardless of their assigned role. As an Owner or Administrator of your New Relic account, you can control the permissions of individual users or entire roles with RBAC. To find out what is possible and how to make changes, see Users and roles. Security is no doubt of utmost concern to your organization. To simplify password management for your employees and strengthen security, you may already be using SSO with your other systems. You should do the same with New Relic. Using New Relic's SSO integration feature, account administrators will be able to enforce strong passwords and restrict login via a corporate authentication mechanism. This way, New Relic users who have already authenticated using a corporate SSO system will be able to bypass the New Relic login prompt. How to do it Log in to New Relic as an admin and go to the SSO configuration page. From the New Relic title bar, select (your account name) > Account Settings > Integrations > Single Sign On. From the SAML Single Sign On page, review your New Relic SAML Service Provider details. To upload your SAML Identity Provider certificate, select Choose File, and then follow standard procedures to select and save the file. Copy and paste in (or type) the Remove login URL that your users will use for Single Sign-On. If your organization’s SAML integration provides a redirect URL for logout, copy and paste in (or type) the Logout landing URL; otherwise leave blank. Save, test, and enable.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.01745,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Full</em>-<em>stack</em> <em>observability</em>",
        "body": " and dependencies, including databases and external services. Health indicators and performance metrics show you the current operational status for every part of your architecture. How to do it Go to one.newrelic.com &gt; More &gt; service maps. To <em>get</em> <em>started</em>, read the instructions in Introduction to service maps. 8"
      },
      "id": "6044186564441f1f94378ecc"
    }
  ],
  "/docs/full-stack-observability/observe-everything/get-started/new-relic-guided-install-overview": [
    {
      "sections": [
        "Get started with Full-Stack Observability",
        "Tip",
        "You’re in control because you understand your system",
        "All the answers in one place",
        "Start anywhere"
      ],
      "title": "Get started with Full-Stack Observability",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Observe everything",
        "Get started"
      ],
      "external_id": "e7fc0bf91fa26b38a11933b6570c8b1e483a1ff9",
      "image": "https://docs.newrelic.com/static/44970161aec793f3141cfcdc0fc96a57/c1b63/observability.png",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/observe-everything/get-started/get-started-full-stack-observability/",
      "published_at": "2021-05-04T22:30:38Z",
      "updated_at": "2021-03-16T08:30:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Full-Stack Observability is the power of knowing what is happening in your digital system and why, at any time, whatever solution you’re using. It’s getting the whole picture of everything that enables your applications and devices to deliver value to your customers, from the container running a microservice in the cloud to a mobile website's shopping cart button. Tip To use Full-Stack Observability and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. You’re in control because you understand your system New Relic helps you cut through the layers of complexity surrounding your systems by bringing together and connecting data from any instrumented source and environment, without having to jump between tools. You can interrogate your data for patterns, discover them using our data platform, or get proactive results from our machine learning tools. New Relic provides answers to essential questions in one place. All the answers in one place As a full user you get access to our entire set of observability tools. All our tools are interconnected and accessible in New Relic One. All the data you bring to New Relic through agents and integrations are metrics, events, logs, and traces that feed our platform's analytics and monitoring capabilities. New Relic links your data in a meaningful way so that you can explore it, build dashboards, and set up alerts. Full-Stack Observability curated experiences allow to visualize, analyze, and troubleshoot your entire software stack in one unified platform. The New Relic Explorer consolidates all the entities in your system, and how they're connected, in a single place, so you can easily detect performance trends and issues. By automatically connecting infrastructure health with application performance and end-user behavior, you can cut through the noise to find the signal. Start anywhere Being fully-connected, New Relic allows you to start your observability journey from any element of your stack. For example, you can get to crucial infrastructure logs from traces of an application running on a problematic Kubernetes pod. The more entities you instrument, the more data you'll bring in. The more data you've brought to New Relic, the more you'll understand your metrics, events, logs, and traces. You want to instrument Start with Keep exploring Front-end applications Mobile applications User behavior and flows New Relic Explorer Browser monitoring Mobile monitoring Synthetic monitoring Single page monitoring Scripted browsers Containerized minions Workloads Backend applications Serverless applications New Relic Explorer Application monitoring Serverless monitoring Learning about Apdex Distributed tracing Logs in context APM data to infrastructure Workloads Infrastructure hosts and services (on-premise, cloud, orchestrated) Container environments and orchestration tools (Kubernetes, ECS, etc.) Infrastructure monitoring Infrastructure integrations Kubernetes integration Docker integration ECS integration Log forwarding APM data to infrastructure Custom integrations Kubernetes cluster explorer Infrastructure alerts Workloads",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 236.09904,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Get</em> <em>started</em> with <em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "sections": "<em>Get</em> <em>started</em> with <em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": "<em>Full</em>-<em>Stack</em> <em>Observability</em> is the power of knowing what is happening in your digital system and why, at any time, whatever solution you’re using. It’s getting the whole picture of <em>everything</em> that enables your applications and devices to deliver value to your customers, from the container running"
      },
      "id": "603e891528ccbce6d9eba765"
    },
    {
      "sections": [
        "Introduction to New Relic integrations",
        "Tip",
        "Choose what's right for you",
        "Create your own solutions"
      ],
      "title": "Introduction to New Relic integrations",
      "type": "docs",
      "tags": [
        "Full-Stack Observability",
        "Instrument everything",
        "Get started"
      ],
      "external_id": "03217983a29af22737c1163da9ef0811b29c2bcd",
      "image": "",
      "url": "https://docs.newrelic.com/docs/full-stack-observability/instrument-everything/get-started-new-relic-instrumentation/introduction-new-relic-integrations/",
      "published_at": "2021-05-06T04:28:30Z",
      "updated_at": "2021-03-16T07:30:04Z",
      "document_type": "page",
      "popularity": 1,
      "body": "We provide hundreds of solutions to get your data into New Relic so you can analyze the data in one place. They give you a steady flow of useful data to fix problems quickly, maintain complex systems, improve your code, and accelerate your digital transformation. You can bring in data from hundreds of applications, frameworks, services, operating systems, and other technologies. Our integrations gather the data, and the agents send it to New Relic. The solution you need may require you to install both an integration and an agent. In some cases, you can just install our agents that contain integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to get you started. Tip To use integrations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Choose what's right for you We offer a wide range of solutions so you can easily collect data across your environment. You may only need one of our solutions to get the data you need, or you can choose a variety of options to capture a broader range of data types. Go to New Relic Integrations to find solutions that fit your environment. Here is a sample of what you’ll find there: Application performance monitoring (APM): C, Go, Java, Node, .NET, PHP, Python, and Ruby Mobile apps: Android and iOS Browser monitoring: Google Chrome, Mozilla Firefox, Microsoft Internet Explorer, and Apple Safari Host monitoring: Linux and Microsoft Windows Cloud platform monitoring: Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP) Core infrastructure services: Kubernetes, NGINX, MySQL, and more Open source telemetry integrations: Prometheus, Micrometer, OpenTelemetry, and more Create your own solutions If you are looking for custom options, we have tools to help you create your own: Use New Relic Flex to create lightweight monitoring solutions using infrastructure monitoring. Use New Relic Telemetry SDKs to build custom solutions for sending metrics, traces, and more. Build your own New Relic One applications that you can share with your colleagues, or edit open source applications in our catalog.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 152.13278,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Full</em>-<em>Stack</em> <em>Observability</em>",
        "body": " integrations, such as our APM agents. Whatever data you need to bring in, chances are that we have options for your environment. If you prefer to make your own solutions, we also offer tools to <em>get</em> you <em>started</em>. Tip To use integrations and infrastructure monitoring, as well as the rest of our"
      },
      "id": "603e817f28ccbc4857eba798"
    },
    {
      "sections": [
        "APM best practices guide",
        "Tip",
        "1. Standardize application names",
        "How to do it",
        "2. Add tags to your applications",
        "Caution",
        "3. Create and evaluate alert policies",
        "4. Identify and set up key transactions",
        "5. Track deployment history",
        "6. Review APM reports",
        "7. Review your environment with service maps",
        "8. Keep current",
        "9. Manage user access",
        "Important"
      ],
      "title": "APM best practices guide",
      "type": "docs",
      "tags": [
        "New Relic solutions",
        "Best practices guides",
        "Full-stack observability"
      ],
      "external_id": "368a1a5688384d5bebf128604a9b8f190d335524",
      "image": "",
      "url": "https://docs.newrelic.com/docs/new-relic-solutions/best-practices-guides/full-stack-observability/apm-best-practices-guide/",
      "published_at": "2021-05-04T23:58:05Z",
      "updated_at": "2021-03-30T13:08:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Application Monitoring Tips You Need To Know It's one thing to know how to use APM, but it's another thing to know how to use New Relic's application performance monitoring software well. Here are some best practices designed to help you become an APM master—and a key asset to your team! Tip To get a high-level overview of all your applications and services, use the New Relic Explorer in New Relic One. 1. Standardize application names Most New Relic agents provide a default application name, such as \"My Application\" or \"PHP Application,\" if you don't specify one in your New Relic configuration file. You don't want to end up with 20 identically named applications, be sure to select a descriptive identifier for your apps as soon you deploy them. To keep things consistent and easy to navigate, New Relic recommends standardizing your application naming (for example, all apps in Staging append [ Staging] or the like at the end of their names). Ideally, you want your new Java applications to be named automatically to reduce the chances of typographical errors and misnaming. How to do it For Java applications, automatic application naming can come from the following sources: Request attribute Servlet init parameter Filter init parameter Web app context parameter Web app context name (display name) Web app context path Choose the method that fits best your needs and follow these steps. For non-Java applications, there are no automatic naming methods, so refer to the documentation for your APM agent. 2. Add tags to your applications When several different applications use the same account, and each application spans multiple environments (for example, development, test, pre-production, production), it can be hard to find a specific application in your overview dashboard. That's why we recommend adding tags to your apps so that you can segment them into logical groups. The two most common tags that mature APM customers use are application name and environment. So, for example, if you wanted to view the billing application in Test, you could simply filter by \"billing app\" (name tag) and \"test\" (environment tag). Caution In the APM agent configuration settings files, use the labels field to add tags to your data. For example, see this description of the Python labels setting. APM is designed so that apps can roll up into an unlimited number of meaningful tag categories. How to do it Read the overview: Use tags to organize and group what you monitor. Follow these instructions for creating tags: NerdGraph tagging API tutorial. 3. Create and evaluate alert policies When key performance indicators spike or drop, individuals and teams in your organization need to be notified. Alerting in New Relic provides a set of tools including dynamic baselines that allow you to detect problems before they impact your end users. Alert policies can be set up in two primary ways: Static threshold alerts are great when you already know the nature of an application and its normal behaviors aren't likely to change anytime soon. Apdex score, response time, error rate, throughput are some of the static thresholds you can create alert policies on. Dynamic baseline alerts make it easy to determine and set dynamic alert thresholds for applications with varying seasonal patterns and growth trends (which make it difficult to set thresholds that define normal behavior). These alerts use baselines modeled from your application’s historical metric data. Each alert policy can contain as many conditions as you need, and each alert condition includes three components: Type of condition (metric, external service, and so on) Entities that the policy targets (for example, apps monitored by New Relic APM or New Relic Browser, hosts monitored by New Relic Infrastructure, and so on) Thresholds that escalate into alerting situations with increasing severity Once you have your alerting set up, you then want to make sure you're taking advantage of all viable notification channels. After all, what good are alerts if no one knows about them? You can manage alerts by creating specific user groups and by leveraging New Relic's integrated alert channels, including Slack, PagerDuty, webhooks, and email. Be sure to evaluate alert policies on a regular basis to ensure that they are always valid. How to do it See the detailed documentation: To set up dynamic baseline alerts and choose an application, follow standard procedures. You will see a preview of the metric with the predicted baseline You can select a metric for that application and see the corresponding baseline. Then, using the threshold sliders, you can set how closely you want your threshold to follow the baseline prediction. To set up static threshold alerts for your Apdex settings, follow standard procedures. To set up your alert notification channels, follow standard procedures. 4. Identify and set up key transactions Depending on the nature of your application, some transactions may be more important to you than others. New Relic's key transactions feature is designed to help you closely monitor what you consider to be your app's most business-critical transactions, whether that's end-user or app response time, call counts, error rates, or something else. You can also set alert threshold levels for notifications when your key transactions are performing poorly. How to do it In the menu bar, select More > Key transactions, and then select Add more. Then select the app and web transaction or, from the selected transaction, select Track as key transaction. Type a name for the key transaction, and select Track key transaction. Optional: If the agent for the selected app supports custom alerting, use the default values that New Relic automatically fills, or select Edit key alert transaction policy to set the Apdex and alert threshold values. To view the key transactions dashboard details, select View new key transaction. 5. Track deployment history When development teams are pushing new code out as frequently as possible, it can be hard to measure the impact that each deployment is having on performance. One way to stay in tune with how these changes are affecting your application is with deployment reports. These reports list recent deployments and their impact on end-users and app servers' Apdex scores, along with response times, throughput, and errors. You can also view and drill down into the details to catch errors related to recent deployments, or file a ticket and share details with your team. How to do it From the New Relic menu bar, select APM > (selected app) > Events > Deployments. To view performance after a deployment, go to the selected app's Overview dashboard in the Recent events section. A blue vertical bar on a chart indicates a deployment. To view summary information about the deployment, point to the blue bar. 6. Review APM reports From SLA, deployment, and capacity to scalability, host usage reports, and more, New Relic APM offers a variety of downloadable reporting tools surfacing historical trends—all great ways to report to senior executive teams or customers. Take a look at the full list of reports and use them to your advantage. How to do it From the New Relic APM menu bar, select Applications > (selected app) > Reports. Select the report you'd like to see. If you want to save or export a report to share, select Download this report as .csv, which will create a report with comma-separated values. 7. Review your environment with service maps Use New Relic service maps, a feature included in APM, to understand how apps and services in your architecture connect and talk to each other. Service maps are visual, customizable representations of your application architecture. Maps automatically show you your app's connections and dependencies, including databases and external services. Health indicators and performance metrics show you the current operational status for every part of your architecture. How to do it Go to one.newrelic.com > More > service maps. To get started, read the instructions in Introduction to service maps. 8. Keep current With New Relic’s SaaS platform, getting new features is as easy as updating your agent. Most likely your organization already has a set of scripts for deploying application upgrades into your environment. In a similar fashion, you can also automate your New Relic agent deployment to ensure that your systems are up to date. Both Puppet and Chef scripts are great examples of deployment frameworks that make life easier by allowing you to automate your entire deployment and management process. How to do it Regularly review which version of the agent you're using so that you know when an update is needed. If the latest agent release contains a needed fix or added functionality, download it. To deploy the agent automatically (preferred as a method to avoid errors): Use existing deployment scripts, provided they can be adapted to handle the deployment. OR Create and maintain a script that specifically deploys and configures the New Relic agent. Ideally, the script would pull the agent files from a repository where the files are versioned (for rollback purposes). Once the script has been created, shut down the application (unless script handles this). Run the deployment script. Start the application (unless script handles this). If problems arise, run the script to roll back to the previous version. To deploy the agent manually: Back up the current agent directory. Deploy the updated agent into the existing agent directory. Modify configuration files by comparing new files with existing files. In particular, make sure things like license key and custom extensions are copied over to the new configuration. Restart the application. If problems arise, restore the old agent using the backup and restart. 9. Manage user access Important This is available only for accounts on our original product-based pricing plan. Enable role-based access control (RBAC) and single sign-on (SSO) New Relic allows authorized individuals to access the broadest possible amount of data, regardless of their assigned role. As an Owner or Administrator of your New Relic account, you can control the permissions of individual users or entire roles with RBAC. To find out what is possible and how to make changes, see Users and roles. Security is no doubt of utmost concern to your organization. To simplify password management for your employees and strengthen security, you may already be using SSO with your other systems. You should do the same with New Relic. Using New Relic's SSO integration feature, account administrators will be able to enforce strong passwords and restrict login via a corporate authentication mechanism. This way, New Relic users who have already authenticated using a corporate SSO system will be able to bypass the New Relic login prompt. How to do it Log in to New Relic as an admin and go to the SSO configuration page. From the New Relic title bar, select (your account name) > Account Settings > Integrations > Single Sign On. From the SAML Single Sign On page, review your New Relic SAML Service Provider details. To upload your SAML Identity Provider certificate, select Choose File, and then follow standard procedures to select and save the file. Copy and paste in (or type) the Remove login URL that your users will use for Single Sign-On. If your organization’s SAML integration provides a redirect URL for logout, copy and paste in (or type) the Logout landing URL; otherwise leave blank. Save, test, and enable.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 107.01744,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Full</em>-<em>stack</em> <em>observability</em>",
        "body": " and dependencies, including databases and external services. Health indicators and performance metrics show you the current operational status for every part of your architecture. How to do it Go to one.newrelic.com &gt; More &gt; service maps. To <em>get</em> <em>started</em>, read the instructions in Introduction to service maps. 8"
      },
      "id": "6044186564441f1f94378ecc"
    }
  ],
  "/docs/gateway-api-import-data-other-observability-platforms": [
    {
      "image": "https://developer.newrelic.com/static/93a294bc7852a6eea64a7baa55a0aca3/ba3ac/api-token-prompt.png",
      "url": "https://developer.newrelic.com/build-apps/ab-test/nerdstoragevault/",
      "sections": [
        "Access NerdStorageVault from your Nerdlet",
        "Course",
        "Store your API token",
        "Important",
        "Query your API token",
        "Create your token prompt",
        "Tip",
        "Pass your API token to TotalCancellations"
      ],
      "published_at": "2021-05-07T01:44:42Z",
      "title": "Access NerdStorageVault from your Nerdlet",
      "updated_at": "2021-05-05T01:54:57Z",
      "type": "developer",
      "external_id": "d35788d1b5e58689f48c12aa4cbdc2f446550102",
      "document_type": "page",
      "popularity": 1,
      "info": "Access NerdStorageVault from your Nerdlet",
      "body": "Course This lesson is part of a course that teaches you how to build a New Relic One application from the ground up. If you haven't already, check out the course introduction. Each lesson in the course builds upon the last, so make sure you've completed the last lesson, Access NerdStorage from your nerdlet, before starting this one. Throughout this course, you're building a New Relic One application that gathers telemetry data from a demo web service that's running an A/B test on a newsletter signup form. The purpose of your New Relic One application is to help you understand how design changes impact the number of high quality newsletter subscriptions your service gets. The business objective, to increase your service's high quality newsletter subscriptions, relies primarily on three key pieces of information: Number of page views per version Number of subscriptions per version Number of cancellations Cancellations are important because if one design version of your newsletter signup form results in a high number of subscriptions but also a high number of cancellations, then those subscriptions are not as valuable. In previous lessons, you gathered data for page views and subscriptions from New Relic's database (NRDB), but you still need cancellation data. Your demo application does not report cancellation data to New Relic, so you need to fetch that data from an external source. We've provided a service at https://api.nerdsletter.net/cancellations to return fake cancellation data for the purposes of this course. If you visit this URL in your browser, you'll see a brief message: \"Unauthorized\". This is because we created this service with a requirement that whoever requests its data must pass an Authorization header with the bearer token ABC123. So before you can request cancellation data from api.nerdsletter.net, you need to implement a few new behaviors in your application: Provide a mechanism for inputting an authorization token Persist the authorization token in a secure data store To input your authorization token, you'll use a Modal with a TextField. The secure data store you'll use is called NerdStorageVault. It's different from NerdStorage, which you used in the last lesson, because it only supports user storage and encrypts its data. Store your API token Step 1 of 3 Change to the add-nerdstoragevault/ab-test directory of the coursework repository: bash Copy $ cd nru-programmability-course/add-nerdstoragevault/ab-test This directory contains the code that we expect your application to have at this point in the course. By navigating to the correct directory at the start of each lesson, you leave your custom code behind, thereby protecting yourself from carrying incorrect code from one lesson to the next. Step 2 of 3 In your Nerdlet's index.js file, initialize state in AbTestNerdletNerdlet with a null token default: import React from 'react'; import { ChartGroup, Grid, GridItem } from 'nr1'; import EndTestSection from './end-test'; import NewsletterSignups from './newsletter-signups'; import PastTests from './past-tests'; import TotalCancellations from './total-cancellations'; import TotalSubscriptions from './total-subscriptions'; import VersionDescription from './description'; import VersionPageViews from './page-views'; import VersionTotals from './totals'; const ACCOUNT_ID = 123456 // <YOUR NEW RELIC ACCOUNT ID> const VERSION_A_DESCRIPTION = 'The newsletter signup message says, \"Sign up for our newsletter\"' const VERSION_B_DESCRIPTION = 'The newsletter signup message says, \"Sign up for our newsletter and get a free shirt!\"' export default class AbTestNerdletNerdlet extends React.Component { constructor() { super(...arguments); this.state = { token: null, } } render() { return <div> <Grid className=\"wrapper\"> <GridItem columnSpan={6}> <VersionDescription description={VERSION_A_DESCRIPTION} version=\"A\" /> </GridItem> <GridItem columnSpan={6}> <VersionDescription description={VERSION_B_DESCRIPTION} version=\"B\" /> </GridItem> <GridItem columnSpan={12}><hr /></GridItem> <GridItem columnSpan={12}><NewsletterSignups /></GridItem> <GridItem columnSpan={6}><TotalSubscriptions /></GridItem> <GridItem columnSpan={6}><TotalCancellations /></GridItem> <GridItem columnSpan={6}> <VersionTotals version='a' accountId={ACCOUNT_ID} /> </GridItem> <GridItem columnSpan={6}> <VersionTotals version='b' accountId={ACCOUNT_ID} /> </GridItem> <ChartGroup> <GridItem columnSpan={6}> <VersionPageViews version='a' /> </GridItem> <GridItem columnSpan={6}> <VersionPageViews version='b' /> </GridItem> </ChartGroup> <GridItem columnSpan={12}> <EndTestSection accountId={ACCOUNT_ID} versionADescription={VERSION_A_DESCRIPTION} versionBDescription={VERSION_B_DESCRIPTION} /> </GridItem> <GridItem columnSpan={12}> <PastTests accountId={ACCOUNT_ID} /> </GridItem> </Grid> </div> } } nerdlets/ab-test-nerdlet/index.js Copy Your Nerdlet uses this token state to manage the authorization token you'll later pass to the third-party service. However, a component's state is not a long-term solution for data management. For that, you need NerdStorageVault. Step 3 of 3 Implement a method, called storeToken(), which mutates NerdStorageVault data, and bind that method to the AbTestNerdletNerdlet instance: import React from 'react'; import { ChartGroup, Grid, GridItem, NerdGraphMutation, } from 'nr1'; import EndTestSection from './end-test'; import NewsletterSignups from './newsletter-signups'; import PastTests from './past-tests'; import TotalCancellations from './total-cancellations'; import TotalSubscriptions from './total-subscriptions'; import VersionDescription from './description'; import VersionPageViews from './page-views'; import VersionTotals from './totals'; const ACCOUNT_ID = 123456 // <YOUR NEW RELIC ACCOUNT ID> const VERSION_A_DESCRIPTION = 'The newsletter signup message says, \"Sign up for our newsletter\"' const VERSION_B_DESCRIPTION = 'The newsletter signup message says, \"Sign up for our newsletter and get a free shirt!\"' export default class AbTestNerdletNerdlet extends React.Component { constructor() { super(...arguments); this.state = { token: null, } this.storeToken = this.storeToken.bind(this); } storeToken(newToken) { if (newToken != this.state.token) { const mutation = ` mutation($key: String!, $token: SecureValue!) { nerdStorageVaultWriteSecret( scope: { actor: CURRENT_USER } secret: { key: $key, value: $token } ) { status errors { message type } } } `; const variables = { key: \"api_token\", token: newToken, }; NerdGraphMutation.mutate({ mutation: mutation, variables: variables }).then( (data) => { if (data.data.nerdStorageVaultWriteSecret.status === \"SUCCESS\") { this.setState({token: newToken}) } } ); } } render() { return <div> <Grid className=\"wrapper\"> <GridItem columnSpan={6}> <VersionDescription description={VERSION_A_DESCRIPTION} version=\"A\" /> </GridItem> <GridItem columnSpan={6}> <VersionDescription description={VERSION_B_DESCRIPTION} version=\"B\" /> </GridItem> <GridItem columnSpan={12}><hr /></GridItem> <GridItem columnSpan={12}><NewsletterSignups /></GridItem> <GridItem columnSpan={6}><TotalSubscriptions /></GridItem> <GridItem columnSpan={6}><TotalCancellations /></GridItem> <GridItem columnSpan={6}> <VersionTotals version='a' accountId={ACCOUNT_ID} /> </GridItem> <GridItem columnSpan={6}> <VersionTotals version='b' accountId={ACCOUNT_ID} /> </GridItem> <ChartGroup> <GridItem columnSpan={6}> <VersionPageViews version='a' /> </GridItem> <GridItem columnSpan={6}> <VersionPageViews version='b' /> </GridItem> </ChartGroup> <GridItem columnSpan={12}> <EndTestSection accountId={ACCOUNT_ID} versionADescription={VERSION_A_DESCRIPTION} versionBDescription={VERSION_B_DESCRIPTION} /> </GridItem> <GridItem columnSpan={12}> <PastTests accountId={ACCOUNT_ID} /> </GridItem> </Grid> </div> } } nerdlets/ab-test-nerdlet/index.js Copy When you call storeToken() with a new token value, your Nerdlet uses NerdGraph APIs to mutate NerdStorageVault data for the api_token key. If the request to NerdGraph is successful, storeToken() updates state.token so that the new token is locally accessible. Unlike NerdStorage, which has query and mutation components for your convenience, NerdStorageVault has no components in the SDK. Instead, you have to use NerdGraphQuery and NerdGraphMutation to interact with it Important It's important to remember that NerdStorageVault is limited to the user scope. Any other users of your New Relic One application will have their own NerdStorageVault data. This means that even other users on your account will need to enter their token separately. Query your API token Step 1 of 2 First, create methods and state to show and hide your API token prompt: import React from 'react'; import { ChartGroup, Grid, GridItem, NerdGraphMutation, } from 'nr1'; import EndTestSection from './end-test'; import NewsletterSignups from './newsletter-signups'; import PastTests from './past-tests'; import TotalCancellations from './total-cancellations'; import TotalSubscriptions from './total-subscriptions'; import VersionDescription from './description'; import VersionPageViews from './page-views'; import VersionTotals from './totals'; const ACCOUNT_ID = 123456 // <YOUR NEW RELIC ACCOUNT ID> const VERSION_A_DESCRIPTION = 'The newsletter signup message says, \"Sign up for our newsletter\"' const VERSION_B_DESCRIPTION = 'The newsletter signup message says, \"Sign up for our newsletter and get a free shirt!\"' export default class AbTestNerdletNerdlet extends React.Component { constructor() { super(...arguments); this.state = { hideTokenPrompt: true, token: null, } this.storeToken = this.storeToken.bind(this); this.showPrompt = this.showPrompt.bind(this); this.hidePrompt = this.hidePrompt.bind(this); } storeToken(newToken) { if (newToken != this.state.token) { const mutation = ` mutation($key: String!, $token: SecureValue!) { nerdStorageVaultWriteSecret( scope: { actor: CURRENT_USER } secret: { key: $key, value: $token } ) { status errors { message type } } } `; const variables = { key: \"api_token\", token: newToken, }; NerdGraphMutation.mutate({ mutation: mutation, variables: variables }).then( (data) => { if (data.data.nerdStorageVaultWriteSecret.status === \"SUCCESS\") { this.setState({token: newToken}) } } ); } } showPrompt() { this.setState({ hideTokenPrompt: false }); } hidePrompt() { this.setState({ hideTokenPrompt: true }); } render() { return <div> <Grid className=\"wrapper\"> <GridItem columnSpan={6}> <VersionDescription description={VERSION_A_DESCRIPTION} version=\"A\" /> </GridItem> <GridItem columnSpan={6}> <VersionDescription description={VERSION_B_DESCRIPTION} version=\"B\" /> </GridItem> <GridItem columnSpan={12}><hr /></GridItem> <GridItem columnSpan={12}><NewsletterSignups /></GridItem> <GridItem columnSpan={6}><TotalSubscriptions /></GridItem> <GridItem columnSpan={6}><TotalCancellations /></GridItem> <GridItem columnSpan={6}> <VersionTotals version='a' accountId={ACCOUNT_ID} /> </GridItem> <GridItem columnSpan={6}> <VersionTotals version='b' accountId={ACCOUNT_ID} /> </GridItem> <ChartGroup> <GridItem columnSpan={6}> <VersionPageViews version='a' /> </GridItem> <GridItem columnSpan={6}> <VersionPageViews version='b' /> </GridItem> </ChartGroup> <GridItem columnSpan={12}> <EndTestSection accountId={ACCOUNT_ID} versionADescription={VERSION_A_DESCRIPTION} versionBDescription={VERSION_B_DESCRIPTION} /> </GridItem> <GridItem columnSpan={12}> <PastTests accountId={ACCOUNT_ID} /> </GridItem> </Grid> </div> } } nerdlets/ab-test-nerdlet/index.js Copy state.hideTokenPrompt determines whether or not the prompt is visible. Now, you need a mechanism for revealing the prompt, which is hidden by default. Step 2 of 2 Query NerdStorageVault for your api_token: import React from 'react'; import { ChartGroup, Grid, GridItem, NerdGraphMutation, NerdGraphQuery, } from 'nr1'; import EndTestSection from './end-test'; import NewsletterSignups from './newsletter-signups'; import PastTests from './past-tests'; import TotalCancellations from './total-cancellations'; import TotalSubscriptions from './total-subscriptions'; import VersionDescription from './description'; import VersionPageViews from './page-views'; import VersionTotals from './totals'; const ACCOUNT_ID = 123456 // <YOUR NEW RELIC ACCOUNT ID> const VERSION_A_DESCRIPTION = 'The newsletter signup message says, \"Sign up for our newsletter\"' const VERSION_B_DESCRIPTION = 'The newsletter signup message says, \"Sign up for our newsletter and get a free shirt!\"' export default class AbTestNerdletNerdlet extends React.Component { constructor() { super(...arguments); this.state = { hideTokenPrompt: true, token: null, } this.storeToken = this.storeToken.bind(this); this.showPrompt = this.showPrompt.bind(this); this.hidePrompt = this.hidePrompt.bind(this); } storeToken(newToken) { if (newToken != this.state.token) { const mutation = ` mutation($key: String!, $token: SecureValue!) { nerdStorageVaultWriteSecret( scope: { actor: CURRENT_USER } secret: { key: $key, value: $token } ) { status errors { message type } } } `; const variables = { key: \"api_token\", token: newToken, }; NerdGraphMutation.mutate({ mutation: mutation, variables: variables }).then( (data) => { if (data.data.nerdStorageVaultWriteSecret.status === \"SUCCESS\") { this.setState({token: newToken}) } } ); } } showPrompt() { this.setState({ hideTokenPrompt: false }); } hidePrompt() { this.setState({ hideTokenPrompt: true }); } componentDidMount() { const query = ` query($key: String!) { actor { nerdStorageVault { secret(key: $key) { value } } } } `; const variables = { key: \"api_token\", }; NerdGraphQuery.query( { query: query, variables: variables, } ).then( ({ loading, error, data }) => { if (error) { console.error(error); this.showPrompt(); } if (data && data.actor.nerdStorageVault.secret) { this.setState({ token: data.actor.nerdStorageVault.secret.value }) } else { this.showPrompt(); } } ) } render() { return <div> <Grid className=\"wrapper\"> <GridItem columnSpan={6}> <VersionDescription description={VERSION_A_DESCRIPTION} version=\"A\" /> </GridItem> <GridItem columnSpan={6}> <VersionDescription description={VERSION_B_DESCRIPTION} version=\"B\" /> </GridItem> <GridItem columnSpan={12}><hr /></GridItem> <GridItem columnSpan={12}><NewsletterSignups /></GridItem> <GridItem columnSpan={6}><TotalSubscriptions /></GridItem> <GridItem columnSpan={6}><TotalCancellations /></GridItem> <GridItem columnSpan={6}> <VersionTotals version='a' accountId={ACCOUNT_ID} /> </GridItem> <GridItem columnSpan={6}> <VersionTotals version='b' accountId={ACCOUNT_ID} /> </GridItem> <ChartGroup> <GridItem columnSpan={6}> <VersionPageViews version='a' /> </GridItem> <GridItem columnSpan={6}> <VersionPageViews version='b' /> </GridItem> </ChartGroup> <GridItem columnSpan={12}> <EndTestSection accountId={ACCOUNT_ID} versionADescription={VERSION_A_DESCRIPTION} versionBDescription={VERSION_B_DESCRIPTION} /> </GridItem> <GridItem columnSpan={12}> <PastTests accountId={ACCOUNT_ID} /> </GridItem> </Grid> </div> } } nerdlets/ab-test-nerdlet/index.js Copy Here, in componentDidMount(), you've queried NerdGraph for your api_token data. componentDidMount() is a React lifecycle method that's called when your component is mounted in the component tree. This means that early in it's setup process, your application will request your api_token. If the NerdGraph query responds successfully with your token from NerdStorageVault, it sets the token in state. Otherwise, it shows the prompt so that you can enter a token. This is great for storing an initial token, but what if you enter the wrong token or the API changes? You need a way reveal the prompt on-demand. Next, you'll create the actual token prompt and a button to manually invoke the prompt. Create your token prompt Step 1 of 9 In nerdlets/ab-test-nerdlet, add a new Javascript file named token-prompt.js: bash Copy $ touch token-prompt.js Step 2 of 9 In this new file, create a button that allows you to enter a new token on demand: import React from 'react'; import { Button } from 'nr1'; class ApiTokenButton extends React.Component { constructor(props) { super(props) } render() { return ( <Button onClick={this.props.showPrompt}>Update API token</Button> ) } } nerdlets/ab-test-nerdlet/token-prompt.js Copy ApiTokenButton receives showPrompt() in its props and calls that method when its Button is clicked. Step 3 of 9 Create a token prompt using a Modal with a TextField: import React from 'react'; import { Button, Modal, TextField, } from 'nr1'; class ApiTokenButton extends React.Component { constructor(props) { super(props) } render() { return ( <Button onClick={this.props.showPrompt}>Update API token</Button> ) } } class ApiTokenPrompt extends React.Component { constructor() { super(...arguments); this.state = { token: null, tokenError: false, }; this.submitToken = this.submitToken.bind(this); this.hideTokenError = this.hideTokenError.bind(this); this.changeToken = this.changeToken.bind(this); this.keyPress = this.keyPress.bind(this); } showTokenError() { this.setState({ tokenError: true }); } hideTokenError() { this.setState({ tokenError: false }); } changeToken(event) { this.setState({ token: event.target.value }); } submitToken(event) { event.preventDefault(); if (this.state.token) { this.props.storeToken(this.state.token) this.hideTokenError() this.props.hidePrompt() } else { this.showTokenError() } } keyPress(event) { if(event.keyCode == 13) { event.preventDefault(); this.submitToken(event); } } render() { return <Modal hidden={this.props.hideTokenPrompt} onClose={this.props.hidePrompt}> To see cancellation data, you need to enter an API token for your backend service: <form> <TextField label=\"API token\" onChange={this.changeToken} onKeyDown={this.keyPress} invalid={this.state.tokenError ? \"Token required\" : false} /> <Button type={Button.TYPE.PRIMARY} onClick={this.submitToken}>Submit</Button> </form> </Modal> } } nerdlets/ab-test-nerdlet/token-prompt.js Copy ApiTokenPrompt renders a Modal with a TextField, a Button, and an explanatory prompt. You use the Modal to enter your API token. It also provides basic error handling if you try to submit the form with no token value. It's important to distinguish the token in AbTestNerdletNerdlet.state and the token in ApiTokenPrompt.state. The token in your Nerdlet's state is the current token that your Nerdlet knows about. It's this token that matches what's in NerdStorageVault. The token in ApiTokenPrompt.state is a fluid value that changes as you update the text in the TextField. When you press Submit in the modal, ApiTokenPrompt submits its token to your Nerdlet's storeToken() method. Then, storeToken() mutates NerdStorageVault with the new token. You also implemented a few methods to improve the user experience: keyPress() submits the token when the RETURN key is pressed showTokenError() and hideTokenError() remind the user that they must enter a token before submitting the form Step 4 of 9 Export your components, so you can use them in your Nerdlet: import React from 'react'; import { Button, Modal, TextField, } from 'nr1'; class ApiTokenButton extends React.Component { constructor(props) { super(props) } render() { return ( <Button onClick={this.props.showPrompt}>Update API token</Button> ) } } class ApiTokenPrompt extends React.Component { constructor() { super(...arguments); this.state = { token: null, tokenError: false, }; this.submitToken = this.submitToken.bind(this); this.hideTokenError = this.hideTokenError.bind(this); this.changeToken = this.changeToken.bind(this); this.keyPress = this.keyPress.bind(this); } showTokenError() { this.setState({ tokenError: true }); } hideTokenError() { this.setState({ tokenError: false }); } changeToken(event) { this.setState({ token: event.target.value }); } submitToken(event) { event.preventDefault(); if (this.state.token) { this.props.storeToken(this.state.token) this.hideTokenError() this.props.hidePrompt() } else { this.showTokenError() } } keyPress(event) { if(event.keyCode == 13) { event.preventDefault(); this.submitToken(event); } } render() { return <Modal hidden={this.props.hideTokenPrompt} onClose={this.props.hidePrompt}> To see cancellation data, you need to enter an API token for your backend service: <form> <TextField label=\"API token\" onChange={this.changeToken} onKeyDown={this.keyPress} invalid={this.state.tokenError ? \"Token required\" : false} /> <Button type={Button.TYPE.PRIMARY} onClick={this.submitToken}>Submit</Button> </form> </Modal> } } export { ApiTokenButton, ApiTokenPrompt } nerdlets/ab-test-nerdlet/token-prompt.js Copy Step 5 of 9 In your Nerdlet's index.js file, import ApiTokenButton and ApiTokenPrompt and add them to render(): import React from 'react'; import { ChartGroup, Grid, GridItem, NerdGraphMutation, NerdGraphQuery, } from 'nr1'; import EndTestSection from './end-test'; import NewsletterSignups from './newsletter-signups'; import PastTests from './past-tests'; import TotalCancellations from './total-cancellations'; import TotalSubscriptions from './total-subscriptions'; import VersionDescription from './description'; import VersionPageViews from './page-views'; import VersionTotals from './totals'; import { ApiTokenButton, ApiTokenPrompt } from './token-prompt'; const ACCOUNT_ID = 123456 // <YOUR NEW RELIC ACCOUNT ID> const VERSION_A_DESCRIPTION = 'The newsletter signup message says, \"Sign up for our newsletter\"' const VERSION_B_DESCRIPTION = 'The newsletter signup message says, \"Sign up for our newsletter and get a free shirt!\"' export default class AbTestNerdletNerdlet extends React.Component { constructor() { super(...arguments); this.state = { hideTokenPrompt: true, token: null, } this.storeToken = this.storeToken.bind(this); this.showPrompt = this.showPrompt.bind(this); this.hidePrompt = this.hidePrompt.bind(this); } storeToken(newToken) { if (newToken != this.state.token) { const mutation = ` mutation($key: String!, $token: SecureValue!) { nerdStorageVaultWriteSecret( scope: { actor: CURRENT_USER } secret: { key: $key, value: $token } ) { status errors { message type } } } `; const variables = { key: \"api_token\", token: newToken, }; NerdGraphMutation.mutate({ mutation: mutation, variables: variables }).then( (data) => { if (data.data.nerdStorageVaultWriteSecret.status === \"SUCCESS\") { this.setState({token: newToken}) } } ); } } showPrompt() { this.setState({ hideTokenPrompt: false }); } hidePrompt() { this.setState({ hideTokenPrompt: true }); } componentDidMount() { const query = ` query($key: String!) { actor { nerdStorageVault { secret(key: $key) { value } } } } `; const variables = { key: \"api_token\", }; NerdGraphQuery.query( { query: query, variables: variables, } ).then( ({ loading, error, data }) => { if (error) { console.error(error); this.showPrompt(); } if (data && data.actor.nerdStorageVault.secret) { this.setState({ token: data.actor.nerdStorageVault.secret.value }) } else { this.showPrompt(); } } ) } render() { return <div> <ApiTokenPrompt hideTokenPrompt={this.state.hideTokenPrompt} hidePrompt={this.hidePrompt} showPrompt={this.showPrompt} storeToken={this.storeToken} /> <Grid className=\"wrapper\"> <GridItem columnSpan={6}> <VersionDescription description={VERSION_A_DESCRIPTION} version=\"A\" /> </GridItem> <GridItem columnSpan={6}> <VersionDescription description={VERSION_B_DESCRIPTION} version=\"B\" /> </GridItem> <GridItem columnSpan={12}><hr /></GridItem> <GridItem columnSpan={12}><NewsletterSignups /></GridItem> <GridItem columnSpan={6}><TotalSubscriptions /></GridItem> <GridItem columnSpan={6}><TotalCancellations /></GridItem> <GridItem columnSpan={6}> <VersionTotals version='a' accountId={ACCOUNT_ID} /> </GridItem> <GridItem columnSpan={6}> <VersionTotals version='b' accountId={ACCOUNT_ID} /> </GridItem> <ChartGroup> <GridItem columnSpan={6}> <VersionPageViews version='a' /> </GridItem> <GridItem columnSpan={6}> <VersionPageViews version='b' /> </GridItem> </ChartGroup> <GridItem columnSpan={12}> <EndTestSection accountId={ACCOUNT_ID} versionADescription={VERSION_A_DESCRIPTION} versionBDescription={VERSION_B_DESCRIPTION} /> </GridItem> <GridItem columnSpan={12}> <PastTests accountId={ACCOUNT_ID} /> </GridItem> <GridItem columnSpan={12}> <ApiTokenButton showPrompt={this.showPrompt} /> </GridItem> </Grid> </div> } } nerdlets/ab-test-nerdlet/index.js Copy Step 6 of 9 Navigate to the root of your Nerdpack at nru-programmability-course/add-nerdstoragevault/ab-test. Step 7 of 9 Generate a new UUID for your Nerdpack: bash Copy $ nr1 nerdpack:uuid -gf Because you cloned the coursework repository that contained an existing Nerdpack, you need to generate your own unique identifier. This UUID maps your Nerdpack to your New Relic account. It also allows your app to make Nerdgraph requests on behalf of your account. Step 8 of 9 Serve your application locally: bash Copy $ nr1 nerdpack:serve Step 9 of 9 Go to https://one.newrelic.com?nerdpacks=local, and view your application under Apps > Your apps: When you visit your application for the first time, the prompt is automatically revealed. Enter \"ABC123\" in the TextField, as that's the token that the third-party service expects. Once you submit your token and your Nerdlet hides the prompt, click Update API token at the bottom of your New Relic One application to reveal it again: Tip If something doesn't work, use your browser's debug tools to try to identify the problem. Make sure you: Copied the code correctly from the lesson Generated a new UUID Replaced all instances of <YOUR NEW RELIC ACCOUNT ID> in your project with your actual New Relic account ID Pass your API token to TotalCancellations Step 1 of 3 In index.js, pass the API token to TotalCancellations so you're prepared to make a request to the third-party service: import React from 'react'; import { ChartGroup, Grid, GridItem, NerdGraphMutation, NerdGraphQuery, } from 'nr1'; import EndTestSection from './end-test'; import NewsletterSignups from './newsletter-signups'; import PastTests from './past-tests'; import TotalCancellations from './total-cancellations'; import TotalSubscriptions from './total-subscriptions'; import VersionDescription from './description'; import VersionPageViews from './page-views'; import VersionTotals from './totals'; import { ApiTokenButton, ApiTokenPrompt } from './token-prompt'; const ACCOUNT_ID = 123456 // <YOUR NEW RELIC ACCOUNT ID> const VERSION_A_DESCRIPTION = 'The newsletter signup message says, \"Sign up for our newsletter\"' const VERSION_B_DESCRIPTION = 'The newsletter signup message says, \"Sign up for our newsletter and get a free shirt!\"' export default class AbTestNerdletNerdlet extends React.Component { constructor() { super(...arguments); this.state = { hideTokenPrompt: true, token: null, } this.storeToken = this.storeToken.bind(this); this.showPrompt = this.showPrompt.bind(this); this.hidePrompt = this.hidePrompt.bind(this); } storeToken(newToken) { if (newToken != this.state.token) { const mutation = ` mutation($key: String!, $token: SecureValue!) { nerdStorageVaultWriteSecret( scope: { actor: CURRENT_USER } secret: { key: $key, value: $token } ) { status errors { message type } } } `; const variables = { key: \"api_token\", token: newToken, }; NerdGraphMutation.mutate({ mutation: mutation, variables: variables }).then( (data) => { if (data.data.nerdStorageVaultWriteSecret.status === \"SUCCESS\") { this.setState({token: newToken}) } } ); } } showPrompt() { this.setState({ hideTokenPrompt: false }); } hidePrompt() { this.setState({ hideTokenPrompt: true }); } componentDidMount() { const query = ` query($key: String!) { actor { nerdStorageVault { secret(key: $key) { value } } } } `; const variables = { key: \"api_token\", }; NerdGraphQuery.query( { query: query, variables: variables, } ).then( ({ loading, error, data }) => { if (error) { console.error(error); this.showPrompt(); } if (data && data.actor.nerdStorageVault.secret) { this.setState({ token: data.actor.nerdStorageVault.secret.value }) } else { this.showPrompt(); } } ) } render() { return <div> <ApiTokenPrompt hideTokenPrompt={this.state.hideTokenPrompt} hidePrompt={this.hidePrompt} showPrompt={this.showPrompt} storeToken={this.storeToken} /> <Grid className=\"wrapper\"> <GridItem columnSpan={6}> <VersionDescription description={VERSION_A_DESCRIPTION} version=\"A\" /> </GridItem> <GridItem columnSpan={6}> <VersionDescription description={VERSION_B_DESCRIPTION} version=\"B\" /> </GridItem> <GridItem columnSpan={12}><hr /></GridItem> <GridItem columnSpan={12}><NewsletterSignups /></GridItem> <GridItem columnSpan={6}><TotalSubscriptions /></GridItem> <GridItem columnSpan={6}> <TotalCancellations token={this.state.token} /> </GridItem> <GridItem columnSpan={6}> <VersionTotals version='a' accountId={ACCOUNT_ID} /> </GridItem> <GridItem columnSpan={6}> <VersionTotals version='b' accountId={ACCOUNT_ID} /> </GridItem> <ChartGroup> <GridItem columnSpan={6}> <VersionPageViews version='a' /> </GridItem> <GridItem columnSpan={6}> <VersionPageViews version='b' /> </GridItem> </ChartGroup> <GridItem columnSpan={12}> <EndTestSection accountId={ACCOUNT_ID} versionADescription={VERSION_A_DESCRIPTION} versionBDescription={VERSION_B_DESCRIPTION} /> </GridItem> <GridItem columnSpan={12}> <PastTests accountId={ACCOUNT_ID} /> </GridItem> <GridItem columnSpan={12}> <ApiTokenButton showPrompt={this.showPrompt} /> </GridItem> </Grid> </div> } } nerdlets/ab-test-nerdlet/index.js Copy Step 2 of 3 In total-cancellations.js, log the token to your browser console: import React from 'react'; import { HeadingText, PieChart } from 'nr1'; export default class TotalCancellations extends React.Component { constructor() { super(...arguments); this.state = { lastToken: null } } componentDidUpdate() { if (this.props.token && this.props.token != this.state.lastToken) { console.log(`requesting data with api token ${this.props.token}`) this.setState({lastToken: this.props.token}) } } render() { const cancellationsA = { metadata: { id: 'cancellations-A', name: 'Version A', viz: 'main', color: 'blue', }, data: [ { y: 118 }, ], } const cancellationsB = { metadata: { id: 'cancellations-B', name: 'Version B', viz: 'main', color: 'green', }, data: [ { y: 400 }, ], } return <div> <HeadingText className=\"chartHeader\"> Total cancellations per version </HeadingText> <PieChart data={[cancellationsA, cancellationsB]} fullWidth /> </div> } } nerdlets/ab-test-nerdlet/total-cancellations.js Copy Here, you've implemented another React lifecycle method, called componentDidUpdate(). Now, every time your Nerdlet's state.token changes, TotalCancellations gets a new token prop, which triggers componentDidUpdate(). In componentDidUpdate(), you check that the incoming token is not the same as the last token it knew about, which is stored in local state. If the incoming token is different, you log a message with the new token and update state.lastToken. This logic prepares your code for a future change to use your API token in a request to a third-party service. Step 3 of 3 With your Nerdpack served locally, view your application to see the log from TotalCancellations in your browser's console: If you change your token, you'll see another log from TotalCancellations with your updated token. Tip If something doesn't work, use your browser's debug tools to try to identify the problem. Make sure you: Copied the code correctly from the lesson Generated a new UUID Replaced all instances of <YOUR NEW RELIC ACCOUNT ID> in your project with your actual New Relic account ID When you're finished, stop serving your New Relic One application by pressing CTRL+C in your local server's terminal window. Now you know how to use NerdGraphQuery and NerdGraphMutation to manage data in NerdStorageVault! Remember, use NerdStorage for your New Relic One application's non-sensitive data and NerdStorageVault for the sensitive stuff like tokens, passwords, and other secrets. As a bonus, you created a way to manage your token in NerdStorageVault from the user interface. You also passed the token to your TotalCancellations component for later use. Whether it's been with NrqlQuery, AccountStorageQuery, AccountStorageMutation, NerdGraphQuery, or NerdGraphMutation, you've learned several ways to interact with New Relic data in your New Relic One application. But New Relic One applications aren't just another way to show New Relic data. The purpose of New Relic One applications is to show you how your software is helping you meet your business objectives. Sometimes, New Relic data is all you need to make that happen, but other times you need to look beyond New Relic for data to fill in the gaps. Course This lesson is part of a course that teaches you how to build a New Relic One application from the ground up. Continue on to the next lesson: Fetch data from a third-party service.",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 127.53547,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Access NerdStorageVault <em>from</em> your Nerdlet",
        "sections": "Access NerdStorageVault <em>from</em> your Nerdlet",
        "info": "Access NerdStorageVault <em>from</em> your Nerdlet",
        "body": " and subscriptions <em>from</em> New Relic&#x27;s database (NRDB), but you still need cancellation <em>data</em>. Your demo application does not report cancellation <em>data</em> to New Relic, so you need to fetch that <em>data</em> <em>from</em> an external source. We&#x27;ve provided a service at https:&#x2F;&#x2F;<em>api</em>.nerdsletter.net&#x2F;cancellations to return fake"
      },
      "id": "6091faf264441f1cbe2f36e1"
    },
    {
      "image": "https://developer.newrelic.com/static/5972bbab1eb47533d3d2171defc47fce/ba3ac/chart-group-syncing.png",
      "url": "https://developer.newrelic.com/build-apps/ab-test/chart-group/",
      "sections": [
        "Add a chart group",
        "Course",
        "Tip"
      ],
      "published_at": "2021-05-07T01:44:40Z",
      "title": "Add a chart group",
      "updated_at": "2021-05-05T01:54:57Z",
      "type": "developer",
      "external_id": "6ad93629eaf632d87125194995d2b7caa432f548",
      "document_type": "page",
      "popularity": 1,
      "info": "Add a chart group",
      "body": "Course This lesson is part of a course that teaches you how to build a New Relic One application from the ground up. If you haven't already, check out the course introduction. Each lesson in the course builds upon the last, so make sure you've completed the last lesson, Add tables, before starting this one. In previous lessons, you added a variety of charts to your A/B test application. These charts presented different facets of information about your A/B test, and they behaved independently from one other. In this lesson, you’ll create a new pair of line charts and learn how to synchronize their behaviors. Before you write any new code, consult your design guide to understand the role that these new charts will play in your application: In the design guide, there is a line graph under each of the tables you created in the last lesson. These line graphs show the version-specific response times for newsletter signup requests. Now, you’ll build two more line charts, but this time, you’ll group them with a ChartGroup and specify that their values are measured in milliseconds. Step 1 of 9 Change to the add-a-chart-group/ab-test directory of the coursework repository: bash Copy $ cd nru-programmability-course/add-a-chart-group/ab-test Step 2 of 9 In nerdlets/ab-test-nerdlet, add a new Javascript file named page-views.js: bash Copy $ touch page-views.js Step 3 of 9 In this new file, create a component called VersionPageViews to hold a LineChart, which shows the number of times a page is viewed: import React from 'react'; import { LineChart } from 'nr1'; export default class VersionPageViews extends React.Component { render() { const versionPageViews = { metadata: { id: `page-views-${this.props.version}`, name: `Version ${this.props.version.toUpperCase()}`, viz: 'main', color: 'blue', units_data: { y: 'MS' } }, data: [ { x: 0, y: 10 }, { x: 10, y: 13 }, { x: 20, y: 11.5 }, { x: 30, y: 10 }, { x: 40, y: 8.75 }, { x: 50, y: 9 }, ], } return <LineChart data={[versionPageViews]} fullWidth /> } } nerdlets/ab-test-nerdlet/page-views.js Copy Notice the new attribute in the series' metadata fields: units_data. This attribute describes the unit type for a specified data axis. In this case, you set the unit type for the y axis to 'MS', which stands for milliseconds. Other options for unit types, include: 'PERCENTAGE', 'TIMESTAMP', and 'BYTES_PER_SECOND'. Step 4 of 9 In your Nerdlet's index.js file, import your new component and update your Nerdlet's render() method: import React from 'react'; import NewsletterSignups from './newsletter-signups'; import PastTests from './past-tests'; import TotalCancellations from './total-cancellations'; import TotalSubscriptions from './total-subscriptions'; import VersionPageViews from './page-views'; import VersionTotals from './totals'; export default class AbTestNerdletNerdlet extends React.Component { render() { return <div> <NewsletterSignups /> <TotalSubscriptions /> <TotalCancellations /> <VersionTotals version='a' /> <VersionTotals version='b' /> <VersionPageViews version='a' /> <VersionPageViews version='b' /> <PastTests /> </div> } } nerdlets/ab-test-nerdlet/index.js Copy Step 5 of 9 In index.js, import ChartGroup from nr1 and group your VersionPageViews: import React from 'react'; import { ChartGroup } from 'nr1'; import NewsletterSignups from './newsletter-signups'; import PastTests from './past-tests'; import TotalCancellations from './total-cancellations'; import TotalSubscriptions from './total-subscriptions'; import VersionPageViews from './page-views'; import VersionTotals from './totals'; export default class AbTestNerdletNerdlet extends React.Component { render() { return <div> <NewsletterSignups /> <TotalSubscriptions /> <TotalCancellations /> <VersionTotals version='a' /> <VersionTotals version='b' /> <ChartGroup> <VersionPageViews version='a' /> <VersionPageViews version='b' /> </ChartGroup> <PastTests /> </div> } } nerdlets/ab-test-nerdlet/index.js Copy Because the tables are conceptually related, as they show contrasting performance metrics over the same timeslice, it makes sense to group them in a ChartGroup. This means that the two charts behave synchronously. For example, when you hover over one chart, the other chart shows a hover indicator at the same x coordinate. Step 6 of 9 Navigate to the root of your Nerdpack at nru-programmability-course/add-a-chart-group/ab-test. Step 7 of 9 Generate a new UUID for your Nerdpack: bash Copy $ nr1 nerdpack:uuid -gf Because you cloned the coursework repository that contained an existing Nerdpack, you need to generate your own unique identifier. This UUID maps your Nerdpack to your New Relic account. Step 8 of 9 Serve your application locally: bash Copy $ nr1 nerdpack:serve Step 9 of 9 View your changes in New Relic: Here, you see the LineChart components synchronized in your application. When you're finished, stop serving your New Relic One application by pressing CTRL+C in your local server's terminal window. Tip Each of the chart component types you've used in this lesson have had different series configurations. Most chart components share the same metadata attributes, like LineChart and PieChart, but differ in their data formats. It's helpful to be aware of the different data formats, for when you create your own charts. You can read more about the data formats in the chart component documentation. Now your application is filled with charts, but it doesn't look great. The charts are stacked on top of one another in an unhelpful way. In the next lesson, you'll learn about the user interface components from the SDK and how you can use them to organize your charts. Course This lesson is part of a course that teaches you how to build a New Relic One application from the ground up. When you're ready, continue on to the next lesson: Add user interface components to your application.",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 106.87963,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "body": " this one. In previous lessons, you added a variety of charts to your A&#x2F;B test application. These charts presented different facets of information about your A&#x2F;B test, and they behaved independently <em>from</em> one <em>other</em>. In this lesson, you’ll create a new pair of line charts and learn how to synchronize"
      },
      "id": "6091faf1196a6726c2d52a20"
    },
    {
      "sections": [
        "Introduction to automated user management (AUM) and single-sign on (SSO)",
        "Requirements",
        "Import user groups from an identity provider using AUM"
      ],
      "title": "Introduction to automated user management (AUM) and single-sign on (SSO)",
      "type": "docs",
      "tags": [
        "Accounts",
        "Accounts and billing",
        "Automated user management"
      ],
      "external_id": "831a5f1137eccac9540d716302645b4e976a6332",
      "image": "",
      "url": "https://docs.newrelic.com/docs/accounts/accounts/automated-user-management/automated-user-provisioning-single-sign/",
      "published_at": "2021-05-06T04:53:56Z",
      "updated_at": "2021-05-06T04:53:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic lets you set up automated user management (AUM), which allows you to import, update, and deactivate your New Relic users from your identity provider. Once this automated provisioning is complete, your users can log in to New Relic via their identity provider. Requirements Requirements and impacts: Requires Enterprise edition. User model-related requirements: This feature creates users on our New Relic One user model. To implement AUM, you must be on our New Relic One user model and have user management roles assigned. If you're on our original user model (or otherwise can't seem to implement this feature), talk to your New Relic account representative. Supports SAML 2.0 standard for single sign on (SSO). Supports SCIM 2.0 standard. Supported identity providers: Okta, Azure AD, OneLogin. For unsupported identity providers, we have a SCIM API. Notes on initial enabling of AUM: We don't currently support toggling SCIM on or off. If an authentication domain has already been set up with the source of users as Manual, you can't change it to SCIM. When first enabled, the bearer token is generated and only shown once. If you need to view a bearer token later, the only way to do this is to generate a new one, and that will invalidate the old one and any integrations using the old token. Import user groups from an identity provider using AUM For an explanation of how your identity provider groups map over to New Relic groups, see Group and role mapping. To use automated user management to import users from your identity provider: It's important to first review the requirements. Use the Organization and access UI to enable SCIM and configure SAML SSO. Configure your identity provider using one of our relevant guides:Azure AD | Okta | OneLogin. If you don't use one of those, we also have a SCIM API. Note that your users are created in New Relic as full users. If your organization is on New Relic One pricing, these users are billable. To convert users to free basic users, use the User management UI. Recommended: Set a time zone in your identity provider. If not specified, our UI shows date/times with the UTC time zone. Time zone is specified in IANA Time Zone database format, also known as the \"Olson\" time zone database format (e.g., \"America/Los_Angeles\"). If you have issues, contact your account representative. After being provisioned, your users can click on the New Relic SCIM/SSO application tile in their identity provider to be logged into New Relic. To learn more about New Relic's roles and capabilities, see Standard roles.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 99.34808,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "<em>Import</em> user groups <em>from</em> an identity provider using AUM",
        "body": "New Relic lets you set up automated user management (AUM), which allows you to <em>import</em>, update, and deactivate your New Relic users <em>from</em> your identity provider. Once this automated provisioning is complete, your users can log in to New Relic via their identity provider. Requirements Requirements"
      },
      "id": "6043d60e64441ff8f5378f37"
    }
  ],
  "/docs/infrastructure/index": [
    {
      "sections": [
        "REST API calls for infrastructure alerts",
        "Requirements",
        "Tip",
        "Using infrastructure API calls",
        "GET infrastructure conditions",
        "GET a list of infrastructure conditions",
        "Example GET a list of conditions",
        "GET a specific infrastructure condition",
        "Example GET a specific condition",
        "Create (POST) an infrastructure condition",
        "Important",
        "Update (PUT) an infrastructure condition",
        "Example update (PUT) a condition",
        "Remove (DELETE) an infrastructure condition",
        "Types of conditions",
        "Process running conditions API data",
        "Example condition types",
        "Metric conditions API data",
        "Example",
        "Host not reporting condition",
        "Definitions",
        "value",
        "duration_minutes",
        "time_function"
      ],
      "title": "REST API calls for infrastructure alerts",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "c35aa43cdb6645473d02886a49d6f9aeb37e577f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/rest-api-calls-new-relic-infrastructure-alerts/",
      "published_at": "2021-05-04T17:32:15Z",
      "updated_at": "2021-04-06T06:04:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the infrastructure REST API to add, update, delete, and list alerting conditions. You can also manage individual alerting conditions using the infrastructure monitoring UI. REST API calls for infrastructure alerts are not available in the API Explorer. Why use the API Examples Consistency Define the same set of conditions for every cluster without having to set up identical conditions in the Infrastructure monitoring UI each time. Manage multiple conditions quickly, without having to update them one by one using the UI. Flexibility Create conditions for an arbitrary group of hosts. Disable or delete conditions for hosts taken offline anytime. Create a condition with exclusionary filtering (for instance, environment NOT LIKE x). For more on this, see this post on exclusion filtering. For AWS Cloud integrations, select attributes that haven't been sent up by AWS yet. Create compound alert conditions by using the where_clause, which allows you to specify the limits on a secondary or tertiary metric. Exceed the 500-facet limitation on NRQL alert conditions. Reliability Audit when a condition was last updated. Requirements In order to use the Infrastructure REST API, you need: An API key The alerting condition's related policy_id from New Relic, available via GET list of conditions or via the Alerts REST API The condition id, available via GET list of conditions, or via the condition's URL in the Infrastructure monitoring UI Tip If your account hosts data in the EU data center, make sure you are using the proper API endpoints for EU region accounts. Using infrastructure API calls Here are some basic cURL commands and their responses for Infrastructure alert conditions. Depending on the type of condition, the DATA information you provide in the call will vary for POST (add) and PUT (update) calls. Definitions of each attribute used in the data blocks can be found in the Definitions section. GET infrastructure conditions You can either GET a list of infrastructure conditions or GET a specific infrastructure condition. Here are a few tips for listing infrastructure conditions. For pagination, use limit (records per page) and offset (how many records to skip) parameters. Default is 50 records per page, and offset starts at 0 (skip no records). To scope the results to a specific policy, use policy_id. Tip If you want to use the GET response as a template for your PUT or POST input, be sure to remove the created_at_epoch_millis, updated_at_epoch_millis and id information. GET a list of infrastructure conditions curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111\" Copy Example GET a list of conditions Response showing 2 of the 3 conditions for the example policy (formatted for readability and truncated): HTTP/1.1 200 OK Content-Length: 622 Content-Type: application/json { \"data\":[ { \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(`hostname` LIKE '%cassandra%')\", \"id\":13890, \"created_at_epoch_millis\":1490996713872, \"updated_at_epoch_millis\":1490996713872, \"policy_id\":111111, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(`commandName` = 'java')\" }, { \"created_at_epoch_millis\": 1501704525462, \"critical_threshold\": { \"duration_minutes\": 5 }, \"enabled\": true, \"filter\": { \"and\": [ { \"like\": { \"fullHostname\": \"Production_1\" } } ] }, \"id\": 448036, \"name\": \"PROD - Host Machine's Agent Not Responding ....\", \"policy_id\": 98485, \"type\": \"infra_host_not_reporting\", \"updated_at_epoch_millis\": 1504879191220 } . . . ], \"meta\":{ \"limit\":50, \"offset\":0, \"total\":3 }, \"links\":{} } Copy To get a list of the 10 Infrastructure conditions beyond the 50 limit: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111&offset=50&list=10\" Copy GET a specific infrastructure condition To get information about a single Infrastructure condition: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition-id\" Copy Example GET a specific condition Response (formatted for readability): HTTP/1.1 200 OK Content-Length: 246 Content-Type: application/json { \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"demo condition\", \"enabled\":false, \"id\":13887, \"created_at_epoch_millis\":1490981583580, \"updated_at_epoch_millis\":1490981583580, \"policy_id\":23635, \"critical_threshold\":{ \"duration_minutes\":100 } } } Copy Create (POST) an infrastructure condition Important Do not include an \"id\": when adding a new condition (POST). It will be generated when the condition is created. To add an infrastructure condition, use this basic cURL command: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are adding: Process running conditions API data Metric conditions API data Host not reporting conditions API data Update (PUT) an infrastructure condition You only need to include the fields that need to be changed when updating an infrastructure condition. The API keeps the existing values for any missing fields. Important If you want to change the condition type, do not use PUT. Instead, delete the existing condition, then add (POST) a new condition with the new condition type and all fields. To update an infrastructure condition, use this basic cURL command. To indicate which condition is to be updated, be sure to include the \"id\": . Example update (PUT) a condition curl -X PUT 'https://infra-api.newrelic.com/v2/alerts/conditions/condition-id' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are updating: Process running conditions API data Metric conditions API data Host not reporting conditions API data Remove (DELETE) an infrastructure condition To delete an infrastructure condition, use this basic cURL command: curl -v -X DELETE --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition_id\" Copy Types of conditions Process running conditions API data A process running condition alerts you when the number of processes is above, below, or equal to the threshold you define. To add (POST) or update (PUT) a process running condition, use your API key, and refer to the definitions to customize your values in the API call. Example condition types For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(commandName = '\\''java'\\'')\" } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause and process_where_clause Metric conditions API data A metric condition alerts you when the metric of your choice is above, below, or equal to the threshold you define. This includes: System metrics Process metrics Network metrics Storage metrics Cloud integration metrics To add (POST) or update (PUT) a metric condition, use your API key, and refer to the definitions to customize your values in the API call. If you are adding or updating a cloud integration alert condition: For the event_type field, enter the event type generated by your selected cloud integration service (for example, ComputeSample for the AWS EC2 integration). If you are setting up an alert condition on a cloud integration service that requires a provider value (for example, AWS RDS uses DatastoreSample with a provider value of RdsDbInstance or RdsDbCluster), you will need to add the \"integration_provider\" field and use the value that is appropriate for the service your alert condition is targeting (for example, \"integration_provider\":\"RdsDbInstance\"). For the select_value field, build the metric name by using the following syntax, where provider is a standard prefix string: provider.metric.aggregation_type Copy metric: Use the metric name as described in the New Relic documentation for your integration. aggregation_type: Use Sum, Average, Minimum, or Maximum. Refer to the original documentation by the integration's cloud provider to see which statistic aggregations are available for each metric. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_metric\", \"name\":\"Disk Space Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"event_type\":\"StorageSample\", \"select_value\":\"diskFreePercent\", \"comparison\":\"below\", \"critical_threshold\":{ \"value\":10, \"duration_minutes\":1, \"time_function\":\"any\" }, \"warning_threshold\":{ \"value\":30, \"duration_minutes\":2, \"time_function\":\"any\" } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Host not reporting condition A host not reporting condition alerts you when a host stops reporting. To add (POST) or update (PUT) a host not reporting condition, use your API key, and refer to the definitions to customize your values in the API call. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"Cassandra Host Reporting Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"critical_threshold\":{ \"duration_minutes\":12 } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Definitions When formatting your cURL commands, use these values as needed. These are listed in alphabetical order, not the order they appear in your API calls. Field Definition comparison (enum) Condition type: infra_metric, infra_process_running The value used to define the threshold; for example, \"[\"above\", \"below\", \"equal\"]. critical_threshold and warning_threshold Condition type: all This object identifies the threshold value before opening a violation. The critical_threshold is required. The warning_threshold is optional and may only be used with infra_metric conditions. The keys of this object depend on the condition type. Condition type: infra_metric format: \"critical_threshold\":{ \"value\":<number>, \"duration_minutes\":<integer>, \"time_function\":\"any\" or \"all\" }, Copy Condition type: infra_process_running format: \"critical_threshold\":{ \"value\":<integer>, \"duration_minutes\":<integer>, }, Copy Condition type: infra_host_not_reporting format: \"critical_threshold\":{ \"duration_minutes\":<integer>, }, Copy value The numeric value that must be breached for the condition to open a violation duration_minutes The number of minutes the value must be passed or met for the condition to open a violation time_function Indicates if the condition needs to be sustained for a certain period of time to create a violation, or if it only needs to break the threshold once within a certain period of time. If you're setting up a for at least x minutes threshold, use all; for an at least once in x minutes threshold, use any. enabled (boolean) Condition type: all Whether the condition is turned on or off; true or false. event_type (string) Condition type: infra_metric The metric event; for example, system metrics, process metrics, storage metrics, or network metrics. This automatically populates for infrastructure integrations; for example, StorageSample or SystemSample. filter (string) Condition type: all If the condition was made in the UI, filter appears instead of where_clause; for example: {and: [{is: {ec2InstanceType: \"m3.medium\"}}]} Copy Recommendation: Use where_clause when creating a new condition. id (integer) Condition type: all The condition ID located in the URL. GET: This value appears in the GET response. PUT: Include this value in the DATA section. POST: Do not include this in the DATA section. DELETE: Include this value in the -X DELETE call. integration_provider (string) Condition type: infra_metric For alerts on integrations, use integration_provider instead of event_type. To see valid values: From the New Relic documentation for your cloud service, check the Find and use data section. Example: In the AWS RDS monitoring integration documentation, you can see that the DatastoreSample event type can be used with an integration_provider value of either RdsDbInstance for DB instances, or RdsDbCluster for Aurora DB clusters. name (string) Condition type: all The infrastructure alerting condition's name; for example: \"[test] process running\" Copy policy_id (integer) Condition type: all The unique ID for the alert policy's account ID associated with the condition; for example, 1234567890. This is not the policy's global ID. process_where_clause (string) Condition type: infra_process_running Any filters applied to processes, specifically in process running alert conditions. This parameter is mandatory for those types of alert conditions. For example: \"commandName = '\\''java'\\''\" Copy runbook_url (string) Condition type: all The runbook URL to display in notifications. select_value (string) Condition type: infra_metric The attribute name to identify the metric being targeted; for example, \"cpuPercent\", \"diskFreePercent\", \"memoryResidentSizeBytes\", or \"memoryFreeBytes/memoryTotalBytes*100\". This automatically populates for Infrastructure Integrations; for example, diskFreePercent. type (enum) Condition type: all The type of infrastructure alert condition: \"infra_process_running\", \"infra_metric\", or \"infra_host_not_reporting\". violation_close_timer (integer) Condition type: all The Violation time limit setting, expressed as hours. Possible values are 0, 1, 2, 4, 8,12, 24, 48, 72. This determines how much time will pass before a violation is automatically closed. Setting the time limit to 0 prevents a violation from being force-closed. For new conditions, if a value is not provided, the following default values are used: Host Not Responding (HNR) conditions: 0 (disabled) All other conditions: 24 When updating existing conditions, if a value is provided, it overrides the existing value, but does not affect already opened violations. where_clause (string) Condition type: all If applicable, this identifies any infrastructure host filters used; for example: \"(`hostname` LIKE '\\''%cassandra%'\\'')\", Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 46.24811,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "REST API calls for <em>infrastructure</em> alerts",
        "sections": "REST API calls for <em>infrastructure</em> alerts",
        "tags": "<em>Infrastructure</em>",
        "body": "Use the <em>infrastructure</em> REST API to add, update, delete, and list alerting conditions. You can also manage individual alerting conditions using the <em>infrastructure</em> monitoring UI. REST API calls for <em>infrastructure</em> alerts are not available in the API Explorer. Why use the API Examples Consistency"
      },
      "id": "6043fa6c196a678ae2960f31"
    },
    {
      "sections": [
        "Infrastructure and security",
        "Secure agent communication",
        "Running modes",
        "Linux",
        "Run as root",
        "Run as privileged user",
        "Run as unprivileged user",
        "Windows",
        "Sources of data collection",
        "Default directories accessed by the agent",
        "Packages and commands used by the agent",
        "Proxies",
        "External agent connections",
        "External agent commands",
        "Deliverables"
      ],
      "title": "Infrastructure and security",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring",
        "Infrastructure and security"
      ],
      "external_id": "67cdc13e3b29b66199549af64e21cafcf851a31e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-monitoring/infrastructure-security/infrastructure-security/",
      "published_at": "2021-05-05T03:33:03Z",
      "updated_at": "2021-03-13T04:00:44Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring provides comprehensive data from your entire system by running comfortably on your hosts, especially when it runs with administrator privileges. What follows is an overview of our agent's security and some recommendations. Secure agent communication Every piece of information exchanged between your hosts and the infrastructure agent is delivered securely. All communication from the agent occurs over HTTPS, using Transport Layer Security (TLS). To ensure secure communication, the infrastructure agent was designed with the following protective measures: All communication is established directly from the agent to the service. The agent does not require any incoming ports to be opened. The agent is read-only and cannot make changes to your system. The infrastructure agent does not support high security mode. For more information about New Relic's security measures, see our security and privacy documentation, or visit the New Relic security website. Running modes New Relic is committed to the security of your data. All data derived while running the infrastructure agent is protected, and used only to deliver information related to your infrastructure back to you. Linux You can run the infrastructure Linux agent in three different modes: Run as root When the agent runs as the root user it has total access to all the system metrics and inventory. Run as privileged user The agent runs a non-privileged user, named the nri-agent, which is granted extended kernel capabilities during the installation process. The privileged nri-agent user is therefore able to collect some metrics and most of the inventory. These permissions are read-only. The installation scripts in privileged mode will make the following changes in your system: Create the nri-agent user and group. Set the nri-agent user and group as the owners of the following directories: /var/run/newrelic-infra /var/db/newrelic-infra /var/log/newrelic-infra /etc/newrelic-infra Add the following kernel capabilities to the /usr/bin/newrelic-infra executable: CAP_SYS_PTRACE, which allows inspecting and tracing arbitrary processes CAP_DAC_READ_SEARCH, to bypass file and directory read and execute permission checks Run as unprivileged user The agent runs with a non-privileged user, nri-agent, which is automatically created during the upgrade/installation process, and will not have read access to all the system metrics. This provides visibility into environments with very strict security or regulatory policies. There are no special permissions or access granted to the user in this run mode. The installation scripts in unprivileged mode will make the following changes in your system: Create the nri-agent user and group. Set the nri-agent and group as the owners of the following system files and folders: /var/run/newrelic-infra /var/db/newrelic-infra /var/log/newrelic-infra /etc/newrelic-infra For more details on the different running modes, see the comparison in the Infrastructure installation documentation. Windows In Windows systems, the agent must be executed with Administrator permissions. Sources of data collection The infrastructure agent gathers metrics, events, and inventory data from a variety of OS sources. While some of these sources can be read from a non-privileged account, others require elevated privileges. For current agent versions, New Relic requires that it run as the root user (on Linux) or with full Administrator access (Windows). Here are additional details about how the infrastructure agent accesses default directories and what packages and commands it uses. Default directories accessed by the agent Unless otherwise noted, this information applies to any Linux operating system. Directory or file Purpose Linux OS /etc/newrelic-infra.yml Default configuration file Any /usr/bin/newrelic-infra-service Default agent service wrapper binary install location Any /usr/bin/newrelic-infra Default agent binary install location Any /usr/bin/newrelic-infra-ctl Default CLI agent control binary location Any /var/db/newrelic-infra/ Default inventory cache and plugin binaries Any /var/run/newrelic-infra.pid Default pid file Any stdout, stderr, logs Depending on configuration, the agent writes logs to stdout, which may connect to your system logging service Any Packages and commands used by the agent Some data sources are specific to particular operating systems. Unless otherwise mentioned, New Relic uses the source on all variations of an operating system when the related software is detected. Many of the tools are on a path accessible to the agent. If not otherwise indicated, the infrastructure agent typically searches for them in /usr/bin, /bin, or /sbin. New Relic uses some tools or data sources to gather information for multiple infrastructure features. Here are some primary use cases. Unless otherwise noted, New Relic uses this information primarily for the Inventory page. Plugin or data Tool, directory, or file Linux OS systemd initctl list Any upstart systemctl -l, systemctl show, modinfo, lsmod Any selinux sestatus -b, semodule -l Any dpkg dpkg-query -W -f Debian rpm rpm -qa Redhat hostinfo /sys/devices/virtual/dmi/id/sys_vendor, /sys/devices/virtual/dmi/id/product_name Any hostinfo /proc/sys/kernel/osrelease Any hostinfo uptime -s Any hostinfo /etc/lsb_release Debian hostinfo /etc/redhat-release Redhat facter facter -p -j Any daemontool svstat Any kernel_modules /sbin/modinfo, /sbin/lsmod Any users /usr/bin/env who Any Various plugins gather system-wide details through this directory. Used for infrastructure Inventory and metrics. /proc/ Any Various plugins gather sysctl settings through this directory. Used for infrastructure Inventory and metrics. /sys/ Any Various plugins (sshd_config, hostinfo) read specific configs in this directory. Used for infrastructure Inventory and metrics. /etc/ Any Proxies New Relic includes optional settings so that you can configure the agent to communicate through a proxy. To define proxy settings, see the configuration documentation for infrastructure monitoring. External agent connections The agent periodically sends JSON-formatted data describing compute metrics, events, and your inventory configuration to the infrastructure endpoint at New Relic. These communications are associated with the agent using the license key generated for your account. Once New Relic receives data from any external agent, it will display the new metrics, events, or configuration data in the infrastructure monitoring UI. External agent commands The agent handles two different sources of commands, newrelic-infra-ctl and command-API: CLI commands submitted with newrelic-infra-ctl are sent into the agent with Linux or Docker signaling or with Windows named-pipes. Using New Relic's platform command-API endpoint, the agent polls for platform-provided commands every 60 seconds. The connection is always open from the agent into the New Relic platform command-API endpoint, never the opposite way. The command-API endpoint is used only to force enable or disable of dynamic entities' registration. It also applies to the Docker integration that comes with the infrastructure agent version 1.9.0 or higher. Deliverables The infrastructure agent and all on-host integrations that run on top of it are provided using standard operating system repositories and packages. New Relic cryptographically signs all packages, and verification steps are provided by default in the installation scripts. All code is checked for dependency vulnerabilities through standard security tools (Snyk, Dependabot, Trivy). New Relic official downloads site is hosted on AWS via S3, and fronted with Fastly, our trusted CDN provider.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 45.252132,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> and security",
        "sections": "<em>Infrastructure</em> and security",
        "tags": "<em>Infrastructure</em>",
        "body": "New Relic&#x27;s <em>infrastructure</em> monitoring provides comprehensive data from your entire system by running comfortably on your hosts, especially when it runs with administrator privileges. What follows is an overview of our agent&#x27;s security and some recommendations. Secure agent communication Every piece"
      },
      "id": "6043efa0e7b9d268c5579a11"
    },
    {
      "sections": [
        "Zip assisted install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Zip assisted install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "fcadbeed626401863b6b16e5c52e9a472a5ac13e",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-04-16T02:59:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. Install the agent Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Once it's unpacked, access and edit the installation PowerShell script installer.ps1. Update your license key. Optional: Update any other parameters. Execute installer.ps1 with admin rights. Configure your installation Important Make sure any custom folder defined in the installation settings has permissions limitations properly defined. The infrastructure agent might execute any integration defined in the NRIA_PLUGIN_DIR directory with Administrator permissions. You can configure the following parameters during the assisted install for Windows: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: C:\\Program Files\\New Relic\\newrelic-infra Copy NRIA_APP_DATA_DIR This configures the data directory to store inventory and other agent files. Default: C:\\%ProgramData%\\New Relic\\newrelic-infra Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The New Relic license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy NRIA_OVERWRITE By default and for security reasons, Windows does not install a service if there's another service with the same name already installed. To bypass this check, make sure this setting NRIA_OVERWRITE is TRUE. Default: TRUE Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: C:\\Program Files\\NewRelic\\newrelic-infra\\inregrations.d Copy NRIA_SERVICE_NAME This provides the name for the Windows service. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 43.037014,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip assisted install of the <em>infrastructure</em> agent for Windows",
        "sections": "Zip assisted install of the <em>infrastructure</em> agent for Windows",
        "tags": "<em>Infrastructure</em>",
        "body": "With the assisted install of the <em>infrastructure</em> agent for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. Install the agent Important As of version"
      },
      "id": "603ea7af196a67dab0a83d9d"
    }
  ],
  "/docs/infrastructure/infrastructure-alerts/infrastructure-alert-conditions/create-infrastructure-host-not-reporting-condition": [
    {
      "sections": [
        "REST API calls for infrastructure alerts",
        "Requirements",
        "Tip",
        "Using infrastructure API calls",
        "GET infrastructure conditions",
        "GET a list of infrastructure conditions",
        "Example GET a list of conditions",
        "GET a specific infrastructure condition",
        "Example GET a specific condition",
        "Create (POST) an infrastructure condition",
        "Important",
        "Update (PUT) an infrastructure condition",
        "Example update (PUT) a condition",
        "Remove (DELETE) an infrastructure condition",
        "Types of conditions",
        "Process running conditions API data",
        "Example condition types",
        "Metric conditions API data",
        "Example",
        "Host not reporting condition",
        "Definitions",
        "value",
        "duration_minutes",
        "time_function"
      ],
      "title": "REST API calls for infrastructure alerts",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "c35aa43cdb6645473d02886a49d6f9aeb37e577f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/rest-api-calls-new-relic-infrastructure-alerts/",
      "published_at": "2021-05-04T17:32:15Z",
      "updated_at": "2021-04-06T06:04:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the infrastructure REST API to add, update, delete, and list alerting conditions. You can also manage individual alerting conditions using the infrastructure monitoring UI. REST API calls for infrastructure alerts are not available in the API Explorer. Why use the API Examples Consistency Define the same set of conditions for every cluster without having to set up identical conditions in the Infrastructure monitoring UI each time. Manage multiple conditions quickly, without having to update them one by one using the UI. Flexibility Create conditions for an arbitrary group of hosts. Disable or delete conditions for hosts taken offline anytime. Create a condition with exclusionary filtering (for instance, environment NOT LIKE x). For more on this, see this post on exclusion filtering. For AWS Cloud integrations, select attributes that haven't been sent up by AWS yet. Create compound alert conditions by using the where_clause, which allows you to specify the limits on a secondary or tertiary metric. Exceed the 500-facet limitation on NRQL alert conditions. Reliability Audit when a condition was last updated. Requirements In order to use the Infrastructure REST API, you need: An API key The alerting condition's related policy_id from New Relic, available via GET list of conditions or via the Alerts REST API The condition id, available via GET list of conditions, or via the condition's URL in the Infrastructure monitoring UI Tip If your account hosts data in the EU data center, make sure you are using the proper API endpoints for EU region accounts. Using infrastructure API calls Here are some basic cURL commands and their responses for Infrastructure alert conditions. Depending on the type of condition, the DATA information you provide in the call will vary for POST (add) and PUT (update) calls. Definitions of each attribute used in the data blocks can be found in the Definitions section. GET infrastructure conditions You can either GET a list of infrastructure conditions or GET a specific infrastructure condition. Here are a few tips for listing infrastructure conditions. For pagination, use limit (records per page) and offset (how many records to skip) parameters. Default is 50 records per page, and offset starts at 0 (skip no records). To scope the results to a specific policy, use policy_id. Tip If you want to use the GET response as a template for your PUT or POST input, be sure to remove the created_at_epoch_millis, updated_at_epoch_millis and id information. GET a list of infrastructure conditions curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111\" Copy Example GET a list of conditions Response showing 2 of the 3 conditions for the example policy (formatted for readability and truncated): HTTP/1.1 200 OK Content-Length: 622 Content-Type: application/json { \"data\":[ { \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(`hostname` LIKE '%cassandra%')\", \"id\":13890, \"created_at_epoch_millis\":1490996713872, \"updated_at_epoch_millis\":1490996713872, \"policy_id\":111111, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(`commandName` = 'java')\" }, { \"created_at_epoch_millis\": 1501704525462, \"critical_threshold\": { \"duration_minutes\": 5 }, \"enabled\": true, \"filter\": { \"and\": [ { \"like\": { \"fullHostname\": \"Production_1\" } } ] }, \"id\": 448036, \"name\": \"PROD - Host Machine's Agent Not Responding ....\", \"policy_id\": 98485, \"type\": \"infra_host_not_reporting\", \"updated_at_epoch_millis\": 1504879191220 } . . . ], \"meta\":{ \"limit\":50, \"offset\":0, \"total\":3 }, \"links\":{} } Copy To get a list of the 10 Infrastructure conditions beyond the 50 limit: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111&offset=50&list=10\" Copy GET a specific infrastructure condition To get information about a single Infrastructure condition: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition-id\" Copy Example GET a specific condition Response (formatted for readability): HTTP/1.1 200 OK Content-Length: 246 Content-Type: application/json { \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"demo condition\", \"enabled\":false, \"id\":13887, \"created_at_epoch_millis\":1490981583580, \"updated_at_epoch_millis\":1490981583580, \"policy_id\":23635, \"critical_threshold\":{ \"duration_minutes\":100 } } } Copy Create (POST) an infrastructure condition Important Do not include an \"id\": when adding a new condition (POST). It will be generated when the condition is created. To add an infrastructure condition, use this basic cURL command: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are adding: Process running conditions API data Metric conditions API data Host not reporting conditions API data Update (PUT) an infrastructure condition You only need to include the fields that need to be changed when updating an infrastructure condition. The API keeps the existing values for any missing fields. Important If you want to change the condition type, do not use PUT. Instead, delete the existing condition, then add (POST) a new condition with the new condition type and all fields. To update an infrastructure condition, use this basic cURL command. To indicate which condition is to be updated, be sure to include the \"id\": . Example update (PUT) a condition curl -X PUT 'https://infra-api.newrelic.com/v2/alerts/conditions/condition-id' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are updating: Process running conditions API data Metric conditions API data Host not reporting conditions API data Remove (DELETE) an infrastructure condition To delete an infrastructure condition, use this basic cURL command: curl -v -X DELETE --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition_id\" Copy Types of conditions Process running conditions API data A process running condition alerts you when the number of processes is above, below, or equal to the threshold you define. To add (POST) or update (PUT) a process running condition, use your API key, and refer to the definitions to customize your values in the API call. Example condition types For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(commandName = '\\''java'\\'')\" } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause and process_where_clause Metric conditions API data A metric condition alerts you when the metric of your choice is above, below, or equal to the threshold you define. This includes: System metrics Process metrics Network metrics Storage metrics Cloud integration metrics To add (POST) or update (PUT) a metric condition, use your API key, and refer to the definitions to customize your values in the API call. If you are adding or updating a cloud integration alert condition: For the event_type field, enter the event type generated by your selected cloud integration service (for example, ComputeSample for the AWS EC2 integration). If you are setting up an alert condition on a cloud integration service that requires a provider value (for example, AWS RDS uses DatastoreSample with a provider value of RdsDbInstance or RdsDbCluster), you will need to add the \"integration_provider\" field and use the value that is appropriate for the service your alert condition is targeting (for example, \"integration_provider\":\"RdsDbInstance\"). For the select_value field, build the metric name by using the following syntax, where provider is a standard prefix string: provider.metric.aggregation_type Copy metric: Use the metric name as described in the New Relic documentation for your integration. aggregation_type: Use Sum, Average, Minimum, or Maximum. Refer to the original documentation by the integration's cloud provider to see which statistic aggregations are available for each metric. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_metric\", \"name\":\"Disk Space Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"event_type\":\"StorageSample\", \"select_value\":\"diskFreePercent\", \"comparison\":\"below\", \"critical_threshold\":{ \"value\":10, \"duration_minutes\":1, \"time_function\":\"any\" }, \"warning_threshold\":{ \"value\":30, \"duration_minutes\":2, \"time_function\":\"any\" } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Host not reporting condition A host not reporting condition alerts you when a host stops reporting. To add (POST) or update (PUT) a host not reporting condition, use your API key, and refer to the definitions to customize your values in the API call. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"Cassandra Host Reporting Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"critical_threshold\":{ \"duration_minutes\":12 } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Definitions When formatting your cURL commands, use these values as needed. These are listed in alphabetical order, not the order they appear in your API calls. Field Definition comparison (enum) Condition type: infra_metric, infra_process_running The value used to define the threshold; for example, \"[\"above\", \"below\", \"equal\"]. critical_threshold and warning_threshold Condition type: all This object identifies the threshold value before opening a violation. The critical_threshold is required. The warning_threshold is optional and may only be used with infra_metric conditions. The keys of this object depend on the condition type. Condition type: infra_metric format: \"critical_threshold\":{ \"value\":<number>, \"duration_minutes\":<integer>, \"time_function\":\"any\" or \"all\" }, Copy Condition type: infra_process_running format: \"critical_threshold\":{ \"value\":<integer>, \"duration_minutes\":<integer>, }, Copy Condition type: infra_host_not_reporting format: \"critical_threshold\":{ \"duration_minutes\":<integer>, }, Copy value The numeric value that must be breached for the condition to open a violation duration_minutes The number of minutes the value must be passed or met for the condition to open a violation time_function Indicates if the condition needs to be sustained for a certain period of time to create a violation, or if it only needs to break the threshold once within a certain period of time. If you're setting up a for at least x minutes threshold, use all; for an at least once in x minutes threshold, use any. enabled (boolean) Condition type: all Whether the condition is turned on or off; true or false. event_type (string) Condition type: infra_metric The metric event; for example, system metrics, process metrics, storage metrics, or network metrics. This automatically populates for infrastructure integrations; for example, StorageSample or SystemSample. filter (string) Condition type: all If the condition was made in the UI, filter appears instead of where_clause; for example: {and: [{is: {ec2InstanceType: \"m3.medium\"}}]} Copy Recommendation: Use where_clause when creating a new condition. id (integer) Condition type: all The condition ID located in the URL. GET: This value appears in the GET response. PUT: Include this value in the DATA section. POST: Do not include this in the DATA section. DELETE: Include this value in the -X DELETE call. integration_provider (string) Condition type: infra_metric For alerts on integrations, use integration_provider instead of event_type. To see valid values: From the New Relic documentation for your cloud service, check the Find and use data section. Example: In the AWS RDS monitoring integration documentation, you can see that the DatastoreSample event type can be used with an integration_provider value of either RdsDbInstance for DB instances, or RdsDbCluster for Aurora DB clusters. name (string) Condition type: all The infrastructure alerting condition's name; for example: \"[test] process running\" Copy policy_id (integer) Condition type: all The unique ID for the alert policy's account ID associated with the condition; for example, 1234567890. This is not the policy's global ID. process_where_clause (string) Condition type: infra_process_running Any filters applied to processes, specifically in process running alert conditions. This parameter is mandatory for those types of alert conditions. For example: \"commandName = '\\''java'\\''\" Copy runbook_url (string) Condition type: all The runbook URL to display in notifications. select_value (string) Condition type: infra_metric The attribute name to identify the metric being targeted; for example, \"cpuPercent\", \"diskFreePercent\", \"memoryResidentSizeBytes\", or \"memoryFreeBytes/memoryTotalBytes*100\". This automatically populates for Infrastructure Integrations; for example, diskFreePercent. type (enum) Condition type: all The type of infrastructure alert condition: \"infra_process_running\", \"infra_metric\", or \"infra_host_not_reporting\". violation_close_timer (integer) Condition type: all The Violation time limit setting, expressed as hours. Possible values are 0, 1, 2, 4, 8,12, 24, 48, 72. This determines how much time will pass before a violation is automatically closed. Setting the time limit to 0 prevents a violation from being force-closed. For new conditions, if a value is not provided, the following default values are used: Host Not Responding (HNR) conditions: 0 (disabled) All other conditions: 24 When updating existing conditions, if a value is provided, it overrides the existing value, but does not affect already opened violations. where_clause (string) Condition type: all If applicable, this identifies any infrastructure host filters used; for example: \"(`hostname` LIKE '\\''%cassandra%'\\'')\", Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 223.77585,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "sections": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use the <em>infrastructure</em> REST API to add, update, delete, and list alerting <em>conditions</em>. You can also manage individual alerting <em>conditions</em> using the <em>infrastructure</em> monitoring UI. REST API calls for <em>infrastructure</em> <em>alerts</em> are not available in the API Explorer. Why use the API Examples Consistency"
      },
      "id": "6043fa6c196a678ae2960f31"
    },
    {
      "sections": [
        "Alert on infrastructure processes",
        "Important",
        "Examples",
        "Ensure enough processes are running to satisfy load",
        "Ensure that critical services run constantly",
        "Monitor startup for critical processes that require special attention",
        "Make sure a job doesn't take too long",
        "Validate that services started successfully",
        "Watch for runaway processes or configuration problems",
        "Create an infrastructure process running condition"
      ],
      "title": "Alert on infrastructure processes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "5fcbe11b9beb16723ff2521fca981f19a4c716ce",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/alert-infrastructure-processes/",
      "published_at": "2021-05-05T03:39:35Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use New Relic infrastructure's Process running alert condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process you expected to start on a host (such as a new program) is not actually running This feature's flexibility allows you to easily filter what hosts and processes to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Examples By applying filters to the hosts and processes that are important to your business, you can define alerting thresholds to decide when violations open and New Relic sends an email notification to you depending on the policy's incident preferences. These examples illustrate how to use infrastructure monitoring's Process running condition to monitor your processes. Ensure enough processes are running to satisfy load Problem: Some load balancers and application servers work by running many worker processes in parallel. Here, for example, you may want an alert violation when fewer than eight processes are running for a service like gunicorn. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Ensure that critical services run constantly Problem: A service, such as a database or application server, is expected to be running constantly on certain hosts, and you need to know when it has stopped. Solution: Use the No processes are running (default) threshold. Monitor startup for critical processes that require special attention Problem: You have processes requiring special attention due to security or potential performance impact. Solution: Use the At least one process is running threshold with condition filters set to a username and specific executable so that New Relic can open a violation when the process is running. Make sure a job doesn't take too long Problem: You have a job that runs periodically, and you want to open a violation when it has been running longer than an expected number of minutes. Solution: Use the At least one process is running threshold. Validate that services started successfully Problem: When provisioning new hosts, you want to open a violation if a required service fails to successfully start up. Solution: Use the No processes are running (default) threshold. Watch for runaway processes or configuration problems Problem: Sometimes problems with processes can be solved with changes to your configuration. For example, you have more than one Chef process running, and you may need to address an issue with how that service is configured. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Create an infrastructure process running condition To define the Process running alert criteria: Follow standard procedures to create an infrastructure alert condition. Select Process running as the Alert type. Filter what hosts and processes you want the alert condition to apply to. Define the Critical threshold for triggering the alert notification: minimum 1 minute, default 5 minutes, maximum 60 minutes. If you create the alert condition directly with infrastructure monitoring, New Relic will send an email notification when the defined threshold for the alert condition passes depending on the policy's incident preferences. Your alert policy defines which personnel or teams and which notification channels we use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.6369,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alert</em> on <em>infrastructure</em> processes",
        "sections": "<em>Alert</em> on <em>infrastructure</em> processes",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use New Relic <em>infrastructure</em>&#x27;s Process running <em>alert</em> condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process you expected to start"
      },
      "id": "603eb49128ccbca939eba74a"
    },
    {
      "sections": [
        "Infrastructure alerting examples",
        "Examples: Infrastructure pages",
        "Examples: Threshold options",
        "Integrations providers",
        "CPU, disk, load average, memory, swap",
        "Byte size"
      ],
      "title": "Infrastructure alerting examples",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "1ec5f86b745413b2a8d6a5b676ecbe622c674ab1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/infrastructure-alerting-examples/",
      "published_at": "2021-05-05T03:39:35Z",
      "updated_at": "2021-03-16T07:35:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Alert type field in infrastructure monitoring's Settings > Alerts page shows what options you can select to create infrastructure alert conditions. You can also create alert conditions from any infrastructure chart by selecting the ellipses icon and then Create alert. Examples: Infrastructure pages Here are some examples of how to create alert conditions within the context of the Infrastructure monitoring UI page you are currently viewing. To create an alerts condition from any chart, select the ellipses icon and then Create alert. New Relic will automatically select the appropriate Alert type. Example Problem and solution High CPU usage Problem: Your Ops team monitors a filtered set of host clusters in your eastern region and notices that the CPU usage is constantly high. Solution: Use the CPU chart on Infrastructure monitoring's Hosts page to create an alert condition for system metrics. Virtual memory capacity Problem: Your night shift needs to be alerted when virtual memory for a set of background workers reaches an average of 10G for at least two minutes. Solution: Use the Top memory consumers chart on Infrastructure monitoring's Processes page to create an alert condition for process metrics. Limited bandwidth Problem: You want to monitor performance based on the average number of errors received or transmitted. Solution: Use the Top bandwidth chart on Infrastructure monitoring's Network page to create an alert condition for network metrics. I/O read and write operations Problem: You are testing a new set of hosts in your staging environment, and you want to be notified when their read or write capacity rises above your test threshold level. Solution: Use the Top I/O operations chart on Infrastructure monitoring's Storage page to create an alert condition for storage metrics. Host not reporting Problem: You want to be notified when we have stopped receiving data from an infrastructure agent. Solution: From the Hosts, Processes, Network, or Storage pages, create a host not reporting alert condition. Processes not running as expected Problem: You want to be notified if any of the processes on your hosts stop reporting. OR A process you expected to start on a host (such as a new program) is not actually running. Solution: From the Processes page (or from the Hosts, Network, or Storage pages), create a process running alert condition. Examples: Threshold options Use the thresholds dropdown for the selected Alert type to further define how you want to be alerted. Here are some examples of the options available. Integrations providers With infrastructure integrations, you can create an alert condition from your Integrations page. Depending on the type of provider selected (CloudFront, DynamoDB, EBS, etc.), options will vary from the Define thresholds dropdown; for example, bytes, errors, requests, CPU, connections, memory, records, latency, etc. CPU, disk, load average, memory, swap The System metrics thresholds dropdown allows you to select various criteria for CPU, disk, load average, memory, and swap metrics. Byte size The Network metrics thresholds provide flexibility with your business needs. Depending on the size of your network, you can easily set the threshold in bytes, KB, MB, GB, or TB.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.61485,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>alerting</em> examples",
        "sections": "<em>Infrastructure</em> <em>alerting</em> examples",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "The <em>Alert</em> type field in <em>infrastructure</em> monitoring&#x27;s Settings &gt; <em>Alerts</em> page shows what options you can select to create <em>infrastructure</em> <em>alert</em> <em>conditions</em>. You can also create <em>alert</em> <em>conditions</em> from any <em>infrastructure</em> chart by selecting the ellipses icon and then Create <em>alert</em>. Examples: <em>Infrastructure</em>"
      },
      "id": "603eb52a28ccbc9027eba7bb"
    }
  ],
  "/docs/infrastructure/infrastructure-monitoring/get-started/get-started-infrastructure-monitoring": [
    {
      "sections": [
        "Install the infrastructure agent",
        "Tip",
        "Quick start: Use our guided install",
        "One agent, many capabilities",
        "Install the infrastructure monitoring agent",
        "Linux",
        "Windows Server and 10",
        "Other installation scenarios",
        "Check the source code",
        "What's next"
      ],
      "title": "Install the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "ccb11bfd79824202d189a3e743771cfc81e77710",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent/",
      "published_at": "2021-05-05T03:42:54Z",
      "updated_at": "2021-03-30T08:28:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring agent is a lightweight executable file that collects data about your hosts. It also forwards data from infrastructure integrations to New Relic, as well as log data for log analytics. There are multiple ways to install and deploy the infrastructure monitoring agent, depending on your setup and needs. This document describes how the infrastructure monitoring agent works and how to install it. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. One agent, many capabilities Our infrastructure monitoring agent collects performance and health data about the system resources and processes of the host where it's enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-host integrations, and logs. If you want to collect data about core services running on your host, you need to install the infrastructure monitoring agent first, and then install or enable on-host integrations. Our infrastructure monitoring agent and its integrations collect data from the system and core services. It can also forward logs to New Relic. Backend application metrics (APM) are collected by separate language agents. Notice how each integration and forwarder feed different data types in the New Relic database (NRDB). Install the infrastructure monitoring agent If our guided install doesn't work for your setup, follow the instructions for your Linux, Windows, or other setup. The infrastructure monitoring agent can currently run on many Linux distributions, as well as Windows Server. For more information on where you can run the agent, check the compatibility and requirements page. Linux The preferred way to install the Linux agent is through the package manager of your distribution. Select your distribution from the list for step-by-step instructions. Amazon Linux CentOS Container (Docker) Debian RHEL SLES Ubuntu Other Linux distros To use these installation links, you must be logged to your New Relic account. If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial. For advanced install needs, you can deploy the agent using our tarball files in assisted or manual mode. Windows Server and 10 To deploy the agent on a Windows Server host, install it using our MSI installer. For a guided procedure, click the button below and follow the step-by-step instructions: Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, see our tutorial. For advanced installation needs, you can deploy the agent using our zip files in assisted or manual mode. Other installation scenarios The infrastructure monitoring agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Check the source code The infrastructure monitoring agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README. What's next After you've installed the infrastructure monitoring agent: Learn how to configure the agent or edit the config template. Install on-host integrations (for example, for Apache or MySQL). Enable log forwarding using the infrastructure agent. Learn how to manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 126.452835,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Install the <em>infrastructure</em> agent",
        "sections": "Install the <em>infrastructure</em> <em>monitoring</em> agent",
        "tags": "<em>Get</em> <em>started</em>",
        "body": ". Then ingest up to 100GB of data for free each month. Forever. Quick <em>start</em>: Use our guided install The quickest way to <em>get</em> <em>started</em> with our <em>infrastructure</em> <em>monitoring</em> agent is through our guided install. Ready to <em>get</em> <em>started</em>? Click one of these button to try it out. Guided install EU Guided install"
      },
      "id": "603e79bd64441f99814e8888"
    },
    {
      "sections": [
        "Ruby agent requirements and supported frameworks",
        "Operating systems",
        "Security requirements",
        "Ruby versions",
        "Web servers",
        "Web frameworks",
        "Databases",
        "Other APM software",
        "Instance details",
        "Background jobs",
        "HTTP clients",
        "Message queuing",
        "Other",
        "Connect the agent to other parts of New Relic"
      ],
      "title": "Ruby agent requirements and supported frameworks",
      "type": "docs",
      "tags": [
        "Agents",
        "Ruby agent",
        "Getting started"
      ],
      "external_id": "fcbaf26ace3ac2dba3d30693820665ad5ae23b36",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/ruby-agent/getting-started/ruby-agent-requirements-supported-frameworks/",
      "published_at": "2021-05-04T17:31:38Z",
      "updated_at": "2021-04-28T01:27:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic's Ruby agent, make sure you meet these requirements for compatible operating systems, security requirements, and supported frameworks. Operating systems The Ruby agent supports UNIX-like operating systems such as Linux, Solaris, FreeBSD, and macOS. Security requirements As a standard security measure for data collection, your app server must support SHA-2 (256-bit). SHA-1 is not supported. Ruby versions The New Relic Ruby agent does not support experimental versions. Ruby versions supported by the Ruby agent include: Ruby versions Supported Deprecated JRuby 9.0.x 9.1.x 9.2.x 1.7.x or earlier: Last supported agent was 3.18.1.330. MRI 1.8.7 1.9.x 2.0.x 2.1.x 2.2.x 2.3.x 2.4.x 2.5.x 2.6.x 2.7.x 2.0.x, 2.1.x Last supported agent: 6.15.0. 1.8.7, 1.9.2, 1.9.3: Last supported agent was 3.18.1.330. 1.8.6: Last supported agent was 3.6.8.168. Web servers Web servers supported by the Ruby agent include: Web servers Supported Deprecated Experimental Passenger 2.2.x 3.0.x 4.0.x 5.x.x 6.0.x Puma 2.0.x 3.x.x 1.0.x Rainbows! 4.5.0 Thin 1.x.x Unicorn 4.x.x 5.x.x 6.0.x 1.0.x 2.0.x 3.0.x Webrick Supported for all agent-supported versions of Ruby Web frameworks The Ruby agent does not support experimental versions. Web frameworks supported by the Ruby agent include: Web frameworks Supported Deprecated Grape 0.2.0 1.2.x 1.3.x 1.4.x Padrino 0.14.x 0.15.x Rack 1.1.0 or higher 2.0.3 or higher 1.0.x Rails 3.2.x 4.0.x 4.1.x 4.2.x 5.0.x 5.1.x 5.2.x 6.0.x 2.1.x, 2.2.x, 2.3.x: Last supported agent was 3.18.1.330. 2.0.x: Last supported agent was 3.6.8.168. 3.0.x, 3.1.x: Last supported agent was 6.15.0. Sinatra 1.4.x 1.5.x 2.0.x 2.1.x 1.2.x, 1.3.x: Last supported in agent version 6.15.0 Databases The Ruby agent does not support experimental versions. Databases supported by the Ruby agent include: Databases Supported Deprecated ActiveRecord 3.2.x 4.0.x 4.1.x 4.2.x 5.0.x 5.1.x 5.2.x 6.0.x 6.1.x 3.0.x, 3.1.x: Last supported in agent version 6.15.0. 2.1.x, 2.2.x, 2.3.x: Last supported agent was 3.18.1.330. 2.0.x Last supported agent was 3.6.8.168. DataMapper 1.0 Mongo 1.8.x or higher 1.9.x 2.0.x or higher Redis 3.x 4.0.x 4.1.x 4.2.x Sequel 3.37.x 4.0.x 5.5.x or higher Other APM software If your application uses other application performance monitoring (APM) software besides our agent, we cannot guarantee that our agent will work correctly and we cannot offer technical support. For more information, see Errors when using other monitoring software. Instance details New Relic collects instance details for a variety of databases and database drivers. The ability to view specific instances and the types of database information in APM depends on your New Relic agent version. New Relic's Ruby agent version 3.17.0 or higher supports the following ORM databases: ORM Database Adapter name Minimum agent version ActiveRecord 5 or higher PostgreSQL pg 3.17.0 MySQL mysql2 3.17.0 ActiveRecord 2.1 to 4 PostgreSQL pg 3.17.0 MySQL mysql 3.17.0 MySQL mysql2 3.17.0 The Ruby agent also supports these gem databases: Gem database Gem name Minimum gem version Minimum agent version Memcached Dalli memcached memcache-client 2.7.6 1.8.0 1.5.0 3.17.0 Mongo DB mongo 1.8.6 3.17.0 Redis redis-rb 3.0.7 3.17.0 To request instance-level information from datastores currently not listed for your New Relic agent, get support at support.newrelic.com. Background jobs Background jobs supported by the New Relic Ruby agent include: Background jobs Supported Deprecated Delayed_Job 2.0.x 3.0.x 4.0.x 4.1.x Rake 12.3.3 or higher 13.x 10.x 11.x 12.3.2 or earlier. NOTE: Only 12.3.3 or higher tested due to exploit potential in earlier versions. Resque 1.23.x 1.27.x 2.0.0 or higher 1.22.x and earlier Sidekiq 4.2.x 5.0.x 6.0.x 6.1.x 2.8.x, 3.4.x, 4.0.x, and 4.1.x: Last supported in agent version 6.15.0 HTTP clients HTTP clients supported by the Ruby agent include: Net::HTTP : Supported for all agent-supported versions of Ruby. Curb: 0.8.1 or higher Excon: 0.10.1 or higher HttpClient: 2.2.0 or higher HttpRb: 0.9.9 or higher Typhoeus: 0.5.3 or higher Message queuing Message queue instrumentation is only available with the Ruby agent version 4.3.0 or higher. Currently supported message brokers: RabbitMQ Other APM's Ruby agent also supports: ActiveMerchant:1.25.0 or higher Acts_as_Solr authlogic Bunny: 2.0 or higher Dalli Memcache-Client Sunspot Connect the agent to other parts of New Relic The Ruby agent integrates with other New Relic capabilities to give you end-to-end visibility. Capability Integration Browser monitoring The Ruby agent automatically injects the Browser JavaScript agent when you enable auto-instrumentation. After enabling Browser injection, you can view Browser data in the APM Summary page and quickly switch between the APM and Browser data for a particular app. For configuration options and manual instrumentation, see the browser and Ruby agent documentation. Infrastructure monitoring When you install the Infrastructure and APM agents on the same host, they automatically detect one another. You can then view a list of hosts in the APM UI, and filter your Infrastructure hosts by APM app in the Infrastructure UI. For more information, see APM data in infrastructure monitoring. Synthetic monitoring Synthetic transaction traces connect requests from Synthetics monitors to the underlying APM transaction.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 119.86438,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Getting</em> <em>started</em>",
        "body": " for a particular app. For configuration options and manual instrumentation, see the browser and Ruby agent documentation. <em>Infrastructure</em> <em>monitoring</em> When you install the <em>Infrastructure</em> and APM agents on the same host, they automatically detect one another. You can then view a list of hosts in the APM UI"
      },
      "id": "603ebad028ccbc6835eba79d"
    },
    {
      "sections": [
        "PHP agent compatibility and requirements",
        "PHP releases",
        "Important",
        "Permissions",
        "License key",
        "Processor type",
        "Operating systems",
        "Web servers",
        "Frameworks",
        "Databases and libraries",
        "Other APM software",
        "Instance details",
        "Message queuing",
        "Security requirements",
        "Connect the agent to other New Relic features",
        "For more help"
      ],
      "title": "PHP agent compatibility and requirements",
      "type": "docs",
      "tags": [
        "Agents",
        "PHP agent",
        "Getting started"
      ],
      "external_id": "7dcc0dfcba63adaf1c406559a68dac52fe817f7f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/agents/php-agent/getting-started/php-agent-compatibility-requirements/",
      "published_at": "2021-05-04T17:55:53Z",
      "updated_at": "2021-04-28T01:18:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before you install New Relic for PHP, make sure your system meets the applicable requirements. PHP releases New Relic supports PHP versions 5.5, 5.6, 7.0, 7.1, 7.2, 7.3, 7.4, and 8.0. Important Compatibility note: When PHP 8.0 detects the New Relic agent, it disables Just-In-Time compilation. We recommend using a supported release of PHP, especially 7.3, 7.4, and 8.0. PHP 5.1 support was deprecated in release 4.0 of the agent, and removed in release 4.5. PHP 5.2 support was deprecated in release 6.8 of the agent, and removed in release 7.0. PHP 5.3 and PHP 5.4 support was deprecated in release 9.15 of the agent, and removed in release 9.17. Permissions Installation: Root access is required for most installations. Running: Root access is not required. License key For any installation, you will need your New Relic license key. This is a 40-character hexadecimal string that New Relic provides when you sign up for your account. For more information, see the PHP install script and license key settings in the PHP INI file. Processor type Intel (and compatible) platforms only Support for SSE2 instructions is required Operating systems Important The Windows operating system is not supported. Operating system Supported by New Relic's PHP agent Linux (x86 and x86_64) AWS Linux 2 Red Hat Enterprise Linux (RHEL) 5 or higher CentOS 5.5 or higher (except 5.6) Important As of January 2021, we will discontinue support for CentOS 5. For more information, see our PHP agent release notes v9.15.0 and our Explorers Hub post. Debian 5.0 (\"lenny\") or higher Ubuntu 9.10 (\"Karmic Koala\") or higher Any other Linux distribution with: Kernel version 2.6.13 or higher (2.6.26 or higher highly recommended) glibc 2.5 or higher with NPTL support; or musl libc version 1.1 or higher macOS (x86_64 only) Important As of January 2021, we will discontinue support for macOS. For more information, see our PHP agent release notes v9.15.0 and our Explorers Hub post. macOS 10.6 or higher (See macOS troubleshooting notes.) Since modern versions of macOS can't run 32-bit applications, New Relic removed support for 32-bit macOS with PHP agent release 4.6. FreeBSD (x64) The latest agent supports the latest Production Release. SmartOS and Solaris (x64) (DEPRECATED) Last supported PHP Agent for Solaris and SmartOS is 8.1 SmartOS: All versions OpenSolaris 10: snv_134b or higher Must use i86pc architecture. SPARC architectures are not supported. Web servers Apache 2.2 or 2.4 via mod_php Any web server that supports FastCGI using PHP-FPM Frameworks Supported PHP frameworks include: Frameworks CakePHP 1.2, 1.3, and 2.x Magento 1.x and 2.x, CE and EE CodeIgniter 2.x MediaWiki Drupal 6.x, 7.x, 8.x, and 9.x Silex 1.x and 2.x Joomla 1.5, 1.6, 2.x, and 3.x Slim 2.x, 3.x, and 4.x Kohana 3.2 and 3.3 Symfony 1.x, 2.x, 3.x, 4.x, 5.x Laminas 3.x Wordpress Laravel 4.x, 5.x, 6.x, and 7.x Yii 1.x Laravel Lumen 6.x, 7.x, and 8.x Zend Framework 1.x, 2.x, and 3.x Important Joomla 3.x is not supported on PHP 8. Important As of PHP agent version 9.17, the following frameworks or framework versions are no longer supported and may be removed from future agent builds: Cake PHP 1.x Joomla 1.5, 1.6, and 2.x Kohana Silex 1.x and 2.x Symfony 1.x and 2.x The PHP agent's list of frameworks continues to grow. Even if the framework you are using is not listed here, the PHP agent may be able to provide you with useful information about your app. For more information, see PHP frameworks: Integrating support for New Relic. If you want to suggest support for other popular PHP frameworks, visit us at the Explorers Hub and create a Feature Idea! Databases and libraries Supported databases and libraries: Databases and Libraries Firebird Oracle Guzzle PHPUnit Informix PDO Memcached Postgres MongoDB Predis Microsoft SQL Server Redis MySQL SQLite ODBC Sybase Other APM software If your application uses other application performance monitoring (APM) software besides our agent, we cannot guarantee that our agent will work correctly and we cannot offer technical support. For more information, see Errors when using other monitoring software. Instance details New Relic collects instance details for a variety of databases and database drivers. The ability to view specific instances and the types of database information in APM depends on your New Relic agent version. New Relic's PHP agent version 6.8 or higher supports instance details for the following: Database Extension Minimum agent version MongoDB mongodb 7.1 MySQL mysql 6.8 MySQL mysqli 6.8 MySQL pdo_mysql 6.8 PostgreSQL pgsql 6.9 PostgreSQL pdo_pgsql 6.9 Redis predis 7.1 Redis redis 7.1 To disable collection of host information, use either of these options: Set newrelic.datastore_tracer.instance_reporting.enabled to false in the newrelic.ini. Omit the database name with newrelic.datastore_tracer.database_name_reporting.enabled = false. To request instance-level information from datastores currently not listed for your New Relic agent, get support at support.newrelic.com. Message queuing HTTP Laravel Queuing, available as an experimental feature in the PHP Agent 6.6.0.169 release, enabled by default since PHP Agent 8.0.0.204. Security requirements As a standard security measure for data collection, your app server must support SHA-2 (256-bit). SHA-1 is not supported. Connect the agent to other New Relic features The PHP agent integrates with other New Relic features to give you end-to-end visibility: Product Integration Browser monitoring The PHP agent automatically injects the browser agent's JS code when you enable auto-instrumentation. After enabling browser injection, you can view data in the APM Summary page and quickly switch between the APM and browser data for a particular app. For configuration options and manual instrumentation, see Browser monitoring and the PHP agent. Infrastructure monitoring When you install the infrastructure and APM agents on the same host, they automatically detect one another. You can then view a list of hosts in the APM UI, and filter your Infrastructure hosts by APM app in the Infrastructure UI. For more information, see APM data in infrastructure monitoring. New Relic Dashboards The PHP agent sends default events and attributes for NRQL queries. You can also record custom events for advanced analysis. Synthetic monitoring Synthetic transaction traces connect requests from synthetic monitors to the underlying APM transaction. For more help If you need more help, check out these support and learning resources: Suggest a change and learn how to contribute to our PHP agent open source repository. Browse the Explorers Hub to get help from the community and join in discussions. Find answers on our sites and learn how to use our support portal. Run New Relic Diagnostics, our troubleshooting tool for Linux, Windows, and macOS. Review New Relic's data security and licenses documentation.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 119.84767,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Getting</em> <em>started</em>",
        "body": " between the APM and browser data for a particular app. For configuration options and manual instrumentation, see Browser <em>monitoring</em> and the PHP agent. <em>Infrastructure</em> <em>monitoring</em> When you install the <em>infrastructure</em> and APM agents on the same host, they automatically detect one another. You can then view"
      },
      "id": "60441ccae7b9d21edf5799c4"
    }
  ],
  "/docs/infrastructure/infrastructure-monitoring/infrastructure-security/infrastructure-security": [
    {
      "sections": [
        "Get started with infrastructure monitoring",
        "Tip",
        "Quick start: Use our guided install",
        "Reduce MTTR with actionable data",
        "Install and configure the infrastructure agent",
        "Add integrations to gather data about your services",
        "Filter your hosts any way you want",
        "Explore your infrastructure data",
        "Check the source code"
      ],
      "title": "Get started with infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring",
        "Get started"
      ],
      "external_id": "0451d55a1af096a3b33ceda0bb08b9d956e0472a",
      "image": "https://docs.newrelic.com/static/0552c14ad1f127cde466137f57e562ba/c1b63/nrone-infrastructure-hosts.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-monitoring/get-started/get-started-infrastructure-monitoring/",
      "published_at": "2021-05-05T03:28:38Z",
      "updated_at": "2021-03-30T09:50:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic provides flexible, dynamic observability of your entire infrastructure, from services running in the cloud or on dedicated hosts to containers running in orchestrated environments. You can connect the health and performance of all your hosts to application context, logs, and configuration changes. With infrastructure monitoring, modern operations teams get complete observability of complex and hybrid systems, from a datacenter to thousands of Amazon, Google Cloud, or Azure instances. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. The Hosts page for your infrastructure in New Relic One shows performance indicators for systems, networks, processes, and storage. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Guided install EU Guided install Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. Reduce MTTR with actionable data Real-time metrics and analytics reduce your mean-time-to-resolution (MTTR) by connecting changes in host performance to changes in your configuration. From the Inventory page you can search across your entire estate to find exactly which of your hosts contain particular packages, configs, or startup scripts. The Inventory page in New Relic One focuses on all the infrastructure entities in your estate. From the Events page you can track config changes, restarts, SSH sessions, and other key event changes. A real-time feed gives you a changelog for your entire infrastructure. Our solution securely collects and displays your data in five seconds, so your monitoring never lags behind reality. The Events page in New Relic One contains a real-time feed of all that is happening in your hosts. With our infrastructure monitoring tools you can also: Query your data to drill down into your infrastructure events and build custom dashboards that you can share with your team. Troubleshoot performance issues, no matter where the issue occurs—server-side or application-side—thanks to APM data connection with infrastructure monitoring. Create, view, or update alert settings directly from the relevant infrastructure chart. For example, you can create a host not reporting alert condition. Forward logs to our logs monitoring by adding as many configuration files as log sources you need to push to our platform. Install and configure the infrastructure agent You can send metric data to our platform using our open source infrastructure agent, a lightweight executable file that works in the background to collect the information you need. The simplest way to install and run the infrastructure agent is using a package manager (Linux) or the MSI installer (Windows). You can also use our installation assistants: Amazon Linux CentOS Container (Docker) Debian RHEL SLES Ubuntu Windows To use the links above, you must be logged to your New Relic account. If you don't have a New Relic account yet, or prefer to follow the procedure step-by-step, see our tutorials to install the agent for Linux | Windows | Elastic Beanstalk | Ansible | Chef | Puppet. Add integrations to gather data about your services Integrations give you access to the metrics of many popular systems, including Amazon Web Services (AWS), Google Cloud Platform, Microsoft Azure, Kubernetes, MySQL, Cassandra, and more. You can also make your own integrations using New Relic Flex or the Integrations SDK to collect data from other applications. With New Relic Flex, you can instrument any command-line tool without coding. Our Kubernetes cluster explorer in New Relic One gives you a powerful and innovative solution to the challenges associated with running Kubernetes at a large scale. Filter your hosts any way you want Filter sets allow you to organize your hosts based on criteria that matter most to you. You can filter your hosts by any infrastructure attribute, such as geographic location, hostname, or Linux distribution. You can also filter on Amazon attributes, such as region or instance type, and add custom attributes to define unique metadata, such as what team manages the host or the services running. Explore your infrastructure data You can query and visualize your infrastructure data in New Relic One: Sample and search infrastructure events, such as to gain full understanding of the data being collected. Browse your data visually with the data explorer. Create custom SQL-like queries of your data using the New Relic Query Language (NRQL), or query your data using our PromQL-style queries. Use dashboards to build advanced data visualizations, contextualize data, and understand what's going on in your system, real-time. Check the source code The infrastructure agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 90.302124,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Get started with <em>infrastructure</em> <em>monitoring</em>",
        "sections": "Get started with <em>infrastructure</em> <em>monitoring</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em>",
        "body": " changes. With <em>infrastructure</em> <em>monitoring</em>, modern operations teams get complete observability of complex and hybrid systems, from a datacenter to thousands of Amazon, Google Cloud, or Azure instances. Tip To use <em>infrastructure</em> <em>monitoring</em> and the rest of our observability platform, join the New Relic"
      },
      "id": "6043fb59e7b9d272d9579a16"
    },
    {
      "sections": [
        "Reduce the infrastructure agent's CPU footprint",
        "Problem",
        "Solution",
        "Reduce event sampling",
        "Important",
        "Reduce agent plugin reporting",
        "How to enable and disable plugins",
        "Disable SELinux semodule -l (Linux only)",
        "Reduce or disable Sysctl (Linux only)",
        "Additional plugins to reduce or disable",
        "Review on-host integrations"
      ],
      "title": "Reduce the infrastructure agent's CPU footprint",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "4eea817bfabb6b698ea3ce001b8c5eeca20d475e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint/",
      "published_at": "2021-05-05T03:40:45Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The New Relic infrastructure agent is consuming too much CPU. Solution The New Relic infrastructure agent is designed to report a broad range of system data with minimal CPU and memory consumption. However, if you have a need to reduce your CPU consumption, you can disable or decrease the sampling frequency of various samplers and plugins. This topic highlights some newrelic-infra.yml configurations that may reduce your CPU usage: Reduce event sampling Reduce agent plugin reporting Review on-host integrations Reduce event sampling The infrastructure agent reports several default events at specific frequencies. To lower the overhead, you can reduce the sampling frequency in seconds, or you can completely disable the samplers by setting the corresponding property value to -1. Important We don't recommend a sample rate larger than 60 seconds because you may see gaps in the New Relic user interface charts. The table below lists some samplers to configure: Event Sampling frequency Allow/deny list Network Network sampling rate Not available Process Process sampling rate Allow list (Windows only) Storage Storage sampling rate Deny list System System sampling rate Not available Reduce agent plugin reporting The infrastructure agent has built-in plugins that collect inventory data (specific system configuration and state information). For some systems, the CPU consumption may be relatively high if the plugins are gathering a lot of data. To reduce the footprint, you can disable or decrease the sampling frequency for specific plugins that report data you don’t want. How to enable and disable plugins Disable a single plugin: To disable a plugin, set the corresponding property value to -1. Disable all plugins: disable_all_plugins: true Enable selected plugins: To enable certain plugins, insert an exception in disable_all_plugins. For example, the following configuration disables all plugins, but the Network Interfaces plugin reports every 120 seconds: disable_all_plugins: true network_interface_interval_sec: 120 Copy Disable SELinux semodule -l (Linux only) The SELinux plugin periodically invokes the semodule -l system command to get information about the existing SELinux modules. In most CentOS/RedHat distributions, this command will generate CPU consumption peaks. To disable this functionality, insert the following configuration option in your /etc/newrelic-infra.yml file: selinux_enable_semodule: false Reduce or disable Sysctl (Linux only) The Sysctl plugin walks the whole /sys directory structure and reads values from all the files there. Disabling it or reducing the interval may decrease some CPU System time in the Infrastructure agent. You can disable inventory frequency by setting it to a negative number or reduce the frequeny by setting the sysctl_interval_sec configuration value to the number of seconds between consecutive executions of the plugin. For example, to execute the plugin once every 10 minutes: sysctl_interval_sec: 600 Copy To disable the Sysctl plugin: sysctl_interval_sec: -1 Copy The current default value for the sysctl_interval_sec property is 60. Additional plugins to reduce or disable The following inventory plugins are not especially CPU consuming, but you can still reduce their frequency or disable them by setting the corresponding configuration options. Linux plugins For configuration of these Linux plugins, see Plugin variables: Cloud Security Groups Daemon Tools DPKG Facter Kernel Modules Network interfaces RPM SELinux Supervisord Sysctl Systemd SysV Upstart Users SSHD configuration Windows plugins For configuration of these Windows plugins, see Plugin variables: Network interfaces Windows services Windows updates Review on-host integrations If you use infrastructure on-host integrations, this may have additional impacts on CPU usage. The nature of the impact and the methods to adjust the impact depend on the integration you're using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the monitoring load by adding additional infrastructure agents. For example, the Kafka integration allows a multi-agent deployment.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 84.52898,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "sections": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> troubleshooting",
        "body": " on the integration you&#x27;re using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the <em>monitoring</em> load by adding additional <em>infrastructure</em> agents. For example, the Kafka integration allows a multi-agent deployment."
      },
      "id": "603eb9dc64441fbf1f4e8847"
    },
    {
      "sections": [
        "Events heatmap: Examine patterns in time range"
      ],
      "title": "Events heatmap: Examine patterns in time range",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "bc50e789884c9c4eea404d558d4070519a3eab0c",
      "image": "https://docs.newrelic.com/static/96c3e087c9dfb8b4cb4ad72b79c47e94/c1b63/infra-events-timeline.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/events-heatmap-examine-patterns-time-range/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "he events heatmap provides a snapshot of the infrastructure events occurring within the same time range as the displayed metrics. The darker the color on the heatmap, the more events occurred during that time period. By comparing the heatmap to the charts on the infrastructure page, you can quickly pinpoint issues in your ecosystem. For example, if a massive CPU spike occurs, you can click on the events heatmap for that time range to find the event that caused it. From there you can dive deeper to uncover the real issue. one.newrelic.com > Infrastructure: The heatmap on Infrastructure monitoring UI pages visually shows patterns with events occurring at the same time period for the displayed metrics. The heatmap appears on several Infrastructure UI pages, including: System Network Processes Storage Events",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 84.52898,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> UI",
        "body": " <em>monitoring</em> UI pages visually shows patterns with events occurring at the same time period for the displayed metrics. The heatmap appears on several <em>Infrastructure</em> UI pages, including: System Network Processes Storage Events"
      },
      "id": "603e8455196a67833da83dc2"
    }
  ],
  "/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure": [
    {
      "sections": [
        "Reduce the infrastructure agent's CPU footprint",
        "Problem",
        "Solution",
        "Reduce event sampling",
        "Important",
        "Reduce agent plugin reporting",
        "How to enable and disable plugins",
        "Disable SELinux semodule -l (Linux only)",
        "Reduce or disable Sysctl (Linux only)",
        "Additional plugins to reduce or disable",
        "Review on-host integrations"
      ],
      "title": "Reduce the infrastructure agent's CPU footprint",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "4eea817bfabb6b698ea3ce001b8c5eeca20d475e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint/",
      "published_at": "2021-05-05T03:40:45Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The New Relic infrastructure agent is consuming too much CPU. Solution The New Relic infrastructure agent is designed to report a broad range of system data with minimal CPU and memory consumption. However, if you have a need to reduce your CPU consumption, you can disable or decrease the sampling frequency of various samplers and plugins. This topic highlights some newrelic-infra.yml configurations that may reduce your CPU usage: Reduce event sampling Reduce agent plugin reporting Review on-host integrations Reduce event sampling The infrastructure agent reports several default events at specific frequencies. To lower the overhead, you can reduce the sampling frequency in seconds, or you can completely disable the samplers by setting the corresponding property value to -1. Important We don't recommend a sample rate larger than 60 seconds because you may see gaps in the New Relic user interface charts. The table below lists some samplers to configure: Event Sampling frequency Allow/deny list Network Network sampling rate Not available Process Process sampling rate Allow list (Windows only) Storage Storage sampling rate Deny list System System sampling rate Not available Reduce agent plugin reporting The infrastructure agent has built-in plugins that collect inventory data (specific system configuration and state information). For some systems, the CPU consumption may be relatively high if the plugins are gathering a lot of data. To reduce the footprint, you can disable or decrease the sampling frequency for specific plugins that report data you don’t want. How to enable and disable plugins Disable a single plugin: To disable a plugin, set the corresponding property value to -1. Disable all plugins: disable_all_plugins: true Enable selected plugins: To enable certain plugins, insert an exception in disable_all_plugins. For example, the following configuration disables all plugins, but the Network Interfaces plugin reports every 120 seconds: disable_all_plugins: true network_interface_interval_sec: 120 Copy Disable SELinux semodule -l (Linux only) The SELinux plugin periodically invokes the semodule -l system command to get information about the existing SELinux modules. In most CentOS/RedHat distributions, this command will generate CPU consumption peaks. To disable this functionality, insert the following configuration option in your /etc/newrelic-infra.yml file: selinux_enable_semodule: false Reduce or disable Sysctl (Linux only) The Sysctl plugin walks the whole /sys directory structure and reads values from all the files there. Disabling it or reducing the interval may decrease some CPU System time in the Infrastructure agent. You can disable inventory frequency by setting it to a negative number or reduce the frequeny by setting the sysctl_interval_sec configuration value to the number of seconds between consecutive executions of the plugin. For example, to execute the plugin once every 10 minutes: sysctl_interval_sec: 600 Copy To disable the Sysctl plugin: sysctl_interval_sec: -1 Copy The current default value for the sysctl_interval_sec property is 60. Additional plugins to reduce or disable The following inventory plugins are not especially CPU consuming, but you can still reduce their frequency or disable them by setting the corresponding configuration options. Linux plugins For configuration of these Linux plugins, see Plugin variables: Cloud Security Groups Daemon Tools DPKG Facter Kernel Modules Network interfaces RPM SELinux Supervisord Sysctl Systemd SysV Upstart Users SSHD configuration Windows plugins For configuration of these Windows plugins, see Plugin variables: Network interfaces Windows services Windows updates Review on-host integrations If you use infrastructure on-host integrations, this may have additional impacts on CPU usage. The nature of the impact and the methods to adjust the impact depend on the integration you're using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the monitoring load by adding additional infrastructure agents. For example, the Kafka integration allows a multi-agent deployment.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.39183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "sections": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " on the integration you&#x27;re using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the <em>monitoring</em> load by adding additional <em>infrastructure</em> agents. For example, the Kafka integration allows a multi-agent deployment."
      },
      "id": "603eb9dc64441fbf1f4e8847"
    },
    {
      "sections": [
        "APM data missing from infrastructure monitoring",
        "Problem",
        "Solution",
        "Restart the app server.",
        "Make sure the hostnames are the same in APM and Infrastructure.",
        "Check for replacement host FQDN recognition problems."
      ],
      "title": "APM data missing from infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "68c926e4922c558a2ab2b0f9557f2fe7973ee0af",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/apm-data-missing-infrastructure/",
      "published_at": "2021-05-05T03:34:17Z",
      "updated_at": "2021-03-16T07:35:33Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When APM and infrastructure agents are installed on the same hosts and use the same New Relic license key, APM data should appear in Infrastructure, and vice versa. If you do not see this APM-Infrastructure linkage, follow these troubleshooting tips. Solution If you completed the APM/Infrastructure integration but do not see New Relic APM data in Infrastructure, try these troubleshooting procedures. Restart the app server. If you have not restarted your APM-monitored application in a few weeks or months, the data streams from Infrastructure and APM may not be linked. Restart your app server. Generate some traffic for your app. Wait a few minutes, and then check for APM data in infrastructure monitoring. Make sure the hostnames are the same in APM and Infrastructure. If the hostnames are different in APM and infrastructure monitoring, New Relic cannot integrate the data. One common cause for this issue is that the default hostnames are different. For example, infrastructure monitoring uses a host's FQDN (such as myhost1.example.com), while APM uses the host's name (such as myhost1). Go to one[.newrelic.com](http://one.newrelic.com) > APM > (select an app). From the app's APM Overview page, look at the app's associated host name. Compare that name with the same host's name in infrastructure monitoring. If the names are different, either set the APM agent host's display_name to match its FQDN, or set the host's display_name in Infrastructure to match the one set in APM. Check for replacement host FQDN recognition problems. If the APM-Infrastructure integration previously worked but has stopped, the server may have been replaced by another server that has the same FQDN. If both servers existed simultaneously for a period of time, New Relic cannot automatically recognize the new server. That will break the connection between APM and infrastructure data. To solve this problem, get help at support.newrelic.com. To prevent this problem, make sure there is a time gap between taking down an old server going down and creating a new server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.37183,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "sections": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem When APM and <em>infrastructure</em> agents are installed on the same hosts and use the same New Relic license key, APM data should appear in <em>Infrastructure</em>, and vice versa. If you do not see this APM-<em>Infrastructure</em> linkage, follow these <em>troubleshooting</em> tips. Solution If you completed the APM"
      },
      "id": "603e9100e7b9d2b2962a07e8"
    },
    {
      "sections": [
        "HTTPS proxy configuration not working",
        "Problem",
        "Solution",
        "Version 1.3.1 or higher",
        "Version 1.0.1002 to 1.2.25",
        "Version 1.0.956 to 1.0.989",
        "Version 1.0.944 or lower"
      ],
      "title": "HTTPS proxy configuration not working",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "1eb071405ac556f5d0f577c999dec6bc0871cbfb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/https-proxy-configuration-missing/",
      "published_at": "2021-05-05T03:41:54Z",
      "updated_at": "2021-03-16T06:36:55Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The infrastructure agent's HTTP/HTTPS proxy configuration is not working. For best proxy results, we recommend using the most recent infrastructure agent. Solution If the proxy configuration for your infrastructure agent isn't working, the problem may be due to proxy precedence. Review the section below for your infrastructure agent version: Version 1.3.1 or higher Infrastructure agent versions 1.3.1 or higher uses the following proxy precedence: Configuration Precedence Windows Linux NRIA_PROXY 1 HTTP/HTTPS HTTP/HTTPS proxy (in newrelic-infra.yml) 2 HTTP/HTTPS HTTP/HTTPS HTTPS_PROXY 3 HTTPS HTTPS HTTP_PROXY 4 HTTP HTTP The following options affect TLS certificate configuration: If you use an HTTPS proxy, you probably need to set one of these: The configuration option proxy_validate_certificates: true The environment variable NRIA_PROXY_VALIDATE_CERTIFICATES=true If you use your own self-signed certificates for the HTTPS proxy, keep the default value proxy_validate_certificates: false Here are some additional factors that affect proxy configuration: The NRIA_PROXY and proxy configuration options enable HTTP or HTTPS depending on the URL scheme. The HTTP_PROXY and HTTPS_PROXY environment variables are ignored if either of the these configuration options is set: NRIA_IGNORE_SYSTEM_PROXY=true ignore_system_proxy: true The certificate validation won't work in Centos 5 systems. Version 1.0.1002 to 1.2.25 For infrastructure agent versions 1.0.1002 to 1.2.25, review the section below based on your configuration: Ignore proxy is false Ignore proxy is true Ignore proxy is false If NRIA_IGNORE_SYSTEM_PROXY=FALSE or ignore_system_proxy=FALSE: Configuration Precedence Windows Linux HTTPS_PROXY 1 HTTP 1 HTTP 1 NRIA_PROXY 2 HTTP 1 HTTP 1 proxy (in newrelic-infra.yml) 3 HTTP 1 HTTP 1 HTTP_PROXY 4 HTTP HTTP 1 HTTPS proxy not supported (redirected to HTTP) Ignore proxy is true If NRIA_IGNORE_SYSTEM_PROXY=TRUE or ignore_system_proxy=TRUE: Configuration Precedence Windows Linux NRIA_PROXY 1 HTTP 1 HTTP 1 proxy (in newrelic-infra.yml) 2 HTTP 1 HTTP 1 1 HTTPS proxy not supported (redirected to HTTP) Version 1.0.956 to 1.0.989 Infrastructure agent versions 1.0.956 to 1.0.989 uses the following proxy precedence: Configuration Precedence Windows Linux HTTPS_PROXY 1 HTTPS HTTP 1 NRIA_PROXY 2 HTTP/HTTPS 2 HTTP 1 proxy (in newrelic-infra.yml) 3 HTTP/HTTPS HTTP 1 HTTP_PROXY 4 HTTP/HTTPS TTP 1 HTTPS proxy not supported (redirected to HTTP) 2 When using a HTTPS proxy with a custom TLS/SSL or self-signed certificate, provide either: The certificate file location in ca_bundle_file The certificates directory path in ca_bundle_dir Version 1.0.944 or lower Infrastructure agent versions 1.0.944 or lower uses the following proxy precedence: Configuration Precedence Windows Linux HTTPS_PROXY 1 HTTP 1 HTTP 1 NRIA_PROXY 2 HTTP 1 HTTP 1 proxy (in newrelic-infra.yml) 3 HTTP 1 HTTP 1 HTTP_PROXY 4 HTTP HTTP 1 HTTPS proxy not supported (redirected to HTTP) 2 When using a HTTPS proxy with a custom TLS/SSL or self-signed certificate, provide either: The certificate file location in ca_bundle_file The certificates directory path in ca_bundle_dir",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.35214,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The <em>infrastructure</em> agent&#x27;s HTTP&#x2F;HTTPS proxy configuration is not working. For best proxy results, we recommend using the most recent <em>infrastructure</em> agent. Solution If the proxy configuration for your <em>infrastructure</em> agent isn&#x27;t working, the problem may be due to proxy precedence. Review"
      },
      "id": "603ebc24196a67c28ba83d83"
    }
  ],
  "/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-logs/generate-logs-troubleshooting-infrastructure": [
    {
      "sections": [
        "The agent is not starting and there are no logs",
        "Problem",
        "Solution",
        "Check requiretty",
        "Important",
        "Review log permissions"
      ],
      "title": "The agent is not starting and there are no logs",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot logs"
      ],
      "external_id": "fefb6cf577c3c825a6908eba8e378de3ceca4cd7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/agent-not-starting-there-are-no-logs/",
      "published_at": "2021-05-05T03:29:41Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The infrastructure agent is not starting, and logs are not created. Solution Here are some troubleshooting options for non-root users if the infrastructure agent is not starting and there are no logs: Check requiretty To see if requiretty is causing logging issues: In /var/log/messages or /var/log/syslog, look for the message sudo: sorry, you must have a tty to run sudo . Important When using old Linux versions, sometimes the nri-agent user fails to execute a service because it does not have any TTY attached. If you find this message, edit your /etc/sudoers file with the visudo command and comment or remove the following line: Defaults requiretty Save and exit the file. Restart the newrelic-infra service. Review log permissions Check the agent's permission to open log_file. It's possible that the log file you are using was created when the agent was running as root, and now the nri-agent user does not have permissions to write it. To solve this, try one of these options: Change the owner of the log file. Change the log_file entry in the /etc/newrelic-infra.yml configuration file. Our installation scripts create the /var/log/newrelic-infra/ folder for that purpose, so we recommend the following value: log_file: /var/log/newrelic-infra/newrelic-infra.log Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.31435,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "The agent is not starting and there are no <em>logs</em>",
        "sections": "The agent is not starting and there are no <em>logs</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The <em>infrastructure</em> agent is not starting, and <em>logs</em> are not created. Solution Here are some <em>troubleshooting</em> options for non-root users if the <em>infrastructure</em> agent is not starting and there are no <em>logs</em>: Check requiretty To see if requiretty is causing logging issues: In &#x2F;var&#x2F;<em>log</em>&#x2F;messages"
      },
      "id": "603eba9e28ccbc5f64eba786"
    },
    {
      "sections": [
        "Infrastructure agent logging behavior",
        "Logging severity levels",
        "Important",
        "Log formatting",
        "Log rotation",
        "Logrotate config file sample",
        "Tip",
        "Smart verbose mode",
        "Logging before Infrastructure agent v1.4.9",
        "Integration log management",
        "Integration STDERR expected format"
      ],
      "title": "Infrastructure agent logging behavior",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot logs"
      ],
      "external_id": "0dc6570e893e47c4d5b5c4232283432926c6476a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-logs/infrastructure-agent-logging-behavior/",
      "published_at": "2021-05-05T03:30:52Z",
      "updated_at": "2021-03-16T07:31:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure agent gathers its own data as well as integrations's logs and consolidates them in a single source. By default, logs appear in standard-output and are added to a log file. To disable logs in standard output, see the agent's config options. Logging severity levels Infrastructure uses a subset of the standard Syslog severity levels: ERROR: Error conditions met WARN: Warning conditions met INFO: Informational messages DEBUG: Contains debug-level messages (useful when troubleshooting) Important DEBUG level is only shown when the verbose mode is enabled. Log formatting For infrastructure agent v1.4.9 or higher, log messages are inlined with context values. This offers better grouping and filtering; for example: containerized agent found in container containerID: VALUE Copy By default, Infrastructure logs are formatted as text: In foreground mode, log output is colored, without a timestamp: DEBUG Sending deltas divided in blocks component=PatchSender mentityKey=ohaimaci mnumberOfBlocks=1 Copy In background mode, logs are timestamped output, used when running as a service or dumping logs to a file: time=\"2019-07-12T09:54:15+02:00\" level=info msg=\"Agent service manager shutdown completed successfully.\" component=AgentService service=newrelic-infra Copy Alternatively, logs can be formatted as a JSON file: {\"context\":{},\"level\":\"info\",\"msg\":\"upstart_interval_sec: 0\",\"timestamp\":\"2019-07-11T18:24:03+02:00\"} {\"context\":{},\"level\":\"info\",\"msg\":\"plugin_dir: \",\"timestamp\":\"2019-07-11T18:24:03+02:00\"} Copy To change the log format, see the agent configuration settings. Log rotation The infrastructure agent does not provide any native log rotation or compression mechanism. Instead, we encourage you to use consolidated log rotation tools, such as the Linux logrotate tool, which is usually installed by default in most Linux distributions. Logrotate can be configured as an entry in /etc/logrotate.conf, or as a file in the /etc/logrotate.d directory. Logrotate config file sample A sample logrotate config file looks like this: /var/log/newrelic-infra/newrelic-infra.log { copytruncate compress daily dateext maxage 7 } Copy Where: /var/log/newrelic-infra/newrelic-infra.log: The Infrastructure agent log file. It must match the log_file configuration parameter in the /etc/newrelic-infra.yml file. copytruncate: Indicates that the log file is truncated but not deleted when it is rotated. This configuration option is mandatory, otherwise the log file will be deleted and won’t be recreated. compress: Compresses (usually in Gzip format) the rotated log files. daily: The agent rotates logs daily. dateext: Appends a date (by default, in the format YYYYMMDD) to the rotated log file (e.g. newrelic-infra.log-20190708.gz) maxage 7: Makes logrotate remove rotated files after 7 days. Tip For a complete description of the logrotate configuration options, see the Linux Logrotate documentation. Since logrotate is usually executed automatically as a cron job, verify that there is a logrotate entry in cron (for example, /etc/cron.daily/logrotate) similar to: #!/bin/sh /usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \"ALERT exited abnormally with [$EXITVALUE]\" fi exit 0 Copy Smart verbose mode For infrastructure agent versions 1.9.0 or higher, you can enable smart verbose mode for logs. Smart verbose mode prevents debug messages being logged until an error message is logged. Once an error has been logged, the cached debug messages are logged, but only the most recent number of configured debug messages. For example, if you have a configured limit of 10, after an error is logged, only the 10 most recent debug messages are logged, and older logs are discarded. For more information on how to enable smart verbose mode and the debug message limit, see Infrastructure configuration settings. Logging before Infrastructure agent v1.4.9 Here is a comparison of functionality for Infrastructure agent versions before and after v1.4.9: Agent v1.4.9 and higher Before v1.4.9 Foreground mode logged. The agent couldn't log some entries in foreground mode because the logging service wasn't able to write data until the agent was completely configured. Logs in text and JSON formats. Logs in text only. Logs displayed as inline text. Logs displayed as static literals in a single, decontextualized line. Integration log management Integrations write JSON payloads into STDOUT and plain-text (JSON structured in the future) logs into STDERR. The infrastructure agent handles integration STDERR lines and forward this output into the agent one, usually the service log. Agent handles each STDERR line as follows: when agent runs in verbose mode: it just forwards the full STDERR line as a DEBUG agent log entry placing integration line contexts within the ` msg ` field. otherwise: it parses the line against the expected format (see below) and only logs as agent ERROR level, entries produced by integrations with ` fatal ` or ` error ` severity levels. In this case fields are extracted and forwarded in structured manner (therefore if JSON output is enabled for the agent fields become queryable. Integration STDERR expected format A line is expected to be a list of key-value pairs separated by an equal character. Keys can contain any character, whereas values can have three different formats: string: < quote>any character including escaped quotes \\ \" < quote> map: & { any character} word: any character except spaces Internally agent used this regex to extract the fields: ([^\\s]*?)=(\".*?[^\\\\]\"|&{.*?}|[^\\s]*) Copy For instance, this line: time=\"2015-03-26T01:27:38-04:00\" level=error msg=\"Foo bar baz\" foo=bar Copy Will generate a structured agent log line with these fields: - \"time\": \"2015-03-26T01:27:38-04:00\" - \"level\": \"error\" - \"msg\": \"Foo bar baz\" - \"foo\": \"bar\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.29306,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> agent <em>logging</em> behavior",
        "sections": "<em>Infrastructure</em> agent <em>logging</em> behavior",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "New Relic&#x27;s <em>infrastructure</em> agent gathers its own data as well as integrations&#x27;s <em>logs</em> and consolidates them in a single source. By default, <em>logs</em> appear in standard-output and are added to a <em>log</em> file. To disable <em>logs</em> in standard output, see the agent&#x27;s config options. Logging severity levels"
      },
      "id": "603eb3a228ccbc6badeba7a5"
    },
    {
      "sections": [
        "Reduce the infrastructure agent's CPU footprint",
        "Problem",
        "Solution",
        "Reduce event sampling",
        "Important",
        "Reduce agent plugin reporting",
        "How to enable and disable plugins",
        "Disable SELinux semodule -l (Linux only)",
        "Reduce or disable Sysctl (Linux only)",
        "Additional plugins to reduce or disable",
        "Review on-host integrations"
      ],
      "title": "Reduce the infrastructure agent's CPU footprint",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "4eea817bfabb6b698ea3ce001b8c5eeca20d475e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint/",
      "published_at": "2021-05-05T03:40:45Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The New Relic infrastructure agent is consuming too much CPU. Solution The New Relic infrastructure agent is designed to report a broad range of system data with minimal CPU and memory consumption. However, if you have a need to reduce your CPU consumption, you can disable or decrease the sampling frequency of various samplers and plugins. This topic highlights some newrelic-infra.yml configurations that may reduce your CPU usage: Reduce event sampling Reduce agent plugin reporting Review on-host integrations Reduce event sampling The infrastructure agent reports several default events at specific frequencies. To lower the overhead, you can reduce the sampling frequency in seconds, or you can completely disable the samplers by setting the corresponding property value to -1. Important We don't recommend a sample rate larger than 60 seconds because you may see gaps in the New Relic user interface charts. The table below lists some samplers to configure: Event Sampling frequency Allow/deny list Network Network sampling rate Not available Process Process sampling rate Allow list (Windows only) Storage Storage sampling rate Deny list System System sampling rate Not available Reduce agent plugin reporting The infrastructure agent has built-in plugins that collect inventory data (specific system configuration and state information). For some systems, the CPU consumption may be relatively high if the plugins are gathering a lot of data. To reduce the footprint, you can disable or decrease the sampling frequency for specific plugins that report data you don’t want. How to enable and disable plugins Disable a single plugin: To disable a plugin, set the corresponding property value to -1. Disable all plugins: disable_all_plugins: true Enable selected plugins: To enable certain plugins, insert an exception in disable_all_plugins. For example, the following configuration disables all plugins, but the Network Interfaces plugin reports every 120 seconds: disable_all_plugins: true network_interface_interval_sec: 120 Copy Disable SELinux semodule -l (Linux only) The SELinux plugin periodically invokes the semodule -l system command to get information about the existing SELinux modules. In most CentOS/RedHat distributions, this command will generate CPU consumption peaks. To disable this functionality, insert the following configuration option in your /etc/newrelic-infra.yml file: selinux_enable_semodule: false Reduce or disable Sysctl (Linux only) The Sysctl plugin walks the whole /sys directory structure and reads values from all the files there. Disabling it or reducing the interval may decrease some CPU System time in the Infrastructure agent. You can disable inventory frequency by setting it to a negative number or reduce the frequeny by setting the sysctl_interval_sec configuration value to the number of seconds between consecutive executions of the plugin. For example, to execute the plugin once every 10 minutes: sysctl_interval_sec: 600 Copy To disable the Sysctl plugin: sysctl_interval_sec: -1 Copy The current default value for the sysctl_interval_sec property is 60. Additional plugins to reduce or disable The following inventory plugins are not especially CPU consuming, but you can still reduce their frequency or disable them by setting the corresponding configuration options. Linux plugins For configuration of these Linux plugins, see Plugin variables: Cloud Security Groups Daemon Tools DPKG Facter Kernel Modules Network interfaces RPM SELinux Supervisord Sysctl Systemd SysV Upstart Users SSHD configuration Windows plugins For configuration of these Windows plugins, see Plugin variables: Network interfaces Windows services Windows updates Review on-host integrations If you use infrastructure on-host integrations, this may have additional impacts on CPU usage. The nature of the impact and the methods to adjust the impact depend on the integration you're using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the monitoring load by adding additional infrastructure agents. For example, the Kafka integration allows a multi-agent deployment.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 133.18265,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "sections": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " on the integration you&#x27;re using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the <em>monitoring</em> load by adding additional <em>infrastructure</em> agents. For example, the Kafka integration allows a multi-agent deployment."
      },
      "id": "603eb9dc64441fbf1f4e8847"
    }
  ],
  "/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-logs/infrastructure-agent-logging-behavior": [
    {
      "sections": [
        "The agent is not starting and there are no logs",
        "Problem",
        "Solution",
        "Check requiretty",
        "Important",
        "Review log permissions"
      ],
      "title": "The agent is not starting and there are no logs",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot logs"
      ],
      "external_id": "fefb6cf577c3c825a6908eba8e378de3ceca4cd7",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/agent-not-starting-there-are-no-logs/",
      "published_at": "2021-05-05T03:29:41Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The infrastructure agent is not starting, and logs are not created. Solution Here are some troubleshooting options for non-root users if the infrastructure agent is not starting and there are no logs: Check requiretty To see if requiretty is causing logging issues: In /var/log/messages or /var/log/syslog, look for the message sudo: sorry, you must have a tty to run sudo . Important When using old Linux versions, sometimes the nri-agent user fails to execute a service because it does not have any TTY attached. If you find this message, edit your /etc/sudoers file with the visudo command and comment or remove the following line: Defaults requiretty Save and exit the file. Restart the newrelic-infra service. Review log permissions Check the agent's permission to open log_file. It's possible that the log file you are using was created when the agent was running as root, and now the nri-agent user does not have permissions to write it. To solve this, try one of these options: Change the owner of the log file. Change the log_file entry in the /etc/newrelic-infra.yml configuration file. Our installation scripts create the /var/log/newrelic-infra/ folder for that purpose, so we recommend the following value: log_file: /var/log/newrelic-infra/newrelic-infra.log Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.31435,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "The agent is not starting and there are no <em>logs</em>",
        "sections": "The agent is not starting and there are no <em>logs</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The <em>infrastructure</em> agent is not starting, and <em>logs</em> are not created. Solution Here are some <em>troubleshooting</em> options for non-root users if the <em>infrastructure</em> agent is not starting and there are no <em>logs</em>: Check requiretty To see if requiretty is causing logging issues: In &#x2F;var&#x2F;<em>log</em>&#x2F;messages"
      },
      "id": "603eba9e28ccbc5f64eba786"
    },
    {
      "sections": [
        "Generate logs for troubleshooting the infrastructure agent",
        "Problem",
        "Important",
        "Solution",
        "Smart verbose mode",
        "Forward the agent logs to New Relic Logs",
        "Notes for specific systems",
        "Containerized agent on CoreOS"
      ],
      "title": "Generate logs for troubleshooting the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot logs"
      ],
      "external_id": "a0c2ca22e3fca2b3add8c94d211adffce686661c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-logs/generate-logs-troubleshooting-infrastructure/",
      "published_at": "2021-05-05T03:31:48Z",
      "updated_at": "2021-03-16T06:35:54Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When troubleshooting your infrastructure agent, generate verbose logs for a few minutes to find and investigate errors. This can be useful for your own troubleshooting or when working with New Relic Support. Important Verbose logging generates a lot of data very quickly. When finished generating logs, be sure to set verbose: 0 to reduce disk space consumption. If you have New Relic infrastructure agent 1.4.0 or higher, you can automate this process by using the newrelic-infra-ctl command. For more information, see the troubleshooting binary documentation. Solution Generating verbose log files requires editing your configuration file. For a sample config file that includes all applicable settings, see the example template. To generate detailed logs: Step Procedures Edit your newrelic-infra.yml file: Enable verbose logging: verbose: 1. (If you use a containerized infrastructure agent on CoreOS, see system-specific notes.) Set log_file to a convenient log file location. Restart the agent so the agent notices the new settings. Let your host run at normal load for about three minutes to generate sufficient logging data. Return your settings to default: Disable verbose logging by setting verbose: 0 in newrelic-infra.yml. Optional: Disable logging to a custom file by removing the log_file line from newrelic-infra.yml. Restart the agent so the agent notices the new settings. Examine the log file for errors. If you need to send your log file to New Relic Support: Include the line in your log file that contains the agent version: New Relic infrastructure agent version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your newrelic-infra.yml. Smart verbose mode Sometimes errors don't occur until after quite some time has passed. This makes debugging difficult, because typically verbose logs are only enabled for a short period time; otherwise there will be many debug logs. For example, if an error occurs one hour after the infrastructure agent has started, getting debug logs around the time of the error can be tricky or impractical. As of infrastructure agent v1.9.0 or higher, you can use smart verbose mode for logs. Smart verbose mode only logs the most recent debug messages after an error has been logged. This allows you to leave smart verbose mode running until an error occurs, without logging lots of irrelevant debug messages, and only logging the most recent debug messages. (The number of messages is determined by your configuration.) For more information on smart verbose mode, see the Infrastructure agent logging behavior docs, and use the Infrastructure configuration settings documentation for details on how to enable smart verbose mode. Forward the agent logs to New Relic Logs The Infrastructure agent can be configured to send its own logs to New Relic Logs. This can be useful for troubleshooting issues with log forwarding, the Infrastructure agent, or when contacting support. For details on how to enable log forwarding for the Infrastructure agent, see Troubleshoot log forwarding. Notes for specific systems These are some additional notes and requirements for specific systems, used to supplement the general logging instructions: Containerized agent on CoreOS If you are using a containerized infrastructure agent on CoreOS: Choose one of these options to change the log level to verbose: Recommended: Set the environment variable NRIA_VERBOSE to 1. Running this on the command line would look like: -e NRIA_VERBOSE=1 Copy OR Edit the config file to set verbose: 1. (Editing the config file in a container is not recommended, because it requires rebuilding the image twice: once to add verbose logging and once to remove it.) Use journalctl to collect the logs: journalctl -u newrelic-infra > newrelic-infra.log Copy Set the verbose logging level back to 0 after collecting logs for a few minutes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.27434,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Generate <em>logs</em> for <em>troubleshooting</em> the <em>infrastructure</em> agent",
        "sections": "Generate <em>logs</em> for <em>troubleshooting</em> the <em>infrastructure</em> agent",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " verbose mode. Forward the agent <em>logs</em> to New Relic <em>Logs</em> The <em>Infrastructure</em> agent can be configured to send its own <em>logs</em> to New Relic <em>Logs</em>. This can be useful for <em>troubleshooting</em> issues with <em>log</em> forwarding, the <em>Infrastructure</em> agent, or when contacting support. For details on how to enable <em>log</em> forwarding"
      },
      "id": "603e910028ccbc6304eba76d"
    },
    {
      "sections": [
        "Reduce the infrastructure agent's CPU footprint",
        "Problem",
        "Solution",
        "Reduce event sampling",
        "Important",
        "Reduce agent plugin reporting",
        "How to enable and disable plugins",
        "Disable SELinux semodule -l (Linux only)",
        "Reduce or disable Sysctl (Linux only)",
        "Additional plugins to reduce or disable",
        "Review on-host integrations"
      ],
      "title": "Reduce the infrastructure agent's CPU footprint",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "4eea817bfabb6b698ea3ce001b8c5eeca20d475e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint/",
      "published_at": "2021-05-05T03:40:45Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The New Relic infrastructure agent is consuming too much CPU. Solution The New Relic infrastructure agent is designed to report a broad range of system data with minimal CPU and memory consumption. However, if you have a need to reduce your CPU consumption, you can disable or decrease the sampling frequency of various samplers and plugins. This topic highlights some newrelic-infra.yml configurations that may reduce your CPU usage: Reduce event sampling Reduce agent plugin reporting Review on-host integrations Reduce event sampling The infrastructure agent reports several default events at specific frequencies. To lower the overhead, you can reduce the sampling frequency in seconds, or you can completely disable the samplers by setting the corresponding property value to -1. Important We don't recommend a sample rate larger than 60 seconds because you may see gaps in the New Relic user interface charts. The table below lists some samplers to configure: Event Sampling frequency Allow/deny list Network Network sampling rate Not available Process Process sampling rate Allow list (Windows only) Storage Storage sampling rate Deny list System System sampling rate Not available Reduce agent plugin reporting The infrastructure agent has built-in plugins that collect inventory data (specific system configuration and state information). For some systems, the CPU consumption may be relatively high if the plugins are gathering a lot of data. To reduce the footprint, you can disable or decrease the sampling frequency for specific plugins that report data you don’t want. How to enable and disable plugins Disable a single plugin: To disable a plugin, set the corresponding property value to -1. Disable all plugins: disable_all_plugins: true Enable selected plugins: To enable certain plugins, insert an exception in disable_all_plugins. For example, the following configuration disables all plugins, but the Network Interfaces plugin reports every 120 seconds: disable_all_plugins: true network_interface_interval_sec: 120 Copy Disable SELinux semodule -l (Linux only) The SELinux plugin periodically invokes the semodule -l system command to get information about the existing SELinux modules. In most CentOS/RedHat distributions, this command will generate CPU consumption peaks. To disable this functionality, insert the following configuration option in your /etc/newrelic-infra.yml file: selinux_enable_semodule: false Reduce or disable Sysctl (Linux only) The Sysctl plugin walks the whole /sys directory structure and reads values from all the files there. Disabling it or reducing the interval may decrease some CPU System time in the Infrastructure agent. You can disable inventory frequency by setting it to a negative number or reduce the frequeny by setting the sysctl_interval_sec configuration value to the number of seconds between consecutive executions of the plugin. For example, to execute the plugin once every 10 minutes: sysctl_interval_sec: 600 Copy To disable the Sysctl plugin: sysctl_interval_sec: -1 Copy The current default value for the sysctl_interval_sec property is 60. Additional plugins to reduce or disable The following inventory plugins are not especially CPU consuming, but you can still reduce their frequency or disable them by setting the corresponding configuration options. Linux plugins For configuration of these Linux plugins, see Plugin variables: Cloud Security Groups Daemon Tools DPKG Facter Kernel Modules Network interfaces RPM SELinux Supervisord Sysctl Systemd SysV Upstart Users SSHD configuration Windows plugins For configuration of these Windows plugins, see Plugin variables: Network interfaces Windows services Windows updates Review on-host integrations If you use infrastructure on-host integrations, this may have additional impacts on CPU usage. The nature of the impact and the methods to adjust the impact depend on the integration you're using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the monitoring load by adding additional infrastructure agents. For example, the Kafka integration allows a multi-agent deployment.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 133.18265,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "sections": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " on the integration you&#x27;re using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the <em>monitoring</em> load by adding additional <em>infrastructure</em> agents. For example, the Kafka integration allows a multi-agent deployment."
      },
      "id": "603eb9dc64441fbf1f4e8847"
    }
  ],
  "/docs/infrastructure/infrastructure-ui-pages/infra-ui-pages/infrastructure-inventory-page-search-your-entire-infrastructure": [
    {
      "sections": [
        "Events heatmap: Examine patterns in time range"
      ],
      "title": "Events heatmap: Examine patterns in time range",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "bc50e789884c9c4eea404d558d4070519a3eab0c",
      "image": "https://docs.newrelic.com/static/96c3e087c9dfb8b4cb4ad72b79c47e94/c1b63/infra-events-timeline.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/events-heatmap-examine-patterns-time-range/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "he events heatmap provides a snapshot of the infrastructure events occurring within the same time range as the displayed metrics. The darker the color on the heatmap, the more events occurred during that time period. By comparing the heatmap to the charts on the infrastructure page, you can quickly pinpoint issues in your ecosystem. For example, if a massive CPU spike occurs, you can click on the events heatmap for that time range to find the event that caused it. From there you can dive deeper to uncover the real issue. one.newrelic.com > Infrastructure: The heatmap on Infrastructure monitoring UI pages visually shows patterns with events occurring at the same time period for the displayed metrics. The heatmap appears on several Infrastructure UI pages, including: System Network Processes Storage Events",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.15839,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": " <em>monitoring</em> <em>UI</em> pages visually shows patterns with events occurring at the same time period for the displayed metrics. The heatmap appears on several <em>Infrastructure</em> <em>UI</em> pages, including: System Network Processes Storage Events"
      },
      "id": "603e8455196a67833da83dc2"
    },
    {
      "sections": [
        "Infrastructure Events page: Live feed of config changes",
        "Event types",
        "Events page features",
        "Chart data attributes"
      ],
      "title": "Infrastructure Events page: Live feed of config changes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "e4a87670c8671072ae7cc6531721f46edc7f925d",
      "image": "https://docs.newrelic.com/static/75373d03d819516d3cbe23f1ea65957b/c1b63/infra-events-ui.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/infrastructure-events-page-live-feed-every-config-change/",
      "published_at": "2021-05-05T03:28:38Z",
      "updated_at": "2021-03-11T11:47:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Events page is a live feed of important system and host activity, including inventory change events, configuration changes, and log analytics events. The event feed helps you understand correlations between these events and system performance. Search and filter through your events to decrease the mean time to detect and repair infrastructure issues. You can access the Events page by going to one.newrelic.com > Infrastructure > Events. Event types New Relic collects a variety of change events so you can understand each change in your environment: Events Comments Alert incidents When a violation is opened or closed, New Relic generates an event indicating the host and associated alert condition. Agent connection When an infrastructure agent connects to New Relic, our platform generates an Agent connected event. If New Relic doesn't receive data from an agent for three minutes, the platform generates an Agent disconnected event. Inventory changes These events are generated when inventory data is added, removed, or modified. Select the source icon to to understand which category corresponds to the altered inventory path. For additional details, select an inventory event to see a side-by-side comparison of the old and new state. Inventory events can include: Kernel (includes modules and configuration): Can be added, modified, or deleted. Metadata (includes various additional information about hosts): Can be added, modified, or deleted. Packages: Can be installed, removed, or modified. Services: Can be started, stopped, or restarted. Sessions (includes users): Can be connected or disconnected. Events page features To view the live event feed: Go to one.newrelic.com > Infrastructure > E * *vents. The Events * * page includes a heatmap, which provides a snapshot of the events occurring within the selected time range. one.newrelic.com > Infrastructure > Events: Use the Events to view important, real-time activity in your infrastructure. With the Events page, you can easily search through your event log to quickly find vulnerable packages. If you want to... Do this... Focus on specific events Use the Search events field to look for specific events, config changes or agent installations. To focus on a specific set of events, select or change the filter set. Search within a particular time range Enter a time range to the right of the search bar to investigate events within a specific time range. For example, if you encountered a CPU spike around 11am on the previous day, search Yesterday at 11 am to investigate the possible cause. Compare events with host load, memory, CPU, and more View the events feed on the Hosts page. To compare infrastructure events and performance for a specific time, select a range via the time picker or drag and select a range on a chart. View events specifically related to agents, config, metadata, services, or sessions Group or sort events by selecting the filter icon be the search bar. Drill down into additional details Select an event to view additional details, such as attributes and values. To drill down further, select View in Inventory to see additional details in the Inventory page. View host's alert threshold violation Select the host's Critical icon or Warning icon. Chart data attributes For a technical explanation of the attributes used to populate the Events page, see InfrastructureEvent attributes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.99223,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> Events page: Live feed of config changes",
        "sections": "<em>Infrastructure</em> Events page: Live feed of config changes",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": " the mean time to detect and repair <em>infrastructure</em> issues. You can access the Events page by going to one.newrelic.com &gt; <em>Infrastructure</em> &gt; Events. Event types New Relic collects a variety of change events so you can understand each change in your environment: Events Comments Alert incidents When"
      },
      "id": "6043fa6c28ccbc13742c60a5"
    },
    {
      "sections": [
        "Infrastructure monitoring Hosts page",
        "System tab",
        "System tab functions",
        "APM and infrastructure data",
        "Important",
        "Network tab",
        "Network tab functions",
        "Processes tab",
        "Tip",
        "Processes tab functions",
        "Storage tab",
        "Storage tab functions",
        "Docker containers tab",
        "Docker containers tab functions"
      ],
      "title": "Infrastructure monitoring Hosts page",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "41d8a2ac3ecbbdee164fd0bec6ac94bb3e8def64",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-ui-pages/infrastructure-ui/infrastructure-hosts-page/",
      "published_at": "2021-05-05T00:33:11Z",
      "updated_at": "2021-03-09T04:04:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Hosts page to better understand important performance metrics, like resource usage, network and processes performance, and data about your containers. You can filter your hosts by attributes and compare their performance with inventory change events to help determine root causes. You can also set alert notifications to help prevent problems. To view your hosts information, go to one.newrelic.com > Infrastructure > Hosts, then click any of the following tabs: System: Overview of your hosts' performance Network: Bandwidth and error data about your network interfaces Processes: Data about CPU percentage, I/O bytes, and memory usage for individual or groups of processes Storage: Resources' capacity and efficiency, including your devices' overall utilization, disk usage, or I/O operations Docker containers: CPU percentage, I/O bytes, and memory usage for individual or group of containers System tab Here are the default graphs shown on the Hosts page. For additional chart options, select the dropdown for any chart. CPU %: On the System tab, CPU Percent is a derived metric that is part of the SystemSample event. The CPU percentage is not collected by New Relic, but derived from several other metrics. Specifically, the cpuPercent attribute is an aggregation of cpuUserPercent, cpuSystemPercent, cpuIoWaitPercent and cpuStealPercent. Load average five minute: represents an average of the number of tasks waiting to do work on a system over the last 5 minutes. Memory free %: compares the amount of free memory bytes to the amount of used memory bytes. For explanations of different chart metrics, see Default infrastructure attributes and events. Functions for adjusting chart data include: Select different host groups: Use the host filter. Change time range: Use the time picker above the charts, or drag and select a time range on a chart. Change the number of elements on charts: use the Chart shows dropdown. Change data used to order hosts: Use the table below the charts to order the chart elements by different metrics; for example, from highest to lowest CPU user % to lowest, or from lowest to highest Disk free. System tab functions Here are some of the things you can do from the System tab: If you want to... Do this... Filter and group hosts Use filter sets to show only hosts matching certain criteria, or use group by to aggregate the results. Understand host health Use the Health column of the table. To see more details about specific violations, select the health status icons. Find root causes of issues Use the Events heatmap at the top of the page to compare performance to important events happening in your infrastructure. For more, see Events heatmap. Set an alert condition for a metric Mouse over a chart, select and then Create alert. View host's alert threshold violation If present, select the host's Critical icon or Warning icon. APM and infrastructure data If you have APM data associated with your infrastructure data, there are several ways to access your APM data on the Hosts page: Use the hosts filter to filter on hosts running specific applications. In the host list below the charts, select the Application column to filter on specific applications. From the chart selector dropdown beside a chart's name, select one of the application-related charts. Important APM charts in infrastructure monitoring do not have View query or Create alert options like the other infrastructure charts do. For more about using APM and infrastructure monitoring together, see APM data in infrastructure. Network tab The Network page provides real-time visibility into the health and performance of individual hosts, web servers, or other groups of resources across your network. Default charts show bandwidth metrics by packet, bandwidth by bytes, and errors per second. Details about individual interfaces can help you: Examine saturation levels across your network or for selected filter sets. Compare load balances between specific resources. Identify unexpected differences in capacity patterns between similar hosts. Evaluate the top five network errors that New Relic automatically presents for the selected time period. This real-time network data can then help you determine whether to resolve errors by reconfiguring your network, rebalancing the loads, or taking other preventative maintenance actions before needing to make a purchase decision. From the Network tab you can view bandwidth and error data about your network interfaces. The Network page includes an Events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. For a technical explanation of the attributes used to populate the Network page chart, see NetworkSample attributes. Network tab functions Here are some of the things you can do from the Network tab: If you want to... Do this... Filter and group Use filter sets to show only hosts matching certain criteria, or use Group by to aggregate the results. Select a time range Use the time picker to change the range of time selected. You can also click and drag on the chart to select a custom time range. When you select a time range, it carries over when you go from one infrastructure page to another. Change metrics displayed in chart Use the sorting dropdown to switch what metric the chart is displaying for the chosen process groupings, including: Received Bytes Received Dropped Received Errors Received Packets Transmitted Bytes Transmitted Dropped Transmitted Errors Transmitted Packets Search and filter the list Type in the Search interfaces search bar to filter the list to only those items containing the text you've typed. You can also filter the list by simply selecting the list item or user name you want to filter for. The chosen filters are displayed beside the filter icon above the chart. Add items to chart The chart, by default, displays the top five results. Use the Chart shows dropdown to display more results. To remove an item from a chart, select its name below the chart. Set an alert condition for a metric Mouse over a chart, select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Processes tab Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Use the Processes tab to get information about processes running on your hosts, and to set alerts on process metrics. The Processes tab shows data such as CPU percentage, I/O bytes, and memory usage for individual processes or groupings of processes. The page also includes an events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. Use this information to: Pinpoint processes that are causing performance issues. Create alerts for conditions such as CPU percentage and memory usage. On the Processes page, CPU percent is scoped to individual processes, rather than hosts. Because of this, the CPU percent metric does not take into account the resources of the entire system. Instead, it shows how much of a single CPU core each process is taking. Example Here's an example of how to pinpoint an issue and set an alert: You notice on the Hosts page that a cluster has a CPU percentage much higher than normal. You go to the Processes page and filter down to that cluster's processes. You notice that several instances of the same process have excessive CPU percentage. After fixing the issue, you decide to create an alert condition that triggers when the CPU percentage for that process type exceeds a certain threshold. For a technical explanation of the attributes used to populate the Processes page chart, see ProcessSample attributes. Tip You cannot start, stop, or restart processes from the UI. Instead, follow standard procedures to start, stop, check, or restart the infrastructure agent manually. Processes tab functions Here are some of the things you can do from the Processes tab: If you want to... Do this... Only show hosts matching certain criteria Use filter sets. Aggregate results or group by host attributes Use Group by to change how the processes are grouped in the chart and the list. The dropdown contains host-related attributes such as hostID and entityName. If you are monitoring AWS EC2 instances, EC2-related attributes such as awsRegion will be available. Select a time range Use either of these options: Select any of the time picker options at the top of the page. Click and drag on the chart to select a custom time range. After you select a time range, it carries over from one infrastructure page to another. View Docker metrics To view process usage by Docker containers, see Docker instrumentation. Change charts Select the dropdown beside the chart's name to switch what metric the chart displays. Choices include: Memory: Resident or virtual size CPU percentage Disk I/O: Read or write Search and filter the list Use either of these options: Type in the Search processes search bar. Select the list item or user name you want to filter for. The selected filters appear beside the filter icon above the chart, where you can select and remove them as needed. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Storage tab The Storage page allows you to monitor your resources' capacity and efficiency, including your devices' overall utilization, disk usage, or I/O operations. This can help you to: Examine unexpected patterns; for example, a cluster of mirrored machines that do not process I/O tasks uniformly. Monitor usage levels before disk space completely runs out. Set alert conditions to notify you about problems with one or more hosts; for example, long processing times for read or write operations, disk availability or utilization based on percentage or bytes, etc. Make informed decisions about where to redistribute hosts with higher or lower than normal processing requests. Use data to help plan for additions or changes to your infrastructure budget before an emergency arises. The Storage page includes an Events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. For a technical explanation of the attributes used to populate the Storage page chart, see StorageSample attributes. Storage tab functions Here are some of the things you can do from the Storage tab: If you want to... Do this... Filter and group Use filter sets to show only hosts matching certain criteria, or use Group by to aggregate the results. Select a time range Use the time picker on the upper right to change the range of time selected. You can also click and drag on the chart to select a custom time range. When you select a time range, it carries over when you go from one infrastructure page to another. Change metrics displayed in chart Use the sorting dropdown to switch what metric the chart is displaying for the chosen process groupings. Choices include: Total Utilization % Read Utilization % Write Utilization % Disk Used Bytes Disk Free Bytes I/O Read Operations I/O Write Operations Search and filter the list Type in the Search devices search bar to filter the list. You can also filter the list by simply selecting the list item or user name you want to filter for. The chosen filters are displayed beside the filter icon above the chart. Add/remove chart items The chart, by default, displays the top five results. Use the Chart shows dropdown to display more results. To remove an item, select its name below the chart. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Docker containers tab Use the Docker containers tab to get information about the containers running on your hosts, and to set alerts on container metrics. The Docker containers tab shows data such as CPU percentage, I/O bytes, and memory usage for individual containers or groupings of containers. The page also includes an events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. Use this information to: Identify containers that are experiencing performance issues. Create alerts for conditions such as CPU percentage and memory usage. Docker containers tab functions Here are some of the things you can do from the Docker containers tab: If you want to... Do this... Only show hosts matching certain criteria Use filter sets. Aggregate results or group by host attributes Use Group by to change how the processes are grouped in the chart and the list. The dropdown contains host-related attributes such as image and operatingSystem. Select a time range Use either of these options: Select any of the time picker options at the top of the page. Click and drag on the chart to select a custom time range. After you select a time range, it carries over from one infrastructure page to another. View Docker integration dashboard To open the Docker integration dashboard, click the Dashboard link above the data table. Change charts Select the dropdown beside the chart's name to switch what metric the chart displays. Choices include: CPU: Used cores, kernel percentage Memory: Size limit, cache bytes I/O: Write count per second, total bytes Network: Errors per second, packets Process: Process count, restart count Search and filter the list Use either of these options: Type in the Search processes search bar. Select the list item or user name you want to filter for. The selected filters appear beside the filter icon above the chart, where you can select and remove them as needed. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. View host's alert threshold violation Select the host's Critical icon or Warning icon.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.0694,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>monitoring</em> Hosts page",
        "sections": "<em>Infrastructure</em> <em>monitoring</em> Hosts page",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": "-related charts. Important APM charts in <em>infrastructure</em> <em>monitoring</em> do not have View query or Create alert options like the other <em>infrastructure</em> charts do. For more about using APM and <em>infrastructure</em> <em>monitoring</em> together, see APM data in <em>infrastructure</em>. Network tab The Network page provides real-time"
      },
      "id": "60440a6d196a675f6c960f58"
    }
  ],
  "/docs/infrastructure/infrastructure-ui-pages/infrastructure-ui/infrastructure-hosts-page": [
    {
      "sections": [
        "Events heatmap: Examine patterns in time range"
      ],
      "title": "Events heatmap: Examine patterns in time range",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "bc50e789884c9c4eea404d558d4070519a3eab0c",
      "image": "https://docs.newrelic.com/static/96c3e087c9dfb8b4cb4ad72b79c47e94/c1b63/infra-events-timeline.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/events-heatmap-examine-patterns-time-range/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "he events heatmap provides a snapshot of the infrastructure events occurring within the same time range as the displayed metrics. The darker the color on the heatmap, the more events occurred during that time period. By comparing the heatmap to the charts on the infrastructure page, you can quickly pinpoint issues in your ecosystem. For example, if a massive CPU spike occurs, you can click on the events heatmap for that time range to find the event that caused it. From there you can dive deeper to uncover the real issue. one.newrelic.com > Infrastructure: The heatmap on Infrastructure monitoring UI pages visually shows patterns with events occurring at the same time period for the displayed metrics. The heatmap appears on several Infrastructure UI pages, including: System Network Processes Storage Events",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.15839,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": " <em>monitoring</em> <em>UI</em> pages visually shows patterns with events occurring at the same time period for the displayed metrics. The heatmap appears on several <em>Infrastructure</em> <em>UI</em> pages, including: System Network Processes Storage Events"
      },
      "id": "603e8455196a67833da83dc2"
    },
    {
      "sections": [
        "Infrastructure Inventory page: Search your entire infrastructure",
        "Inventory item naming",
        "Tip",
        "Page functions",
        "Filter the data",
        "Search inventory",
        "View inventory item details",
        "View host's alert threshold violations",
        "Inventory data collection",
        "Linux built-in agent data",
        "Windows built-in agent data",
        "Amazon AWS cloud integrations inventory",
        "Inventory data retention",
        "Chart data attributes"
      ],
      "title": "Infrastructure Inventory page: Search your entire infrastructure",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "64aef10b24b74ac3c0f070358d37f3cab099e5b2",
      "image": "https://docs.newrelic.com/static/2d17c192725956ff09b5e987be5b997b/747d8/inventory-name-source-path.jpg",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-ui-pages/infra-ui-pages/infrastructure-inventory-page-search-your-entire-infrastructure/",
      "published_at": "2021-05-05T03:40:44Z",
      "updated_at": "2021-03-11T12:47:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic can collect detailed information about a system's configuration per host, including system modules, configuration files, metadata, packages, services, user sessions, etc. The Inventory page provides a real-time, filterable, searchable view into each host's configuration. Use the Inventory page to: Ensure a version update was applied successfully across all your hosts. Audit version discrepancies across your hosts. Quickly identify which hosts require an update to fix a security vulnerability. To view and search your inventory data: Go to one.newrelic.com > Infrastructure > Inventory. Inventory item naming The infrastructure inventory is a qualified namespace (structured like a directory) that organizes inventory items into names that resemble a source path. The inventory item name is comprised of three elements: Element Description Category Basic, top level type of data source, typically based on its role in the system. Common examples include config, package, kernel, user session, services, and modules. Source The specific data source for the inventory item. Label The name of the specific inventory item; for example, the filename, package name, or system setting name. Tip For detailed metadata and other information about your hosts, use tagging with New Relic One. Page functions Use Inventory page functions to find information about a particular item on your hosts: Filter the data Use Filter Sets to show only hosts matching certain criteria. Search inventory Search for an inventory item using the search function. For example, if you want to find information related to OpenSSL, search openssl. The search term is matched again the inventory item name. View inventory item details Inventory item details provide host and system information for each host it resides on according to the New Relic inventory item name. If you have different versions of the same item on other hosts, New Relic detects that and flags them on the Inventory page with the variant hosts label and lists each host running each version. Item details are attributes (key/value pairs) that are dictated by their source. Specific attributes are generally stable over time, but new ones may be added and others could be deprecated. Attributes carry the critical metadata that are at the heart of each inventory item. Common inventory item attributes include: Variant hosts (hostname) Architecture Description Essential Priority Status Version View host's alert threshold violations To view one or more host's alert threshold violations, select the host's Critical icon or Warning icon. Inventory data collection Inventory is collected from the infrastructure agent's built-in data collectors, Amazon Elastic Compute Cloud (EC2) integrations, agent integrations provided by New Relic, and customer-built integrations. The data appears on the Inventory page and in other places within the Infrastructure monitoring's user interface. Linux built-in agent data The infrastructure agent collects this data for Linux systems. Category Source Data collected using... applications apm APM Language Agent metadata config selinux sestatus -b, semodule -l selinux-policies sestatus -b, semodule -l selinux-modules sestatus -b, semodule -l sshd /etc/sshd_config (PermitRootLogin, PermitEmptyPasswords, PasswordAuthentication, and ChallengeResponseAuthentication only) kernel modules /sbin/modinfo, /sbin/lsmod, /proc/modules sysctl /proc/sys metadata agent_config Agent's complete config file attributes Agent's custom_attributes host_aliases Agent's display_name, Cloud provider instance-id, os.Hostname(), hostname -f, hostname cloud_security_groups Cloud provider security-groups system uptime -s, /etc/redhat-release, /proc/cpuinfo, /etc/os-release, /proc/sys/kernel/random/boot_id, /proc/sys/kernel/osrelease, /sys/class/dmi/id/product_uuid, /sys/devices/virtual/dmi/id/sys_vendor, /sys/devices/virtual/dmi/id/product_name facter facter -p -j services daemontool ps -e, svstat systemd initctl list upstart systemctl -l, systemctl show, modinfo, lsmod supervisord /var/run/supervisor.sock unix socket connection, supervisor.getAllProcessInfo pidfile var/run, find -L -name, /proc/N/status, /proc/N/stat sessions users who system network_interfaces net.Interfaces() packages dpkg dpkg-query -W -f rpm rpm -qa Windows built-in agent data The infrastructure agent collects this data for Windows systems. Category Source Data collected using... applications apm APM language agent metadata metadata agent_config Agent's complete config file attributes Agent's custom_attributes host_aliases Agent's display_name, Cloud provider instance-id, os.Hostname(), Registry (SYSTEM \\ CurrentControlSet \\ Services \\ Tcpip \\ Parameters (Domain, DhcpDomain, Hostname) system kernel32.dll (GetPhysicallyInstalledSystemMemory), WMI (Win32_OperatingSystem, Win32_Processor), os.Hostname() services windows_services WMI (Win32_Service WHERE State = \"Running\" AND StartMode = \"Auto\") system network_interfaces net.Interfaces() packages windows_programs Registry (SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\, SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\) windows_updates WMI (Win32_QuickFixEngineering) (off by default) Amazon AWS cloud integrations inventory Data collected varies by Amazon Elastic Compute Cloud (EC2) integration. For more information, see New Relic's individual Amazon Integrations documentation. Inventory data retention Inventory data is real-time. If a host stops reporting, its inventory data still displays for up to 24 hours. Chart data attributes For a technical explanation about attributes used to populate the Inventory page, see Default infrastructure attributes and events. This includes a summary of common events by operating system.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.00941,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> Inventory page: Search your entire <em>infrastructure</em>",
        "sections": "<em>Infrastructure</em> Inventory page: Search your entire <em>infrastructure</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": ", Amazon Elastic Compute Cloud (EC2) integrations, agent integrations provided by New Relic, and customer-built integrations. The data appears on the Inventory page and in other places within the <em>Infrastructure</em> <em>monitoring</em>&#x27;s user interface. Linux built-in agent data The <em>infrastructure</em> agent collects"
      },
      "id": "60440a6d64441fdf50378ee7"
    },
    {
      "sections": [
        "Infrastructure Events page: Live feed of config changes",
        "Event types",
        "Events page features",
        "Chart data attributes"
      ],
      "title": "Infrastructure Events page: Live feed of config changes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "e4a87670c8671072ae7cc6531721f46edc7f925d",
      "image": "https://docs.newrelic.com/static/75373d03d819516d3cbe23f1ea65957b/c1b63/infra-events-ui.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/infrastructure-events-page-live-feed-every-config-change/",
      "published_at": "2021-05-05T03:28:38Z",
      "updated_at": "2021-03-11T11:47:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Events page is a live feed of important system and host activity, including inventory change events, configuration changes, and log analytics events. The event feed helps you understand correlations between these events and system performance. Search and filter through your events to decrease the mean time to detect and repair infrastructure issues. You can access the Events page by going to one.newrelic.com > Infrastructure > Events. Event types New Relic collects a variety of change events so you can understand each change in your environment: Events Comments Alert incidents When a violation is opened or closed, New Relic generates an event indicating the host and associated alert condition. Agent connection When an infrastructure agent connects to New Relic, our platform generates an Agent connected event. If New Relic doesn't receive data from an agent for three minutes, the platform generates an Agent disconnected event. Inventory changes These events are generated when inventory data is added, removed, or modified. Select the source icon to to understand which category corresponds to the altered inventory path. For additional details, select an inventory event to see a side-by-side comparison of the old and new state. Inventory events can include: Kernel (includes modules and configuration): Can be added, modified, or deleted. Metadata (includes various additional information about hosts): Can be added, modified, or deleted. Packages: Can be installed, removed, or modified. Services: Can be started, stopped, or restarted. Sessions (includes users): Can be connected or disconnected. Events page features To view the live event feed: Go to one.newrelic.com > Infrastructure > E * *vents. The Events * * page includes a heatmap, which provides a snapshot of the events occurring within the selected time range. one.newrelic.com > Infrastructure > Events: Use the Events to view important, real-time activity in your infrastructure. With the Events page, you can easily search through your event log to quickly find vulnerable packages. If you want to... Do this... Focus on specific events Use the Search events field to look for specific events, config changes or agent installations. To focus on a specific set of events, select or change the filter set. Search within a particular time range Enter a time range to the right of the search bar to investigate events within a specific time range. For example, if you encountered a CPU spike around 11am on the previous day, search Yesterday at 11 am to investigate the possible cause. Compare events with host load, memory, CPU, and more View the events feed on the Hosts page. To compare infrastructure events and performance for a specific time, select a range via the time picker or drag and select a range on a chart. View events specifically related to agents, config, metadata, services, or sessions Group or sort events by selecting the filter icon be the search bar. Drill down into additional details Select an event to view additional details, such as attributes and values. To drill down further, select View in Inventory to see additional details in the Inventory page. View host's alert threshold violation Select the host's Critical icon or Warning icon. Chart data attributes For a technical explanation of the attributes used to populate the Events page, see InfrastructureEvent attributes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.99223,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> Events page: Live feed of config changes",
        "sections": "<em>Infrastructure</em> Events page: Live feed of config changes",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": " the mean time to detect and repair <em>infrastructure</em> issues. You can access the Events page by going to one.newrelic.com &gt; <em>Infrastructure</em> &gt; Events. Event types New Relic collects a variety of change events so you can understand each change in your environment: Events Comments Alert incidents When"
      },
      "id": "6043fa6c28ccbc13742c60a5"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-aws-elastic-beanstalk": [
    {
      "sections": [
        "Configure the infrastructure agent with Puppet",
        "Requirements",
        "Module description",
        "Run newrelic-infra module",
        "Install the infrastructure agent with the module",
        "Puppet parameters",
        "For more help"
      ],
      "title": "Configure the infrastructure agent with Puppet",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Config management tools"
      ],
      "external_id": "d78919080b3cac0164fd79d2f4e4c36009e0711a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-puppet/",
      "published_at": "2021-05-05T03:34:16Z",
      "updated_at": "2021-03-16T08:31:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how to use Puppet to install and configure New Relic's infrastructure agent using the newrelic-infra module. For an explanation of how to use Puppet, see Puppet documentation. This is a community-supported effort. Here we provide basic information needed to use this configuration management tool. Detailed configuration will have to conform to your company standards. Requirements The Infrastructure Puppet module has these requirements: Infrastructure-supported Linux operating systems Puppet version 3.0 or higher Module description Use the newrelic-infra module to: Add the New Relic's infrastructure agent package repository source. Install, configure, and manage the New Relic infrastructure agent. The New Relic Puppet module is available on Puppet Forge. Run newrelic-infra module To run the default newrelic-infra module, declare the main ::agent class. Install the infrastructure agent with the module All interactions with newrelic-infra are done through the main agent class. To install New Relic's infrastructure agent using Puppet, use: class { 'newrelic_infra::agent': ensure => 'latest', license_key => 'YOUR_LICENSE_KEY', } Copy Puppet parameters Here are the parameters for the newrelic_infra::agent public class: Parameter Parameter description custom_configs A hash of key-value pairs. Corresponds directly with the available general configuration settings. ensure Specifies the Infrastructure agent ensure status. Valid values include: 'latest' - (default) Installs the latest agent version 'absent' - Uninstalls the agent VERSION_STRING - A string containing a specific version to pin license_key Specifies the New Relic license key to use. For more help If you need additional help, file an issue at newrelic/infrastructure-agent-puppet on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 368.40012,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>infrastructure</em> <em>agent</em> with Puppet",
        "sections": "<em>Install</em> the <em>infrastructure</em> <em>agent</em> with the module",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Read on to learn how to use Puppet to <em>install</em> and configure New Relic&#x27;s <em>infrastructure</em> <em>agent</em> using the newrelic-infra module. For an explanation of how to use Puppet, see Puppet documentation. This is a community-supported effort. Here we provide basic information needed to use this configuration"
      },
      "id": "603e88b4e7b9d299092a07d9"
    },
    {
      "sections": [
        "Configure the infrastructure agent using Chef",
        "Compatibility and requirements",
        "Chef recipes",
        "Chef attributes",
        "default\\['newrelic_infra']\\['config']\\['license_key'] (REQUIRED)",
        "default\\['newrelic_infra']\\['packages']\\['agent']\\['action']",
        "default\\['newrelic_infra']\\['packages']\\['agent']\\['retries']",
        "default\\['newrelic_infra']\\['packages']\\['agent']\\['version']",
        "Use the basic recipe",
        "For more help"
      ],
      "title": "Configure the infrastructure agent using Chef",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Config management tools"
      ],
      "external_id": "c73f19ee6533c7028bdf2ba595ea88436df6c5c6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-using-chef/",
      "published_at": "2021-05-05T03:27:27Z",
      "updated_at": "2021-03-13T07:15:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how to use New Relic's Chef recipes to install and configure New Relic's infrastructure agent. For instructions on how to use Chef recipes, see the Chef documentation. The New Relic cookbook is available from the public Chef Supermarket. This is a community-supported effort. Compatibility and requirements The Infrastructure Chef recipe has the following requirements: Chef versions 12 or higher Supports all operating systems compatible with the infrastructure agent Chef recipes Infrastructure monitoring has one default recipe: default. Include this recipe to install and configure the infrastructure agent. If this recipe detects an unsupported platform or version, the Chef run fails. Configuration depends on your specific setup and standards. Chef attributes The default recipe supplies the following Chef attributes: default\\['newrelic_infra']\\['config']\\['license_key'] (REQUIRED) Type String Default (none) Defines your New Relic license key. default\\['newrelic_infra']\\['packages']\\['agent']\\['action'] Type String Default install Valid values install, upgrade, or remove Select what type of package resource actions action you want to perform: install: Installs the infrastructure agent. If [agent_version] is specified, that version will be installed. The first time the cookbook runs on each host, it will install the latest infrastructure agent. However, the agent will not be upgraded with install on subsequent Chef runs. Use upgrade to install newer versions. upgrade: Upgrades hosts to the latest infrastructure agent version. remove: Uninstalls the infrastructure agent. default\\['newrelic_infra']\\['packages']\\['agent']\\['retries'] Type Integer Default 0 The number of times to catch exceptions and retry the resource. default\\['newrelic_infra']\\['packages']\\['agent']\\['version'] Type String Default (none) Use with 'install' to set a specific agent version. If no value is set, the recipe defaults to the latest agent version. Use the basic recipe The New Relic cookbook is available from the public Chef Supermarket. To install and configure New Relic's infrastructure agent using Chef: Add the newrelic-infra dependency in your own Chef metadata.rb or Berksfile. Set the New Relic license key attribute. For example, add the following to your recipes/default.rb: default['newrelic_infra']['config']['license_key'] = 'YOUR_LICENSE_KEY' Copy Optional: To control version usage and updating, customize the recipe with Chef attributes. Include the default New Relic recipe by using include_recipe ‘newrelic-infra::default' or by adding the recipe to your run list. For more help If you need additional help, file an issue at newrelic/infrastructure-agent-chef on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 365.634,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>infrastructure</em> <em>agent</em> using Chef",
        "sections": "Configure the <em>infrastructure</em> <em>agent</em> using Chef",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Read on to learn how to use New Relic&#x27;s Chef recipes to <em>install</em> and configure New Relic&#x27;s <em>infrastructure</em> <em>agent</em>. For instructions on how to use Chef recipes, see the Chef documentation. The New Relic cookbook is available from the public Chef Supermarket. This is a community-supported effort"
      },
      "id": "60440aa264441f8cff378ee5"
    },
    {
      "sections": [
        "Configure the infrastructure agent using Ansible",
        "Sample code",
        "Compatibility and requirements",
        "Set up Ansible with New Relic",
        "Tip",
        "Role configuration variables",
        "For more help"
      ],
      "title": "Configure the infrastructure agent using Ansible",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Config management tools"
      ],
      "external_id": "1f13326e09d6da78f08f645bc069c22342fbac6c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-using-ansible/",
      "published_at": "2021-05-05T03:27:29Z",
      "updated_at": "2021-03-13T02:47:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use New Relic's Ansible role to install and configure our infrastructure monitoring agent. For instructions on how to use Ansible, see the Ansible documentation. This is a community-supported effort. Sample code New Relic provides configuration management sample code to help you install our infrastructure monitoring agent using workflows and tools that are common with many Ops teams. This is a basic Ansible role and is intended to be used as a starting place for creating your own customized workflow. Configuration depends on your specific setup and standards. To view an Ansible sample role and more integration information, see the Ansible Galaxy documentation. Compatibility and requirements The Ansible role with New Relic's infrastructure monitoring agent requires a supported Linux operating system. Set up Ansible with New Relic Tip To use Ansible configurations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. The newrelic.newrelic-infra role: Adds the New Relic infrastructure agent package repository source. Installs and configures the infrastructure agent. To get started using this role: Include the role in your playbook. Customize the required variables. All typical interactions with newrelic.newrelic-infra use role configuration. Here is an example of configuring your role to install the infrastructure agent: - hosts: ap_northeast_1 roles: - name: newrelic.newrelic-infra vars: nrinfragent_os_name: YOUR_OS_NAME nrinfragent_os_version: YOUR_OS_VERSION nrinfragent_config: license_key: YOUR_LICENSE_KEY log_file: /var/log/newrelic/nr-infra.log log_to_stdout: false Copy Role configuration variables Here are available variables for configuring the newrelic.newrelic-infra role: Variable Description nrinfragent_config Required. A map of key-value pairs. Corresponds directly with the available general configuration settings. nrinfragent_state Describes what you want to do with the agent: 'latest': Default. Installs the latest version of the infrastructure agent. 'absent': Uninstall the agent. nrinfragent_version The version of the agent you want to install: '*': Default. Installs the latest version of the infrastructure agent. 'x.y.zzz': String specifying a specific agent version number you want to install; for example, 1.0.682. nrinfragent_os_name Specifies the target OS that the infrastructure agent will be installed on. See the meta/main.yml file for the latest list. nrinfragent_os_version Specifies the OS version of the installer package needed for this machine. See the meta/main.yml file for the latest list. For more help If you need additional help, file an issue at newrelic/infrastructure-agent-ansible on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 365.4736,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>infrastructure</em> <em>agent</em> using Ansible",
        "sections": "Configure the <em>infrastructure</em> <em>agent</em> using Ansible",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "You can use New Relic&#x27;s Ansible role to <em>install</em> and configure our <em>infrastructure</em> monitoring <em>agent</em>. For instructions on how to use Ansible, see the Ansible documentation. This is a community-supported effort. Sample code New Relic provides configuration <em>management</em> sample code to help you <em>install</em> our"
      },
      "id": "60440aa3196a675fb6960f5c"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-puppet": [
    {
      "sections": [
        "Configure the infrastructure agent using Chef",
        "Compatibility and requirements",
        "Chef recipes",
        "Chef attributes",
        "default\\['newrelic_infra']\\['config']\\['license_key'] (REQUIRED)",
        "default\\['newrelic_infra']\\['packages']\\['agent']\\['action']",
        "default\\['newrelic_infra']\\['packages']\\['agent']\\['retries']",
        "default\\['newrelic_infra']\\['packages']\\['agent']\\['version']",
        "Use the basic recipe",
        "For more help"
      ],
      "title": "Configure the infrastructure agent using Chef",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Config management tools"
      ],
      "external_id": "c73f19ee6533c7028bdf2ba595ea88436df6c5c6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-using-chef/",
      "published_at": "2021-05-05T03:27:27Z",
      "updated_at": "2021-03-13T07:15:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how to use New Relic's Chef recipes to install and configure New Relic's infrastructure agent. For instructions on how to use Chef recipes, see the Chef documentation. The New Relic cookbook is available from the public Chef Supermarket. This is a community-supported effort. Compatibility and requirements The Infrastructure Chef recipe has the following requirements: Chef versions 12 or higher Supports all operating systems compatible with the infrastructure agent Chef recipes Infrastructure monitoring has one default recipe: default. Include this recipe to install and configure the infrastructure agent. If this recipe detects an unsupported platform or version, the Chef run fails. Configuration depends on your specific setup and standards. Chef attributes The default recipe supplies the following Chef attributes: default\\['newrelic_infra']\\['config']\\['license_key'] (REQUIRED) Type String Default (none) Defines your New Relic license key. default\\['newrelic_infra']\\['packages']\\['agent']\\['action'] Type String Default install Valid values install, upgrade, or remove Select what type of package resource actions action you want to perform: install: Installs the infrastructure agent. If [agent_version] is specified, that version will be installed. The first time the cookbook runs on each host, it will install the latest infrastructure agent. However, the agent will not be upgraded with install on subsequent Chef runs. Use upgrade to install newer versions. upgrade: Upgrades hosts to the latest infrastructure agent version. remove: Uninstalls the infrastructure agent. default\\['newrelic_infra']\\['packages']\\['agent']\\['retries'] Type Integer Default 0 The number of times to catch exceptions and retry the resource. default\\['newrelic_infra']\\['packages']\\['agent']\\['version'] Type String Default (none) Use with 'install' to set a specific agent version. If no value is set, the recipe defaults to the latest agent version. Use the basic recipe The New Relic cookbook is available from the public Chef Supermarket. To install and configure New Relic's infrastructure agent using Chef: Add the newrelic-infra dependency in your own Chef metadata.rb or Berksfile. Set the New Relic license key attribute. For example, add the following to your recipes/default.rb: default['newrelic_infra']['config']['license_key'] = 'YOUR_LICENSE_KEY' Copy Optional: To control version usage and updating, customize the recipe with Chef attributes. Include the default New Relic recipe by using include_recipe ‘newrelic-infra::default' or by adding the recipe to your run list. For more help If you need additional help, file an issue at newrelic/infrastructure-agent-chef on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 365.634,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>infrastructure</em> <em>agent</em> using Chef",
        "sections": "Configure the <em>infrastructure</em> <em>agent</em> using Chef",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Read on to learn how to use New Relic&#x27;s Chef recipes to <em>install</em> and configure New Relic&#x27;s <em>infrastructure</em> <em>agent</em>. For instructions on how to use Chef recipes, see the Chef documentation. The New Relic cookbook is available from the public Chef Supermarket. This is a community-supported effort"
      },
      "id": "60440aa264441f8cff378ee5"
    },
    {
      "sections": [
        "Configure the infrastructure agent on AWS Elastic Beanstalk",
        "Requirements",
        "Install the infrastructure agent",
        "Amazon Linux AMI",
        "Amazon Linux 2",
        "Windows",
        "Uninstall the infrastructure agent"
      ],
      "title": "Configure the infrastructure agent on AWS Elastic Beanstalk",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Config management tools"
      ],
      "external_id": "63fee84da30d8fb761d1cab41d31aa7bad9f3adf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-aws-elastic-beanstalk/",
      "published_at": "2021-05-05T03:34:17Z",
      "updated_at": "2021-03-13T03:06:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Amazon Web Services (AWS) Elastic Beanstalk is a dynamic service that allows easy deployment and scalability for your applications. Follow these instructions to deploy the infrastructure agent to the instances launched with your AWS Elastic Beanstalk applications. In addition to deploying the infrastructure agent you can also integrate New Relic with AWS and bring Elastic Beanstalk monitoring information into New Relic. If you haven't already done so, follow these instructions for Amazon integrations with infrastructure monitoring. Requirements Make sure you have a supported Amazon Web Services account. In addition, for any New Relic installation, you will need your New Relic license key. This is a 40-character hexadecimal string that New Relic provides when you sign up for your account. Install the infrastructure agent To install the infrastructure agent on instances launched with AWS Elastic Beanstalk: In the .ebextensions folder inside your Elastic BeanStalk application, create a new file named newrelic.config. Based on the operating system, add the following content to the file, replacing YOUR_LICENSE_KEY with your New Relic license key. Amazon Linux AMI files: \"/etc/newrelic-infra.yml\" : mode: \"000644\" owner: root group: root content: | license_key: YOUR_LICENSE_KEY commands: # Create the agent’s yum repository \"01-agent-repository\": command: sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64/newrelic-infra.repo # # Update your yum cache \"02-update-yum-cache\": command: yum -q makecache -y --disablerepo='*' --enablerepo='newrelic-infra' # # Run the installation script \"03-run-installation-script\": command: sudo yum install newrelic-infra -y Copy Amazon Linux 2 files: \"/etc/newrelic-infra.yml\" : mode: \"000644\" owner: root group: root content: | license_key: YOUR_LICENSE_KEY commands: # Create the agent’s yum repository \"01-agent-repository\": command: sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64/newrelic-infra.repo # # Update your yum cache \"02-update-yum-cache\": command: yum -q makecache -y --disablerepo='*' --enablerepo='newrelic-infra' # # Run the installation script \"03-run-installation-script\": command: sudo yum install newrelic-infra -y Copy Windows packages: msi: infrastructure: https://download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi files: \"C:\\\\Program Files\\\\New Relic\\\\newrelic-infra\\\\newrelic-infra.yml\": content: | license_key: YOUR_LICENSE_KEY commands: 01_stop-newrelic-infra: command: net stop newrelic-infra ignoreErrors: true 02_start-newrelic-infra: command: net start newrelic-infra ignoreErrors: true Copy Push your app to Elastic BeanStalk: In general, use eb deploy. If you are still using Eb CLI 2.6 , use git aws.push if required. Optional: Use the AWS Console UI. After a successful setup, it can take up to fifteen minutes before metrics begin to appear in New Relic. View your host's infrastructure pages at one.newrelic.com. Uninstall the infrastructure agent To uninstall the agent, remove newrelic.config from .ebextensions, then deploy using the CLI.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 365.48495,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>infrastructure</em> <em>agent</em> on AWS Elastic Beanstalk",
        "sections": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " Services account. In addition, for any New Relic installation, you will need your New Relic license key. This is a 40-character hexadecimal string that New Relic provides when you sign up for your account. <em>Install</em> the <em>infrastructure</em> <em>agent</em> To <em>install</em> the <em>infrastructure</em> <em>agent</em> on instances launched with AWS"
      },
      "id": "60440a6d28ccbc37982c60c5"
    },
    {
      "sections": [
        "Configure the infrastructure agent using Ansible",
        "Sample code",
        "Compatibility and requirements",
        "Set up Ansible with New Relic",
        "Tip",
        "Role configuration variables",
        "For more help"
      ],
      "title": "Configure the infrastructure agent using Ansible",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Config management tools"
      ],
      "external_id": "1f13326e09d6da78f08f645bc069c22342fbac6c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-using-ansible/",
      "published_at": "2021-05-05T03:27:29Z",
      "updated_at": "2021-03-13T02:47:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use New Relic's Ansible role to install and configure our infrastructure monitoring agent. For instructions on how to use Ansible, see the Ansible documentation. This is a community-supported effort. Sample code New Relic provides configuration management sample code to help you install our infrastructure monitoring agent using workflows and tools that are common with many Ops teams. This is a basic Ansible role and is intended to be used as a starting place for creating your own customized workflow. Configuration depends on your specific setup and standards. To view an Ansible sample role and more integration information, see the Ansible Galaxy documentation. Compatibility and requirements The Ansible role with New Relic's infrastructure monitoring agent requires a supported Linux operating system. Set up Ansible with New Relic Tip To use Ansible configurations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. The newrelic.newrelic-infra role: Adds the New Relic infrastructure agent package repository source. Installs and configures the infrastructure agent. To get started using this role: Include the role in your playbook. Customize the required variables. All typical interactions with newrelic.newrelic-infra use role configuration. Here is an example of configuring your role to install the infrastructure agent: - hosts: ap_northeast_1 roles: - name: newrelic.newrelic-infra vars: nrinfragent_os_name: YOUR_OS_NAME nrinfragent_os_version: YOUR_OS_VERSION nrinfragent_config: license_key: YOUR_LICENSE_KEY log_file: /var/log/newrelic/nr-infra.log log_to_stdout: false Copy Role configuration variables Here are available variables for configuring the newrelic.newrelic-infra role: Variable Description nrinfragent_config Required. A map of key-value pairs. Corresponds directly with the available general configuration settings. nrinfragent_state Describes what you want to do with the agent: 'latest': Default. Installs the latest version of the infrastructure agent. 'absent': Uninstall the agent. nrinfragent_version The version of the agent you want to install: '*': Default. Installs the latest version of the infrastructure agent. 'x.y.zzz': String specifying a specific agent version number you want to install; for example, 1.0.682. nrinfragent_os_name Specifies the target OS that the infrastructure agent will be installed on. See the meta/main.yml file for the latest list. nrinfragent_os_version Specifies the OS version of the installer package needed for this machine. See the meta/main.yml file for the latest list. For more help If you need additional help, file an issue at newrelic/infrastructure-agent-ansible on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 365.4736,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>infrastructure</em> <em>agent</em> using Ansible",
        "sections": "Configure the <em>infrastructure</em> <em>agent</em> using Ansible",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "You can use New Relic&#x27;s Ansible role to <em>install</em> and configure our <em>infrastructure</em> monitoring <em>agent</em>. For instructions on how to use Ansible, see the Ansible documentation. This is a community-supported effort. Sample code New Relic provides configuration <em>management</em> sample code to help you <em>install</em> our"
      },
      "id": "60440aa3196a675fb6960f5c"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-using-ansible": [
    {
      "sections": [
        "Configure the infrastructure agent with Puppet",
        "Requirements",
        "Module description",
        "Run newrelic-infra module",
        "Install the infrastructure agent with the module",
        "Puppet parameters",
        "For more help"
      ],
      "title": "Configure the infrastructure agent with Puppet",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Config management tools"
      ],
      "external_id": "d78919080b3cac0164fd79d2f4e4c36009e0711a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-puppet/",
      "published_at": "2021-05-05T03:34:16Z",
      "updated_at": "2021-03-16T08:31:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how to use Puppet to install and configure New Relic's infrastructure agent using the newrelic-infra module. For an explanation of how to use Puppet, see Puppet documentation. This is a community-supported effort. Here we provide basic information needed to use this configuration management tool. Detailed configuration will have to conform to your company standards. Requirements The Infrastructure Puppet module has these requirements: Infrastructure-supported Linux operating systems Puppet version 3.0 or higher Module description Use the newrelic-infra module to: Add the New Relic's infrastructure agent package repository source. Install, configure, and manage the New Relic infrastructure agent. The New Relic Puppet module is available on Puppet Forge. Run newrelic-infra module To run the default newrelic-infra module, declare the main ::agent class. Install the infrastructure agent with the module All interactions with newrelic-infra are done through the main agent class. To install New Relic's infrastructure agent using Puppet, use: class { 'newrelic_infra::agent': ensure => 'latest', license_key => 'YOUR_LICENSE_KEY', } Copy Puppet parameters Here are the parameters for the newrelic_infra::agent public class: Parameter Parameter description custom_configs A hash of key-value pairs. Corresponds directly with the available general configuration settings. ensure Specifies the Infrastructure agent ensure status. Valid values include: 'latest' - (default) Installs the latest agent version 'absent' - Uninstalls the agent VERSION_STRING - A string containing a specific version to pin license_key Specifies the New Relic license key to use. For more help If you need additional help, file an issue at newrelic/infrastructure-agent-puppet on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 368.4001,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>infrastructure</em> <em>agent</em> with Puppet",
        "sections": "<em>Install</em> the <em>infrastructure</em> <em>agent</em> with the module",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Read on to learn how to use Puppet to <em>install</em> and configure New Relic&#x27;s <em>infrastructure</em> <em>agent</em> using the newrelic-infra module. For an explanation of how to use Puppet, see Puppet documentation. This is a community-supported effort. Here we provide basic information needed to use this configuration"
      },
      "id": "603e88b4e7b9d299092a07d9"
    },
    {
      "sections": [
        "Configure the infrastructure agent using Chef",
        "Compatibility and requirements",
        "Chef recipes",
        "Chef attributes",
        "default\\['newrelic_infra']\\['config']\\['license_key'] (REQUIRED)",
        "default\\['newrelic_infra']\\['packages']\\['agent']\\['action']",
        "default\\['newrelic_infra']\\['packages']\\['agent']\\['retries']",
        "default\\['newrelic_infra']\\['packages']\\['agent']\\['version']",
        "Use the basic recipe",
        "For more help"
      ],
      "title": "Configure the infrastructure agent using Chef",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Config management tools"
      ],
      "external_id": "c73f19ee6533c7028bdf2ba595ea88436df6c5c6",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-using-chef/",
      "published_at": "2021-05-05T03:27:27Z",
      "updated_at": "2021-03-13T07:15:54Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how to use New Relic's Chef recipes to install and configure New Relic's infrastructure agent. For instructions on how to use Chef recipes, see the Chef documentation. The New Relic cookbook is available from the public Chef Supermarket. This is a community-supported effort. Compatibility and requirements The Infrastructure Chef recipe has the following requirements: Chef versions 12 or higher Supports all operating systems compatible with the infrastructure agent Chef recipes Infrastructure monitoring has one default recipe: default. Include this recipe to install and configure the infrastructure agent. If this recipe detects an unsupported platform or version, the Chef run fails. Configuration depends on your specific setup and standards. Chef attributes The default recipe supplies the following Chef attributes: default\\['newrelic_infra']\\['config']\\['license_key'] (REQUIRED) Type String Default (none) Defines your New Relic license key. default\\['newrelic_infra']\\['packages']\\['agent']\\['action'] Type String Default install Valid values install, upgrade, or remove Select what type of package resource actions action you want to perform: install: Installs the infrastructure agent. If [agent_version] is specified, that version will be installed. The first time the cookbook runs on each host, it will install the latest infrastructure agent. However, the agent will not be upgraded with install on subsequent Chef runs. Use upgrade to install newer versions. upgrade: Upgrades hosts to the latest infrastructure agent version. remove: Uninstalls the infrastructure agent. default\\['newrelic_infra']\\['packages']\\['agent']\\['retries'] Type Integer Default 0 The number of times to catch exceptions and retry the resource. default\\['newrelic_infra']\\['packages']\\['agent']\\['version'] Type String Default (none) Use with 'install' to set a specific agent version. If no value is set, the recipe defaults to the latest agent version. Use the basic recipe The New Relic cookbook is available from the public Chef Supermarket. To install and configure New Relic's infrastructure agent using Chef: Add the newrelic-infra dependency in your own Chef metadata.rb or Berksfile. Set the New Relic license key attribute. For example, add the following to your recipes/default.rb: default['newrelic_infra']['config']['license_key'] = 'YOUR_LICENSE_KEY' Copy Optional: To control version usage and updating, customize the recipe with Chef attributes. Include the default New Relic recipe by using include_recipe ‘newrelic-infra::default' or by adding the recipe to your run list. For more help If you need additional help, file an issue at newrelic/infrastructure-agent-chef on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 365.634,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>infrastructure</em> <em>agent</em> using Chef",
        "sections": "Configure the <em>infrastructure</em> <em>agent</em> using Chef",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Read on to learn how to use New Relic&#x27;s Chef recipes to <em>install</em> and configure New Relic&#x27;s <em>infrastructure</em> <em>agent</em>. For instructions on how to use Chef recipes, see the Chef documentation. The New Relic cookbook is available from the public Chef Supermarket. This is a community-supported effort"
      },
      "id": "60440aa264441f8cff378ee5"
    },
    {
      "sections": [
        "Configure the infrastructure agent on AWS Elastic Beanstalk",
        "Requirements",
        "Install the infrastructure agent",
        "Amazon Linux AMI",
        "Amazon Linux 2",
        "Windows",
        "Uninstall the infrastructure agent"
      ],
      "title": "Configure the infrastructure agent on AWS Elastic Beanstalk",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Config management tools"
      ],
      "external_id": "63fee84da30d8fb761d1cab41d31aa7bad9f3adf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-aws-elastic-beanstalk/",
      "published_at": "2021-05-05T03:34:17Z",
      "updated_at": "2021-03-13T03:06:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Amazon Web Services (AWS) Elastic Beanstalk is a dynamic service that allows easy deployment and scalability for your applications. Follow these instructions to deploy the infrastructure agent to the instances launched with your AWS Elastic Beanstalk applications. In addition to deploying the infrastructure agent you can also integrate New Relic with AWS and bring Elastic Beanstalk monitoring information into New Relic. If you haven't already done so, follow these instructions for Amazon integrations with infrastructure monitoring. Requirements Make sure you have a supported Amazon Web Services account. In addition, for any New Relic installation, you will need your New Relic license key. This is a 40-character hexadecimal string that New Relic provides when you sign up for your account. Install the infrastructure agent To install the infrastructure agent on instances launched with AWS Elastic Beanstalk: In the .ebextensions folder inside your Elastic BeanStalk application, create a new file named newrelic.config. Based on the operating system, add the following content to the file, replacing YOUR_LICENSE_KEY with your New Relic license key. Amazon Linux AMI files: \"/etc/newrelic-infra.yml\" : mode: \"000644\" owner: root group: root content: | license_key: YOUR_LICENSE_KEY commands: # Create the agent’s yum repository \"01-agent-repository\": command: sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64/newrelic-infra.repo # # Update your yum cache \"02-update-yum-cache\": command: yum -q makecache -y --disablerepo='*' --enablerepo='newrelic-infra' # # Run the installation script \"03-run-installation-script\": command: sudo yum install newrelic-infra -y Copy Amazon Linux 2 files: \"/etc/newrelic-infra.yml\" : mode: \"000644\" owner: root group: root content: | license_key: YOUR_LICENSE_KEY commands: # Create the agent’s yum repository \"01-agent-repository\": command: sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64/newrelic-infra.repo # # Update your yum cache \"02-update-yum-cache\": command: yum -q makecache -y --disablerepo='*' --enablerepo='newrelic-infra' # # Run the installation script \"03-run-installation-script\": command: sudo yum install newrelic-infra -y Copy Windows packages: msi: infrastructure: https://download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi files: \"C:\\\\Program Files\\\\New Relic\\\\newrelic-infra\\\\newrelic-infra.yml\": content: | license_key: YOUR_LICENSE_KEY commands: 01_stop-newrelic-infra: command: net stop newrelic-infra ignoreErrors: true 02_start-newrelic-infra: command: net start newrelic-infra ignoreErrors: true Copy Push your app to Elastic BeanStalk: In general, use eb deploy. If you are still using Eb CLI 2.6 , use git aws.push if required. Optional: Use the AWS Console UI. After a successful setup, it can take up to fifteen minutes before metrics begin to appear in New Relic. View your host's infrastructure pages at one.newrelic.com. Uninstall the infrastructure agent To uninstall the agent, remove newrelic.config from .ebextensions, then deploy using the CLI.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 365.48495,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>infrastructure</em> <em>agent</em> on AWS Elastic Beanstalk",
        "sections": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " Services account. In addition, for any New Relic installation, you will need your New Relic license key. This is a 40-character hexadecimal string that New Relic provides when you sign up for your account. <em>Install</em> the <em>infrastructure</em> <em>agent</em> To <em>install</em> the <em>infrastructure</em> <em>agent</em> on instances launched with AWS"
      },
      "id": "60440a6d28ccbc37982c60c5"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-using-chef": [
    {
      "sections": [
        "Configure the infrastructure agent with Puppet",
        "Requirements",
        "Module description",
        "Run newrelic-infra module",
        "Install the infrastructure agent with the module",
        "Puppet parameters",
        "For more help"
      ],
      "title": "Configure the infrastructure agent with Puppet",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Config management tools"
      ],
      "external_id": "d78919080b3cac0164fd79d2f4e4c36009e0711a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-puppet/",
      "published_at": "2021-05-05T03:34:16Z",
      "updated_at": "2021-03-16T08:31:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how to use Puppet to install and configure New Relic's infrastructure agent using the newrelic-infra module. For an explanation of how to use Puppet, see Puppet documentation. This is a community-supported effort. Here we provide basic information needed to use this configuration management tool. Detailed configuration will have to conform to your company standards. Requirements The Infrastructure Puppet module has these requirements: Infrastructure-supported Linux operating systems Puppet version 3.0 or higher Module description Use the newrelic-infra module to: Add the New Relic's infrastructure agent package repository source. Install, configure, and manage the New Relic infrastructure agent. The New Relic Puppet module is available on Puppet Forge. Run newrelic-infra module To run the default newrelic-infra module, declare the main ::agent class. Install the infrastructure agent with the module All interactions with newrelic-infra are done through the main agent class. To install New Relic's infrastructure agent using Puppet, use: class { 'newrelic_infra::agent': ensure => 'latest', license_key => 'YOUR_LICENSE_KEY', } Copy Puppet parameters Here are the parameters for the newrelic_infra::agent public class: Parameter Parameter description custom_configs A hash of key-value pairs. Corresponds directly with the available general configuration settings. ensure Specifies the Infrastructure agent ensure status. Valid values include: 'latest' - (default) Installs the latest agent version 'absent' - Uninstalls the agent VERSION_STRING - A string containing a specific version to pin license_key Specifies the New Relic license key to use. For more help If you need additional help, file an issue at newrelic/infrastructure-agent-puppet on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 368.4001,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>infrastructure</em> <em>agent</em> with Puppet",
        "sections": "<em>Install</em> the <em>infrastructure</em> <em>agent</em> with the module",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Read on to learn how to use Puppet to <em>install</em> and configure New Relic&#x27;s <em>infrastructure</em> <em>agent</em> using the newrelic-infra module. For an explanation of how to use Puppet, see Puppet documentation. This is a community-supported effort. Here we provide basic information needed to use this configuration"
      },
      "id": "603e88b4e7b9d299092a07d9"
    },
    {
      "sections": [
        "Configure the infrastructure agent on AWS Elastic Beanstalk",
        "Requirements",
        "Install the infrastructure agent",
        "Amazon Linux AMI",
        "Amazon Linux 2",
        "Windows",
        "Uninstall the infrastructure agent"
      ],
      "title": "Configure the infrastructure agent on AWS Elastic Beanstalk",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Config management tools"
      ],
      "external_id": "63fee84da30d8fb761d1cab41d31aa7bad9f3adf",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-aws-elastic-beanstalk/",
      "published_at": "2021-05-05T03:34:17Z",
      "updated_at": "2021-03-13T03:06:19Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Amazon Web Services (AWS) Elastic Beanstalk is a dynamic service that allows easy deployment and scalability for your applications. Follow these instructions to deploy the infrastructure agent to the instances launched with your AWS Elastic Beanstalk applications. In addition to deploying the infrastructure agent you can also integrate New Relic with AWS and bring Elastic Beanstalk monitoring information into New Relic. If you haven't already done so, follow these instructions for Amazon integrations with infrastructure monitoring. Requirements Make sure you have a supported Amazon Web Services account. In addition, for any New Relic installation, you will need your New Relic license key. This is a 40-character hexadecimal string that New Relic provides when you sign up for your account. Install the infrastructure agent To install the infrastructure agent on instances launched with AWS Elastic Beanstalk: In the .ebextensions folder inside your Elastic BeanStalk application, create a new file named newrelic.config. Based on the operating system, add the following content to the file, replacing YOUR_LICENSE_KEY with your New Relic license key. Amazon Linux AMI files: \"/etc/newrelic-infra.yml\" : mode: \"000644\" owner: root group: root content: | license_key: YOUR_LICENSE_KEY commands: # Create the agent’s yum repository \"01-agent-repository\": command: sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64/newrelic-infra.repo # # Update your yum cache \"02-update-yum-cache\": command: yum -q makecache -y --disablerepo='*' --enablerepo='newrelic-infra' # # Run the installation script \"03-run-installation-script\": command: sudo yum install newrelic-infra -y Copy Amazon Linux 2 files: \"/etc/newrelic-infra.yml\" : mode: \"000644\" owner: root group: root content: | license_key: YOUR_LICENSE_KEY commands: # Create the agent’s yum repository \"01-agent-repository\": command: sudo curl -o /etc/yum.repos.d/newrelic-infra.repo https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64/newrelic-infra.repo # # Update your yum cache \"02-update-yum-cache\": command: yum -q makecache -y --disablerepo='*' --enablerepo='newrelic-infra' # # Run the installation script \"03-run-installation-script\": command: sudo yum install newrelic-infra -y Copy Windows packages: msi: infrastructure: https://download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi files: \"C:\\\\Program Files\\\\New Relic\\\\newrelic-infra\\\\newrelic-infra.yml\": content: | license_key: YOUR_LICENSE_KEY commands: 01_stop-newrelic-infra: command: net stop newrelic-infra ignoreErrors: true 02_start-newrelic-infra: command: net start newrelic-infra ignoreErrors: true Copy Push your app to Elastic BeanStalk: In general, use eb deploy. If you are still using Eb CLI 2.6 , use git aws.push if required. Optional: Use the AWS Console UI. After a successful setup, it can take up to fifteen minutes before metrics begin to appear in New Relic. View your host's infrastructure pages at one.newrelic.com. Uninstall the infrastructure agent To uninstall the agent, remove newrelic.config from .ebextensions, then deploy using the CLI.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 365.48495,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>infrastructure</em> <em>agent</em> on AWS Elastic Beanstalk",
        "sections": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " Services account. In addition, for any New Relic installation, you will need your New Relic license key. This is a 40-character hexadecimal string that New Relic provides when you sign up for your account. <em>Install</em> the <em>infrastructure</em> <em>agent</em> To <em>install</em> the <em>infrastructure</em> <em>agent</em> on instances launched with AWS"
      },
      "id": "60440a6d28ccbc37982c60c5"
    },
    {
      "sections": [
        "Configure the infrastructure agent using Ansible",
        "Sample code",
        "Compatibility and requirements",
        "Set up Ansible with New Relic",
        "Tip",
        "Role configuration variables",
        "For more help"
      ],
      "title": "Configure the infrastructure agent using Ansible",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Config management tools"
      ],
      "external_id": "1f13326e09d6da78f08f645bc069c22342fbac6c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-using-ansible/",
      "published_at": "2021-05-05T03:27:29Z",
      "updated_at": "2021-03-13T02:47:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use New Relic's Ansible role to install and configure our infrastructure monitoring agent. For instructions on how to use Ansible, see the Ansible documentation. This is a community-supported effort. Sample code New Relic provides configuration management sample code to help you install our infrastructure monitoring agent using workflows and tools that are common with many Ops teams. This is a basic Ansible role and is intended to be used as a starting place for creating your own customized workflow. Configuration depends on your specific setup and standards. To view an Ansible sample role and more integration information, see the Ansible Galaxy documentation. Compatibility and requirements The Ansible role with New Relic's infrastructure monitoring agent requires a supported Linux operating system. Set up Ansible with New Relic Tip To use Ansible configurations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. The newrelic.newrelic-infra role: Adds the New Relic infrastructure agent package repository source. Installs and configures the infrastructure agent. To get started using this role: Include the role in your playbook. Customize the required variables. All typical interactions with newrelic.newrelic-infra use role configuration. Here is an example of configuring your role to install the infrastructure agent: - hosts: ap_northeast_1 roles: - name: newrelic.newrelic-infra vars: nrinfragent_os_name: YOUR_OS_NAME nrinfragent_os_version: YOUR_OS_VERSION nrinfragent_config: license_key: YOUR_LICENSE_KEY log_file: /var/log/newrelic/nr-infra.log log_to_stdout: false Copy Role configuration variables Here are available variables for configuring the newrelic.newrelic-infra role: Variable Description nrinfragent_config Required. A map of key-value pairs. Corresponds directly with the available general configuration settings. nrinfragent_state Describes what you want to do with the agent: 'latest': Default. Installs the latest version of the infrastructure agent. 'absent': Uninstall the agent. nrinfragent_version The version of the agent you want to install: '*': Default. Installs the latest version of the infrastructure agent. 'x.y.zzz': String specifying a specific agent version number you want to install; for example, 1.0.682. nrinfragent_os_name Specifies the target OS that the infrastructure agent will be installed on. See the meta/main.yml file for the latest list. nrinfragent_os_version Specifies the OS version of the installer package needed for this machine. See the meta/main.yml file for the latest list. For more help If you need additional help, file an issue at newrelic/infrastructure-agent-ansible on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 365.47357,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>infrastructure</em> <em>agent</em> using Ansible",
        "sections": "Configure the <em>infrastructure</em> <em>agent</em> using Ansible",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "You can use New Relic&#x27;s Ansible role to <em>install</em> and configure our <em>infrastructure</em> monitoring <em>agent</em>. For instructions on how to use Ansible, see the Ansible documentation. This is a community-supported effort. Sample code New Relic provides configuration <em>management</em> sample code to help you <em>install</em> our"
      },
      "id": "60440aa3196a675fb6960f5c"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/configuration/config-file-template-newrelic-infrayml": [
    {
      "sections": [
        "Infrastructure agent configuration settings",
        "Important",
        "Agent variables",
        "license_key (REQUIRED)",
        "fedramp",
        "max_procs",
        "payload_compression_level",
        "startup_connection_retries",
        "startup_connection_retry_time",
        "startup_connection_timeout",
        "Cloud variables",
        "cloud_max_retry_count",
        "cloud_metadata_expiry_sec",
        "cloud_retry_backoff_sec",
        "disable_cloud_instance_id",
        "disable_cloud_metadata",
        "Debug variables",
        "trace",
        "Docker variables",
        "container_cache_metadata_limit",
        "docker_api_version",
        "File system variables",
        "custom_supported_file_systems",
        "file_devices_ignored",
        "Hostname variables",
        "display_name",
        "dns_hostname_resolution",
        "override_hostname",
        "override_hostname_short",
        "Installation variables",
        "agent_dir",
        "plugin_dir",
        "custom_plugin_installation_dir",
        "Integrations variables",
        "passthrough_environment",
        "entityname_integrations_v2_update",
        "http_server_enabled",
        "http_server_host",
        "http_server_port",
        "remove_entities_period",
        "Inventory variables",
        "offline_time_to_reset",
        "ignored_inventory",
        "Linux variables",
        "pid_file",
        "ignore_reclaimable",
        "Logging variables",
        "log_file",
        "log_format",
        "log_to_stdout",
        "verbose",
        "smart_verbose_mode_entry_limit",
        "Metrics variables",
        "custom_attributes",
        "Tip",
        "enable_process_metrics",
        "include_matching_metrics",
        "network_interface_filters",
        "Linux",
        "Windows",
        "disable_zero_mem_process_filter",
        "Plugins variables",
        "disable_all_plugins",
        "cloud_security_group_refresh_sec",
        "daemontools_interval_sec",
        "dpkg_interval_sec",
        "facter_interval_sec",
        "kernel_modules_refresh_sec",
        "network_interface_interval_sec",
        "rpm_interval_sec",
        "selinux_interval_sec",
        "sshd_config_refresh_sec",
        "supervisor_interval_sec",
        "sysctl_interval_sec",
        "systemd_interval_sec",
        "sysvinit_interval_sec",
        "upstart_interval_sec",
        "users_refresh_sec",
        "supervisor_rpc_sock",
        "facter_home_dir",
        "Proxy variables",
        "proxy",
        "ignore_system_proxy",
        "ca_bundle_dir",
        "ca_bundle_file",
        "proxy_validate_certificates",
        "proxy_config_plugin",
        "Samples variables",
        "metrics_network_sample_rate",
        "metrics_process_sample_rate",
        "metrics_storage_sample_rate",
        "metrics_system_sample_rate",
        "metrics_nfs_sample_rate",
        "detailed_nfs",
        "Security variables",
        "selinux_enable_semodule",
        "strip_command_line",
        "Windows variables",
        "windows_services_refresh_sec",
        "windows_updates_refresh_sec",
        "app_data_dir",
        "enable_win_update_plugin",
        "legacy_storage_sampler",
        "win_process_priority_class",
        "win_removable_drives",
        "What's next?"
      ],
      "title": "Infrastructure agent configuration settings ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Configuration"
      ],
      "external_id": "7d016d3f5af6ed1615904578f0ed8bce02aa335b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/configuration/infrastructure-agent-configuration-settings/",
      "published_at": "2021-05-05T02:35:20Z",
      "updated_at": "2021-03-30T19:18:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent has a large set of configuration settings to fine-tune its behavior. Here we: List all the configuration options (in both their YAML and the environment variable names). Explain what the settings do and when to use them. Give the variable type and default value (if any). List the minimum required agent version as applicable. For an example of how all these variables can be used, see our sample configuration template. Important license_key is the only required setting. Agent variables license_key (REQUIRED) Specifies the license key for your New Relic account. The agent uses this key to associate your server's metrics with your New Relic account. This setting is created as part of the standard installation process. YML option name Environment variable Type Default Version license_key NRIA_LICENSE_KEY string Example: license_key: 1234567890abcdefghijklmnopqrstuvwxyz1234 Copy fedramp Specifies whether Fedramp endpoints should be used. YML option name Environment variable Type Default Version fedramp NRIA_FEDRAMP boolean false 1.15.0 max_procs Specifies the number of logical processors available to the agent. Increasing this value helps to distribute the load between different cores. If set to -1, the agent will try to read the environment variable GOMAXPROCS. If this variable is not set, the default value will be the total number of cores available in the host. YML option name Environment variable Type Default Version max_procs NRIA_MAX_PROCS integer 1 1.0.1002 payload_compression_level Since version 1.0.804 or higher, data sent from the agent is compressed by default. To disable payload compression, set payload_compression_level to 0. Important Recommendation: Do not change this setting. YML option name Environment variable Type Default Version payload_compression_level NRIA_PAYLOAD_COMPRESSION_LEVEL integer 6 1.0.804 startup_connection_retries Number of times the agent will retry the request to check New Relic's platform availability at startup before throwing an error. If set to a negative value, the agent will keep checking the connection until it succeeds. YML option name Environment variable Type Default Version startup_connection_retries NRIA_STARTUP_CONNECTION_RETRIES integer 6 1.0.936 startup_connection_retry_time After a request has timed out, time the agent waits to retry a request to check New Relic's platform availability at startup. YML option name Environment variable Type Default Version startup_connection_retry_time NRIA_STARTUP_CONNECTION_RETRY_TIME string 5s 1.0.936 - 1.2.30 startup_connection_timeout Time the agent waits until a request to check New Relic's platform availability at startup is considered timed out. YML option name Environment variable Type Default Version startup_connection_timeout NRIA_STARTUP_CONNECTION_TIMEOUT string 10s 1.0.936 Cloud variables If the agent is running in a cloud instance, the agent will try to detect the cloud type and fetch metadata. cloud_max_retry_count Sets the number of times the agent retries to connect in case that cloud detection fails. If cloud detection fails during the initialization of the agent, the agent will retry after waiting for CloudRetryBackOffSec seconds. YML option name Environment variable Type Default Version cloud_max_retry_count NRIA_CLOUD_MAX_RETRY_COUNT integer 10 1.2.6 cloud_metadata_expiry_sec Sets the interval of time the agent will wait until discarding the metadata, in seconds. After this period metadata expires and the agent will fetch it again. YML option name Environment variable Type Default Version cloud_metadata_expiry_sec NRIA_CLOUD_METADATA_EXPIRY_SEC integer 300 1.2.6 cloud_retry_backoff_sec Sets the interval of time the agent waits between cloud detection retries in case that cloud detection failed, in seconds. If cloud detection fails during the initialization of the agent, it will retry for CloudMaxRetryCount times. YML option name Environment variable Type Default Version cloud_retry_backoff_sec NRIA_CLOUD_RETRY_BACKOFF_SEC integer 60 1.2.6 disable_cloud_instance_id Similar to DisableCloudMetadata, but it disables the collection of cloud metadata only for the host alias plugin. YML option name Environment variable Type Default Version disable_cloud_instance_id NRIA_DISABLE_CLOUD_INSTANCE_ID boolean false 1.0.220 disable_cloud_metadata Disables the collection of cloud metadata. YML option name Environment variable Type Default Version disable_cloud_metadata NRIA_DISABLE_CLOUD_METADATA boolean false 1.0.690 Debug variables trace Enables traces to be printed in the logs for the given features. Traces are additional log entries used for debugging purposes of specific features, they are only shown when the verbose log config option is enabled. Currently two features have traces defined: connect and attributes. YML option name Environment variable Type Default Version trace NRIA_TRACE [ ]string [] 1.0.1015 Example as a YAML attribute: trace: [connect, attributes] Copy Example as an environment variable: NRIA_TRACE='connect, attributes' Copy Docker variables container_cache_metadata_limit Time, in seconds, before the cached containers metadata expires and the agent needs to fetch them again. YML option name Environment variable Type Default Version container_cache_metadata_limit NRIA_CONTAINER_CACHE_METADATA_LIMIT integer 60 1.0.801 docker_api_version Specifies the Docker client API version. YML option name Environment variable Type Default Version docker_api_version NRIA_DOCKER_API_VERSION string 1.24 1.1.4 File system variables custom_supported_file_systems List of the types of file systems that the agent supports. This value needs to be a subset of the default list, and items that are not in the default list will be discarded. YML option name Environment variable Type Default Version custom_supported_file_systems NRIA_CUSTOM_SUPPORTED_FILESYSTEMS [ ]string Linux: [\"xfs\", \"btrfs\", \"ext\", \"ext2\", \"ext3\", \"ext4\", \"hfs\", \"vxfs\"] Windows: [\"NTFS\", \"ReFS\"] 1.0.220 file_devices_ignored List of storage devices to be ignored by the agent when gathering StorageSample data. YML option name Environment variable Type Default Version file_devices_ignored NRIA_FILE_DEVICES_IGNORED [ ]string 1.0.220 Example as a YAML attribute: file_devices_ignored: - sda1 - sda2 Copy Example as an environment variable: FILE_DEVICES_IGNORED=\"sda1,sda2\" Copy Hostname variables display_name Overrides the auto-generated hostname for reporting. This is useful when you have multiple hosts with the same name, since our infrastructure monitoring uses the hostname as the unique identifier for each host. Keep in mind this value is also used for the loopback address replacement on entity names. For more information, see our documentation on how entity name resolution works. YML option name Environment variable Type Default Version display_name NRIA_DISPLAY_NAME string empty 1.0.266 Example: display_name: teslaOne Copy dns_hostname_resolution When true, the full hostname is resolved by performing a reverse lookup of the host's address. Otherwise, it will be retrieved with the hostname command on Linux and from the TCP/IP parameters of the registry on Windows. YML option name Environment variable Type Default Version dns_hostname_resolution NRIA_DNS_HOSTNAME_RESOLUTION boolean true 1.2.6 override_hostname When set, this is the value that will be reported for the full hostname; otherwise, the agent will perform the normal lookup behavior. YML option name Environment variable Type Default Version override_hostname NRIA_OVERRIDE_HOSTNAME string 1.0.1015 Example: my.custom-hostname.co.org Copy override_hostname_short When set, this is the value that will be reported for the hostname; otherwise, the agent will perform the normal lookup behavior. YML option name Environment variable Type Default Version override_hostname_short NRIA_OVERRIDE_HOSTNAME_SHORT string 1.0.1015 Example: my.custom-hostname Copy Installation variables agent_dir Directory where the agent stores files for cache, inventory, integrations, etc. YML option name Environment variable Type Default Version agent_dir NRIA_AGENT_DIR string Linux: /var/db/newrelic-infra Windows: C:\\Program Files\\NewRelic\\newrelic-infra\\ 1.0.2 plugin_dir Directory containing the configuration files of the integrations. Each integration has its own configuration file, named by default <integration_name>-config.yml, placed in a predefined location from which the agent loads on initialization. YML option name Environment variable Type Default Version plugin_dir NRIA_PLUGIN_DIR string Linux: etc/newrelic-infra/integrations.d/ Windows: \\Program Files\\NewRelic\\newrelic-infra\\inregrations.d 1.0.2 Important With secrets management, you can configure on-host integrations with New Relic Infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. custom_plugin_installation_dir Specifies a custom path to install integrations, which allows to install them outside the agent_dir. It has priority when the agent is looking for installed integrations. YML option name Environment variable Type Default Version custom_plugin_installation_dir NRIA_CUSTOM_PLUGIN_INSTALLATION_DIR string Empty 1.0.2 Integrations variables passthrough_environment A list of environment variables that will be passed to all integrations. If an integration already has an existing configuration option with the same name, then the environment variable takes precedence. YML option name Environment variable Type Default Version passthrough_environment NRIA_PASSTHROUGH_ENVIRONMENT [ ]string Empty 1.0.719 Example as a YAML attribute (inside the agent's configuration file, located by default in /etc/newrelic-infra.yml): passthrough_environment: - HOST - PORT Copy Example as an environment variable: NRIA_PASSTHROUGH_ENVIRONMENT=\"HOST,PORT\" Copy entityname_integrations_v2_update The agent enables loopback-address replacement on the entity name (and therefore key) automatically for version 3 of the integration protocol. If you are using version 2 of the protocol and want this behavior, enable the entityname_integrations_v2_update option. YML option name Environment variable Type Default Version entityname_integrations_v2_update NRIA_ENTITYNAME_INTEGRATIONS_V2_UPDATE boolean false 1.2.15 http_server_enabled By setting this configuration parameter to true the agent will open an HTTP port (by default, 8001) to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_enabled NRIA_HTTP_SERVER_ENABLED boolean false 1.0.818 http_server_host By setting this value, the agent will start listening on the HTTPServerPort to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_host NRIA_HTTP_SERVER_HOST string localhost 1.0.818 http_server_port Sets the port for the http server to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_port NRIA_HTTP_SERVER_PORT integer 8001 1.0.818 remove_entities_period Starts the process of deleting entities that haven't reported information during this interval. Valid time units: s (seconds), m (minutes), and h (hour). YML option name Environment variable Type Default Version remove_entities_period NRIA_REMOVE_ENTITIES_PERIOD string 48h 1.0.859 Example: 1h Copy Inventory variables offline_time_to_reset If the cached inventory becomes older than this value (for example, because the agent is offline), the agent automatically removes and recreates the delta store. YML option name Environment variable Type Default Version offline_time_to_reset NRIA_OFFLINE_TIME_TO_RESET string 24h 1.0.888 ignored_inventory The list of inventory paths ignored by the agent. YML option name Environment variable Type Default Version ignored_inventory NRIA_IGNORED_INVENTORY string [ ] Empty list 1.0.336 Example as a YAML attribute: ignored_inventory: - files/config/stuff.bar - files/config/stuff.foo Copy Example as an environment variable: NRIA_IGNORED_INVENTORY=\"files/config/stuff.bar,files/config/stuff.foo\" Copy Linux variables pid_file Location on Linux where the pid file of the agent process is created. It is used at startup to ensure that no other instances of the agent are running. YML option name Environment variable Type Default Version pid_file NRIA_PID_FILE string /var/run/newrelic-infra/newrelic-infra.pid 1.0.2 ignore_reclaimable When true, formulation of the host virtual memory considers SReclaimable as available memory; otherwise SReclaimable will be considered part of the used memory. YML option name Environment variable Type Default Version ignore_reclaimable NRIA_IGNORE_RECLAIMABLE boolean false 1.2.6 Logging variables log_file Defines the file path for the logs. The default installation creates a log directory and it sets this filepath value in the log_file configuration option for you. This log directory is different for each OS, as shown below. Change this configuration option to customize the file path for the logs. YML option name Environment variable Type Default Version log_file NRIA_LOG_FILE string See below * Default paths: Linux: If not defined, it logs only in the standard output. Windows, agent version 1.0.752 or lower: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy Windows, agent version 1.0.775 to 1.0.944: C:\\%APPDATA%\\Roaming\\New Relic\\newrelic-infra\\newrelic-infra.log Copy Windows, agent version 1.0.944 or higher: C:\\%ProgramData%\\New Relic\\newrelic-infra\\newrelic-infra.log Copy If the directory can't be created: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy log_format Defines the log output format. Available values: text: Plain text output, one line per log entry. json: JSON-formatted output, one line per log entry. YML option name Environment variable Type Default Version log_format NRIA_LOG_FORMAT string text 1.4.9 log_to_stdout By default all logs are displayed in both standard output and a log file. To disable logs in the standard output, set this configuration option to false. YML option name Environment variable Type Default Version log_to_stdout NRIA_LOG_TO_STDOUT boolean true 1.0.703 verbose When verbose is set to 0, verbose logging is off, but the agent still creates logs. Set this to 1 to create verbose logs to use in troubleshooting the agent; set verbose to 2 ito use the smart verbose mode logging. Smart verbose mode logs the last smart_verbose_mode_entry_limit debug messages when an error is logged. Setting verbose to 3 enables forwarding the agent logs to New Relic Logs. Important Verbose logging can generate a lot of data very quickly. Run the agent in verbose mode only for as long as necessary to reproduce your issue, then set verbose: 0 and restart your agent to disable verbose logging. Alternatively, you can set verbose: 2, which will enable smart verbose mode. YML option name Environment variable Type Default Version verbose NRIA_VERBOSE Integer (0, 1, 2 or 3) 0 1.9.0 when using smart mode (2) 1.11.4 when forwarding the New Relic Logs (3) smart_verbose_mode_entry_limit smart_verbose_mode_entry_limit refers to the number of previous debug messages that will be logged when an error is logged. For example, if the limit is set to 5, debug logs will be cached in memory until an error is logged, at which point the previous 5 debug messages will also be logged Important This configuration option is only used when verbose is set to 2 (Smart Verbose Mode enabled). YML option name Environment variable Type Default Version smart_verbose_mode_entry_limit NRIA_SMART_VERBOSE_MODE_ENTRY_LIMIT Integer 1000 1.9.0 Metrics variables custom_attributes Custom attributes are key-value pairs (similar to tags in other tools) used to annotate the data from the Infrastructure agent. You can use this metadata to build filter sets, group your results, and annotate your data. For example, you might indicate a machine's environment (staging or production), the service a machine hosts (login service, for example), or the team responsible for that machine. Tip The agent collects many details about your environment as part of its default attributes, including Amazon Elastic Compute Cloud (Amazon EC2) tags. YML option name Environment variable Type custom_attributes NRIA_CUSTOM_ATTRIBUTES map [ string]interface { } Use a list of custom attributes to annotate the data from this agent instance. Separate keys and values with colons :, as in KEY: VALUE, and separate each key-value pair with a line break. Keys can be any valid YAML except slashes /. Values can be any YAML string, including spaces. Example as a YAML attribute: custom_attributes: environment: production service: login service team: alpha-team Copy Example as an environment variable: NRIA_CUSTOM_ATTRIBUTES='{\"customAttribute_1\":\"SOME_ATTRIBUTE\",\"customAttribute_2\": \"SOME_ATTRIBUTE_2\"}' Copy NRQL example filtering by custom attribute: FROM SystemSample SELECT * WHERE environment = 'production' Copy enable_process_metrics Important Requires infrastructure agent version 1.12.0 or higher. Accounts created before July 20, 2020 and/or infrastructure agents installed using the new Guided Install have this variable enabled by default. Enables the sending of process metrics to New Relic. By default, the infrastructure agent doesn't send data about the operating system's processes. The agent still collects such data, unless metrics_process_sample_rate is set to -1. To report metric data about all the operating system's processes, set enable_process_metrics to true. To disable, set to false. Sending all process data could increase the volume of data sent to New Relic. To fine-tune which processes you want to monitor, configure include_matching_metrics. By default, processes using low memory are excluded from being sampled. For more information, see disable-zero-mem-process-filter. YML option name Environment variable Type Default Version enable_process_metrics NRIA_ENABLE_PROCESS_METRICS boolean false include_matching_metrics Important Currently, this setting only applies to an operating system's processes metrics. You can control how much data is sent to New Relic by configuring include_matching_metrics, which allows you to restrict the transmission of metric data based on the values of metric attributes. You include metric data by defining literal or partial values for any of the attributes of the metric. For example, you can choose to send the host.process.cpuPercent of all processes whose process.name match the ^java regular expression. In this example, we include process metrics using executable files and names: include_matching_metrics: # You can combine attributes from different metrics process.name: - regex “^java” # Include all processes starting with \"java\" process.executable: - “/usr/bin/python2” # Include the Python 2.x executable - regex “\\\\System32\\\\svchost” # Include all svchost executables Copy If you need to include command line arguments in any of the values, set strip_command_line to false (the infrastructure agents removes CLI arguments by default to prevent secrets from leaking). To configure include_matching_metrics as an environment variable for the Kubernetes integration, add it in the manifest inside the env: object: env: - name: NRIA_INCLUDE_MATCHING_METRICS value: | process.name: - regex \"^java\" process.executable: - \"/usr/bin/python2\" - regex \"\\\\System32\\\\svchost\" Copy Default YML option name Environment variable Type Default Version include_matching_metrics NRIA_INCLUDE_MATCHING_METRICS metric.attribute: - regex \"pattern\" - \"string\" - \"string-with-wildcard * \" network_interface_filters You can use the network interface filters configuration to hide unused or uninteresting network interfaces from the infrastructure agent. This helps reduce resource usage, work, and noise in your data. Important Environment variables are not supported for this configuration setting. The configuration uses a simple pattern-matching mechanism that can look for interfaces that start with a specific sequence of letters or numbers following either pattern: {name}[other characters], where you specify the name using the prefix option [number]{name}[other characters], where you specify the name using the index-1 option New Relic infrastructure implements a curated default list of filters, available for both Linux and Windows, that you can modify. YML option name Environment variable Type Default Version network_interface_filters not supported map [ string] [ ]string 1.0.220 Linux Default network interface filters for Linux: Network interfaces that start with dummy, lo, vmnet, sit, tun, tap, or veth Network interfaces that contain tun or tap The following example (added to your configuration file) overrides the default filters. This will ignore network interfaces that start with dummy or lo , or contain tun preceded by a sequence of numbers, and followed by other characters: network_interface_filters: prefix: - dummy - lo index-1: - tun Copy Windows Default network interface filters for Windows: Network interfaces that start with Loop, isatap, or Local The following example (added to your configuration file) overrides the default filters. This will ignore network interfaces that start with Loop: network_interface_filters: prefix: - Loop Copy disable_zero_mem_process_filter The ZeroRSSFilter excludes processes that are not using memory from being sampled. Disable the filter so that the agent includes these processes in the ProcessSample. YML option name Environment variable Type Default Version disable_zero_mem_process_filter NRIA_DISABLE_ZERO_MEM_PROCESS_FILTER boolean false 1.0.832 Plugins variables Tip You can quickly disable all the variables by setting DisableAllPlugins to true, and turn on only those options you need. disable_all_plugins To disable all the plugins, set this option to true. YML option name Environment variable Type Default Version disable_all_plugins NRIA_DISABLE_ALL_PLUGINS boolean false 1.2.1 cloud_security_group_refresh_sec Sampling period for the CloudSecurityGroups plugin, in seconds. The minimum value is 30. To disable it, set it to -1. Important This plugin is activated only if the agent is running in an AWS instance. YML option name Environment variable Type Default Version cloud_security_group_refresh_sec NRIA_CLOUD_SECURITY_GROUP_REFRESH_SEC int64 60 1.0.692 daemontools_interval_sec Sampling period for the Daemontoolsplugin, in seconds. The minimum value is a 10. To disable it, set it to -1. YML option name Environment variable Type Default Version daemontools_interval_sec NRIA_DAEMONTOOLS_INTERVAL_SEC int64 15 1.0.316 dpkg_interval_sec Sampling period for the Dpkg plugin, in seconds. The minimum value is 30. To disable it, set it to -1. If the parameter is not explicitly set in the config file, it can be disabled by setting DisableAllPlugins to true. Important This is only activated in root or privileged running modes and on Debian-based distributions. YML option name Environment variable Type Default Version dpkg_interval_sec NRIA_DPKG_INTERVAL_SEC int64 30 1.0.316 facter_interval_sec Sampling period for the Facter plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version facter_interval_sec NRIA_FACTER_INTERVAL_SEC int64 30 1.0.316 kernel_modules_refresh_sec Sampling period for the CloudSecurityGroups plugin, in seconds. The minimum value is 10. To disable it, set it to -1. Important kernel_modules_refresh_sec is only activated in root or privileged running modes. YML option name Environment variable Type Default Version kernel_modules_refresh_sec NRIA_KERNEL_MODULES_REFRESH_SEC int64 10 1.0.755 network_interface_interval_sec Sampling period for the NetworkInterface plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version network_interface_interval_sec NRIA_NETWORK_INTERFACE_INTERVAL_SEC int64 60 1.0.329 rpm_interval_sec Sampling period for the Rpm plugin, in seconds. The minimum value is 30. To disable it, set it to -1. Important rpm_interval_sec is only activated when the agent runs in root or privileged modes for RedHat, RedHat AWS, or SUSE distributions. YML option name Environment variable Type Default Version rpm_interval_sec NRIA_RPM_INTERVAL_SEC int64 30 1.0.316 selinux_interval_sec Sampling period for the SELinux plugin, in seconds. The minimum value is 30. To disable it, set it to -1. This option is ignored if SelinuxEnableSemodule is set to false. For more information, see our troubleshooting doc on disabling the SELinux module. Important SELinux is only activated when the agent runs in root mode. YML option name Environment variable Type Default Version selinux_interval_sec NRIA_SELINUX_INTERVAL_SEC int64 30 1.0.316 sshd_config_refresh_sec Sampling period for the Sshd plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version sshd_config_refresh_sec NRIA_SSHD_CONFIG_REFRESH_SEC int64 15 1.0.755 supervisor_interval_sec Sampling period for the Supervisor plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version supervisor_interval_sec NRIA_SUPERVISOR_INTERVAL_SEC int64 15 1.0.316 sysctl_interval_sec Sampling period for the Sysctl plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version sysctl_interval_sec NRIA_SYSCTL_INTERVAL_SEC int64 60 1.0.316 systemd_interval_sec Sampling period for the Systemd plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version systemd_interval_sec NRIA_SYSTEMD_INTERVAL_SEC int64 30 1.0.316 sysvinit_interval_sec Sampling period for the sysv plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version sysvinit_interval_sec NRIA_SYSVINIT_INTERVAL_SEC int64 30 1.0.316 upstart_interval_sec Sampling period for the Upstart plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version upstart_interval_sec NRIA_UPSTART_INTERVAL_SEC int64 30 1.0.316 users_refresh_sec Sampling period for the Users plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version users_refresh_sec NRIA_USERS_REFRESH_SEC int64 30 1.0.755 supervisor_rpc_sock Location of the supervisor socket. YML option name Environment variable Type Default Version supervisor_rpc_sock NRIA_SUPERVISOR_RPC_SOCK string /var/run/supervisor.sock 1.0.2 facter_home_dir Sets the HOME environment variable for Puppet's Facter. If not defined, it defaults to the current user's home directory. YML option name Environment variable Type Default Version facter_home_dir NRIA_FACTER_HOME_DIR string 1.1.7 Proxy variables For infrastructure agent version 1.3.1 or higher, the precedence of the proxy configuration settings is: NRIA_PROXY proxy HTTP_PROXY HTTPS_PROXY proxy Your system may have firewall rules that require the agent to use a proxy to communicate with New Relic. If so, set the proxy URL in the form https://user:password@hostname:port. It can be HTTP or HTTPS. YML option name Environment variable Type Default Version proxy NRIA_PROXY string Empty 1.0.308 Example: https://proxy_user.access_10@proxy_01:1080 Copy ignore_system_proxy When set to true, the HTTPS_PROXY and HTTP_PROXY environment variables are ignored. This is useful when the agent needs to connect directly to the metrics collector and skip the existing system proxy. YML option name Environment variable Type Default Version ignore_system_proxy NRIA_IGNORE_SYSTEM_PROXY boolean false 1.0.1002 ca_bundle_dir If the HTTPS_PROXY option references to a proxy with self-signed certificates, this option specifies the path to the the directory where the proxy certificate is available. The certificates in the directory must end with the .pem extension. YML option name Environment variable Type Default Version ca_bundle_dir NRIA_CA_BUNDLE_DIR string 1.0.296 ca_bundle_file If the HTTPS_PROXY option references to a proxy with self-signed certificates, this option specifies the path to the certificate file. YML option name Environment variable Type Default Version ca_bundle_file NRIA_CA_BUNDLE_FILE string 1.0.296 proxy_validate_certificates If set to true, when the proxy is configured to use an HTTPS connection, it will only work: If the HTTPS proxy has certificates from a valid Certificate Authority. If the ca_bundle_file or ca_bundle_dir configuration properties contain the HTTPS proxy certificates. YML option name Environment variable Type Default Version proxy_validate_certificates NRIA_PROXY_VALIDATE_CERTIFICATES boolean false 1.3.0 proxy_config_plugin Sends the following proxy configuration information as inventory: HTTPS_PROXY HTTP_PROXY proxy ca_bundle_dir ca_bundle_file ignore_system_proxy proxy_validate_certificates YML option name Environment variable Type Default Version proxy_config_plugin NRIA_PROXY_CONFIG_PLUGIN boolean true 1.3.0 If you are having problems with proxy configuration, see Proxy troubleshooting. Samples variables metrics_network_sample_rate Sample rate of network samples, in seconds. Minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_network_sample_rate NRIA_METRICS_NETWORK_SAMPLE_RATE integer 10 1.0.308 metrics_process_sample_rate Sample rate of process samples, in seconds. Minimum value is 20. To disable process samples entirely, set metrics_process_sample_rate to -1. YML option name Environment variable Type Default Version metrics_process_sample_rate NRIA_METRICS_PROCESS_SAMPLE_RATE integer 20 1.0.308 metrics_storage_sample_rate Sample rate of storage samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_storage_sample_rate NRIA_METRICS_STORAGE_SAMPLE_RATE integer 5 1.0.308 metrics_system_sample_rate Sample rate of system samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_system_sample_rate NRIA_METRICS_SYSTEM_SAMPLE_RATE integer 5 1.0.308 metrics_nfs_sample_rate Sample rate of NFS storage samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_nfs_sample_rate NRIA_METRICS_NFS_SAMPLE_RATE integer 20 1.5.40 detailed_nfs Detailed NFS metrics. When enabled, the agent will provide a complete list of NFS metrics. YML option name Environment variable Type Default Version detailed_nfs NRIA_DETAILED_NFS boolean false 1.5.40 Security variables selinux_enable_semodule Get versions of policy modules installed using SEModule. If disabled, the SELinux plugin will only retrieve the status using SEStatus. YML option name Environment variable Type Default Version selinux_enable_semodule NRIA_SELINUX_ENABLE_SEMODULE boolean true 1.0.864 strip_command_line When true, the agent removes the command arguments from the commandLine attribute of the ProcessSample. Tip This is a security measure to prevent leaking sensitive information. YML option name Environment variable Type Default Version strip_command_line NRIA_STRIP_COMMAND_LINE boolean true 1.0.149 Windows variables windows_services_refresh_sec Sampling period for the Windows services plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version windows_services_refresh_sec NRIA_WINDOWS_SERVICES_REFRESH_SEC int64 30 1.0.755 windows_updates_refresh_sec Sampling period for the Windows updates plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version windows_updates_refresh_sec NRIA_WINDOWS_UPDATES_REFRESH_SEC int64 60 1.0.755 app_data_dir Defines the path to store data in a different path than the program files directory: %AppDir%/data: Used for storing the delta data %AppDir%/user_data: External directory for user-generated JSON files %AppDir%/newrelic-infra.log: If log file config option is not defined, then we use this directory path as default. YML option name Environment variable Type Default Version app_data_dir NRIA_APP_DATA_DIR string Windows: env(ProgramData)\\New Relic\\newrelic-infra Linux: Not applicable 1.0.755 enable_win_update_plugin Enables the Windows updates plugin, which retrieves the lists of hotfixes that are installed on the host. YML option name Environment variable Type Default Version enable_win_update_plugin NRIA_ENABLE_WIN_UPDATE_PLUGIN boolean false 1.0.274 legacy_storage_sampler If true, the agent will be forced to use Windows WMI (the agent's legacy method to grab metrics for Windows; for example, StorageSampler) and will disable the new method (which uses the PDH library). YML option name Environment variable Type Default Version legacy_storage_sampler NRIA_LEGACY_STORAGE_SAMPLER boolean 1.0.1051 win_process_priority_class This configuration option allows to increase the newrelic-infra.exe process priority to any of the following values: Normal Idle High RealTime BelowNormal AboveNormal YML option name Environment variable Type Default Version win_process_priority_class NRIA_WIN_PROCESS_PRIORITY_CLASS string 1.0.989 Example: Normal Copy win_removable_drives Enables the Windows agent to report drives A: and B: when they are mapped as removable drives. YML option name Environment variable Type Default Version win_removable_drives NRIA_WIN_REMOVABLE_DRIVES boolean true 1.3.1 What's next? You can also: Further understand the configuration of the agent. Create a configuration file using our template. See how you can manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 240.49,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> <em>configuration</em> settings ",
        "sections": "<em>Infrastructure</em> <em>agent</em> <em>configuration</em> settings",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> <em>agent</em> has a large set of <em>configuration</em> settings to fine-tune its behavior. Here we: List all the <em>configuration</em> options (in both their YAML and the environment variable names). Explain what the settings do and when to use them. Give the variable type and default value (if any"
      },
      "id": "603ea542196a67a38aa83dd8"
    },
    {
      "sections": [
        "Zip assisted install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Zip assisted install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "fcadbeed626401863b6b16e5c52e9a472a5ac13e",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-04-16T02:59:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. Install the agent Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Once it's unpacked, access and edit the installation PowerShell script installer.ps1. Update your license key. Optional: Update any other parameters. Execute installer.ps1 with admin rights. Configure your installation Important Make sure any custom folder defined in the installation settings has permissions limitations properly defined. The infrastructure agent might execute any integration defined in the NRIA_PLUGIN_DIR directory with Administrator permissions. You can configure the following parameters during the assisted install for Windows: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: C:\\Program Files\\New Relic\\newrelic-infra Copy NRIA_APP_DATA_DIR This configures the data directory to store inventory and other agent files. Default: C:\\%ProgramData%\\New Relic\\newrelic-infra Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The New Relic license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy NRIA_OVERWRITE By default and for security reasons, Windows does not install a service if there's another service with the same name already installed. To bypass this check, make sure this setting NRIA_OVERWRITE is TRUE. Default: TRUE Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: C:\\Program Files\\NewRelic\\newrelic-infra\\inregrations.d Copy NRIA_SERVICE_NAME This provides the name for the Windows service. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 239.56793,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "sections": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. <em>Install</em> the <em>agent</em> Important As of version"
      },
      "id": "603ea7af196a67dab0a83d9d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 230.04066,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 1: Enable EC2 to <em>install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "&#x27;s an overview of the process: Enable Amazon EC2 to <em>install</em> our <em>infrastructure</em> <em>agent</em> on your ECS clusters. Enable monitoring of services using a service-specific <em>configuration</em> file. Step 1: Enable EC2 to <em>install</em> the <em>infrastructure</em> <em>agent</em> First, you must enable Amazon EC2 to <em>install</em> our <em>infrastructure</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/configuration/configure-infrastructure-agent": [
    {
      "sections": [
        "Infrastructure agent configuration settings",
        "Important",
        "Agent variables",
        "license_key (REQUIRED)",
        "fedramp",
        "max_procs",
        "payload_compression_level",
        "startup_connection_retries",
        "startup_connection_retry_time",
        "startup_connection_timeout",
        "Cloud variables",
        "cloud_max_retry_count",
        "cloud_metadata_expiry_sec",
        "cloud_retry_backoff_sec",
        "disable_cloud_instance_id",
        "disable_cloud_metadata",
        "Debug variables",
        "trace",
        "Docker variables",
        "container_cache_metadata_limit",
        "docker_api_version",
        "File system variables",
        "custom_supported_file_systems",
        "file_devices_ignored",
        "Hostname variables",
        "display_name",
        "dns_hostname_resolution",
        "override_hostname",
        "override_hostname_short",
        "Installation variables",
        "agent_dir",
        "plugin_dir",
        "custom_plugin_installation_dir",
        "Integrations variables",
        "passthrough_environment",
        "entityname_integrations_v2_update",
        "http_server_enabled",
        "http_server_host",
        "http_server_port",
        "remove_entities_period",
        "Inventory variables",
        "offline_time_to_reset",
        "ignored_inventory",
        "Linux variables",
        "pid_file",
        "ignore_reclaimable",
        "Logging variables",
        "log_file",
        "log_format",
        "log_to_stdout",
        "verbose",
        "smart_verbose_mode_entry_limit",
        "Metrics variables",
        "custom_attributes",
        "Tip",
        "enable_process_metrics",
        "include_matching_metrics",
        "network_interface_filters",
        "Linux",
        "Windows",
        "disable_zero_mem_process_filter",
        "Plugins variables",
        "disable_all_plugins",
        "cloud_security_group_refresh_sec",
        "daemontools_interval_sec",
        "dpkg_interval_sec",
        "facter_interval_sec",
        "kernel_modules_refresh_sec",
        "network_interface_interval_sec",
        "rpm_interval_sec",
        "selinux_interval_sec",
        "sshd_config_refresh_sec",
        "supervisor_interval_sec",
        "sysctl_interval_sec",
        "systemd_interval_sec",
        "sysvinit_interval_sec",
        "upstart_interval_sec",
        "users_refresh_sec",
        "supervisor_rpc_sock",
        "facter_home_dir",
        "Proxy variables",
        "proxy",
        "ignore_system_proxy",
        "ca_bundle_dir",
        "ca_bundle_file",
        "proxy_validate_certificates",
        "proxy_config_plugin",
        "Samples variables",
        "metrics_network_sample_rate",
        "metrics_process_sample_rate",
        "metrics_storage_sample_rate",
        "metrics_system_sample_rate",
        "metrics_nfs_sample_rate",
        "detailed_nfs",
        "Security variables",
        "selinux_enable_semodule",
        "strip_command_line",
        "Windows variables",
        "windows_services_refresh_sec",
        "windows_updates_refresh_sec",
        "app_data_dir",
        "enable_win_update_plugin",
        "legacy_storage_sampler",
        "win_process_priority_class",
        "win_removable_drives",
        "What's next?"
      ],
      "title": "Infrastructure agent configuration settings ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Configuration"
      ],
      "external_id": "7d016d3f5af6ed1615904578f0ed8bce02aa335b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/configuration/infrastructure-agent-configuration-settings/",
      "published_at": "2021-05-05T02:35:20Z",
      "updated_at": "2021-03-30T19:18:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent has a large set of configuration settings to fine-tune its behavior. Here we: List all the configuration options (in both their YAML and the environment variable names). Explain what the settings do and when to use them. Give the variable type and default value (if any). List the minimum required agent version as applicable. For an example of how all these variables can be used, see our sample configuration template. Important license_key is the only required setting. Agent variables license_key (REQUIRED) Specifies the license key for your New Relic account. The agent uses this key to associate your server's metrics with your New Relic account. This setting is created as part of the standard installation process. YML option name Environment variable Type Default Version license_key NRIA_LICENSE_KEY string Example: license_key: 1234567890abcdefghijklmnopqrstuvwxyz1234 Copy fedramp Specifies whether Fedramp endpoints should be used. YML option name Environment variable Type Default Version fedramp NRIA_FEDRAMP boolean false 1.15.0 max_procs Specifies the number of logical processors available to the agent. Increasing this value helps to distribute the load between different cores. If set to -1, the agent will try to read the environment variable GOMAXPROCS. If this variable is not set, the default value will be the total number of cores available in the host. YML option name Environment variable Type Default Version max_procs NRIA_MAX_PROCS integer 1 1.0.1002 payload_compression_level Since version 1.0.804 or higher, data sent from the agent is compressed by default. To disable payload compression, set payload_compression_level to 0. Important Recommendation: Do not change this setting. YML option name Environment variable Type Default Version payload_compression_level NRIA_PAYLOAD_COMPRESSION_LEVEL integer 6 1.0.804 startup_connection_retries Number of times the agent will retry the request to check New Relic's platform availability at startup before throwing an error. If set to a negative value, the agent will keep checking the connection until it succeeds. YML option name Environment variable Type Default Version startup_connection_retries NRIA_STARTUP_CONNECTION_RETRIES integer 6 1.0.936 startup_connection_retry_time After a request has timed out, time the agent waits to retry a request to check New Relic's platform availability at startup. YML option name Environment variable Type Default Version startup_connection_retry_time NRIA_STARTUP_CONNECTION_RETRY_TIME string 5s 1.0.936 - 1.2.30 startup_connection_timeout Time the agent waits until a request to check New Relic's platform availability at startup is considered timed out. YML option name Environment variable Type Default Version startup_connection_timeout NRIA_STARTUP_CONNECTION_TIMEOUT string 10s 1.0.936 Cloud variables If the agent is running in a cloud instance, the agent will try to detect the cloud type and fetch metadata. cloud_max_retry_count Sets the number of times the agent retries to connect in case that cloud detection fails. If cloud detection fails during the initialization of the agent, the agent will retry after waiting for CloudRetryBackOffSec seconds. YML option name Environment variable Type Default Version cloud_max_retry_count NRIA_CLOUD_MAX_RETRY_COUNT integer 10 1.2.6 cloud_metadata_expiry_sec Sets the interval of time the agent will wait until discarding the metadata, in seconds. After this period metadata expires and the agent will fetch it again. YML option name Environment variable Type Default Version cloud_metadata_expiry_sec NRIA_CLOUD_METADATA_EXPIRY_SEC integer 300 1.2.6 cloud_retry_backoff_sec Sets the interval of time the agent waits between cloud detection retries in case that cloud detection failed, in seconds. If cloud detection fails during the initialization of the agent, it will retry for CloudMaxRetryCount times. YML option name Environment variable Type Default Version cloud_retry_backoff_sec NRIA_CLOUD_RETRY_BACKOFF_SEC integer 60 1.2.6 disable_cloud_instance_id Similar to DisableCloudMetadata, but it disables the collection of cloud metadata only for the host alias plugin. YML option name Environment variable Type Default Version disable_cloud_instance_id NRIA_DISABLE_CLOUD_INSTANCE_ID boolean false 1.0.220 disable_cloud_metadata Disables the collection of cloud metadata. YML option name Environment variable Type Default Version disable_cloud_metadata NRIA_DISABLE_CLOUD_METADATA boolean false 1.0.690 Debug variables trace Enables traces to be printed in the logs for the given features. Traces are additional log entries used for debugging purposes of specific features, they are only shown when the verbose log config option is enabled. Currently two features have traces defined: connect and attributes. YML option name Environment variable Type Default Version trace NRIA_TRACE [ ]string [] 1.0.1015 Example as a YAML attribute: trace: [connect, attributes] Copy Example as an environment variable: NRIA_TRACE='connect, attributes' Copy Docker variables container_cache_metadata_limit Time, in seconds, before the cached containers metadata expires and the agent needs to fetch them again. YML option name Environment variable Type Default Version container_cache_metadata_limit NRIA_CONTAINER_CACHE_METADATA_LIMIT integer 60 1.0.801 docker_api_version Specifies the Docker client API version. YML option name Environment variable Type Default Version docker_api_version NRIA_DOCKER_API_VERSION string 1.24 1.1.4 File system variables custom_supported_file_systems List of the types of file systems that the agent supports. This value needs to be a subset of the default list, and items that are not in the default list will be discarded. YML option name Environment variable Type Default Version custom_supported_file_systems NRIA_CUSTOM_SUPPORTED_FILESYSTEMS [ ]string Linux: [\"xfs\", \"btrfs\", \"ext\", \"ext2\", \"ext3\", \"ext4\", \"hfs\", \"vxfs\"] Windows: [\"NTFS\", \"ReFS\"] 1.0.220 file_devices_ignored List of storage devices to be ignored by the agent when gathering StorageSample data. YML option name Environment variable Type Default Version file_devices_ignored NRIA_FILE_DEVICES_IGNORED [ ]string 1.0.220 Example as a YAML attribute: file_devices_ignored: - sda1 - sda2 Copy Example as an environment variable: FILE_DEVICES_IGNORED=\"sda1,sda2\" Copy Hostname variables display_name Overrides the auto-generated hostname for reporting. This is useful when you have multiple hosts with the same name, since our infrastructure monitoring uses the hostname as the unique identifier for each host. Keep in mind this value is also used for the loopback address replacement on entity names. For more information, see our documentation on how entity name resolution works. YML option name Environment variable Type Default Version display_name NRIA_DISPLAY_NAME string empty 1.0.266 Example: display_name: teslaOne Copy dns_hostname_resolution When true, the full hostname is resolved by performing a reverse lookup of the host's address. Otherwise, it will be retrieved with the hostname command on Linux and from the TCP/IP parameters of the registry on Windows. YML option name Environment variable Type Default Version dns_hostname_resolution NRIA_DNS_HOSTNAME_RESOLUTION boolean true 1.2.6 override_hostname When set, this is the value that will be reported for the full hostname; otherwise, the agent will perform the normal lookup behavior. YML option name Environment variable Type Default Version override_hostname NRIA_OVERRIDE_HOSTNAME string 1.0.1015 Example: my.custom-hostname.co.org Copy override_hostname_short When set, this is the value that will be reported for the hostname; otherwise, the agent will perform the normal lookup behavior. YML option name Environment variable Type Default Version override_hostname_short NRIA_OVERRIDE_HOSTNAME_SHORT string 1.0.1015 Example: my.custom-hostname Copy Installation variables agent_dir Directory where the agent stores files for cache, inventory, integrations, etc. YML option name Environment variable Type Default Version agent_dir NRIA_AGENT_DIR string Linux: /var/db/newrelic-infra Windows: C:\\Program Files\\NewRelic\\newrelic-infra\\ 1.0.2 plugin_dir Directory containing the configuration files of the integrations. Each integration has its own configuration file, named by default <integration_name>-config.yml, placed in a predefined location from which the agent loads on initialization. YML option name Environment variable Type Default Version plugin_dir NRIA_PLUGIN_DIR string Linux: etc/newrelic-infra/integrations.d/ Windows: \\Program Files\\NewRelic\\newrelic-infra\\inregrations.d 1.0.2 Important With secrets management, you can configure on-host integrations with New Relic Infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. custom_plugin_installation_dir Specifies a custom path to install integrations, which allows to install them outside the agent_dir. It has priority when the agent is looking for installed integrations. YML option name Environment variable Type Default Version custom_plugin_installation_dir NRIA_CUSTOM_PLUGIN_INSTALLATION_DIR string Empty 1.0.2 Integrations variables passthrough_environment A list of environment variables that will be passed to all integrations. If an integration already has an existing configuration option with the same name, then the environment variable takes precedence. YML option name Environment variable Type Default Version passthrough_environment NRIA_PASSTHROUGH_ENVIRONMENT [ ]string Empty 1.0.719 Example as a YAML attribute (inside the agent's configuration file, located by default in /etc/newrelic-infra.yml): passthrough_environment: - HOST - PORT Copy Example as an environment variable: NRIA_PASSTHROUGH_ENVIRONMENT=\"HOST,PORT\" Copy entityname_integrations_v2_update The agent enables loopback-address replacement on the entity name (and therefore key) automatically for version 3 of the integration protocol. If you are using version 2 of the protocol and want this behavior, enable the entityname_integrations_v2_update option. YML option name Environment variable Type Default Version entityname_integrations_v2_update NRIA_ENTITYNAME_INTEGRATIONS_V2_UPDATE boolean false 1.2.15 http_server_enabled By setting this configuration parameter to true the agent will open an HTTP port (by default, 8001) to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_enabled NRIA_HTTP_SERVER_ENABLED boolean false 1.0.818 http_server_host By setting this value, the agent will start listening on the HTTPServerPort to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_host NRIA_HTTP_SERVER_HOST string localhost 1.0.818 http_server_port Sets the port for the http server to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_port NRIA_HTTP_SERVER_PORT integer 8001 1.0.818 remove_entities_period Starts the process of deleting entities that haven't reported information during this interval. Valid time units: s (seconds), m (minutes), and h (hour). YML option name Environment variable Type Default Version remove_entities_period NRIA_REMOVE_ENTITIES_PERIOD string 48h 1.0.859 Example: 1h Copy Inventory variables offline_time_to_reset If the cached inventory becomes older than this value (for example, because the agent is offline), the agent automatically removes and recreates the delta store. YML option name Environment variable Type Default Version offline_time_to_reset NRIA_OFFLINE_TIME_TO_RESET string 24h 1.0.888 ignored_inventory The list of inventory paths ignored by the agent. YML option name Environment variable Type Default Version ignored_inventory NRIA_IGNORED_INVENTORY string [ ] Empty list 1.0.336 Example as a YAML attribute: ignored_inventory: - files/config/stuff.bar - files/config/stuff.foo Copy Example as an environment variable: NRIA_IGNORED_INVENTORY=\"files/config/stuff.bar,files/config/stuff.foo\" Copy Linux variables pid_file Location on Linux where the pid file of the agent process is created. It is used at startup to ensure that no other instances of the agent are running. YML option name Environment variable Type Default Version pid_file NRIA_PID_FILE string /var/run/newrelic-infra/newrelic-infra.pid 1.0.2 ignore_reclaimable When true, formulation of the host virtual memory considers SReclaimable as available memory; otherwise SReclaimable will be considered part of the used memory. YML option name Environment variable Type Default Version ignore_reclaimable NRIA_IGNORE_RECLAIMABLE boolean false 1.2.6 Logging variables log_file Defines the file path for the logs. The default installation creates a log directory and it sets this filepath value in the log_file configuration option for you. This log directory is different for each OS, as shown below. Change this configuration option to customize the file path for the logs. YML option name Environment variable Type Default Version log_file NRIA_LOG_FILE string See below * Default paths: Linux: If not defined, it logs only in the standard output. Windows, agent version 1.0.752 or lower: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy Windows, agent version 1.0.775 to 1.0.944: C:\\%APPDATA%\\Roaming\\New Relic\\newrelic-infra\\newrelic-infra.log Copy Windows, agent version 1.0.944 or higher: C:\\%ProgramData%\\New Relic\\newrelic-infra\\newrelic-infra.log Copy If the directory can't be created: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy log_format Defines the log output format. Available values: text: Plain text output, one line per log entry. json: JSON-formatted output, one line per log entry. YML option name Environment variable Type Default Version log_format NRIA_LOG_FORMAT string text 1.4.9 log_to_stdout By default all logs are displayed in both standard output and a log file. To disable logs in the standard output, set this configuration option to false. YML option name Environment variable Type Default Version log_to_stdout NRIA_LOG_TO_STDOUT boolean true 1.0.703 verbose When verbose is set to 0, verbose logging is off, but the agent still creates logs. Set this to 1 to create verbose logs to use in troubleshooting the agent; set verbose to 2 ito use the smart verbose mode logging. Smart verbose mode logs the last smart_verbose_mode_entry_limit debug messages when an error is logged. Setting verbose to 3 enables forwarding the agent logs to New Relic Logs. Important Verbose logging can generate a lot of data very quickly. Run the agent in verbose mode only for as long as necessary to reproduce your issue, then set verbose: 0 and restart your agent to disable verbose logging. Alternatively, you can set verbose: 2, which will enable smart verbose mode. YML option name Environment variable Type Default Version verbose NRIA_VERBOSE Integer (0, 1, 2 or 3) 0 1.9.0 when using smart mode (2) 1.11.4 when forwarding the New Relic Logs (3) smart_verbose_mode_entry_limit smart_verbose_mode_entry_limit refers to the number of previous debug messages that will be logged when an error is logged. For example, if the limit is set to 5, debug logs will be cached in memory until an error is logged, at which point the previous 5 debug messages will also be logged Important This configuration option is only used when verbose is set to 2 (Smart Verbose Mode enabled). YML option name Environment variable Type Default Version smart_verbose_mode_entry_limit NRIA_SMART_VERBOSE_MODE_ENTRY_LIMIT Integer 1000 1.9.0 Metrics variables custom_attributes Custom attributes are key-value pairs (similar to tags in other tools) used to annotate the data from the Infrastructure agent. You can use this metadata to build filter sets, group your results, and annotate your data. For example, you might indicate a machine's environment (staging or production), the service a machine hosts (login service, for example), or the team responsible for that machine. Tip The agent collects many details about your environment as part of its default attributes, including Amazon Elastic Compute Cloud (Amazon EC2) tags. YML option name Environment variable Type custom_attributes NRIA_CUSTOM_ATTRIBUTES map [ string]interface { } Use a list of custom attributes to annotate the data from this agent instance. Separate keys and values with colons :, as in KEY: VALUE, and separate each key-value pair with a line break. Keys can be any valid YAML except slashes /. Values can be any YAML string, including spaces. Example as a YAML attribute: custom_attributes: environment: production service: login service team: alpha-team Copy Example as an environment variable: NRIA_CUSTOM_ATTRIBUTES='{\"customAttribute_1\":\"SOME_ATTRIBUTE\",\"customAttribute_2\": \"SOME_ATTRIBUTE_2\"}' Copy NRQL example filtering by custom attribute: FROM SystemSample SELECT * WHERE environment = 'production' Copy enable_process_metrics Important Requires infrastructure agent version 1.12.0 or higher. Accounts created before July 20, 2020 and/or infrastructure agents installed using the new Guided Install have this variable enabled by default. Enables the sending of process metrics to New Relic. By default, the infrastructure agent doesn't send data about the operating system's processes. The agent still collects such data, unless metrics_process_sample_rate is set to -1. To report metric data about all the operating system's processes, set enable_process_metrics to true. To disable, set to false. Sending all process data could increase the volume of data sent to New Relic. To fine-tune which processes you want to monitor, configure include_matching_metrics. By default, processes using low memory are excluded from being sampled. For more information, see disable-zero-mem-process-filter. YML option name Environment variable Type Default Version enable_process_metrics NRIA_ENABLE_PROCESS_METRICS boolean false include_matching_metrics Important Currently, this setting only applies to an operating system's processes metrics. You can control how much data is sent to New Relic by configuring include_matching_metrics, which allows you to restrict the transmission of metric data based on the values of metric attributes. You include metric data by defining literal or partial values for any of the attributes of the metric. For example, you can choose to send the host.process.cpuPercent of all processes whose process.name match the ^java regular expression. In this example, we include process metrics using executable files and names: include_matching_metrics: # You can combine attributes from different metrics process.name: - regex “^java” # Include all processes starting with \"java\" process.executable: - “/usr/bin/python2” # Include the Python 2.x executable - regex “\\\\System32\\\\svchost” # Include all svchost executables Copy If you need to include command line arguments in any of the values, set strip_command_line to false (the infrastructure agents removes CLI arguments by default to prevent secrets from leaking). To configure include_matching_metrics as an environment variable for the Kubernetes integration, add it in the manifest inside the env: object: env: - name: NRIA_INCLUDE_MATCHING_METRICS value: | process.name: - regex \"^java\" process.executable: - \"/usr/bin/python2\" - regex \"\\\\System32\\\\svchost\" Copy Default YML option name Environment variable Type Default Version include_matching_metrics NRIA_INCLUDE_MATCHING_METRICS metric.attribute: - regex \"pattern\" - \"string\" - \"string-with-wildcard * \" network_interface_filters You can use the network interface filters configuration to hide unused or uninteresting network interfaces from the infrastructure agent. This helps reduce resource usage, work, and noise in your data. Important Environment variables are not supported for this configuration setting. The configuration uses a simple pattern-matching mechanism that can look for interfaces that start with a specific sequence of letters or numbers following either pattern: {name}[other characters], where you specify the name using the prefix option [number]{name}[other characters], where you specify the name using the index-1 option New Relic infrastructure implements a curated default list of filters, available for both Linux and Windows, that you can modify. YML option name Environment variable Type Default Version network_interface_filters not supported map [ string] [ ]string 1.0.220 Linux Default network interface filters for Linux: Network interfaces that start with dummy, lo, vmnet, sit, tun, tap, or veth Network interfaces that contain tun or tap The following example (added to your configuration file) overrides the default filters. This will ignore network interfaces that start with dummy or lo , or contain tun preceded by a sequence of numbers, and followed by other characters: network_interface_filters: prefix: - dummy - lo index-1: - tun Copy Windows Default network interface filters for Windows: Network interfaces that start with Loop, isatap, or Local The following example (added to your configuration file) overrides the default filters. This will ignore network interfaces that start with Loop: network_interface_filters: prefix: - Loop Copy disable_zero_mem_process_filter The ZeroRSSFilter excludes processes that are not using memory from being sampled. Disable the filter so that the agent includes these processes in the ProcessSample. YML option name Environment variable Type Default Version disable_zero_mem_process_filter NRIA_DISABLE_ZERO_MEM_PROCESS_FILTER boolean false 1.0.832 Plugins variables Tip You can quickly disable all the variables by setting DisableAllPlugins to true, and turn on only those options you need. disable_all_plugins To disable all the plugins, set this option to true. YML option name Environment variable Type Default Version disable_all_plugins NRIA_DISABLE_ALL_PLUGINS boolean false 1.2.1 cloud_security_group_refresh_sec Sampling period for the CloudSecurityGroups plugin, in seconds. The minimum value is 30. To disable it, set it to -1. Important This plugin is activated only if the agent is running in an AWS instance. YML option name Environment variable Type Default Version cloud_security_group_refresh_sec NRIA_CLOUD_SECURITY_GROUP_REFRESH_SEC int64 60 1.0.692 daemontools_interval_sec Sampling period for the Daemontoolsplugin, in seconds. The minimum value is a 10. To disable it, set it to -1. YML option name Environment variable Type Default Version daemontools_interval_sec NRIA_DAEMONTOOLS_INTERVAL_SEC int64 15 1.0.316 dpkg_interval_sec Sampling period for the Dpkg plugin, in seconds. The minimum value is 30. To disable it, set it to -1. If the parameter is not explicitly set in the config file, it can be disabled by setting DisableAllPlugins to true. Important This is only activated in root or privileged running modes and on Debian-based distributions. YML option name Environment variable Type Default Version dpkg_interval_sec NRIA_DPKG_INTERVAL_SEC int64 30 1.0.316 facter_interval_sec Sampling period for the Facter plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version facter_interval_sec NRIA_FACTER_INTERVAL_SEC int64 30 1.0.316 kernel_modules_refresh_sec Sampling period for the CloudSecurityGroups plugin, in seconds. The minimum value is 10. To disable it, set it to -1. Important kernel_modules_refresh_sec is only activated in root or privileged running modes. YML option name Environment variable Type Default Version kernel_modules_refresh_sec NRIA_KERNEL_MODULES_REFRESH_SEC int64 10 1.0.755 network_interface_interval_sec Sampling period for the NetworkInterface plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version network_interface_interval_sec NRIA_NETWORK_INTERFACE_INTERVAL_SEC int64 60 1.0.329 rpm_interval_sec Sampling period for the Rpm plugin, in seconds. The minimum value is 30. To disable it, set it to -1. Important rpm_interval_sec is only activated when the agent runs in root or privileged modes for RedHat, RedHat AWS, or SUSE distributions. YML option name Environment variable Type Default Version rpm_interval_sec NRIA_RPM_INTERVAL_SEC int64 30 1.0.316 selinux_interval_sec Sampling period for the SELinux plugin, in seconds. The minimum value is 30. To disable it, set it to -1. This option is ignored if SelinuxEnableSemodule is set to false. For more information, see our troubleshooting doc on disabling the SELinux module. Important SELinux is only activated when the agent runs in root mode. YML option name Environment variable Type Default Version selinux_interval_sec NRIA_SELINUX_INTERVAL_SEC int64 30 1.0.316 sshd_config_refresh_sec Sampling period for the Sshd plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version sshd_config_refresh_sec NRIA_SSHD_CONFIG_REFRESH_SEC int64 15 1.0.755 supervisor_interval_sec Sampling period for the Supervisor plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version supervisor_interval_sec NRIA_SUPERVISOR_INTERVAL_SEC int64 15 1.0.316 sysctl_interval_sec Sampling period for the Sysctl plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version sysctl_interval_sec NRIA_SYSCTL_INTERVAL_SEC int64 60 1.0.316 systemd_interval_sec Sampling period for the Systemd plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version systemd_interval_sec NRIA_SYSTEMD_INTERVAL_SEC int64 30 1.0.316 sysvinit_interval_sec Sampling period for the sysv plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version sysvinit_interval_sec NRIA_SYSVINIT_INTERVAL_SEC int64 30 1.0.316 upstart_interval_sec Sampling period for the Upstart plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version upstart_interval_sec NRIA_UPSTART_INTERVAL_SEC int64 30 1.0.316 users_refresh_sec Sampling period for the Users plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version users_refresh_sec NRIA_USERS_REFRESH_SEC int64 30 1.0.755 supervisor_rpc_sock Location of the supervisor socket. YML option name Environment variable Type Default Version supervisor_rpc_sock NRIA_SUPERVISOR_RPC_SOCK string /var/run/supervisor.sock 1.0.2 facter_home_dir Sets the HOME environment variable for Puppet's Facter. If not defined, it defaults to the current user's home directory. YML option name Environment variable Type Default Version facter_home_dir NRIA_FACTER_HOME_DIR string 1.1.7 Proxy variables For infrastructure agent version 1.3.1 or higher, the precedence of the proxy configuration settings is: NRIA_PROXY proxy HTTP_PROXY HTTPS_PROXY proxy Your system may have firewall rules that require the agent to use a proxy to communicate with New Relic. If so, set the proxy URL in the form https://user:password@hostname:port. It can be HTTP or HTTPS. YML option name Environment variable Type Default Version proxy NRIA_PROXY string Empty 1.0.308 Example: https://proxy_user.access_10@proxy_01:1080 Copy ignore_system_proxy When set to true, the HTTPS_PROXY and HTTP_PROXY environment variables are ignored. This is useful when the agent needs to connect directly to the metrics collector and skip the existing system proxy. YML option name Environment variable Type Default Version ignore_system_proxy NRIA_IGNORE_SYSTEM_PROXY boolean false 1.0.1002 ca_bundle_dir If the HTTPS_PROXY option references to a proxy with self-signed certificates, this option specifies the path to the the directory where the proxy certificate is available. The certificates in the directory must end with the .pem extension. YML option name Environment variable Type Default Version ca_bundle_dir NRIA_CA_BUNDLE_DIR string 1.0.296 ca_bundle_file If the HTTPS_PROXY option references to a proxy with self-signed certificates, this option specifies the path to the certificate file. YML option name Environment variable Type Default Version ca_bundle_file NRIA_CA_BUNDLE_FILE string 1.0.296 proxy_validate_certificates If set to true, when the proxy is configured to use an HTTPS connection, it will only work: If the HTTPS proxy has certificates from a valid Certificate Authority. If the ca_bundle_file or ca_bundle_dir configuration properties contain the HTTPS proxy certificates. YML option name Environment variable Type Default Version proxy_validate_certificates NRIA_PROXY_VALIDATE_CERTIFICATES boolean false 1.3.0 proxy_config_plugin Sends the following proxy configuration information as inventory: HTTPS_PROXY HTTP_PROXY proxy ca_bundle_dir ca_bundle_file ignore_system_proxy proxy_validate_certificates YML option name Environment variable Type Default Version proxy_config_plugin NRIA_PROXY_CONFIG_PLUGIN boolean true 1.3.0 If you are having problems with proxy configuration, see Proxy troubleshooting. Samples variables metrics_network_sample_rate Sample rate of network samples, in seconds. Minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_network_sample_rate NRIA_METRICS_NETWORK_SAMPLE_RATE integer 10 1.0.308 metrics_process_sample_rate Sample rate of process samples, in seconds. Minimum value is 20. To disable process samples entirely, set metrics_process_sample_rate to -1. YML option name Environment variable Type Default Version metrics_process_sample_rate NRIA_METRICS_PROCESS_SAMPLE_RATE integer 20 1.0.308 metrics_storage_sample_rate Sample rate of storage samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_storage_sample_rate NRIA_METRICS_STORAGE_SAMPLE_RATE integer 5 1.0.308 metrics_system_sample_rate Sample rate of system samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_system_sample_rate NRIA_METRICS_SYSTEM_SAMPLE_RATE integer 5 1.0.308 metrics_nfs_sample_rate Sample rate of NFS storage samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_nfs_sample_rate NRIA_METRICS_NFS_SAMPLE_RATE integer 20 1.5.40 detailed_nfs Detailed NFS metrics. When enabled, the agent will provide a complete list of NFS metrics. YML option name Environment variable Type Default Version detailed_nfs NRIA_DETAILED_NFS boolean false 1.5.40 Security variables selinux_enable_semodule Get versions of policy modules installed using SEModule. If disabled, the SELinux plugin will only retrieve the status using SEStatus. YML option name Environment variable Type Default Version selinux_enable_semodule NRIA_SELINUX_ENABLE_SEMODULE boolean true 1.0.864 strip_command_line When true, the agent removes the command arguments from the commandLine attribute of the ProcessSample. Tip This is a security measure to prevent leaking sensitive information. YML option name Environment variable Type Default Version strip_command_line NRIA_STRIP_COMMAND_LINE boolean true 1.0.149 Windows variables windows_services_refresh_sec Sampling period for the Windows services plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version windows_services_refresh_sec NRIA_WINDOWS_SERVICES_REFRESH_SEC int64 30 1.0.755 windows_updates_refresh_sec Sampling period for the Windows updates plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version windows_updates_refresh_sec NRIA_WINDOWS_UPDATES_REFRESH_SEC int64 60 1.0.755 app_data_dir Defines the path to store data in a different path than the program files directory: %AppDir%/data: Used for storing the delta data %AppDir%/user_data: External directory for user-generated JSON files %AppDir%/newrelic-infra.log: If log file config option is not defined, then we use this directory path as default. YML option name Environment variable Type Default Version app_data_dir NRIA_APP_DATA_DIR string Windows: env(ProgramData)\\New Relic\\newrelic-infra Linux: Not applicable 1.0.755 enable_win_update_plugin Enables the Windows updates plugin, which retrieves the lists of hotfixes that are installed on the host. YML option name Environment variable Type Default Version enable_win_update_plugin NRIA_ENABLE_WIN_UPDATE_PLUGIN boolean false 1.0.274 legacy_storage_sampler If true, the agent will be forced to use Windows WMI (the agent's legacy method to grab metrics for Windows; for example, StorageSampler) and will disable the new method (which uses the PDH library). YML option name Environment variable Type Default Version legacy_storage_sampler NRIA_LEGACY_STORAGE_SAMPLER boolean 1.0.1051 win_process_priority_class This configuration option allows to increase the newrelic-infra.exe process priority to any of the following values: Normal Idle High RealTime BelowNormal AboveNormal YML option name Environment variable Type Default Version win_process_priority_class NRIA_WIN_PROCESS_PRIORITY_CLASS string 1.0.989 Example: Normal Copy win_removable_drives Enables the Windows agent to report drives A: and B: when they are mapped as removable drives. YML option name Environment variable Type Default Version win_removable_drives NRIA_WIN_REMOVABLE_DRIVES boolean true 1.3.1 What's next? You can also: Further understand the configuration of the agent. Create a configuration file using our template. See how you can manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 240.49,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> <em>configuration</em> settings ",
        "sections": "<em>Infrastructure</em> <em>agent</em> <em>configuration</em> settings",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> <em>agent</em> has a large set of <em>configuration</em> settings to fine-tune its behavior. Here we: List all the <em>configuration</em> options (in both their YAML and the environment variable names). Explain what the settings do and when to use them. Give the variable type and default value (if any"
      },
      "id": "603ea542196a67a38aa83dd8"
    },
    {
      "sections": [
        "Zip assisted install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Zip assisted install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "fcadbeed626401863b6b16e5c52e9a472a5ac13e",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-04-16T02:59:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. Install the agent Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Once it's unpacked, access and edit the installation PowerShell script installer.ps1. Update your license key. Optional: Update any other parameters. Execute installer.ps1 with admin rights. Configure your installation Important Make sure any custom folder defined in the installation settings has permissions limitations properly defined. The infrastructure agent might execute any integration defined in the NRIA_PLUGIN_DIR directory with Administrator permissions. You can configure the following parameters during the assisted install for Windows: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: C:\\Program Files\\New Relic\\newrelic-infra Copy NRIA_APP_DATA_DIR This configures the data directory to store inventory and other agent files. Default: C:\\%ProgramData%\\New Relic\\newrelic-infra Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The New Relic license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy NRIA_OVERWRITE By default and for security reasons, Windows does not install a service if there's another service with the same name already installed. To bypass this check, make sure this setting NRIA_OVERWRITE is TRUE. Default: TRUE Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: C:\\Program Files\\NewRelic\\newrelic-infra\\inregrations.d Copy NRIA_SERVICE_NAME This provides the name for the Windows service. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 239.56793,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "sections": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. <em>Install</em> the <em>agent</em> Important As of version"
      },
      "id": "603ea7af196a67dab0a83d9d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 230.04066,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 1: Enable EC2 to <em>install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "&#x27;s an overview of the process: Enable Amazon EC2 to <em>install</em> our <em>infrastructure</em> <em>agent</em> on your ECS clusters. Enable monitoring of services using a service-specific <em>configuration</em> file. Step 1: Enable EC2 to <em>install</em> the <em>infrastructure</em> <em>agent</em> First, you must enable Amazon EC2 to <em>install</em> our <em>infrastructure</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/configuration/infrastructure-agent-configuration-settings": [
    {
      "sections": [
        "Zip assisted install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Zip assisted install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "fcadbeed626401863b6b16e5c52e9a472a5ac13e",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-04-16T02:59:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. Install the agent Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Once it's unpacked, access and edit the installation PowerShell script installer.ps1. Update your license key. Optional: Update any other parameters. Execute installer.ps1 with admin rights. Configure your installation Important Make sure any custom folder defined in the installation settings has permissions limitations properly defined. The infrastructure agent might execute any integration defined in the NRIA_PLUGIN_DIR directory with Administrator permissions. You can configure the following parameters during the assisted install for Windows: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: C:\\Program Files\\New Relic\\newrelic-infra Copy NRIA_APP_DATA_DIR This configures the data directory to store inventory and other agent files. Default: C:\\%ProgramData%\\New Relic\\newrelic-infra Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The New Relic license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy NRIA_OVERWRITE By default and for security reasons, Windows does not install a service if there's another service with the same name already installed. To bypass this check, make sure this setting NRIA_OVERWRITE is TRUE. Default: TRUE Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: C:\\Program Files\\NewRelic\\newrelic-infra\\inregrations.d Copy NRIA_SERVICE_NAME This provides the name for the Windows service. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 239.5679,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "sections": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. <em>Install</em> the <em>agent</em> Important As of version"
      },
      "id": "603ea7af196a67dab0a83d9d"
    },
    {
      "sections": [
        "Monitor services running on Amazon ECS",
        "Requirements",
        "How to enable",
        "Step 1: Enable EC2 to install the infrastructure agent",
        "For CentOS 6, RHEL 6, Amazon Linux 1",
        "CentOS 7, RHEL 7, Amazon Linux 2",
        "Step 2: Enable monitoring of services"
      ],
      "title": "Monitor services running on Amazon ECS",
      "type": "docs",
      "tags": [
        "Integrations",
        "On-host integrations",
        "On-host integrations list"
      ],
      "external_id": "dc178f5c162c1979019d97819db2cc77e0ce220a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/integrations/host-integrations/host-integrations-list/monitor-services-running-amazon-ecs/",
      "published_at": "2021-05-04T16:29:17Z",
      "updated_at": "2021-05-04T16:29:17Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you have services that run on Docker containers in Amazon ECS (like Cassandra, Redis, MySQL, and other supported services), you can use New Relic to report data from those services, from the host, and from the containers. Requirements To monitor services running on ECS, you must meet these requirements: An auto-scaling ECS cluster running Amazon Linux, CentOS, or RHEL that meets the infrastructure agent compatibility and requirements. ECS tasks must have network mode set to none or bridge (awsvpc and host not supported). A supported service running on ECS that meets our integration requirements: Apache (does not report inventory data) Cassandra Couchbase Elasticsearch HAProxy HashiCorp Consul JMX Kafka Memcached MongoDB MySQL NGINX PostgreSQL RabbitMQ (does not report inventory data) Redis SNMP How to enable Before explaining how to enable monitoring of services running in ECS, here's an overview of the process: Enable Amazon EC2 to install our infrastructure agent on your ECS clusters. Enable monitoring of services using a service-specific configuration file. Step 1: Enable EC2 to install the infrastructure agent First, you must enable Amazon EC2 to install our infrastructure agent on ECS clusters. To do this, you'll first need to update your user data to install the infrastructure agent on launch. Here are instructions for changing EC2 launch configuration (taken from Amazon EC2 documentation): Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. Replace user data with one of the following snippets: For CentOS 6, RHEL 6, Amazon Linux 1 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/6/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy CentOS 7, RHEL 7, Amazon Linux 2 Replace the highlighted fields with relevant values: Content-Type: multipart/mixed; boundary=\"MIMEBOUNDARY\" MIME-Version: 1.0 --MIMEBOUNDARY Content-Disposition: attachment; filename=\"init.cfg\" Content-Transfer-Encoding: 7bit Content-Type: text/cloud-config Mime-Version: 1.0 yum_repos: newrelic-infra: baseurl: https://download.newrelic.com/infrastructure_agent/linux/yum/el/7/x86_64 gpgkey: https://download.newrelic.com/infrastructure_agent/gpg/newrelic-infra.gpg gpgcheck: 1 repo_gpgcheck: 1 enabled: true name: New Relic Infrastructure write_files: - content: | --- # New Relic config file license_key: YOUR_LICENSE_KEY path: /etc/newrelic-infra.yml packages: - newrelic-infra - nri-* runcmd: - [ systemctl, daemon-reload ] - [ systemctl, enable, newrelic-infra ] - [ systemctl, start, --no-block, newrelic-infra ] --MIMEBOUNDARY Content-Transfer-Encoding: 7bit Content-Type: text/x-shellscript Mime-Version: 1.0 #!/bin/bash # ECS config { echo \"ECS_CLUSTER=YOUR_ECS_CLUSTER_NAME\" } >> /etc/ecs/ecs.config start ecs echo \"Done\" --MIMEBOUNDARY-- Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop-down menu for Launch configuration, select the new launch configuration created. Click Save. To test if the agent is automatically detecting instances, terminate an EC2 instance in the auto scaling group: the replacement instance will now be launched with the new user data. After five minutes, you should see data from the new host on the Hosts page. Next, move on to enabling the monitoring of services. Step 2: Enable monitoring of services Once you've enabled EC2 to run the infrastructure agent, the agent starts monitoring the containers running on that host. Next, we'll explain how to monitor services deployed on ECS. For example, you can monitor an ECS task containing an NGINX instance that sits in front of your application server. Here's a brief overview of how you'd monitor a supported service deployed on ECS: Create a YAML configuration file for the service you want to monitor. This will eventually be placed in the EC2 user data section via the AWS console. But before doing that, you can test that the config is working by placing that file in the infrastructure agent folder (etc/newrelic-infra/integrations.d) in EC2. That config file must use our container auto-discovery format, which allows it to automatically find containers. The exact config options will depend on the specific integration. Check to see that data from the service is being reported to New Relic. If you are satisfied with the data you see, you can then use the EC2 console to add that configuration to the appropriate launch configuration, in the write_files section, and then update the auto scaling group. Here's a detailed example of doing the above procedure for NGINX: Ensure you have SSH access to the server or access to AWS Systems Manager Session Manager. Log in to the host running the infrastructure agent. Via the command line, change the directory to the integrations configuration folder: cd /etc/newrelic-infra/integrations.d Copy Create a file called nginx-config.yml and add the following snippet: --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 Copy This configuration causes the infrastructure agent to look for containers in ECS that contain nginx. Once a container matches, it then connects to the NGINX status page. For details on how the discovery.ip snippet works, see auto-discovery. For details on general NGINX configuration, see the NGINX integration. If your NGINX status page is set to serve requests from the STATUS_URL on port 80, the infrastructure agent starts monitoring it. After five minutes, verify that NGINX data is appearing in the Infrastructure UI (either: one.newrelic.com > Infrastructure > Third party services, or one.newrelic.com > Explorer > On-host). If the configuration works, place it in the EC2 launch configuration: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Launch configurations. On the next page, select the launch configuration you want to update. Right click and select Copy launch configuration. On the Launch configuration details tab, click Edit details. In the User data section, edit the write_files section (in the part marked text/cloud-config). Add a new file/content entry: - content: | --- discovery: docker: match: image: /nginx/ integrations: - name: nri-nginx env: STATUS_URL: http://${discovery.ip}:/status REMOTE_MONITORING: true METRICS: 1 path: /etc/newrelic-infra/integrations.d/nginx-config.yml Copy Choose Skip to review. Choose Create launch configuration. Next, update the auto scaling group: Open the Amazon EC2 console. On the navigation pane, under Auto scaling, choose Auto scaling groups. Select the auto scaling group you want to update. From the Actions menu, choose Edit. In the drop down menu for Launch configuration, select the new launch configuration created. Click Save. When an EC2 instance is terminated, it is replaced with a new one that automatically looks for new NGINX containers.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 230.04054,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "sections": "Step 1: Enable EC2 to <em>install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "&#x27;s an overview of the process: Enable Amazon EC2 to <em>install</em> our <em>infrastructure</em> <em>agent</em> on your ECS clusters. Enable monitoring of services using a service-specific <em>configuration</em> file. Step 1: Enable EC2 to <em>install</em> the <em>infrastructure</em> <em>agent</em> First, you must enable Amazon EC2 to <em>install</em> our <em>infrastructure</em>"
      },
      "id": "60450959e7b9d2475c579a0f"
    },
    {
      "sections": [
        "Config file template (newrelic-infra.yml)",
        "Tip",
        "Important"
      ],
      "title": "Config file template (newrelic-infra.yml)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Configuration"
      ],
      "external_id": "bddd191ab46f54062379c4c62ffdec8ca7a63e44",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/configuration/config-file-template-newrelic-infrayml/",
      "published_at": "2021-05-05T03:35:29Z",
      "updated_at": "2021-03-13T04:08:20Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent reads the newrelic-infra.yml file for its configuration. You can find the latest configuration template in the infra-agent repository on GitHub. Tip We recommend linting the YAML configuration files before using them to avoid formatting issues. For more information on each setting, see Infrastructure configuration settings. Important Always restart the agent or your web server after changing settings.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 227.41734,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> <em>agent</em> reads the newrelic-infra.yml file for its <em>configuration</em>. You can find the latest <em>configuration</em> template in the infra-<em>agent</em> repository on GitHub. Tip We recommend linting the YAML <em>configuration</em> files before using them to avoid formatting issues. For more information on each setting, see <em>Infrastructure</em> <em>configuration</em> settings. Important Always restart the <em>agent</em> or your web server after changing settings."
      },
      "id": "60440acae7b9d2f1815799ff"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent": [
    {
      "sections": [
        "Requirements for the infrastructure agent",
        "Processor architectures",
        "Operating systems",
        "Important",
        "Tip",
        "Unique hostname",
        "Permissions",
        "Libraries",
        "Network access",
        "Container software",
        "CPU, memory, and disk usage",
        "Configuration management tools"
      ],
      "title": "Requirements for the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "517b5d94efa0139aa3ef5238569d5b04d28fb932",
      "image": "https://docs.newrelic.com/static/8de19e871ebba1c3d12258efc569dc6f/103b3/amazon-linux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/requirements-infrastructure-agent/",
      "published_at": "2021-05-05T03:40:45Z",
      "updated_at": "2021-03-13T05:20:42Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Before installing our infrastructure agent, make sure your system and any on-host integrations you configure meet the requirements. Processor architectures The infrastructure agent supports only x86 processor architectures: Linux: 64-bit architecture (also requires 64-bit package manager and dependencies) Windows: both 32 and 64-bit architectures ARM: AWS Graviton 2 processor (using the tarball setup instructions). Support is limited to infrastructure monitoring. Integrations, such as log forwarding, may not work as expected. Operating systems The infrastructure agent supports these operating systems: Operating system Supported by the infrastructure agent Amazon Linux All versions CentOS Version 7 or higher Debian Version 8 (\"Jessie\") or higher Docker Docker 1.12 Kubernetes Tested with versions 1.10 to 1.16 Red Hat Enterprise Linux (RHEL) Version 6 or higher SUSE Linux Enterprise Server (SLES) Versions 11.4, 12.1, 12.2, 12.3, and 12.4 Ubuntu LTS versions 14.04.x, 16.04.x, 18.04.x, 20.04.x Windows Windows Server 2012, 2016, and 2019, and their service packs. Windows 10 (only the infrastructure agent is supported). Important Operating systems are not supported beyond end-of-life. Tip You can monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized agent. On EKS, install the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The infrastructure agent uses the hostname to uniquely identify each server. To avoid inaccurate metrics from combining multiple servers under a single hostname, make sure that each monitored server has a unique hostname. You can use the optional display_name setting to override the default hostname. Important Servers named localhost are not reported: it's a default name and inherently non-unique. Permissions The infrastructure agent requires these permissions: Linux: By default, the agent runs and installs as root. You can also select privileged or unprivileged run modes. Windows: The agent must be installed from an Administrator account and requires Administrator privileges to run. Libraries For agent versions 1.1.19 or higher, you need the libcap library in order to install Infrastructure. It's available in the official repositories of your distribution. Network access In order to report data to New Relic, our infrastructure agent must have outbound access to certain domains and ports. If your system needs a proxy to connect to these domains, use the proxy setting. Container software The infrastructure agent instruments Docker containers when installed on the host server. We support Docker versions 1.12 or higher. CPU, memory, and disk usage The infrastructure agent is fairly lightweight. For typical CPU, memory, and disk usage, see our page on agent performance overhead. For more information on supported file systems, see Storage sample attributes. Configuration management tools The infrastructure agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Elastic Beanstalk Puppet",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 234.47913,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "sections": "Requirements for the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". Tip You can monitor Amazon BottleRocket workloads: When running EC2 instances, use the containerized <em>agent</em>. On EKS, <em>install</em> the Kubernetes integration. For ECS clusters, deploy the ECS integration. Unique hostname The <em>infrastructure</em> <em>agent</em> uses the hostname to uniquely identify each server. To avoid"
      },
      "id": "60440aca28ccbc8ce02c60cf"
    },
    {
      "sections": [
        "Zip assisted install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Zip assisted install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "fcadbeed626401863b6b16e5c52e9a472a5ac13e",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-04-16T02:59:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. Install the agent Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Once it's unpacked, access and edit the installation PowerShell script installer.ps1. Update your license key. Optional: Update any other parameters. Execute installer.ps1 with admin rights. Configure your installation Important Make sure any custom folder defined in the installation settings has permissions limitations properly defined. The infrastructure agent might execute any integration defined in the NRIA_PLUGIN_DIR directory with Administrator permissions. You can configure the following parameters during the assisted install for Windows: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: C:\\Program Files\\New Relic\\newrelic-infra Copy NRIA_APP_DATA_DIR This configures the data directory to store inventory and other agent files. Default: C:\\%ProgramData%\\New Relic\\newrelic-infra Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The New Relic license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy NRIA_OVERWRITE By default and for security reasons, Windows does not install a service if there's another service with the same name already installed. To bypass this check, make sure this setting NRIA_OVERWRITE is TRUE. Default: TRUE Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: C:\\Program Files\\NewRelic\\newrelic-infra\\inregrations.d Copy NRIA_SERVICE_NAME This provides the name for the Windows service. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.48103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "sections": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. <em>Install</em> the <em>agent</em> Important As of version"
      },
      "id": "603ea7af196a67dab0a83d9d"
    },
    {
      "sections": [
        "Configure the infrastructure agent using Ansible",
        "Sample code",
        "Compatibility and requirements",
        "Set up Ansible with New Relic",
        "Tip",
        "Role configuration variables",
        "For more help"
      ],
      "title": "Configure the infrastructure agent using Ansible",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Config management tools"
      ],
      "external_id": "1f13326e09d6da78f08f645bc069c22342fbac6c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-using-ansible/",
      "published_at": "2021-05-05T03:27:29Z",
      "updated_at": "2021-03-13T02:47:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use New Relic's Ansible role to install and configure our infrastructure monitoring agent. For instructions on how to use Ansible, see the Ansible documentation. This is a community-supported effort. Sample code New Relic provides configuration management sample code to help you install our infrastructure monitoring agent using workflows and tools that are common with many Ops teams. This is a basic Ansible role and is intended to be used as a starting place for creating your own customized workflow. Configuration depends on your specific setup and standards. To view an Ansible sample role and more integration information, see the Ansible Galaxy documentation. Compatibility and requirements The Ansible role with New Relic's infrastructure monitoring agent requires a supported Linux operating system. Set up Ansible with New Relic Tip To use Ansible configurations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. The newrelic.newrelic-infra role: Adds the New Relic infrastructure agent package repository source. Installs and configures the infrastructure agent. To get started using this role: Include the role in your playbook. Customize the required variables. All typical interactions with newrelic.newrelic-infra use role configuration. Here is an example of configuring your role to install the infrastructure agent: - hosts: ap_northeast_1 roles: - name: newrelic.newrelic-infra vars: nrinfragent_os_name: YOUR_OS_NAME nrinfragent_os_version: YOUR_OS_VERSION nrinfragent_config: license_key: YOUR_LICENSE_KEY log_file: /var/log/newrelic/nr-infra.log log_to_stdout: false Copy Role configuration variables Here are available variables for configuring the newrelic.newrelic-infra role: Variable Description nrinfragent_config Required. A map of key-value pairs. Corresponds directly with the available general configuration settings. nrinfragent_state Describes what you want to do with the agent: 'latest': Default. Installs the latest version of the infrastructure agent. 'absent': Uninstall the agent. nrinfragent_version The version of the agent you want to install: '*': Default. Installs the latest version of the infrastructure agent. 'x.y.zzz': String specifying a specific agent version number you want to install; for example, 1.0.682. nrinfragent_os_name Specifies the target OS that the infrastructure agent will be installed on. See the meta/main.yml file for the latest list. nrinfragent_os_version Specifies the OS version of the installer package needed for this machine. See the meta/main.yml file for the latest list. For more help If you need additional help, file an issue at newrelic/infrastructure-agent-ansible on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.13152,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>infrastructure</em> <em>agent</em> using Ansible",
        "sections": "Configure the <em>infrastructure</em> <em>agent</em> using Ansible",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " repository source. Installs and configures the <em>infrastructure</em> <em>agent</em>. To <em>get</em> <em>started</em> using this role: Include the role in your playbook. Customize the required variables. All typical interactions with newrelic.newrelic-infra use role configuration. Here is an example of configuring your role to <em>install</em>"
      },
      "id": "60440aa3196a675fb6960f5c"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/get-started/requirements-infrastructure-agent": [
    {
      "sections": [
        "Install the infrastructure agent",
        "Tip",
        "Quick start: Use our guided install",
        "One agent, many capabilities",
        "Install the infrastructure monitoring agent",
        "Linux",
        "Windows Server and 10",
        "Other installation scenarios",
        "Check the source code",
        "What's next"
      ],
      "title": "Install the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Get started"
      ],
      "external_id": "ccb11bfd79824202d189a3e743771cfc81e77710",
      "image": "https://docs.newrelic.com/static/6bf45ccf002250f7ebaa69cbe3ff706c/c1b63/guided-install-cli.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/get-started/install-infrastructure-agent/",
      "published_at": "2021-05-05T03:42:54Z",
      "updated_at": "2021-03-30T08:28:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure monitoring agent is a lightweight executable file that collects data about your hosts. It also forwards data from infrastructure integrations to New Relic, as well as log data for log analytics. There are multiple ways to install and deploy the infrastructure monitoring agent, depending on your setup and needs. This document describes how the infrastructure monitoring agent works and how to install it. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Quick start: Use our guided install The quickest way to get started with our infrastructure monitoring agent is through our guided install. Ready to get started? Click one of these button to try it out. Guided install EU Guided install Our guided install not only installs the infrastructure agent, but also discovers the applications and log sources running in your environment. It recommends which ones you should instrument. The guided install works with most setups. But if it doesn't suit your needs, you can find other methods below to get started monitoring your infrastructure. One agent, many capabilities Our infrastructure monitoring agent collects performance and health data about the system resources and processes of the host where it's enabled (on-premises or virtualized). At the same time, it acts as a forwarder for two types of data: core services metrics, which are collected by on-host integrations, and logs. If you want to collect data about core services running on your host, you need to install the infrastructure monitoring agent first, and then install or enable on-host integrations. Our infrastructure monitoring agent and its integrations collect data from the system and core services. It can also forward logs to New Relic. Backend application metrics (APM) are collected by separate language agents. Notice how each integration and forwarder feed different data types in the New Relic database (NRDB). Install the infrastructure monitoring agent If our guided install doesn't work for your setup, follow the instructions for your Linux, Windows, or other setup. The infrastructure monitoring agent can currently run on many Linux distributions, as well as Windows Server. For more information on where you can run the agent, check the compatibility and requirements page. Linux The preferred way to install the Linux agent is through the package manager of your distribution. Select your distribution from the list for step-by-step instructions. Amazon Linux CentOS Container (Docker) Debian RHEL SLES Ubuntu Other Linux distros To use these installation links, you must be logged to your New Relic account. If you don't have a New Relic account yet, or if you prefer to follow the procedure manually, see our tutorial. For advanced install needs, you can deploy the agent using our tarball files in assisted or manual mode. Windows Server and 10 To deploy the agent on a Windows Server host, install it using our MSI installer. For a guided procedure, click the button below and follow the step-by-step instructions: Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, see our tutorial. For advanced installation needs, you can deploy the agent using our zip files in assisted or manual mode. Other installation scenarios The infrastructure monitoring agent can be deployed programmatically using several config management and deploy tools: Ansible Chef Docker (install as container) Elastic Beanstalk Puppet Check the source code The infrastructure monitoring agent is open source software. That means you can browse its source code and send improvements, or create your own fork and build it. For more information, see the README. What's next After you've installed the infrastructure monitoring agent: Learn how to configure the agent or edit the config template. Install on-host integrations (for example, for Apache or MySQL). Enable log forwarding using the infrastructure agent. Learn how to manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 247.45473,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "sections": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". Then ingest up to 100GB of data for free each month. Forever. Quick <em>start</em>: Use our guided <em>install</em> The quickest way to <em>get</em> <em>started</em> with our <em>infrastructure</em> monitoring <em>agent</em> is through our guided <em>install</em>. Ready to <em>get</em> <em>started</em>? Click one of these button to try it out. Guided <em>install</em> EU Guided <em>install</em>"
      },
      "id": "603e79bd64441f99814e8888"
    },
    {
      "sections": [
        "Zip assisted install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Zip assisted install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "fcadbeed626401863b6b16e5c52e9a472a5ac13e",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-04-16T02:59:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. Install the agent Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Once it's unpacked, access and edit the installation PowerShell script installer.ps1. Update your license key. Optional: Update any other parameters. Execute installer.ps1 with admin rights. Configure your installation Important Make sure any custom folder defined in the installation settings has permissions limitations properly defined. The infrastructure agent might execute any integration defined in the NRIA_PLUGIN_DIR directory with Administrator permissions. You can configure the following parameters during the assisted install for Windows: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: C:\\Program Files\\New Relic\\newrelic-infra Copy NRIA_APP_DATA_DIR This configures the data directory to store inventory and other agent files. Default: C:\\%ProgramData%\\New Relic\\newrelic-infra Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The New Relic license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy NRIA_OVERWRITE By default and for security reasons, Windows does not install a service if there's another service with the same name already installed. To bypass this check, make sure this setting NRIA_OVERWRITE is TRUE. Default: TRUE Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: C:\\Program Files\\NewRelic\\newrelic-infra\\inregrations.d Copy NRIA_SERVICE_NAME This provides the name for the Windows service. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.48103,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "sections": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. <em>Install</em> the <em>agent</em> Important As of version"
      },
      "id": "603ea7af196a67dab0a83d9d"
    },
    {
      "sections": [
        "Configure the infrastructure agent using Ansible",
        "Sample code",
        "Compatibility and requirements",
        "Set up Ansible with New Relic",
        "Tip",
        "Role configuration variables",
        "For more help"
      ],
      "title": "Configure the infrastructure agent using Ansible",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Config management tools"
      ],
      "external_id": "1f13326e09d6da78f08f645bc069c22342fbac6c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/config-management-tools/configure-infrastructure-agent-using-ansible/",
      "published_at": "2021-05-05T03:27:29Z",
      "updated_at": "2021-03-13T02:47:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can use New Relic's Ansible role to install and configure our infrastructure monitoring agent. For instructions on how to use Ansible, see the Ansible documentation. This is a community-supported effort. Sample code New Relic provides configuration management sample code to help you install our infrastructure monitoring agent using workflows and tools that are common with many Ops teams. This is a basic Ansible role and is intended to be used as a starting place for creating your own customized workflow. Configuration depends on your specific setup and standards. To view an Ansible sample role and more integration information, see the Ansible Galaxy documentation. Compatibility and requirements The Ansible role with New Relic's infrastructure monitoring agent requires a supported Linux operating system. Set up Ansible with New Relic Tip To use Ansible configurations and infrastructure monitoring, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. The newrelic.newrelic-infra role: Adds the New Relic infrastructure agent package repository source. Installs and configures the infrastructure agent. To get started using this role: Include the role in your playbook. Customize the required variables. All typical interactions with newrelic.newrelic-infra use role configuration. Here is an example of configuring your role to install the infrastructure agent: - hosts: ap_northeast_1 roles: - name: newrelic.newrelic-infra vars: nrinfragent_os_name: YOUR_OS_NAME nrinfragent_os_version: YOUR_OS_VERSION nrinfragent_config: license_key: YOUR_LICENSE_KEY log_file: /var/log/newrelic/nr-infra.log log_to_stdout: false Copy Role configuration variables Here are available variables for configuring the newrelic.newrelic-infra role: Variable Description nrinfragent_config Required. A map of key-value pairs. Corresponds directly with the available general configuration settings. nrinfragent_state Describes what you want to do with the agent: 'latest': Default. Installs the latest version of the infrastructure agent. 'absent': Uninstall the agent. nrinfragent_version The version of the agent you want to install: '*': Default. Installs the latest version of the infrastructure agent. 'x.y.zzz': String specifying a specific agent version number you want to install; for example, 1.0.682. nrinfragent_os_name Specifies the target OS that the infrastructure agent will be installed on. See the meta/main.yml file for the latest list. nrinfragent_os_version Specifies the OS version of the installer package needed for this machine. See the meta/main.yml file for the latest list. For more help If you need additional help, file an issue at newrelic/infrastructure-agent-ansible on GitHub.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.13152,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Configure the <em>infrastructure</em> <em>agent</em> using Ansible",
        "sections": "Configure the <em>infrastructure</em> <em>agent</em> using Ansible",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " repository source. Installs and configures the <em>infrastructure</em> <em>agent</em>. To <em>get</em> <em>started</em> using this role: Include the role in your playbook. Customize the required variables. All typical interactions with newrelic.newrelic-infra use role configuration. Here is an example of configuring your role to <em>install</em>"
      },
      "id": "60440aa3196a675fb6960f5c"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/linux-installation/azure-extensions-infrastructure": [
    {
      "sections": [
        "Linux agent running modes",
        "Metrics and inventory provided",
        "Run integrations",
        "On-host integrations",
        "Custom integrations",
        "Set the running mode for your agent",
        "Tip",
        "Switch running modes",
        "From root to privileged/unprivileged",
        "From privileged/unprivileged to any other mode",
        "Update the agent"
      ],
      "title": "Linux agent running modes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "3b639657001e3b6d4456132ec586f24898e8ca99",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/linux-agent-running-modes/",
      "published_at": "2021-05-05T03:28:38Z",
      "updated_at": "2021-03-16T08:32:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent for Linux environments can run as root, privileged, or unprivileged user, which are described below: Mode Overview Root Installed by default. Runs as root and has total access to all the system metrics and inventory. Privileged Runs as a non-privileged user named nri-agent that is created automatically during the installation process. Normal users do not have READ access to all the system metrics, so the agent will not be able to report all the metrics of the root mode. However, privileged mode can collect more metrics than unprivileged mode, including most of the inventory. This is because at installation time, the /usr/bin/newrelic-infra executable is granted with CAP_SYS_PTRACE and CAP_DAC_READ_SEARCH kernel capabilities. Unprivileged Runs as a non-privileged user named nri-agent that is created automatically during the installation process. This mode is the most restricted. Normal users do not have READ access to all the system metrics, so the agent will not be able to report all the metrics of the root or privileged modes. Metrics and inventory provided The agent provides different metrics and inventory depending on the running mode: Mode Metrics and inventory Root All of the documented data and instrumentation values. Privileged All of the values from root mode, except: SELinux inventory: This depends on the semodule command, which requires root access. Docker process metrics: These are not enabled by default. However, you can manually enable them by giving access rights to the nri-agent user. Unprivileged All of the values from privileged mode, except: Process samples do not report these metrics: File descriptor count I/O read bytes per second I/O read count per second I/O total read bytes I/O total read count I/O total write bytes I/O total write count I/O write bytes per second I/O write count per second The following inventory sources are not reported: config/sshd kernel/sysctl packages/rpm packages/dpkg services/pidfile on SysV-based distributions Run integrations As root, integrations will run as usual. When running as privileged or unprivileged user, integrations will execute properly, although some custom integrations (for example, built by customers or technical sales staff) that depend on access to root may need additional configuration. On-host integrations In general, on-host integrations will run with the non-root agent as long as the nri-agent has permissions on the integration cache files. The default path where the integration cache files are stored is /tmp. To change the path, set the environment variable NRIA_CACHE_PATH. In this situation, use the following instructions to target the provided cache path folder instead of /tmp. On-host integrations Cache path folder Apache sudo chown nri-agent:nri-agent -R /tmp/nr-apache.json Copy Cassandra sudo chown nri-agent:nri-agent -R /tmp/nr-integrations Copy MySQL sudo chown nri-agent:nri-agent -R /tmp/nr-mysql.json Copy Nginx sudo chown nri-agent:nri-agent -R /tmp/nr-nginx.json Copy Redis sudo chown nri-agent:nri-agent -R /tmp/nr-redis.json Copy Custom integrations If your custom integration doesn't require root privileges, then it’s compatible with the rootless mode. To run it, you just need to change the owner:group of the cache file as explained above. If your integration requires to be executed with a privileged user, you can use the integration_user argument in the configuration integration. Set the running mode for your agent Tip When deciding which run mode to use, consider how much data you want to be able to collect and analyze, or how much data you want to restrict. For default and assisted installations, you can set the running mode by including the NRIA_MODE environment variable set to either ROOT, PRIVILEGED, or UNPRIVILEGED. For manual installations, follow the instructions described in our docs. Switch running modes From root to privileged/unprivileged To switch the running mode from root to privileged or unprivileged, follow the installation/update instructions in this doc. From privileged/unprivileged to any other mode To change the running mode from privileged or unprivileged to any other mode: Follow these steps: Debian/Ubuntu dpkg --purge newrelic-infra Copy OR sudo apt-get remove --purge newrelic-infra Copy Centos/Suse/RedHat/Amazon rpm -e newrelic-infra Copy OR sudo yum remove newrelic-infra Copy OR sudo zypper rm newrelic-infra Copy After making sure the agent is completely removed, reinstall the agent with the selected mode. Update the agent Follow standard procedures to update the infrastructure agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99216,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Linux</em> <em>agent</em> running modes",
        "sections": "<em>Linux</em> <em>agent</em> running modes",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> <em>agent</em> for <em>Linux</em> environments can run as root, privileged, or unprivileged user, which are described below: Mode Overview Root Installed by default. Runs as root and has total access to all the system metrics and inventory. Privileged Runs as a non-privileged user named nri-<em>agent</em>"
      },
      "id": "603ea28464441f30a54e888a"
    },
    {
      "sections": [
        "Tarball manual install of the infrastructure agent for Linux",
        "Caution",
        "Install the agent",
        "Install: Optional steps",
        "Install the service script",
        "Important",
        "Determine your service manager",
        "Tip",
        "Systemd",
        "SysV",
        "Upstart",
        "Change config file's location",
        "Change the pid-file location",
        "If the agent creates the pid-file",
        "If the agent uses the PIDFILE environment variable",
        "Change the user and runtime mode",
        "Configure the plugin directory",
        "Configure the agent directory",
        "Configure the log file",
        "Change the location of the agent binary",
        "What's next?"
      ],
      "title": "Tarball manual install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "3ed96e6fe181b3f1219f83dbecc10dc6002b69cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-manual-install-infrastructure-agent-linux/",
      "published_at": "2021-05-05T03:42:53Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our custom Linux installation process for infrastructure monitoring allows you to tailor all aspects of the installation process, and to place files and folders on your filesystem. You have full control of the installation. Caution The manual install process is not supervised. If you opt for manual install, you are responsible for placing the different files in the correct folders, providing the correct parameterized configuration values, and ensuring the agent has all the permissions to execute. Install the agent Before installation, check the compatibility and requirements. Additional agent package options Comments Troubleshooting As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend that you add it in your path. Daemon process As of version 1.5.59, the infrastructure agent package includes the additional newrelic-infra-service binary, which is used to safely manage usual agent daemon process newrelic-infra. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Install the service script. Optional: Additional install steps. Install: Optional steps You can also carry out these additional steps: Change the location of the configuration file. Change the location of the PID file. Change the user and runtime mode. Configure the plugin directory. Configure the agent directory. Configure the log file. Change the location of the agent binary. Install the service script Before you proceed to install the service script, you need to determine which service manager your system is using: If you use one of the supported service managers (SystemD, SysV, and Upstart), use the service script provided in the tarball. If you use a service manager we do not support, you must write your own service script. Important In case of doubt, check your Linux distribution's official documentation. Determine your service manager There's no good way you can programmatically know which service manager is being used in your host, but we can give you some heuristics. To determine the service manager, use the following commands: command -v systemctl (used in Systemd) command -v update-rc.d (used in SysV) command -v initctl (used in Upstart) The first command that returns an output indicates which service manager your system uses. Tip For example, run the following sequence: $ command -v systemctl $ command -v initctl /sbin/initctl Copy Based on this output, Upstart is the service manager, since it's the command that obtained a return. Important Before copying the service manager script, check if you need to change the user, the path of the agent’s binary, or the pid file location. All these changes need to be reflected in the service script. If you use one of the supported service managers, install the service script for your host: Systemd Copy the service file ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service to /etc/systemd/system/newrelic-infra.service Enable the service script: systemctl enable newrelic-infra.service Copy SysV Copy the service file ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra to /etc/init.d/system/newrelic-infra Run the following commands: update-rc.d newrelic-infra defaults update-rc.d newrelic-infra enable Copy Upstart Copy the service file ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra to /etc/init/newrelic-infra.conf Run the following command: initctl reload-configuration Copy Your service script is configured. Configure the rest of the options and start the service manually. Change config file's location The infrastructure agent includes a configuration file, usually named newrelic-infra.yml, to fine-tune the agent's behavior. For more information, see a config file template and how to configure the agent. By default, the agent searches for the configuration file in one of these locations: newrelic-infra.yml (working directory root folder) /etc/newrelic-infra.yml /etc/newrelic-infra/newrelic-infra.yml To specify a different location, use the -config flag command-line. For example: usr/bin/newrelic-infra -config /whatever/path/custom_config_name.yml Copy To make this change permanent, edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line ExecStart=/usr/bin/newrelic-infra. Add the config flag and the config file path. In this example the config file is located in the /opt directory ExecStart=/usr/bin/newrelic-infra -config /opt/config.yaml. Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line DAEMON=/usr/bin/$NAME”. Below the DAEMON variable, add this new line: EXTRA_OPTS=\"-config config_file\" (with the quotation marks). Replace config_file with the path to the config file you want to use. Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Add the config flag and the config file path. Here the config file is located in the /opt directory exec /usr/bin/newrelic-infra -config /opt/config.yaml. Save the file. Change the pid-file location The infrastructure agent uses a pid-file to keep the process identification number (pid), which is used to identify a running instance of the agent. How to change the location of the pid-file depends on how the agent is configured. Important By default, we recommend that the agent creates the pid-file. You can edit the location if necessary. To change the location of the pid-file: If the agent creates the pid-file On startup, the agent writes its process pid into the pid-file. If the file doesn’t exist it will create it. By default, the agent creates the pid-file using the path /var/run/newrelic-infra/newrelic-infra.pid. To modify it, use one of these options: Add the pid_file configuration option in the configuration file newrelic-infra.yml. Provide the pid_file using the command line when running the newrelic-infra binary. Set the NRIA_PID_FILE environment variable. Important Since the agent creates or updates the pid-file every time it’s initialized, the user who runs the agent must have read/write permissions over the pid-file location. Use the chown command to give owner rights to a user. For example, if the user nri-agent is running the agent and the pid-file location is set to/var/run/newrelic-infra-custom/nr.pid, then you can give the user rights with: chown nri-agent:nri-agent /var/run/newrelic-infra-custom/ Copy If the agent uses the PIDFILE environment variable Caution We do not recommend using the PIDFILE environment variable to manage the pid-file. If the environment variable PIDFILE is set, the agent will not try to create the pid-file. Create the pid-file in a location of your choice. Tip Use this approach if someone else takes care of the pid-file lifecycle. For example, our init script sets the PIDFILE variable for some service managers, such as SysV, because they handle the creation and content of the pid-file. Change the user and runtime mode The Linux agent runs as root by default, but it also supports running with users with less privileges: PRIVILEGED and UNPRIVILEGED. For more information, see our documentation on agent running modes. Important To execute the agent as a non-root user (PRIVILEGED or UNPRIVILEGED), make sure to grant read/write access to the folders and files provided in the tarball. To change the running mode: Edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line [Service]. Add the line User=user_name, and replace user_name with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line USER=root. Replace root with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Replace it with exec su -s /bin/sh -c ‘exec “$0” “$@“’ user_name-- /usr/bin/newrelic-infra. Replace user_name with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. If you're running the agent as PRIVILEGED, you must give it two additional Linux capabilities: Make sure the libcap library is installed in your host. (You need the setcap and getcap commands that come with it.) Extract the contents of the tarball and execute the following command with root permission: setcap CAP_SYS_PTRACE,CAP_DAC_READ_SEARCH=+ep ./newrelic-infra/usr/bin/newrelic-infra Copy The run mode will be selected based on the current user and the Kernel Capabilities assigned to it. Configure the plugin directory The infrastructure agent allows you to install integrations that monitor and report data from popular services such as Kubernetes, AWS, MySQL, Redis, Kafka, etc. Each integration has its own configuration file, named by default integration-name-config.yml, placed in the predefined location /etc/newrelic-infra/integrations.d/. On initialization, the agent loads the config file. To overwrite the predefined location of the integration configuration file, use one of the following methods: Set the location in the NRIA_PLUGIN_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the plugin_dir field. Pass it as a command line argument using -plugin_dir when you run the newrelic-infra binary. Configure the agent directory The agent requires its own defined directory to run the installed integrations, caching data (inventory), etc. The default location is /var/db/newrelic-infra/. The agent directory has the following structure and content: LICENSE: Text file containing the New Relic Infrastructure agent license. custom-integrations: Directory that stores the installed the custom integrations. newrelic-integrations: Directory that stores the New Relic official integrations. data: Directory where the agent stores cache data (inventory). Important The user running the agent must have read/write permissions to the agent directory. To overwrite the predefined location of the agent directory, use one of the following methods: Set the location in the NRIA_AGENT_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the agent_dir field. Pass it as a command line argument using -agent_dir when you run the newrelic-infra binary. Configure the log file By default the agent stores the log files in /var/db/newrelic-infra/newrelic-infra.log. Important The user running the agent must have write permissions on the log file. To overwrite the predefined location of the log file, use one of the following methods: Set the location in the NRIA_LOG_FILE environment variable. Set the custom path in the newrelic-infra.yml configuration file using the log_file field. Pass it as a command line argument using -log_file when you run the newrelic-infra binary. Change the location of the agent binary To change the location of the executable, edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line [ExecStart=/usr/bin/newrelic-infra]. Replace the path. Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line DAEMON=/usr/bin/$NAME. Replace the path. Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Replace the path. Save the file. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99216,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Our custom <em>Linux</em> <em>installation</em> process for <em>infrastructure</em> monitoring allows you to tailor all aspects of the <em>installation</em> process, and to place files and folders on your filesystem. You have full control of the <em>installation</em>. Caution The manual <em>install</em> process is not supervised. If you opt for manual"
      },
      "id": "603ea506196a671c21a83d8d"
    },
    {
      "sections": [
        "Tarball assisted install of the infrastructure agent for Linux",
        "Important",
        "Install the agent",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Tarball assisted install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "fd6735d0ef7034ddb5435a01658e07dca45efd57",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-assisted-install-infrastructure-agent-linux/",
      "published_at": "2021-05-05T03:42:52Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Linux, you can make the changes you need to the installation script and configuration file we provide so you can adapt it to your environment. Important Assisted install only works for the Systemd, Upstart, and SysV service managers. If you use any other service manager, proceed with the manual install. Before installation, check the compatibility and requirements. Install the agent To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Update your license key in config_defaults.sh. Optional: Update any other environment parameters in the configuration file. Execute installer.sh with admin rights. The script automatically identifies your service manager. If it fails, it will prompt you to manually update it. Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. Configure your installation The configuration file config_defaults.sh serves as a source of reference for all the configuration options. It contains the following environment parameters: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: /var/db/newrelic-infra/ Copy NRIA_BIN_DIR Required at installation. The path to the agent binary folder. Default: /usr/local/bin Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: /etc/newrelic-infra/yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The infrastructure agent license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: /var/run/newrelic-infra/newrelic-infra.log Copy NRIA_MODE Required at installation. The privilege level for the agent. Possible values are ROOT, PRIVILEGED or UNPRIVILEGED. For more info see our documentation on agent running modes. Default: ROOT Copy NRIA_PID_FILE Required at agent startup. The location where the agent will place its PID file. Default: /var/run/newrelic-infra/newrelic-infra.pid Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: /etc/newrelic-infra/integrations.d/ Copy NRIA_USER Required at installation time only when the running mode is set to either PRIVILEGED or UNPRIVILEGED. The user that will run the agent binary. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99216,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>, you can make the changes you need to the <em>installation</em> script and configuration file we provide so you can adapt it to your environment. Important Assisted <em>install</em> only works for the Systemd, Upstart, and SysV service managers. If you"
      },
      "id": "603ea54064441f6bb64e8859"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-container-infrastructure-monitoring": [
    {
      "sections": [
        "Linux agent running modes",
        "Metrics and inventory provided",
        "Run integrations",
        "On-host integrations",
        "Custom integrations",
        "Set the running mode for your agent",
        "Tip",
        "Switch running modes",
        "From root to privileged/unprivileged",
        "From privileged/unprivileged to any other mode",
        "Update the agent"
      ],
      "title": "Linux agent running modes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "3b639657001e3b6d4456132ec586f24898e8ca99",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/linux-agent-running-modes/",
      "published_at": "2021-05-05T03:28:38Z",
      "updated_at": "2021-03-16T08:32:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent for Linux environments can run as root, privileged, or unprivileged user, which are described below: Mode Overview Root Installed by default. Runs as root and has total access to all the system metrics and inventory. Privileged Runs as a non-privileged user named nri-agent that is created automatically during the installation process. Normal users do not have READ access to all the system metrics, so the agent will not be able to report all the metrics of the root mode. However, privileged mode can collect more metrics than unprivileged mode, including most of the inventory. This is because at installation time, the /usr/bin/newrelic-infra executable is granted with CAP_SYS_PTRACE and CAP_DAC_READ_SEARCH kernel capabilities. Unprivileged Runs as a non-privileged user named nri-agent that is created automatically during the installation process. This mode is the most restricted. Normal users do not have READ access to all the system metrics, so the agent will not be able to report all the metrics of the root or privileged modes. Metrics and inventory provided The agent provides different metrics and inventory depending on the running mode: Mode Metrics and inventory Root All of the documented data and instrumentation values. Privileged All of the values from root mode, except: SELinux inventory: This depends on the semodule command, which requires root access. Docker process metrics: These are not enabled by default. However, you can manually enable them by giving access rights to the nri-agent user. Unprivileged All of the values from privileged mode, except: Process samples do not report these metrics: File descriptor count I/O read bytes per second I/O read count per second I/O total read bytes I/O total read count I/O total write bytes I/O total write count I/O write bytes per second I/O write count per second The following inventory sources are not reported: config/sshd kernel/sysctl packages/rpm packages/dpkg services/pidfile on SysV-based distributions Run integrations As root, integrations will run as usual. When running as privileged or unprivileged user, integrations will execute properly, although some custom integrations (for example, built by customers or technical sales staff) that depend on access to root may need additional configuration. On-host integrations In general, on-host integrations will run with the non-root agent as long as the nri-agent has permissions on the integration cache files. The default path where the integration cache files are stored is /tmp. To change the path, set the environment variable NRIA_CACHE_PATH. In this situation, use the following instructions to target the provided cache path folder instead of /tmp. On-host integrations Cache path folder Apache sudo chown nri-agent:nri-agent -R /tmp/nr-apache.json Copy Cassandra sudo chown nri-agent:nri-agent -R /tmp/nr-integrations Copy MySQL sudo chown nri-agent:nri-agent -R /tmp/nr-mysql.json Copy Nginx sudo chown nri-agent:nri-agent -R /tmp/nr-nginx.json Copy Redis sudo chown nri-agent:nri-agent -R /tmp/nr-redis.json Copy Custom integrations If your custom integration doesn't require root privileges, then it’s compatible with the rootless mode. To run it, you just need to change the owner:group of the cache file as explained above. If your integration requires to be executed with a privileged user, you can use the integration_user argument in the configuration integration. Set the running mode for your agent Tip When deciding which run mode to use, consider how much data you want to be able to collect and analyze, or how much data you want to restrict. For default and assisted installations, you can set the running mode by including the NRIA_MODE environment variable set to either ROOT, PRIVILEGED, or UNPRIVILEGED. For manual installations, follow the instructions described in our docs. Switch running modes From root to privileged/unprivileged To switch the running mode from root to privileged or unprivileged, follow the installation/update instructions in this doc. From privileged/unprivileged to any other mode To change the running mode from privileged or unprivileged to any other mode: Follow these steps: Debian/Ubuntu dpkg --purge newrelic-infra Copy OR sudo apt-get remove --purge newrelic-infra Copy Centos/Suse/RedHat/Amazon rpm -e newrelic-infra Copy OR sudo yum remove newrelic-infra Copy OR sudo zypper rm newrelic-infra Copy After making sure the agent is completely removed, reinstall the agent with the selected mode. Update the agent Follow standard procedures to update the infrastructure agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99216,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Linux</em> <em>agent</em> running modes",
        "sections": "<em>Linux</em> <em>agent</em> running modes",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> <em>agent</em> for <em>Linux</em> environments can run as root, privileged, or unprivileged user, which are described below: Mode Overview Root Installed by default. Runs as root and has total access to all the system metrics and inventory. Privileged Runs as a non-privileged user named nri-<em>agent</em>"
      },
      "id": "603ea28464441f30a54e888a"
    },
    {
      "sections": [
        "Tarball manual install of the infrastructure agent for Linux",
        "Caution",
        "Install the agent",
        "Install: Optional steps",
        "Install the service script",
        "Important",
        "Determine your service manager",
        "Tip",
        "Systemd",
        "SysV",
        "Upstart",
        "Change config file's location",
        "Change the pid-file location",
        "If the agent creates the pid-file",
        "If the agent uses the PIDFILE environment variable",
        "Change the user and runtime mode",
        "Configure the plugin directory",
        "Configure the agent directory",
        "Configure the log file",
        "Change the location of the agent binary",
        "What's next?"
      ],
      "title": "Tarball manual install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "3ed96e6fe181b3f1219f83dbecc10dc6002b69cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-manual-install-infrastructure-agent-linux/",
      "published_at": "2021-05-05T03:42:53Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our custom Linux installation process for infrastructure monitoring allows you to tailor all aspects of the installation process, and to place files and folders on your filesystem. You have full control of the installation. Caution The manual install process is not supervised. If you opt for manual install, you are responsible for placing the different files in the correct folders, providing the correct parameterized configuration values, and ensuring the agent has all the permissions to execute. Install the agent Before installation, check the compatibility and requirements. Additional agent package options Comments Troubleshooting As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend that you add it in your path. Daemon process As of version 1.5.59, the infrastructure agent package includes the additional newrelic-infra-service binary, which is used to safely manage usual agent daemon process newrelic-infra. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Install the service script. Optional: Additional install steps. Install: Optional steps You can also carry out these additional steps: Change the location of the configuration file. Change the location of the PID file. Change the user and runtime mode. Configure the plugin directory. Configure the agent directory. Configure the log file. Change the location of the agent binary. Install the service script Before you proceed to install the service script, you need to determine which service manager your system is using: If you use one of the supported service managers (SystemD, SysV, and Upstart), use the service script provided in the tarball. If you use a service manager we do not support, you must write your own service script. Important In case of doubt, check your Linux distribution's official documentation. Determine your service manager There's no good way you can programmatically know which service manager is being used in your host, but we can give you some heuristics. To determine the service manager, use the following commands: command -v systemctl (used in Systemd) command -v update-rc.d (used in SysV) command -v initctl (used in Upstart) The first command that returns an output indicates which service manager your system uses. Tip For example, run the following sequence: $ command -v systemctl $ command -v initctl /sbin/initctl Copy Based on this output, Upstart is the service manager, since it's the command that obtained a return. Important Before copying the service manager script, check if you need to change the user, the path of the agent’s binary, or the pid file location. All these changes need to be reflected in the service script. If you use one of the supported service managers, install the service script for your host: Systemd Copy the service file ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service to /etc/systemd/system/newrelic-infra.service Enable the service script: systemctl enable newrelic-infra.service Copy SysV Copy the service file ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra to /etc/init.d/system/newrelic-infra Run the following commands: update-rc.d newrelic-infra defaults update-rc.d newrelic-infra enable Copy Upstart Copy the service file ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra to /etc/init/newrelic-infra.conf Run the following command: initctl reload-configuration Copy Your service script is configured. Configure the rest of the options and start the service manually. Change config file's location The infrastructure agent includes a configuration file, usually named newrelic-infra.yml, to fine-tune the agent's behavior. For more information, see a config file template and how to configure the agent. By default, the agent searches for the configuration file in one of these locations: newrelic-infra.yml (working directory root folder) /etc/newrelic-infra.yml /etc/newrelic-infra/newrelic-infra.yml To specify a different location, use the -config flag command-line. For example: usr/bin/newrelic-infra -config /whatever/path/custom_config_name.yml Copy To make this change permanent, edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line ExecStart=/usr/bin/newrelic-infra. Add the config flag and the config file path. In this example the config file is located in the /opt directory ExecStart=/usr/bin/newrelic-infra -config /opt/config.yaml. Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line DAEMON=/usr/bin/$NAME”. Below the DAEMON variable, add this new line: EXTRA_OPTS=\"-config config_file\" (with the quotation marks). Replace config_file with the path to the config file you want to use. Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Add the config flag and the config file path. Here the config file is located in the /opt directory exec /usr/bin/newrelic-infra -config /opt/config.yaml. Save the file. Change the pid-file location The infrastructure agent uses a pid-file to keep the process identification number (pid), which is used to identify a running instance of the agent. How to change the location of the pid-file depends on how the agent is configured. Important By default, we recommend that the agent creates the pid-file. You can edit the location if necessary. To change the location of the pid-file: If the agent creates the pid-file On startup, the agent writes its process pid into the pid-file. If the file doesn’t exist it will create it. By default, the agent creates the pid-file using the path /var/run/newrelic-infra/newrelic-infra.pid. To modify it, use one of these options: Add the pid_file configuration option in the configuration file newrelic-infra.yml. Provide the pid_file using the command line when running the newrelic-infra binary. Set the NRIA_PID_FILE environment variable. Important Since the agent creates or updates the pid-file every time it’s initialized, the user who runs the agent must have read/write permissions over the pid-file location. Use the chown command to give owner rights to a user. For example, if the user nri-agent is running the agent and the pid-file location is set to/var/run/newrelic-infra-custom/nr.pid, then you can give the user rights with: chown nri-agent:nri-agent /var/run/newrelic-infra-custom/ Copy If the agent uses the PIDFILE environment variable Caution We do not recommend using the PIDFILE environment variable to manage the pid-file. If the environment variable PIDFILE is set, the agent will not try to create the pid-file. Create the pid-file in a location of your choice. Tip Use this approach if someone else takes care of the pid-file lifecycle. For example, our init script sets the PIDFILE variable for some service managers, such as SysV, because they handle the creation and content of the pid-file. Change the user and runtime mode The Linux agent runs as root by default, but it also supports running with users with less privileges: PRIVILEGED and UNPRIVILEGED. For more information, see our documentation on agent running modes. Important To execute the agent as a non-root user (PRIVILEGED or UNPRIVILEGED), make sure to grant read/write access to the folders and files provided in the tarball. To change the running mode: Edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line [Service]. Add the line User=user_name, and replace user_name with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line USER=root. Replace root with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Replace it with exec su -s /bin/sh -c ‘exec “$0” “$@“’ user_name-- /usr/bin/newrelic-infra. Replace user_name with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. If you're running the agent as PRIVILEGED, you must give it two additional Linux capabilities: Make sure the libcap library is installed in your host. (You need the setcap and getcap commands that come with it.) Extract the contents of the tarball and execute the following command with root permission: setcap CAP_SYS_PTRACE,CAP_DAC_READ_SEARCH=+ep ./newrelic-infra/usr/bin/newrelic-infra Copy The run mode will be selected based on the current user and the Kernel Capabilities assigned to it. Configure the plugin directory The infrastructure agent allows you to install integrations that monitor and report data from popular services such as Kubernetes, AWS, MySQL, Redis, Kafka, etc. Each integration has its own configuration file, named by default integration-name-config.yml, placed in the predefined location /etc/newrelic-infra/integrations.d/. On initialization, the agent loads the config file. To overwrite the predefined location of the integration configuration file, use one of the following methods: Set the location in the NRIA_PLUGIN_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the plugin_dir field. Pass it as a command line argument using -plugin_dir when you run the newrelic-infra binary. Configure the agent directory The agent requires its own defined directory to run the installed integrations, caching data (inventory), etc. The default location is /var/db/newrelic-infra/. The agent directory has the following structure and content: LICENSE: Text file containing the New Relic Infrastructure agent license. custom-integrations: Directory that stores the installed the custom integrations. newrelic-integrations: Directory that stores the New Relic official integrations. data: Directory where the agent stores cache data (inventory). Important The user running the agent must have read/write permissions to the agent directory. To overwrite the predefined location of the agent directory, use one of the following methods: Set the location in the NRIA_AGENT_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the agent_dir field. Pass it as a command line argument using -agent_dir when you run the newrelic-infra binary. Configure the log file By default the agent stores the log files in /var/db/newrelic-infra/newrelic-infra.log. Important The user running the agent must have write permissions on the log file. To overwrite the predefined location of the log file, use one of the following methods: Set the location in the NRIA_LOG_FILE environment variable. Set the custom path in the newrelic-infra.yml configuration file using the log_file field. Pass it as a command line argument using -log_file when you run the newrelic-infra binary. Change the location of the agent binary To change the location of the executable, edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line [ExecStart=/usr/bin/newrelic-infra]. Replace the path. Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line DAEMON=/usr/bin/$NAME. Replace the path. Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Replace the path. Save the file. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99216,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Our custom <em>Linux</em> <em>installation</em> process for <em>infrastructure</em> monitoring allows you to tailor all aspects of the <em>installation</em> process, and to place files and folders on your filesystem. You have full control of the <em>installation</em>. Caution The manual <em>install</em> process is not supervised. If you opt for manual"
      },
      "id": "603ea506196a671c21a83d8d"
    },
    {
      "sections": [
        "Tarball assisted install of the infrastructure agent for Linux",
        "Important",
        "Install the agent",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Tarball assisted install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "fd6735d0ef7034ddb5435a01658e07dca45efd57",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-assisted-install-infrastructure-agent-linux/",
      "published_at": "2021-05-05T03:42:52Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Linux, you can make the changes you need to the installation script and configuration file we provide so you can adapt it to your environment. Important Assisted install only works for the Systemd, Upstart, and SysV service managers. If you use any other service manager, proceed with the manual install. Before installation, check the compatibility and requirements. Install the agent To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Update your license key in config_defaults.sh. Optional: Update any other environment parameters in the configuration file. Execute installer.sh with admin rights. The script automatically identifies your service manager. If it fails, it will prompt you to manually update it. Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. Configure your installation The configuration file config_defaults.sh serves as a source of reference for all the configuration options. It contains the following environment parameters: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: /var/db/newrelic-infra/ Copy NRIA_BIN_DIR Required at installation. The path to the agent binary folder. Default: /usr/local/bin Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: /etc/newrelic-infra/yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The infrastructure agent license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: /var/run/newrelic-infra/newrelic-infra.log Copy NRIA_MODE Required at installation. The privilege level for the agent. Possible values are ROOT, PRIVILEGED or UNPRIVILEGED. For more info see our documentation on agent running modes. Default: ROOT Copy NRIA_PID_FILE Required at agent startup. The location where the agent will place its PID file. Default: /var/run/newrelic-infra/newrelic-infra.pid Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: /etc/newrelic-infra/integrations.d/ Copy NRIA_USER Required at installation time only when the running mode is set to either PRIVILEGED or UNPRIVILEGED. The user that will run the agent binary. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99216,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>, you can make the changes you need to the <em>installation</em> script and configuration file we provide so you can adapt it to your environment. Important Assisted <em>install</em> only works for the Systemd, Upstart, and SysV service managers. If you"
      },
      "id": "603ea54064441f6bb64e8859"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-instrumentation-infrastructure-monitoring": [
    {
      "sections": [
        "Tarball manual install of the infrastructure agent for Linux",
        "Caution",
        "Install the agent",
        "Install: Optional steps",
        "Install the service script",
        "Important",
        "Determine your service manager",
        "Tip",
        "Systemd",
        "SysV",
        "Upstart",
        "Change config file's location",
        "Change the pid-file location",
        "If the agent creates the pid-file",
        "If the agent uses the PIDFILE environment variable",
        "Change the user and runtime mode",
        "Configure the plugin directory",
        "Configure the agent directory",
        "Configure the log file",
        "Change the location of the agent binary",
        "What's next?"
      ],
      "title": "Tarball manual install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "3ed96e6fe181b3f1219f83dbecc10dc6002b69cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-manual-install-infrastructure-agent-linux/",
      "published_at": "2021-05-05T03:42:53Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our custom Linux installation process for infrastructure monitoring allows you to tailor all aspects of the installation process, and to place files and folders on your filesystem. You have full control of the installation. Caution The manual install process is not supervised. If you opt for manual install, you are responsible for placing the different files in the correct folders, providing the correct parameterized configuration values, and ensuring the agent has all the permissions to execute. Install the agent Before installation, check the compatibility and requirements. Additional agent package options Comments Troubleshooting As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend that you add it in your path. Daemon process As of version 1.5.59, the infrastructure agent package includes the additional newrelic-infra-service binary, which is used to safely manage usual agent daemon process newrelic-infra. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Install the service script. Optional: Additional install steps. Install: Optional steps You can also carry out these additional steps: Change the location of the configuration file. Change the location of the PID file. Change the user and runtime mode. Configure the plugin directory. Configure the agent directory. Configure the log file. Change the location of the agent binary. Install the service script Before you proceed to install the service script, you need to determine which service manager your system is using: If you use one of the supported service managers (SystemD, SysV, and Upstart), use the service script provided in the tarball. If you use a service manager we do not support, you must write your own service script. Important In case of doubt, check your Linux distribution's official documentation. Determine your service manager There's no good way you can programmatically know which service manager is being used in your host, but we can give you some heuristics. To determine the service manager, use the following commands: command -v systemctl (used in Systemd) command -v update-rc.d (used in SysV) command -v initctl (used in Upstart) The first command that returns an output indicates which service manager your system uses. Tip For example, run the following sequence: $ command -v systemctl $ command -v initctl /sbin/initctl Copy Based on this output, Upstart is the service manager, since it's the command that obtained a return. Important Before copying the service manager script, check if you need to change the user, the path of the agent’s binary, or the pid file location. All these changes need to be reflected in the service script. If you use one of the supported service managers, install the service script for your host: Systemd Copy the service file ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service to /etc/systemd/system/newrelic-infra.service Enable the service script: systemctl enable newrelic-infra.service Copy SysV Copy the service file ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra to /etc/init.d/system/newrelic-infra Run the following commands: update-rc.d newrelic-infra defaults update-rc.d newrelic-infra enable Copy Upstart Copy the service file ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra to /etc/init/newrelic-infra.conf Run the following command: initctl reload-configuration Copy Your service script is configured. Configure the rest of the options and start the service manually. Change config file's location The infrastructure agent includes a configuration file, usually named newrelic-infra.yml, to fine-tune the agent's behavior. For more information, see a config file template and how to configure the agent. By default, the agent searches for the configuration file in one of these locations: newrelic-infra.yml (working directory root folder) /etc/newrelic-infra.yml /etc/newrelic-infra/newrelic-infra.yml To specify a different location, use the -config flag command-line. For example: usr/bin/newrelic-infra -config /whatever/path/custom_config_name.yml Copy To make this change permanent, edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line ExecStart=/usr/bin/newrelic-infra. Add the config flag and the config file path. In this example the config file is located in the /opt directory ExecStart=/usr/bin/newrelic-infra -config /opt/config.yaml. Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line DAEMON=/usr/bin/$NAME”. Below the DAEMON variable, add this new line: EXTRA_OPTS=\"-config config_file\" (with the quotation marks). Replace config_file with the path to the config file you want to use. Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Add the config flag and the config file path. Here the config file is located in the /opt directory exec /usr/bin/newrelic-infra -config /opt/config.yaml. Save the file. Change the pid-file location The infrastructure agent uses a pid-file to keep the process identification number (pid), which is used to identify a running instance of the agent. How to change the location of the pid-file depends on how the agent is configured. Important By default, we recommend that the agent creates the pid-file. You can edit the location if necessary. To change the location of the pid-file: If the agent creates the pid-file On startup, the agent writes its process pid into the pid-file. If the file doesn’t exist it will create it. By default, the agent creates the pid-file using the path /var/run/newrelic-infra/newrelic-infra.pid. To modify it, use one of these options: Add the pid_file configuration option in the configuration file newrelic-infra.yml. Provide the pid_file using the command line when running the newrelic-infra binary. Set the NRIA_PID_FILE environment variable. Important Since the agent creates or updates the pid-file every time it’s initialized, the user who runs the agent must have read/write permissions over the pid-file location. Use the chown command to give owner rights to a user. For example, if the user nri-agent is running the agent and the pid-file location is set to/var/run/newrelic-infra-custom/nr.pid, then you can give the user rights with: chown nri-agent:nri-agent /var/run/newrelic-infra-custom/ Copy If the agent uses the PIDFILE environment variable Caution We do not recommend using the PIDFILE environment variable to manage the pid-file. If the environment variable PIDFILE is set, the agent will not try to create the pid-file. Create the pid-file in a location of your choice. Tip Use this approach if someone else takes care of the pid-file lifecycle. For example, our init script sets the PIDFILE variable for some service managers, such as SysV, because they handle the creation and content of the pid-file. Change the user and runtime mode The Linux agent runs as root by default, but it also supports running with users with less privileges: PRIVILEGED and UNPRIVILEGED. For more information, see our documentation on agent running modes. Important To execute the agent as a non-root user (PRIVILEGED or UNPRIVILEGED), make sure to grant read/write access to the folders and files provided in the tarball. To change the running mode: Edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line [Service]. Add the line User=user_name, and replace user_name with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line USER=root. Replace root with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Replace it with exec su -s /bin/sh -c ‘exec “$0” “$@“’ user_name-- /usr/bin/newrelic-infra. Replace user_name with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. If you're running the agent as PRIVILEGED, you must give it two additional Linux capabilities: Make sure the libcap library is installed in your host. (You need the setcap and getcap commands that come with it.) Extract the contents of the tarball and execute the following command with root permission: setcap CAP_SYS_PTRACE,CAP_DAC_READ_SEARCH=+ep ./newrelic-infra/usr/bin/newrelic-infra Copy The run mode will be selected based on the current user and the Kernel Capabilities assigned to it. Configure the plugin directory The infrastructure agent allows you to install integrations that monitor and report data from popular services such as Kubernetes, AWS, MySQL, Redis, Kafka, etc. Each integration has its own configuration file, named by default integration-name-config.yml, placed in the predefined location /etc/newrelic-infra/integrations.d/. On initialization, the agent loads the config file. To overwrite the predefined location of the integration configuration file, use one of the following methods: Set the location in the NRIA_PLUGIN_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the plugin_dir field. Pass it as a command line argument using -plugin_dir when you run the newrelic-infra binary. Configure the agent directory The agent requires its own defined directory to run the installed integrations, caching data (inventory), etc. The default location is /var/db/newrelic-infra/. The agent directory has the following structure and content: LICENSE: Text file containing the New Relic Infrastructure agent license. custom-integrations: Directory that stores the installed the custom integrations. newrelic-integrations: Directory that stores the New Relic official integrations. data: Directory where the agent stores cache data (inventory). Important The user running the agent must have read/write permissions to the agent directory. To overwrite the predefined location of the agent directory, use one of the following methods: Set the location in the NRIA_AGENT_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the agent_dir field. Pass it as a command line argument using -agent_dir when you run the newrelic-infra binary. Configure the log file By default the agent stores the log files in /var/db/newrelic-infra/newrelic-infra.log. Important The user running the agent must have write permissions on the log file. To overwrite the predefined location of the log file, use one of the following methods: Set the location in the NRIA_LOG_FILE environment variable. Set the custom path in the newrelic-infra.yml configuration file using the log_file field. Pass it as a command line argument using -log_file when you run the newrelic-infra binary. Change the location of the agent binary To change the location of the executable, edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line [ExecStart=/usr/bin/newrelic-infra]. Replace the path. Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line DAEMON=/usr/bin/$NAME. Replace the path. Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Replace the path. Save the file. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99216,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Our custom <em>Linux</em> <em>installation</em> process for <em>infrastructure</em> monitoring allows you to tailor all aspects of the <em>installation</em> process, and to place files and folders on your filesystem. You have full control of the <em>installation</em>. Caution The manual <em>install</em> process is not supervised. If you opt for manual"
      },
      "id": "603ea506196a671c21a83d8d"
    },
    {
      "sections": [
        "Tarball assisted install of the infrastructure agent for Linux",
        "Important",
        "Install the agent",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Tarball assisted install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "fd6735d0ef7034ddb5435a01658e07dca45efd57",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-assisted-install-infrastructure-agent-linux/",
      "published_at": "2021-05-05T03:42:52Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Linux, you can make the changes you need to the installation script and configuration file we provide so you can adapt it to your environment. Important Assisted install only works for the Systemd, Upstart, and SysV service managers. If you use any other service manager, proceed with the manual install. Before installation, check the compatibility and requirements. Install the agent To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Update your license key in config_defaults.sh. Optional: Update any other environment parameters in the configuration file. Execute installer.sh with admin rights. The script automatically identifies your service manager. If it fails, it will prompt you to manually update it. Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. Configure your installation The configuration file config_defaults.sh serves as a source of reference for all the configuration options. It contains the following environment parameters: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: /var/db/newrelic-infra/ Copy NRIA_BIN_DIR Required at installation. The path to the agent binary folder. Default: /usr/local/bin Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: /etc/newrelic-infra/yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The infrastructure agent license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: /var/run/newrelic-infra/newrelic-infra.log Copy NRIA_MODE Required at installation. The privilege level for the agent. Possible values are ROOT, PRIVILEGED or UNPRIVILEGED. For more info see our documentation on agent running modes. Default: ROOT Copy NRIA_PID_FILE Required at agent startup. The location where the agent will place its PID file. Default: /var/run/newrelic-infra/newrelic-infra.pid Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: /etc/newrelic-infra/integrations.d/ Copy NRIA_USER Required at installation time only when the running mode is set to either PRIVILEGED or UNPRIVILEGED. The user that will run the agent binary. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99216,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>, you can make the changes you need to the <em>installation</em> script and configuration file we provide so you can adapt it to your environment. Important Assisted <em>install</em> only works for the Systemd, Upstart, and SysV service managers. If you"
      },
      "id": "603ea54064441f6bb64e8859"
    },
    {
      "sections": [
        "Linux agent running modes",
        "Metrics and inventory provided",
        "Run integrations",
        "On-host integrations",
        "Custom integrations",
        "Set the running mode for your agent",
        "Tip",
        "Switch running modes",
        "From root to privileged/unprivileged",
        "From privileged/unprivileged to any other mode",
        "Update the agent"
      ],
      "title": "Linux agent running modes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "3b639657001e3b6d4456132ec586f24898e8ca99",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/linux-agent-running-modes/",
      "published_at": "2021-05-05T03:28:38Z",
      "updated_at": "2021-03-16T08:32:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent for Linux environments can run as root, privileged, or unprivileged user, which are described below: Mode Overview Root Installed by default. Runs as root and has total access to all the system metrics and inventory. Privileged Runs as a non-privileged user named nri-agent that is created automatically during the installation process. Normal users do not have READ access to all the system metrics, so the agent will not be able to report all the metrics of the root mode. However, privileged mode can collect more metrics than unprivileged mode, including most of the inventory. This is because at installation time, the /usr/bin/newrelic-infra executable is granted with CAP_SYS_PTRACE and CAP_DAC_READ_SEARCH kernel capabilities. Unprivileged Runs as a non-privileged user named nri-agent that is created automatically during the installation process. This mode is the most restricted. Normal users do not have READ access to all the system metrics, so the agent will not be able to report all the metrics of the root or privileged modes. Metrics and inventory provided The agent provides different metrics and inventory depending on the running mode: Mode Metrics and inventory Root All of the documented data and instrumentation values. Privileged All of the values from root mode, except: SELinux inventory: This depends on the semodule command, which requires root access. Docker process metrics: These are not enabled by default. However, you can manually enable them by giving access rights to the nri-agent user. Unprivileged All of the values from privileged mode, except: Process samples do not report these metrics: File descriptor count I/O read bytes per second I/O read count per second I/O total read bytes I/O total read count I/O total write bytes I/O total write count I/O write bytes per second I/O write count per second The following inventory sources are not reported: config/sshd kernel/sysctl packages/rpm packages/dpkg services/pidfile on SysV-based distributions Run integrations As root, integrations will run as usual. When running as privileged or unprivileged user, integrations will execute properly, although some custom integrations (for example, built by customers or technical sales staff) that depend on access to root may need additional configuration. On-host integrations In general, on-host integrations will run with the non-root agent as long as the nri-agent has permissions on the integration cache files. The default path where the integration cache files are stored is /tmp. To change the path, set the environment variable NRIA_CACHE_PATH. In this situation, use the following instructions to target the provided cache path folder instead of /tmp. On-host integrations Cache path folder Apache sudo chown nri-agent:nri-agent -R /tmp/nr-apache.json Copy Cassandra sudo chown nri-agent:nri-agent -R /tmp/nr-integrations Copy MySQL sudo chown nri-agent:nri-agent -R /tmp/nr-mysql.json Copy Nginx sudo chown nri-agent:nri-agent -R /tmp/nr-nginx.json Copy Redis sudo chown nri-agent:nri-agent -R /tmp/nr-redis.json Copy Custom integrations If your custom integration doesn't require root privileges, then it’s compatible with the rootless mode. To run it, you just need to change the owner:group of the cache file as explained above. If your integration requires to be executed with a privileged user, you can use the integration_user argument in the configuration integration. Set the running mode for your agent Tip When deciding which run mode to use, consider how much data you want to be able to collect and analyze, or how much data you want to restrict. For default and assisted installations, you can set the running mode by including the NRIA_MODE environment variable set to either ROOT, PRIVILEGED, or UNPRIVILEGED. For manual installations, follow the instructions described in our docs. Switch running modes From root to privileged/unprivileged To switch the running mode from root to privileged or unprivileged, follow the installation/update instructions in this doc. From privileged/unprivileged to any other mode To change the running mode from privileged or unprivileged to any other mode: Follow these steps: Debian/Ubuntu dpkg --purge newrelic-infra Copy OR sudo apt-get remove --purge newrelic-infra Copy Centos/Suse/RedHat/Amazon rpm -e newrelic-infra Copy OR sudo yum remove newrelic-infra Copy OR sudo zypper rm newrelic-infra Copy After making sure the agent is completely removed, reinstall the agent with the selected mode. Update the agent Follow standard procedures to update the infrastructure agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99213,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Linux</em> <em>agent</em> running modes",
        "sections": "<em>Linux</em> <em>agent</em> running modes",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> <em>agent</em> for <em>Linux</em> environments can run as root, privileged, or unprivileged user, which are described below: Mode Overview Root Installed by default. Runs as root and has total access to all the system metrics and inventory. Privileged Runs as a non-privileged user named nri-<em>agent</em>"
      },
      "id": "603ea28464441f30a54e888a"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/linux-installation/install-infrastructure-monitoring-agent-linux": [
    {
      "sections": [
        "Tarball manual install of the infrastructure agent for Linux",
        "Caution",
        "Install the agent",
        "Install: Optional steps",
        "Install the service script",
        "Important",
        "Determine your service manager",
        "Tip",
        "Systemd",
        "SysV",
        "Upstart",
        "Change config file's location",
        "Change the pid-file location",
        "If the agent creates the pid-file",
        "If the agent uses the PIDFILE environment variable",
        "Change the user and runtime mode",
        "Configure the plugin directory",
        "Configure the agent directory",
        "Configure the log file",
        "Change the location of the agent binary",
        "What's next?"
      ],
      "title": "Tarball manual install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "3ed96e6fe181b3f1219f83dbecc10dc6002b69cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-manual-install-infrastructure-agent-linux/",
      "published_at": "2021-05-05T03:42:53Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our custom Linux installation process for infrastructure monitoring allows you to tailor all aspects of the installation process, and to place files and folders on your filesystem. You have full control of the installation. Caution The manual install process is not supervised. If you opt for manual install, you are responsible for placing the different files in the correct folders, providing the correct parameterized configuration values, and ensuring the agent has all the permissions to execute. Install the agent Before installation, check the compatibility and requirements. Additional agent package options Comments Troubleshooting As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend that you add it in your path. Daemon process As of version 1.5.59, the infrastructure agent package includes the additional newrelic-infra-service binary, which is used to safely manage usual agent daemon process newrelic-infra. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Install the service script. Optional: Additional install steps. Install: Optional steps You can also carry out these additional steps: Change the location of the configuration file. Change the location of the PID file. Change the user and runtime mode. Configure the plugin directory. Configure the agent directory. Configure the log file. Change the location of the agent binary. Install the service script Before you proceed to install the service script, you need to determine which service manager your system is using: If you use one of the supported service managers (SystemD, SysV, and Upstart), use the service script provided in the tarball. If you use a service manager we do not support, you must write your own service script. Important In case of doubt, check your Linux distribution's official documentation. Determine your service manager There's no good way you can programmatically know which service manager is being used in your host, but we can give you some heuristics. To determine the service manager, use the following commands: command -v systemctl (used in Systemd) command -v update-rc.d (used in SysV) command -v initctl (used in Upstart) The first command that returns an output indicates which service manager your system uses. Tip For example, run the following sequence: $ command -v systemctl $ command -v initctl /sbin/initctl Copy Based on this output, Upstart is the service manager, since it's the command that obtained a return. Important Before copying the service manager script, check if you need to change the user, the path of the agent’s binary, or the pid file location. All these changes need to be reflected in the service script. If you use one of the supported service managers, install the service script for your host: Systemd Copy the service file ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service to /etc/systemd/system/newrelic-infra.service Enable the service script: systemctl enable newrelic-infra.service Copy SysV Copy the service file ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra to /etc/init.d/system/newrelic-infra Run the following commands: update-rc.d newrelic-infra defaults update-rc.d newrelic-infra enable Copy Upstart Copy the service file ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra to /etc/init/newrelic-infra.conf Run the following command: initctl reload-configuration Copy Your service script is configured. Configure the rest of the options and start the service manually. Change config file's location The infrastructure agent includes a configuration file, usually named newrelic-infra.yml, to fine-tune the agent's behavior. For more information, see a config file template and how to configure the agent. By default, the agent searches for the configuration file in one of these locations: newrelic-infra.yml (working directory root folder) /etc/newrelic-infra.yml /etc/newrelic-infra/newrelic-infra.yml To specify a different location, use the -config flag command-line. For example: usr/bin/newrelic-infra -config /whatever/path/custom_config_name.yml Copy To make this change permanent, edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line ExecStart=/usr/bin/newrelic-infra. Add the config flag and the config file path. In this example the config file is located in the /opt directory ExecStart=/usr/bin/newrelic-infra -config /opt/config.yaml. Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line DAEMON=/usr/bin/$NAME”. Below the DAEMON variable, add this new line: EXTRA_OPTS=\"-config config_file\" (with the quotation marks). Replace config_file with the path to the config file you want to use. Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Add the config flag and the config file path. Here the config file is located in the /opt directory exec /usr/bin/newrelic-infra -config /opt/config.yaml. Save the file. Change the pid-file location The infrastructure agent uses a pid-file to keep the process identification number (pid), which is used to identify a running instance of the agent. How to change the location of the pid-file depends on how the agent is configured. Important By default, we recommend that the agent creates the pid-file. You can edit the location if necessary. To change the location of the pid-file: If the agent creates the pid-file On startup, the agent writes its process pid into the pid-file. If the file doesn’t exist it will create it. By default, the agent creates the pid-file using the path /var/run/newrelic-infra/newrelic-infra.pid. To modify it, use one of these options: Add the pid_file configuration option in the configuration file newrelic-infra.yml. Provide the pid_file using the command line when running the newrelic-infra binary. Set the NRIA_PID_FILE environment variable. Important Since the agent creates or updates the pid-file every time it’s initialized, the user who runs the agent must have read/write permissions over the pid-file location. Use the chown command to give owner rights to a user. For example, if the user nri-agent is running the agent and the pid-file location is set to/var/run/newrelic-infra-custom/nr.pid, then you can give the user rights with: chown nri-agent:nri-agent /var/run/newrelic-infra-custom/ Copy If the agent uses the PIDFILE environment variable Caution We do not recommend using the PIDFILE environment variable to manage the pid-file. If the environment variable PIDFILE is set, the agent will not try to create the pid-file. Create the pid-file in a location of your choice. Tip Use this approach if someone else takes care of the pid-file lifecycle. For example, our init script sets the PIDFILE variable for some service managers, such as SysV, because they handle the creation and content of the pid-file. Change the user and runtime mode The Linux agent runs as root by default, but it also supports running with users with less privileges: PRIVILEGED and UNPRIVILEGED. For more information, see our documentation on agent running modes. Important To execute the agent as a non-root user (PRIVILEGED or UNPRIVILEGED), make sure to grant read/write access to the folders and files provided in the tarball. To change the running mode: Edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line [Service]. Add the line User=user_name, and replace user_name with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line USER=root. Replace root with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Replace it with exec su -s /bin/sh -c ‘exec “$0” “$@“’ user_name-- /usr/bin/newrelic-infra. Replace user_name with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. If you're running the agent as PRIVILEGED, you must give it two additional Linux capabilities: Make sure the libcap library is installed in your host. (You need the setcap and getcap commands that come with it.) Extract the contents of the tarball and execute the following command with root permission: setcap CAP_SYS_PTRACE,CAP_DAC_READ_SEARCH=+ep ./newrelic-infra/usr/bin/newrelic-infra Copy The run mode will be selected based on the current user and the Kernel Capabilities assigned to it. Configure the plugin directory The infrastructure agent allows you to install integrations that monitor and report data from popular services such as Kubernetes, AWS, MySQL, Redis, Kafka, etc. Each integration has its own configuration file, named by default integration-name-config.yml, placed in the predefined location /etc/newrelic-infra/integrations.d/. On initialization, the agent loads the config file. To overwrite the predefined location of the integration configuration file, use one of the following methods: Set the location in the NRIA_PLUGIN_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the plugin_dir field. Pass it as a command line argument using -plugin_dir when you run the newrelic-infra binary. Configure the agent directory The agent requires its own defined directory to run the installed integrations, caching data (inventory), etc. The default location is /var/db/newrelic-infra/. The agent directory has the following structure and content: LICENSE: Text file containing the New Relic Infrastructure agent license. custom-integrations: Directory that stores the installed the custom integrations. newrelic-integrations: Directory that stores the New Relic official integrations. data: Directory where the agent stores cache data (inventory). Important The user running the agent must have read/write permissions to the agent directory. To overwrite the predefined location of the agent directory, use one of the following methods: Set the location in the NRIA_AGENT_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the agent_dir field. Pass it as a command line argument using -agent_dir when you run the newrelic-infra binary. Configure the log file By default the agent stores the log files in /var/db/newrelic-infra/newrelic-infra.log. Important The user running the agent must have write permissions on the log file. To overwrite the predefined location of the log file, use one of the following methods: Set the location in the NRIA_LOG_FILE environment variable. Set the custom path in the newrelic-infra.yml configuration file using the log_file field. Pass it as a command line argument using -log_file when you run the newrelic-infra binary. Change the location of the agent binary To change the location of the executable, edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line [ExecStart=/usr/bin/newrelic-infra]. Replace the path. Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line DAEMON=/usr/bin/$NAME. Replace the path. Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Replace the path. Save the file. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99216,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Our custom <em>Linux</em> <em>installation</em> process for <em>infrastructure</em> monitoring allows you to tailor all aspects of the <em>installation</em> process, and to place files and folders on your filesystem. You have full control of the <em>installation</em>. Caution The manual <em>install</em> process is not supervised. If you opt for manual"
      },
      "id": "603ea506196a671c21a83d8d"
    },
    {
      "sections": [
        "Tarball assisted install of the infrastructure agent for Linux",
        "Important",
        "Install the agent",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Tarball assisted install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "fd6735d0ef7034ddb5435a01658e07dca45efd57",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-assisted-install-infrastructure-agent-linux/",
      "published_at": "2021-05-05T03:42:52Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Linux, you can make the changes you need to the installation script and configuration file we provide so you can adapt it to your environment. Important Assisted install only works for the Systemd, Upstart, and SysV service managers. If you use any other service manager, proceed with the manual install. Before installation, check the compatibility and requirements. Install the agent To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Update your license key in config_defaults.sh. Optional: Update any other environment parameters in the configuration file. Execute installer.sh with admin rights. The script automatically identifies your service manager. If it fails, it will prompt you to manually update it. Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. Configure your installation The configuration file config_defaults.sh serves as a source of reference for all the configuration options. It contains the following environment parameters: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: /var/db/newrelic-infra/ Copy NRIA_BIN_DIR Required at installation. The path to the agent binary folder. Default: /usr/local/bin Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: /etc/newrelic-infra/yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The infrastructure agent license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: /var/run/newrelic-infra/newrelic-infra.log Copy NRIA_MODE Required at installation. The privilege level for the agent. Possible values are ROOT, PRIVILEGED or UNPRIVILEGED. For more info see our documentation on agent running modes. Default: ROOT Copy NRIA_PID_FILE Required at agent startup. The location where the agent will place its PID file. Default: /var/run/newrelic-infra/newrelic-infra.pid Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: /etc/newrelic-infra/integrations.d/ Copy NRIA_USER Required at installation time only when the running mode is set to either PRIVILEGED or UNPRIVILEGED. The user that will run the agent binary. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99216,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>, you can make the changes you need to the <em>installation</em> script and configuration file we provide so you can adapt it to your environment. Important Assisted <em>install</em> only works for the Systemd, Upstart, and SysV service managers. If you"
      },
      "id": "603ea54064441f6bb64e8859"
    },
    {
      "sections": [
        "Linux agent running modes",
        "Metrics and inventory provided",
        "Run integrations",
        "On-host integrations",
        "Custom integrations",
        "Set the running mode for your agent",
        "Tip",
        "Switch running modes",
        "From root to privileged/unprivileged",
        "From privileged/unprivileged to any other mode",
        "Update the agent"
      ],
      "title": "Linux agent running modes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "3b639657001e3b6d4456132ec586f24898e8ca99",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/linux-agent-running-modes/",
      "published_at": "2021-05-05T03:28:38Z",
      "updated_at": "2021-03-16T08:32:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent for Linux environments can run as root, privileged, or unprivileged user, which are described below: Mode Overview Root Installed by default. Runs as root and has total access to all the system metrics and inventory. Privileged Runs as a non-privileged user named nri-agent that is created automatically during the installation process. Normal users do not have READ access to all the system metrics, so the agent will not be able to report all the metrics of the root mode. However, privileged mode can collect more metrics than unprivileged mode, including most of the inventory. This is because at installation time, the /usr/bin/newrelic-infra executable is granted with CAP_SYS_PTRACE and CAP_DAC_READ_SEARCH kernel capabilities. Unprivileged Runs as a non-privileged user named nri-agent that is created automatically during the installation process. This mode is the most restricted. Normal users do not have READ access to all the system metrics, so the agent will not be able to report all the metrics of the root or privileged modes. Metrics and inventory provided The agent provides different metrics and inventory depending on the running mode: Mode Metrics and inventory Root All of the documented data and instrumentation values. Privileged All of the values from root mode, except: SELinux inventory: This depends on the semodule command, which requires root access. Docker process metrics: These are not enabled by default. However, you can manually enable them by giving access rights to the nri-agent user. Unprivileged All of the values from privileged mode, except: Process samples do not report these metrics: File descriptor count I/O read bytes per second I/O read count per second I/O total read bytes I/O total read count I/O total write bytes I/O total write count I/O write bytes per second I/O write count per second The following inventory sources are not reported: config/sshd kernel/sysctl packages/rpm packages/dpkg services/pidfile on SysV-based distributions Run integrations As root, integrations will run as usual. When running as privileged or unprivileged user, integrations will execute properly, although some custom integrations (for example, built by customers or technical sales staff) that depend on access to root may need additional configuration. On-host integrations In general, on-host integrations will run with the non-root agent as long as the nri-agent has permissions on the integration cache files. The default path where the integration cache files are stored is /tmp. To change the path, set the environment variable NRIA_CACHE_PATH. In this situation, use the following instructions to target the provided cache path folder instead of /tmp. On-host integrations Cache path folder Apache sudo chown nri-agent:nri-agent -R /tmp/nr-apache.json Copy Cassandra sudo chown nri-agent:nri-agent -R /tmp/nr-integrations Copy MySQL sudo chown nri-agent:nri-agent -R /tmp/nr-mysql.json Copy Nginx sudo chown nri-agent:nri-agent -R /tmp/nr-nginx.json Copy Redis sudo chown nri-agent:nri-agent -R /tmp/nr-redis.json Copy Custom integrations If your custom integration doesn't require root privileges, then it’s compatible with the rootless mode. To run it, you just need to change the owner:group of the cache file as explained above. If your integration requires to be executed with a privileged user, you can use the integration_user argument in the configuration integration. Set the running mode for your agent Tip When deciding which run mode to use, consider how much data you want to be able to collect and analyze, or how much data you want to restrict. For default and assisted installations, you can set the running mode by including the NRIA_MODE environment variable set to either ROOT, PRIVILEGED, or UNPRIVILEGED. For manual installations, follow the instructions described in our docs. Switch running modes From root to privileged/unprivileged To switch the running mode from root to privileged or unprivileged, follow the installation/update instructions in this doc. From privileged/unprivileged to any other mode To change the running mode from privileged or unprivileged to any other mode: Follow these steps: Debian/Ubuntu dpkg --purge newrelic-infra Copy OR sudo apt-get remove --purge newrelic-infra Copy Centos/Suse/RedHat/Amazon rpm -e newrelic-infra Copy OR sudo yum remove newrelic-infra Copy OR sudo zypper rm newrelic-infra Copy After making sure the agent is completely removed, reinstall the agent with the selected mode. Update the agent Follow standard procedures to update the infrastructure agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99213,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Linux</em> <em>agent</em> running modes",
        "sections": "<em>Linux</em> <em>agent</em> running modes",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> <em>agent</em> for <em>Linux</em> environments can run as root, privileged, or unprivileged user, which are described below: Mode Overview Root Installed by default. Runs as root and has total access to all the system metrics and inventory. Privileged Runs as a non-privileged user named nri-<em>agent</em>"
      },
      "id": "603ea28464441f30a54e888a"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/linux-installation/linux-agent-running-modes": [
    {
      "sections": [
        "Tarball manual install of the infrastructure agent for Linux",
        "Caution",
        "Install the agent",
        "Install: Optional steps",
        "Install the service script",
        "Important",
        "Determine your service manager",
        "Tip",
        "Systemd",
        "SysV",
        "Upstart",
        "Change config file's location",
        "Change the pid-file location",
        "If the agent creates the pid-file",
        "If the agent uses the PIDFILE environment variable",
        "Change the user and runtime mode",
        "Configure the plugin directory",
        "Configure the agent directory",
        "Configure the log file",
        "Change the location of the agent binary",
        "What's next?"
      ],
      "title": "Tarball manual install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "3ed96e6fe181b3f1219f83dbecc10dc6002b69cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-manual-install-infrastructure-agent-linux/",
      "published_at": "2021-05-05T03:42:53Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our custom Linux installation process for infrastructure monitoring allows you to tailor all aspects of the installation process, and to place files and folders on your filesystem. You have full control of the installation. Caution The manual install process is not supervised. If you opt for manual install, you are responsible for placing the different files in the correct folders, providing the correct parameterized configuration values, and ensuring the agent has all the permissions to execute. Install the agent Before installation, check the compatibility and requirements. Additional agent package options Comments Troubleshooting As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend that you add it in your path. Daemon process As of version 1.5.59, the infrastructure agent package includes the additional newrelic-infra-service binary, which is used to safely manage usual agent daemon process newrelic-infra. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Install the service script. Optional: Additional install steps. Install: Optional steps You can also carry out these additional steps: Change the location of the configuration file. Change the location of the PID file. Change the user and runtime mode. Configure the plugin directory. Configure the agent directory. Configure the log file. Change the location of the agent binary. Install the service script Before you proceed to install the service script, you need to determine which service manager your system is using: If you use one of the supported service managers (SystemD, SysV, and Upstart), use the service script provided in the tarball. If you use a service manager we do not support, you must write your own service script. Important In case of doubt, check your Linux distribution's official documentation. Determine your service manager There's no good way you can programmatically know which service manager is being used in your host, but we can give you some heuristics. To determine the service manager, use the following commands: command -v systemctl (used in Systemd) command -v update-rc.d (used in SysV) command -v initctl (used in Upstart) The first command that returns an output indicates which service manager your system uses. Tip For example, run the following sequence: $ command -v systemctl $ command -v initctl /sbin/initctl Copy Based on this output, Upstart is the service manager, since it's the command that obtained a return. Important Before copying the service manager script, check if you need to change the user, the path of the agent’s binary, or the pid file location. All these changes need to be reflected in the service script. If you use one of the supported service managers, install the service script for your host: Systemd Copy the service file ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service to /etc/systemd/system/newrelic-infra.service Enable the service script: systemctl enable newrelic-infra.service Copy SysV Copy the service file ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra to /etc/init.d/system/newrelic-infra Run the following commands: update-rc.d newrelic-infra defaults update-rc.d newrelic-infra enable Copy Upstart Copy the service file ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra to /etc/init/newrelic-infra.conf Run the following command: initctl reload-configuration Copy Your service script is configured. Configure the rest of the options and start the service manually. Change config file's location The infrastructure agent includes a configuration file, usually named newrelic-infra.yml, to fine-tune the agent's behavior. For more information, see a config file template and how to configure the agent. By default, the agent searches for the configuration file in one of these locations: newrelic-infra.yml (working directory root folder) /etc/newrelic-infra.yml /etc/newrelic-infra/newrelic-infra.yml To specify a different location, use the -config flag command-line. For example: usr/bin/newrelic-infra -config /whatever/path/custom_config_name.yml Copy To make this change permanent, edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line ExecStart=/usr/bin/newrelic-infra. Add the config flag and the config file path. In this example the config file is located in the /opt directory ExecStart=/usr/bin/newrelic-infra -config /opt/config.yaml. Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line DAEMON=/usr/bin/$NAME”. Below the DAEMON variable, add this new line: EXTRA_OPTS=\"-config config_file\" (with the quotation marks). Replace config_file with the path to the config file you want to use. Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Add the config flag and the config file path. Here the config file is located in the /opt directory exec /usr/bin/newrelic-infra -config /opt/config.yaml. Save the file. Change the pid-file location The infrastructure agent uses a pid-file to keep the process identification number (pid), which is used to identify a running instance of the agent. How to change the location of the pid-file depends on how the agent is configured. Important By default, we recommend that the agent creates the pid-file. You can edit the location if necessary. To change the location of the pid-file: If the agent creates the pid-file On startup, the agent writes its process pid into the pid-file. If the file doesn’t exist it will create it. By default, the agent creates the pid-file using the path /var/run/newrelic-infra/newrelic-infra.pid. To modify it, use one of these options: Add the pid_file configuration option in the configuration file newrelic-infra.yml. Provide the pid_file using the command line when running the newrelic-infra binary. Set the NRIA_PID_FILE environment variable. Important Since the agent creates or updates the pid-file every time it’s initialized, the user who runs the agent must have read/write permissions over the pid-file location. Use the chown command to give owner rights to a user. For example, if the user nri-agent is running the agent and the pid-file location is set to/var/run/newrelic-infra-custom/nr.pid, then you can give the user rights with: chown nri-agent:nri-agent /var/run/newrelic-infra-custom/ Copy If the agent uses the PIDFILE environment variable Caution We do not recommend using the PIDFILE environment variable to manage the pid-file. If the environment variable PIDFILE is set, the agent will not try to create the pid-file. Create the pid-file in a location of your choice. Tip Use this approach if someone else takes care of the pid-file lifecycle. For example, our init script sets the PIDFILE variable for some service managers, such as SysV, because they handle the creation and content of the pid-file. Change the user and runtime mode The Linux agent runs as root by default, but it also supports running with users with less privileges: PRIVILEGED and UNPRIVILEGED. For more information, see our documentation on agent running modes. Important To execute the agent as a non-root user (PRIVILEGED or UNPRIVILEGED), make sure to grant read/write access to the folders and files provided in the tarball. To change the running mode: Edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line [Service]. Add the line User=user_name, and replace user_name with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line USER=root. Replace root with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Replace it with exec su -s /bin/sh -c ‘exec “$0” “$@“’ user_name-- /usr/bin/newrelic-infra. Replace user_name with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. If you're running the agent as PRIVILEGED, you must give it two additional Linux capabilities: Make sure the libcap library is installed in your host. (You need the setcap and getcap commands that come with it.) Extract the contents of the tarball and execute the following command with root permission: setcap CAP_SYS_PTRACE,CAP_DAC_READ_SEARCH=+ep ./newrelic-infra/usr/bin/newrelic-infra Copy The run mode will be selected based on the current user and the Kernel Capabilities assigned to it. Configure the plugin directory The infrastructure agent allows you to install integrations that monitor and report data from popular services such as Kubernetes, AWS, MySQL, Redis, Kafka, etc. Each integration has its own configuration file, named by default integration-name-config.yml, placed in the predefined location /etc/newrelic-infra/integrations.d/. On initialization, the agent loads the config file. To overwrite the predefined location of the integration configuration file, use one of the following methods: Set the location in the NRIA_PLUGIN_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the plugin_dir field. Pass it as a command line argument using -plugin_dir when you run the newrelic-infra binary. Configure the agent directory The agent requires its own defined directory to run the installed integrations, caching data (inventory), etc. The default location is /var/db/newrelic-infra/. The agent directory has the following structure and content: LICENSE: Text file containing the New Relic Infrastructure agent license. custom-integrations: Directory that stores the installed the custom integrations. newrelic-integrations: Directory that stores the New Relic official integrations. data: Directory where the agent stores cache data (inventory). Important The user running the agent must have read/write permissions to the agent directory. To overwrite the predefined location of the agent directory, use one of the following methods: Set the location in the NRIA_AGENT_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the agent_dir field. Pass it as a command line argument using -agent_dir when you run the newrelic-infra binary. Configure the log file By default the agent stores the log files in /var/db/newrelic-infra/newrelic-infra.log. Important The user running the agent must have write permissions on the log file. To overwrite the predefined location of the log file, use one of the following methods: Set the location in the NRIA_LOG_FILE environment variable. Set the custom path in the newrelic-infra.yml configuration file using the log_file field. Pass it as a command line argument using -log_file when you run the newrelic-infra binary. Change the location of the agent binary To change the location of the executable, edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line [ExecStart=/usr/bin/newrelic-infra]. Replace the path. Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line DAEMON=/usr/bin/$NAME. Replace the path. Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Replace the path. Save the file. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99213,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Our custom <em>Linux</em> <em>installation</em> process for <em>infrastructure</em> monitoring allows you to tailor all aspects of the <em>installation</em> process, and to place files and folders on your filesystem. You have full control of the <em>installation</em>. Caution The manual <em>install</em> process is not supervised. If you opt for manual"
      },
      "id": "603ea506196a671c21a83d8d"
    },
    {
      "sections": [
        "Tarball assisted install of the infrastructure agent for Linux",
        "Important",
        "Install the agent",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Tarball assisted install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "fd6735d0ef7034ddb5435a01658e07dca45efd57",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-assisted-install-infrastructure-agent-linux/",
      "published_at": "2021-05-05T03:42:52Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Linux, you can make the changes you need to the installation script and configuration file we provide so you can adapt it to your environment. Important Assisted install only works for the Systemd, Upstart, and SysV service managers. If you use any other service manager, proceed with the manual install. Before installation, check the compatibility and requirements. Install the agent To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Update your license key in config_defaults.sh. Optional: Update any other environment parameters in the configuration file. Execute installer.sh with admin rights. The script automatically identifies your service manager. If it fails, it will prompt you to manually update it. Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. Configure your installation The configuration file config_defaults.sh serves as a source of reference for all the configuration options. It contains the following environment parameters: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: /var/db/newrelic-infra/ Copy NRIA_BIN_DIR Required at installation. The path to the agent binary folder. Default: /usr/local/bin Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: /etc/newrelic-infra/yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The infrastructure agent license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: /var/run/newrelic-infra/newrelic-infra.log Copy NRIA_MODE Required at installation. The privilege level for the agent. Possible values are ROOT, PRIVILEGED or UNPRIVILEGED. For more info see our documentation on agent running modes. Default: ROOT Copy NRIA_PID_FILE Required at agent startup. The location where the agent will place its PID file. Default: /var/run/newrelic-infra/newrelic-infra.pid Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: /etc/newrelic-infra/integrations.d/ Copy NRIA_USER Required at installation time only when the running mode is set to either PRIVILEGED or UNPRIVILEGED. The user that will run the agent binary. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99213,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>, you can make the changes you need to the <em>installation</em> script and configuration file we provide so you can adapt it to your environment. Important Assisted <em>install</em> only works for the Systemd, Upstart, and SysV service managers. If you"
      },
      "id": "603ea54064441f6bb64e8859"
    },
    {
      "sections": [
        "Docker instrumentation for infrastructure monitoring",
        "Requirements",
        "Tip",
        "Enable Docker container monitoring",
        "View your Docker data",
        "Docker attributes",
        "Set alert conditions"
      ],
      "title": "Docker instrumentation for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "7d6febf75c3e6b5a67fdda3226d31132cfc81b43",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-instrumentation-infrastructure-monitoring/",
      "published_at": "2021-05-05T03:34:17Z",
      "updated_at": "2021-03-16T08:31:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure agent automatically monitors your Docker containers. With Docker monitoring you can: Group containers by tags, attributes, and other metadata. Search for containers relevant to your monitoring scenario. Link to related entities that may be affected by issues with the container. Set Docker-related alert conditions. Requirements Tip To use infrastructure monitoring with Docker containers, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirement details for automatic Docker container monitoring for New Relic's infrastructure agent: Infrastructure agent 1.8.32 or higher running on Linux If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable Docker container monitoring If you meet the requirements and have installed the correct infrastructure monitoring agent, there are no additional steps to enable Docker monitoring. If Docker is running, data will automatically be reported. You can also use a Docker image containing the infrastructure monitoring agent. For more information, see Docker container for infrastructure monitoring. View your Docker data To view your Docker data in the New Relic UI, use either of these options: Go to one.newrelic.com > Infrastructure > Hosts > Containers. OR Go to one.newrelic.com > Infrastructure > Third-party services, and select Docker-related links. For more information, see Query your data. Docker attributes Docker attributes (metrics and metadata) are attached to the ContainerSample event. Here's an example of a query to find out how many containers are associated with each Docker image: SELECT uniqueCount(containerId) FROM ContainerSample FACET imageName SINCE 1 HOUR AGO TIMESERIES Copy To see all ContainerSample attributes, use our data dictionary. Attributes include: General metadata (like containerId, name, and image) CPU metrics (like cpuUsedCores, cpuPercent, and cpuThrottleTimeMs) Memory metrics (like memoryUsageBytes, memoryCacheBytes, and memoryResidentSizeBytes) Network metrics (like networkRxBytes, networkRxDropped, and networkTxBytes) Docker metrics are also attached to the ProcessSample event. The reported data does not include information related to the container orchestrator (for example, ECS or Kubernetes). To monitor those, you can add the orchestrator's cluster and task names as labels. Set alert conditions To create Docker-related alert conditions, use either of these options: Go to one.newrelic.com > Alerts & AI. OR Go to one.newrelic.com > Infrastructure > Settings > Alerts. Create a new alert condition. For the condition type, select Container metrics.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99158,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "sections": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". Forever. Requirement details for automatic Docker container monitoring for New Relic&#x27;s <em>infrastructure</em> <em>agent</em>: <em>Infrastructure</em> <em>agent</em> 1.8.32 or higher running on <em>Linux</em> If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable Docker container monitoring If you meet"
      },
      "id": "603e9f3ee7b9d2d57c2a0818"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-assisted-install-infrastructure-agent-linux": [
    {
      "sections": [
        "Linux agent running modes",
        "Metrics and inventory provided",
        "Run integrations",
        "On-host integrations",
        "Custom integrations",
        "Set the running mode for your agent",
        "Tip",
        "Switch running modes",
        "From root to privileged/unprivileged",
        "From privileged/unprivileged to any other mode",
        "Update the agent"
      ],
      "title": "Linux agent running modes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "3b639657001e3b6d4456132ec586f24898e8ca99",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/linux-agent-running-modes/",
      "published_at": "2021-05-05T03:28:38Z",
      "updated_at": "2021-03-16T08:32:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent for Linux environments can run as root, privileged, or unprivileged user, which are described below: Mode Overview Root Installed by default. Runs as root and has total access to all the system metrics and inventory. Privileged Runs as a non-privileged user named nri-agent that is created automatically during the installation process. Normal users do not have READ access to all the system metrics, so the agent will not be able to report all the metrics of the root mode. However, privileged mode can collect more metrics than unprivileged mode, including most of the inventory. This is because at installation time, the /usr/bin/newrelic-infra executable is granted with CAP_SYS_PTRACE and CAP_DAC_READ_SEARCH kernel capabilities. Unprivileged Runs as a non-privileged user named nri-agent that is created automatically during the installation process. This mode is the most restricted. Normal users do not have READ access to all the system metrics, so the agent will not be able to report all the metrics of the root or privileged modes. Metrics and inventory provided The agent provides different metrics and inventory depending on the running mode: Mode Metrics and inventory Root All of the documented data and instrumentation values. Privileged All of the values from root mode, except: SELinux inventory: This depends on the semodule command, which requires root access. Docker process metrics: These are not enabled by default. However, you can manually enable them by giving access rights to the nri-agent user. Unprivileged All of the values from privileged mode, except: Process samples do not report these metrics: File descriptor count I/O read bytes per second I/O read count per second I/O total read bytes I/O total read count I/O total write bytes I/O total write count I/O write bytes per second I/O write count per second The following inventory sources are not reported: config/sshd kernel/sysctl packages/rpm packages/dpkg services/pidfile on SysV-based distributions Run integrations As root, integrations will run as usual. When running as privileged or unprivileged user, integrations will execute properly, although some custom integrations (for example, built by customers or technical sales staff) that depend on access to root may need additional configuration. On-host integrations In general, on-host integrations will run with the non-root agent as long as the nri-agent has permissions on the integration cache files. The default path where the integration cache files are stored is /tmp. To change the path, set the environment variable NRIA_CACHE_PATH. In this situation, use the following instructions to target the provided cache path folder instead of /tmp. On-host integrations Cache path folder Apache sudo chown nri-agent:nri-agent -R /tmp/nr-apache.json Copy Cassandra sudo chown nri-agent:nri-agent -R /tmp/nr-integrations Copy MySQL sudo chown nri-agent:nri-agent -R /tmp/nr-mysql.json Copy Nginx sudo chown nri-agent:nri-agent -R /tmp/nr-nginx.json Copy Redis sudo chown nri-agent:nri-agent -R /tmp/nr-redis.json Copy Custom integrations If your custom integration doesn't require root privileges, then it’s compatible with the rootless mode. To run it, you just need to change the owner:group of the cache file as explained above. If your integration requires to be executed with a privileged user, you can use the integration_user argument in the configuration integration. Set the running mode for your agent Tip When deciding which run mode to use, consider how much data you want to be able to collect and analyze, or how much data you want to restrict. For default and assisted installations, you can set the running mode by including the NRIA_MODE environment variable set to either ROOT, PRIVILEGED, or UNPRIVILEGED. For manual installations, follow the instructions described in our docs. Switch running modes From root to privileged/unprivileged To switch the running mode from root to privileged or unprivileged, follow the installation/update instructions in this doc. From privileged/unprivileged to any other mode To change the running mode from privileged or unprivileged to any other mode: Follow these steps: Debian/Ubuntu dpkg --purge newrelic-infra Copy OR sudo apt-get remove --purge newrelic-infra Copy Centos/Suse/RedHat/Amazon rpm -e newrelic-infra Copy OR sudo yum remove newrelic-infra Copy OR sudo zypper rm newrelic-infra Copy After making sure the agent is completely removed, reinstall the agent with the selected mode. Update the agent Follow standard procedures to update the infrastructure agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99213,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Linux</em> <em>agent</em> running modes",
        "sections": "<em>Linux</em> <em>agent</em> running modes",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> <em>agent</em> for <em>Linux</em> environments can run as root, privileged, or unprivileged user, which are described below: Mode Overview Root Installed by default. Runs as root and has total access to all the system metrics and inventory. Privileged Runs as a non-privileged user named nri-<em>agent</em>"
      },
      "id": "603ea28464441f30a54e888a"
    },
    {
      "sections": [
        "Tarball manual install of the infrastructure agent for Linux",
        "Caution",
        "Install the agent",
        "Install: Optional steps",
        "Install the service script",
        "Important",
        "Determine your service manager",
        "Tip",
        "Systemd",
        "SysV",
        "Upstart",
        "Change config file's location",
        "Change the pid-file location",
        "If the agent creates the pid-file",
        "If the agent uses the PIDFILE environment variable",
        "Change the user and runtime mode",
        "Configure the plugin directory",
        "Configure the agent directory",
        "Configure the log file",
        "Change the location of the agent binary",
        "What's next?"
      ],
      "title": "Tarball manual install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "3ed96e6fe181b3f1219f83dbecc10dc6002b69cb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-manual-install-infrastructure-agent-linux/",
      "published_at": "2021-05-05T03:42:53Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our custom Linux installation process for infrastructure monitoring allows you to tailor all aspects of the installation process, and to place files and folders on your filesystem. You have full control of the installation. Caution The manual install process is not supervised. If you opt for manual install, you are responsible for placing the different files in the correct folders, providing the correct parameterized configuration values, and ensuring the agent has all the permissions to execute. Install the agent Before installation, check the compatibility and requirements. Additional agent package options Comments Troubleshooting As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend that you add it in your path. Daemon process As of version 1.5.59, the infrastructure agent package includes the additional newrelic-infra-service binary, which is used to safely manage usual agent daemon process newrelic-infra. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Install the service script. Optional: Additional install steps. Install: Optional steps You can also carry out these additional steps: Change the location of the configuration file. Change the location of the PID file. Change the user and runtime mode. Configure the plugin directory. Configure the agent directory. Configure the log file. Change the location of the agent binary. Install the service script Before you proceed to install the service script, you need to determine which service manager your system is using: If you use one of the supported service managers (SystemD, SysV, and Upstart), use the service script provided in the tarball. If you use a service manager we do not support, you must write your own service script. Important In case of doubt, check your Linux distribution's official documentation. Determine your service manager There's no good way you can programmatically know which service manager is being used in your host, but we can give you some heuristics. To determine the service manager, use the following commands: command -v systemctl (used in Systemd) command -v update-rc.d (used in SysV) command -v initctl (used in Upstart) The first command that returns an output indicates which service manager your system uses. Tip For example, run the following sequence: $ command -v systemctl $ command -v initctl /sbin/initctl Copy Based on this output, Upstart is the service manager, since it's the command that obtained a return. Important Before copying the service manager script, check if you need to change the user, the path of the agent’s binary, or the pid file location. All these changes need to be reflected in the service script. If you use one of the supported service managers, install the service script for your host: Systemd Copy the service file ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service to /etc/systemd/system/newrelic-infra.service Enable the service script: systemctl enable newrelic-infra.service Copy SysV Copy the service file ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra to /etc/init.d/system/newrelic-infra Run the following commands: update-rc.d newrelic-infra defaults update-rc.d newrelic-infra enable Copy Upstart Copy the service file ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra to /etc/init/newrelic-infra.conf Run the following command: initctl reload-configuration Copy Your service script is configured. Configure the rest of the options and start the service manually. Change config file's location The infrastructure agent includes a configuration file, usually named newrelic-infra.yml, to fine-tune the agent's behavior. For more information, see a config file template and how to configure the agent. By default, the agent searches for the configuration file in one of these locations: newrelic-infra.yml (working directory root folder) /etc/newrelic-infra.yml /etc/newrelic-infra/newrelic-infra.yml To specify a different location, use the -config flag command-line. For example: usr/bin/newrelic-infra -config /whatever/path/custom_config_name.yml Copy To make this change permanent, edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line ExecStart=/usr/bin/newrelic-infra. Add the config flag and the config file path. In this example the config file is located in the /opt directory ExecStart=/usr/bin/newrelic-infra -config /opt/config.yaml. Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line DAEMON=/usr/bin/$NAME”. Below the DAEMON variable, add this new line: EXTRA_OPTS=\"-config config_file\" (with the quotation marks). Replace config_file with the path to the config file you want to use. Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Add the config flag and the config file path. Here the config file is located in the /opt directory exec /usr/bin/newrelic-infra -config /opt/config.yaml. Save the file. Change the pid-file location The infrastructure agent uses a pid-file to keep the process identification number (pid), which is used to identify a running instance of the agent. How to change the location of the pid-file depends on how the agent is configured. Important By default, we recommend that the agent creates the pid-file. You can edit the location if necessary. To change the location of the pid-file: If the agent creates the pid-file On startup, the agent writes its process pid into the pid-file. If the file doesn’t exist it will create it. By default, the agent creates the pid-file using the path /var/run/newrelic-infra/newrelic-infra.pid. To modify it, use one of these options: Add the pid_file configuration option in the configuration file newrelic-infra.yml. Provide the pid_file using the command line when running the newrelic-infra binary. Set the NRIA_PID_FILE environment variable. Important Since the agent creates or updates the pid-file every time it’s initialized, the user who runs the agent must have read/write permissions over the pid-file location. Use the chown command to give owner rights to a user. For example, if the user nri-agent is running the agent and the pid-file location is set to/var/run/newrelic-infra-custom/nr.pid, then you can give the user rights with: chown nri-agent:nri-agent /var/run/newrelic-infra-custom/ Copy If the agent uses the PIDFILE environment variable Caution We do not recommend using the PIDFILE environment variable to manage the pid-file. If the environment variable PIDFILE is set, the agent will not try to create the pid-file. Create the pid-file in a location of your choice. Tip Use this approach if someone else takes care of the pid-file lifecycle. For example, our init script sets the PIDFILE variable for some service managers, such as SysV, because they handle the creation and content of the pid-file. Change the user and runtime mode The Linux agent runs as root by default, but it also supports running with users with less privileges: PRIVILEGED and UNPRIVILEGED. For more information, see our documentation on agent running modes. Important To execute the agent as a non-root user (PRIVILEGED or UNPRIVILEGED), make sure to grant read/write access to the folders and files provided in the tarball. To change the running mode: Edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line [Service]. Add the line User=user_name, and replace user_name with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line USER=root. Replace root with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Replace it with exec su -s /bin/sh -c ‘exec “$0” “$@“’ user_name-- /usr/bin/newrelic-infra. Replace user_name with the user that you want to execute the agent (PRIVILEGED or UNPRIVILEGED). Save the file. If you're running the agent as PRIVILEGED, you must give it two additional Linux capabilities: Make sure the libcap library is installed in your host. (You need the setcap and getcap commands that come with it.) Extract the contents of the tarball and execute the following command with root permission: setcap CAP_SYS_PTRACE,CAP_DAC_READ_SEARCH=+ep ./newrelic-infra/usr/bin/newrelic-infra Copy The run mode will be selected based on the current user and the Kernel Capabilities assigned to it. Configure the plugin directory The infrastructure agent allows you to install integrations that monitor and report data from popular services such as Kubernetes, AWS, MySQL, Redis, Kafka, etc. Each integration has its own configuration file, named by default integration-name-config.yml, placed in the predefined location /etc/newrelic-infra/integrations.d/. On initialization, the agent loads the config file. To overwrite the predefined location of the integration configuration file, use one of the following methods: Set the location in the NRIA_PLUGIN_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the plugin_dir field. Pass it as a command line argument using -plugin_dir when you run the newrelic-infra binary. Configure the agent directory The agent requires its own defined directory to run the installed integrations, caching data (inventory), etc. The default location is /var/db/newrelic-infra/. The agent directory has the following structure and content: LICENSE: Text file containing the New Relic Infrastructure agent license. custom-integrations: Directory that stores the installed the custom integrations. newrelic-integrations: Directory that stores the New Relic official integrations. data: Directory where the agent stores cache data (inventory). Important The user running the agent must have read/write permissions to the agent directory. To overwrite the predefined location of the agent directory, use one of the following methods: Set the location in the NRIA_AGENT_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the agent_dir field. Pass it as a command line argument using -agent_dir when you run the newrelic-infra binary. Configure the log file By default the agent stores the log files in /var/db/newrelic-infra/newrelic-infra.log. Important The user running the agent must have write permissions on the log file. To overwrite the predefined location of the log file, use one of the following methods: Set the location in the NRIA_LOG_FILE environment variable. Set the custom path in the newrelic-infra.yml configuration file using the log_file field. Pass it as a command line argument using -log_file when you run the newrelic-infra binary. Change the location of the agent binary To change the location of the executable, edit the service script: Systemd Open the service script ./newrelic-infra/etc/init_scripts/systemd/newrelic-infra.service. Search for the line [ExecStart=/usr/bin/newrelic-infra]. Replace the path. Save the file. SysV Open the service script ./newrelic-infra/etc/init_scripts/sysv/newrelic-infra. Search for the line DAEMON=/usr/bin/$NAME. Replace the path. Save the file. Upstart Open the service script ./newrelic-infra/etc/init_scripts/upstart/newrelic-infra. Search for the line exec /usr/bin/newrelic-infra. Replace the path. Save the file. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99213,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Our custom <em>Linux</em> <em>installation</em> process for <em>infrastructure</em> monitoring allows you to tailor all aspects of the <em>installation</em> process, and to place files and folders on your filesystem. You have full control of the <em>installation</em>. Caution The manual <em>install</em> process is not supervised. If you opt for manual"
      },
      "id": "603ea506196a671c21a83d8d"
    },
    {
      "sections": [
        "Docker instrumentation for infrastructure monitoring",
        "Requirements",
        "Tip",
        "Enable Docker container monitoring",
        "View your Docker data",
        "Docker attributes",
        "Set alert conditions"
      ],
      "title": "Docker instrumentation for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "7d6febf75c3e6b5a67fdda3226d31132cfc81b43",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-instrumentation-infrastructure-monitoring/",
      "published_at": "2021-05-05T03:34:17Z",
      "updated_at": "2021-03-16T08:31:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure agent automatically monitors your Docker containers. With Docker monitoring you can: Group containers by tags, attributes, and other metadata. Search for containers relevant to your monitoring scenario. Link to related entities that may be affected by issues with the container. Set Docker-related alert conditions. Requirements Tip To use infrastructure monitoring with Docker containers, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirement details for automatic Docker container monitoring for New Relic's infrastructure agent: Infrastructure agent 1.8.32 or higher running on Linux If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable Docker container monitoring If you meet the requirements and have installed the correct infrastructure monitoring agent, there are no additional steps to enable Docker monitoring. If Docker is running, data will automatically be reported. You can also use a Docker image containing the infrastructure monitoring agent. For more information, see Docker container for infrastructure monitoring. View your Docker data To view your Docker data in the New Relic UI, use either of these options: Go to one.newrelic.com > Infrastructure > Hosts > Containers. OR Go to one.newrelic.com > Infrastructure > Third-party services, and select Docker-related links. For more information, see Query your data. Docker attributes Docker attributes (metrics and metadata) are attached to the ContainerSample event. Here's an example of a query to find out how many containers are associated with each Docker image: SELECT uniqueCount(containerId) FROM ContainerSample FACET imageName SINCE 1 HOUR AGO TIMESERIES Copy To see all ContainerSample attributes, use our data dictionary. Attributes include: General metadata (like containerId, name, and image) CPU metrics (like cpuUsedCores, cpuPercent, and cpuThrottleTimeMs) Memory metrics (like memoryUsageBytes, memoryCacheBytes, and memoryResidentSizeBytes) Network metrics (like networkRxBytes, networkRxDropped, and networkTxBytes) Docker metrics are also attached to the ProcessSample event. The reported data does not include information related to the container orchestrator (for example, ECS or Kubernetes). To monitor those, you can add the orchestrator's cluster and task names as labels. Set alert conditions To create Docker-related alert conditions, use either of these options: Go to one.newrelic.com > Alerts & AI. OR Go to one.newrelic.com > Infrastructure > Settings > Alerts. Create a new alert condition. For the condition type, select Container metrics.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99158,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "sections": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". Forever. Requirement details for automatic Docker container monitoring for New Relic&#x27;s <em>infrastructure</em> <em>agent</em>: <em>Infrastructure</em> <em>agent</em> 1.8.32 or higher running on <em>Linux</em> If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable Docker container monitoring If you meet"
      },
      "id": "603e9f3ee7b9d2d57c2a0818"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-manual-install-infrastructure-agent-linux": [
    {
      "sections": [
        "Linux agent running modes",
        "Metrics and inventory provided",
        "Run integrations",
        "On-host integrations",
        "Custom integrations",
        "Set the running mode for your agent",
        "Tip",
        "Switch running modes",
        "From root to privileged/unprivileged",
        "From privileged/unprivileged to any other mode",
        "Update the agent"
      ],
      "title": "Linux agent running modes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "3b639657001e3b6d4456132ec586f24898e8ca99",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/linux-agent-running-modes/",
      "published_at": "2021-05-05T03:28:38Z",
      "updated_at": "2021-03-16T08:32:21Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent for Linux environments can run as root, privileged, or unprivileged user, which are described below: Mode Overview Root Installed by default. Runs as root and has total access to all the system metrics and inventory. Privileged Runs as a non-privileged user named nri-agent that is created automatically during the installation process. Normal users do not have READ access to all the system metrics, so the agent will not be able to report all the metrics of the root mode. However, privileged mode can collect more metrics than unprivileged mode, including most of the inventory. This is because at installation time, the /usr/bin/newrelic-infra executable is granted with CAP_SYS_PTRACE and CAP_DAC_READ_SEARCH kernel capabilities. Unprivileged Runs as a non-privileged user named nri-agent that is created automatically during the installation process. This mode is the most restricted. Normal users do not have READ access to all the system metrics, so the agent will not be able to report all the metrics of the root or privileged modes. Metrics and inventory provided The agent provides different metrics and inventory depending on the running mode: Mode Metrics and inventory Root All of the documented data and instrumentation values. Privileged All of the values from root mode, except: SELinux inventory: This depends on the semodule command, which requires root access. Docker process metrics: These are not enabled by default. However, you can manually enable them by giving access rights to the nri-agent user. Unprivileged All of the values from privileged mode, except: Process samples do not report these metrics: File descriptor count I/O read bytes per second I/O read count per second I/O total read bytes I/O total read count I/O total write bytes I/O total write count I/O write bytes per second I/O write count per second The following inventory sources are not reported: config/sshd kernel/sysctl packages/rpm packages/dpkg services/pidfile on SysV-based distributions Run integrations As root, integrations will run as usual. When running as privileged or unprivileged user, integrations will execute properly, although some custom integrations (for example, built by customers or technical sales staff) that depend on access to root may need additional configuration. On-host integrations In general, on-host integrations will run with the non-root agent as long as the nri-agent has permissions on the integration cache files. The default path where the integration cache files are stored is /tmp. To change the path, set the environment variable NRIA_CACHE_PATH. In this situation, use the following instructions to target the provided cache path folder instead of /tmp. On-host integrations Cache path folder Apache sudo chown nri-agent:nri-agent -R /tmp/nr-apache.json Copy Cassandra sudo chown nri-agent:nri-agent -R /tmp/nr-integrations Copy MySQL sudo chown nri-agent:nri-agent -R /tmp/nr-mysql.json Copy Nginx sudo chown nri-agent:nri-agent -R /tmp/nr-nginx.json Copy Redis sudo chown nri-agent:nri-agent -R /tmp/nr-redis.json Copy Custom integrations If your custom integration doesn't require root privileges, then it’s compatible with the rootless mode. To run it, you just need to change the owner:group of the cache file as explained above. If your integration requires to be executed with a privileged user, you can use the integration_user argument in the configuration integration. Set the running mode for your agent Tip When deciding which run mode to use, consider how much data you want to be able to collect and analyze, or how much data you want to restrict. For default and assisted installations, you can set the running mode by including the NRIA_MODE environment variable set to either ROOT, PRIVILEGED, or UNPRIVILEGED. For manual installations, follow the instructions described in our docs. Switch running modes From root to privileged/unprivileged To switch the running mode from root to privileged or unprivileged, follow the installation/update instructions in this doc. From privileged/unprivileged to any other mode To change the running mode from privileged or unprivileged to any other mode: Follow these steps: Debian/Ubuntu dpkg --purge newrelic-infra Copy OR sudo apt-get remove --purge newrelic-infra Copy Centos/Suse/RedHat/Amazon rpm -e newrelic-infra Copy OR sudo yum remove newrelic-infra Copy OR sudo zypper rm newrelic-infra Copy After making sure the agent is completely removed, reinstall the agent with the selected mode. Update the agent Follow standard procedures to update the infrastructure agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99213,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Linux</em> <em>agent</em> running modes",
        "sections": "<em>Linux</em> <em>agent</em> running modes",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "The <em>infrastructure</em> <em>agent</em> for <em>Linux</em> environments can run as root, privileged, or unprivileged user, which are described below: Mode Overview Root Installed by default. Runs as root and has total access to all the system metrics and inventory. Privileged Runs as a non-privileged user named nri-<em>agent</em>"
      },
      "id": "603ea28464441f30a54e888a"
    },
    {
      "sections": [
        "Tarball assisted install of the infrastructure agent for Linux",
        "Important",
        "Install the agent",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Tarball assisted install of the infrastructure agent for Linux ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "fd6735d0ef7034ddb5435a01658e07dca45efd57",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/tarball-assisted-install-infrastructure-agent-linux/",
      "published_at": "2021-05-05T03:42:52Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Linux, you can make the changes you need to the installation script and configuration file we provide so you can adapt it to your environment. Important Assisted install only works for the Systemd, Upstart, and SysV service managers. If you use any other service manager, proceed with the manual install. Before installation, check the compatibility and requirements. Install the agent To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: newrelic-infra |-- config_defaults.sh |-- etc | |-- init_scripts | | |-- systemd | | | `-- newrelic-infra.service | | |-- sysv | | | `-- newrelic-infra | | `-- upstart | | `-- newrelic-infra | `-- newrelic-infra | `-- integrations.d |-- installer.sh |-- usr | `-- bin | |-- newrelic-infra | |-- newrelic-infra-ctl | `-- newrelic-infra-service `-- var |-- db | `-- newrelic-infra | |-- custom-integrations | |-- integrations.d | |-- LICENSE.txt | `-- newrelic-integrations |-- log | `-- newrelic-infra `-- run `-- newrelic-infra Copy Update your license key in config_defaults.sh. Optional: Update any other environment parameters in the configuration file. Execute installer.sh with admin rights. The script automatically identifies your service manager. If it fails, it will prompt you to manually update it. Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. Configure your installation The configuration file config_defaults.sh serves as a source of reference for all the configuration options. It contains the following environment parameters: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: /var/db/newrelic-infra/ Copy NRIA_BIN_DIR Required at installation. The path to the agent binary folder. Default: /usr/local/bin Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: /etc/newrelic-infra/yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The infrastructure agent license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: /var/run/newrelic-infra/newrelic-infra.log Copy NRIA_MODE Required at installation. The privilege level for the agent. Possible values are ROOT, PRIVILEGED or UNPRIVILEGED. For more info see our documentation on agent running modes. Default: ROOT Copy NRIA_PID_FILE Required at agent startup. The location where the agent will place its PID file. Default: /var/run/newrelic-infra/newrelic-infra.pid Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: /etc/newrelic-infra/integrations.d/ Copy NRIA_USER Required at installation time only when the running mode is set to either PRIVILEGED or UNPRIVILEGED. The user that will run the agent binary. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99213,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em> ",
        "sections": "Tarball assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Linux</em>, you can make the changes you need to the <em>installation</em> script and configuration file we provide so you can adapt it to your environment. Important Assisted <em>install</em> only works for the Systemd, Upstart, and SysV service managers. If you"
      },
      "id": "603ea54064441f6bb64e8859"
    },
    {
      "sections": [
        "Docker instrumentation for infrastructure monitoring",
        "Requirements",
        "Tip",
        "Enable Docker container monitoring",
        "View your Docker data",
        "Docker attributes",
        "Set alert conditions"
      ],
      "title": "Docker instrumentation for infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Linux installation"
      ],
      "external_id": "7d6febf75c3e6b5a67fdda3226d31132cfc81b43",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/linux-installation/docker-instrumentation-infrastructure-monitoring/",
      "published_at": "2021-05-05T03:34:17Z",
      "updated_at": "2021-03-16T08:31:15Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our infrastructure agent automatically monitors your Docker containers. With Docker monitoring you can: Group containers by tags, attributes, and other metadata. Search for containers relevant to your monitoring scenario. Link to related entities that may be affected by issues with the container. Set Docker-related alert conditions. Requirements Tip To use infrastructure monitoring with Docker containers, as well as the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Requirement details for automatic Docker container monitoring for New Relic's infrastructure agent: Infrastructure agent 1.8.32 or higher running on Linux If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable Docker container monitoring If you meet the requirements and have installed the correct infrastructure monitoring agent, there are no additional steps to enable Docker monitoring. If Docker is running, data will automatically be reported. You can also use a Docker image containing the infrastructure monitoring agent. For more information, see Docker container for infrastructure monitoring. View your Docker data To view your Docker data in the New Relic UI, use either of these options: Go to one.newrelic.com > Infrastructure > Hosts > Containers. OR Go to one.newrelic.com > Infrastructure > Third-party services, and select Docker-related links. For more information, see Query your data. Docker attributes Docker attributes (metrics and metadata) are attached to the ContainerSample event. Here's an example of a query to find out how many containers are associated with each Docker image: SELECT uniqueCount(containerId) FROM ContainerSample FACET imageName SINCE 1 HOUR AGO TIMESERIES Copy To see all ContainerSample attributes, use our data dictionary. Attributes include: General metadata (like containerId, name, and image) CPU metrics (like cpuUsedCores, cpuPercent, and cpuThrottleTimeMs) Memory metrics (like memoryUsageBytes, memoryCacheBytes, and memoryResidentSizeBytes) Network metrics (like networkRxBytes, networkRxDropped, and networkTxBytes) Docker metrics are also attached to the ProcessSample event. The reported data does not include information related to the container orchestrator (for example, ECS or Kubernetes). To monitor those, you can add the orchestrator's cluster and task names as labels. Set alert conditions To create Docker-related alert conditions, use either of these options: Go to one.newrelic.com > Alerts & AI. OR Go to one.newrelic.com > Infrastructure > Settings > Alerts. Create a new alert condition. For the condition type, select Container metrics.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 276.99158,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "sections": "Docker instrumentation for <em>infrastructure</em> monitoring",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". Forever. Requirement details for automatic Docker container monitoring for New Relic&#x27;s <em>infrastructure</em> <em>agent</em>: <em>Infrastructure</em> <em>agent</em> 1.8.32 or higher running on <em>Linux</em> If using CentOS, you must have CentOS version 6.0 or higher Docker version 1.12 or higher Enable Docker container monitoring If you meet"
      },
      "id": "603e9f3ee7b9d2d57c2a0818"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/manage-your-agent/agent-message-size": [
    {
      "sections": [
        "Infrastructure agent behavior",
        "Agent service",
        "Agent startup",
        "Monitoring and resource caps",
        "Integration data",
        "Agent shutdown",
        "Maintenance",
        "Retry behavior",
        "Manage data reporting"
      ],
      "title": "Infrastructure agent behavior",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "704d0716fb7aa5a09d0db4a9fff12e53adb31758",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-behavior/",
      "published_at": "2021-05-05T03:35:31Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the infrastructure agent, you can monitor not only individual servers, but also understand how your service performs as a whole. The agent supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these agent versions exhibit a common set of behaviors. Agent service As of infrastructure agent v1.5.59, the agent bundles a binary named newrelic-infra-service. This binary can be managed by the OS service manager. At service startup time, this binary spawns (executes) the usual newrelic-infra process and supervises its child execution. Therefore agent service process should never be restarted, unless triggered via OS service manager. Agent startup During startup the agent will: Register a signal handler. Set the loggers. Load the configuration from file, environment variables, and call arguments. Register plugins for harvesting inventory, samplers, and integrations. StatsD integration with http_server_enabled\" Open an http port (by default, 8001) for receiving data. Startup duration before harvesting and sending data is usually less than six seconds. Monitoring and resource caps By default, the infrastructure agent runs in a single core. Every second it checks if there are events to send and, if there are, it sends them to the New Relic collector. Events that may be sent include: Default infrastructure events Events recorded by New Relic integrations. For descriptions of the default infrastructure events and their collection frequencies, see Infrastructure events. Integration data Integration monitoring is done by executing integration commands at given intervals (set in the config files) and reading their stout/err. The more integrations you enable, the greater the footprint of the agent. For more information, see the documentation for specific integrations. Agent shutdown When a shutdown signal is received, the agent stops all the registered plugins and integration processes. Maintenance The agent runs as a service. On installation, we set up all the service manager-required files, such as the systemD. service file. In case of a catastrophic failure, the service manager configuration will restart the agent. There are no automatic updates to agents. To install a new agent version: Linux: Manually install agent versions through the appropriate package manager (apt, yum, zypper). Windows: Manually download the msi package and install it with msiexec.exe. Retry behavior If a request made to the ingest service is unsuccessful, the payload is discarded; subsequent requests follow an exponential backoff pattern until one succeeds. For inventory, we store the deltas between system states in cache files. On failure, these deltas are not deleted but are reused on requests that follow. Manage data reporting For information about configuring reporting of data, see Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.66418,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> behavior",
        "sections": "<em>Infrastructure</em> <em>agent</em> behavior",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the <em>infrastructure</em> <em>agent</em>, you can monitor not only individual servers, but also understand how <em>your</em> service performs as a whole. The <em>agent</em> supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these <em>agent</em> versions exhibit a common set of behaviors. <em>Agent</em>"
      },
      "id": "603eb68428ccbc8576eba7a5"
    },
    {
      "sections": [
        "Troubleshoot a running infrastructure agent",
        "Linux newrelic-infra-ctl",
        "Important",
        "Windows newrelic-infra-ctl"
      ],
      "title": "Troubleshoot a running infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "e1f206feb686f819c00db0619a8534609fe19c53",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/troubleshoot-running-infrastructure-agent/",
      "published_at": "2021-05-05T03:36:57Z",
      "updated_at": "2021-03-16T07:33:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can troubleshoot our infrastructure agent using our newrelic-infra-ctl utility. This binary is also included in the installation package, in the same directory as the newrelic-infra default binary. Upon receiving the newrelic-infra-ctl command, the agent: Enables verbose logs for a period of five minutes, then reverts the log level to its previous setting. Logs all the agent config options. Logs all the integrations config options. Executes a health check for every loaded integration. (A health check is an immediate execution of the integration with extra logs and output validation.) Linux newrelic-infra-ctl In Linux systems, the troubleshooting binary is /usr/bin/newrelic-infra-ctl, available in both the package manager or the tarball assisted install methods. Important When running on Linux, newrelic-infra-ctl must be executed by either the root user or the same user running the newrelic-infra process. The newrelic-infra-ctl binary can automatically detect the agent process running in the host. It can also detect whether the agent is running inside a container. To change the default settings of newrelic-infra-ctl: To change... Execute pid newrelic-infra-ctl -pid 14580 cid (when using a containerized version of the agent) newrelic-infra-ctl -cid 8fddbcbb101c docker-api-version newrelic-infra-ctl -docker-api-version 1.24 Windows newrelic-infra-ctl In Windows, using the MSI installer, the troubleshooting binary is C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra-ctl.exe. Important In Windows, the agent process is always automatically detected. It does not depend on the pid or the cid.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.63385,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Troubleshoot a running <em>infrastructure</em> <em>agent</em>",
        "sections": "Troubleshoot a running <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "You can troubleshoot our <em>infrastructure</em> <em>agent</em> using our newrelic-infra-ctl utility. This binary is also included in the installation package, in the same directory as the newrelic-infra default binary. Upon receiving the newrelic-infra-ctl command, the <em>agent</em>: Enables verbose logs for a period"
      },
      "id": "603eb3dee7b9d22a5f2f736b"
    },
    {
      "sections": [
        "Start, stop, and restart the infrastructure agent",
        "Linux: Start, stop, restart, or check agent status",
        "Windows: Start, stop, restart, or check agent status",
        "Important",
        "Command prompt (cmd.exe)",
        "PowerShell",
        "Customize agent logs",
        "Determine your init system",
        "Configuration management tools"
      ],
      "title": "Start, stop, and restart the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "81bb8174fac415ee0ec94c51e123c32eba700d6e",
      "image": "https://docs.newrelic.com/static/19e41f9fd7395eb6f1c816aa87182bb9/103b3/otherlinux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/start-stop-restart-infrastructure-agent/",
      "published_at": "2021-05-05T03:35:30Z",
      "updated_at": "2021-03-16T06:36:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent starts automatically after you run the installation script. However, there are situations where you may need to manually restart the agent (for example, after changing your agent configuration). Linux: Start, stop, restart, or check agent status For Linux, ensure you use the correct command for your init system. Select start, stop, restart, or status as appropriate: SystemD (Amazon Linux 2, SLES 12, CentOS 7 or higher, Debian 8 or higher, RHEL 7 or higher, Ubuntu 15.04 or higher): sudo systemctl <start|stop|restart|status> newrelic-infra Copy System V (Debian 7, SLES 11.4, RHEL 5): sudo /etc/init.d/newrelic-infra <start|stop|restart|status> Copy Upstart (Amazon Linux, RHEL 6, Ubuntu 14.04 or lower): sudo initctl <start|stop|restart|status> newrelic-infra Copy Windows: Start, stop, restart, or check agent status Important To start, stop, or restart the agent, you must run cmd.exe or PowerShell as Administrator. For Windows Server, you can use the Windows command prompt or PowerShell. Command prompt (cmd.exe) Start or stop the Windows agent: net <start|stop> newrelic-infra Copy Restart the Windows agent: net stop newrelic-infra ; net start newrelic-infra Copy Check the status of the Windows agent: sc query \"newrelic-infra\" | find \"STATE\" Copy PowerShell Start or stop the Windows agent: Stop-Service -Name \"newrelic-infra\" Start-Service -Name \"newrelic-infra\" Copy You can also use net start|stop newrelic-infra Restart the Windows agent: Restart-Service newrelic-infra Copy Check status of Windows agent: (Get-Service newrelic-infra).Status Copy Customize agent logs The infrastructure agent logs to a default location which depends on your platform. You can customize this location with the log_file setting. You can also generate verbose logs for troubleshooting. Determine your init system For Windows Server, the commands in this document use the Windows command prompt. For Linux, the infrastructure agent selects an init system appropriate for your distribution: Distribution SystemD System V Upstart Amazon Linux Amazon Linux 2 CentOS 7 CentOS 8 Debian 7 (\"Wheezy\") Debian 8 (\"Jessie\") Debian 9 (\"Stretch\") RHEL 5 RHEL 6 RHEL 7 RHEL 8 Ubuntu, 14.04 or lower Ubuntu, 16.04 or higher SLES 12 SLES 11 Configuration management tools To manage the infrastructure agent with your config management tool, see: Ansible configuration Chef configuration AWS Elastic Beanstalk configuration Puppet configuration",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.60464,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Start, stop, and restart the <em>infrastructure</em> <em>agent</em>",
        "sections": "Start, stop, and restart the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " Debian 7 (&quot;Wheezy&quot;) Debian 8 (&quot;Jessie&quot;) Debian 9 (&quot;Stretch&quot;) RHEL 5 RHEL 6 RHEL 7 RHEL 8 Ubuntu, 14.04 or lower Ubuntu, 16.04 or higher SLES 12 SLES 11 Configuration management tools To <em>manage</em> the <em>infrastructure</em> <em>agent</em> with <em>your</em> config management tool, see: Ansible configuration Chef configuration AWS Elastic Beanstalk configuration Puppet configuration"
      },
      "id": "603ec355e7b9d294092a0818"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-behavior": [
    {
      "sections": [
        "Agent message size"
      ],
      "title": "Agent message size",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "a762f70a367322dd6fa16c0825cbf3c3495b6b6c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/agent-message-size/",
      "published_at": "2021-05-05T03:36:57Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Since infrastructure agent version 1.0.989, the maximum message size is 1MB (10^6 bytes), and it can include events, metrics, and inventory integrations indistinctly. Previously, the maximum message size was 5MB, although inventory data was limited to 3MB.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.66418,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Agent</em> message size",
        "sections": "<em>Agent</em> message size",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Since <em>infrastructure</em> <em>agent</em> version 1.0.989, the maximum message size is 1MB (10^6 bytes), and it can include events, metrics, and inventory integrations indistinctly. Previously, the maximum message size was 5MB, although inventory data was limited to 3MB."
      },
      "id": "603ea87628ccbc6062eba74a"
    },
    {
      "sections": [
        "Troubleshoot a running infrastructure agent",
        "Linux newrelic-infra-ctl",
        "Important",
        "Windows newrelic-infra-ctl"
      ],
      "title": "Troubleshoot a running infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "e1f206feb686f819c00db0619a8534609fe19c53",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/troubleshoot-running-infrastructure-agent/",
      "published_at": "2021-05-05T03:36:57Z",
      "updated_at": "2021-03-16T07:33:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can troubleshoot our infrastructure agent using our newrelic-infra-ctl utility. This binary is also included in the installation package, in the same directory as the newrelic-infra default binary. Upon receiving the newrelic-infra-ctl command, the agent: Enables verbose logs for a period of five minutes, then reverts the log level to its previous setting. Logs all the agent config options. Logs all the integrations config options. Executes a health check for every loaded integration. (A health check is an immediate execution of the integration with extra logs and output validation.) Linux newrelic-infra-ctl In Linux systems, the troubleshooting binary is /usr/bin/newrelic-infra-ctl, available in both the package manager or the tarball assisted install methods. Important When running on Linux, newrelic-infra-ctl must be executed by either the root user or the same user running the newrelic-infra process. The newrelic-infra-ctl binary can automatically detect the agent process running in the host. It can also detect whether the agent is running inside a container. To change the default settings of newrelic-infra-ctl: To change... Execute pid newrelic-infra-ctl -pid 14580 cid (when using a containerized version of the agent) newrelic-infra-ctl -cid 8fddbcbb101c docker-api-version newrelic-infra-ctl -docker-api-version 1.24 Windows newrelic-infra-ctl In Windows, using the MSI installer, the troubleshooting binary is C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra-ctl.exe. Important In Windows, the agent process is always automatically detected. It does not depend on the pid or the cid.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.63385,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Troubleshoot a running <em>infrastructure</em> <em>agent</em>",
        "sections": "Troubleshoot a running <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "You can troubleshoot our <em>infrastructure</em> <em>agent</em> using our newrelic-infra-ctl utility. This binary is also included in the installation package, in the same directory as the newrelic-infra default binary. Upon receiving the newrelic-infra-ctl command, the <em>agent</em>: Enables verbose logs for a period"
      },
      "id": "603eb3dee7b9d22a5f2f736b"
    },
    {
      "sections": [
        "Start, stop, and restart the infrastructure agent",
        "Linux: Start, stop, restart, or check agent status",
        "Windows: Start, stop, restart, or check agent status",
        "Important",
        "Command prompt (cmd.exe)",
        "PowerShell",
        "Customize agent logs",
        "Determine your init system",
        "Configuration management tools"
      ],
      "title": "Start, stop, and restart the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "81bb8174fac415ee0ec94c51e123c32eba700d6e",
      "image": "https://docs.newrelic.com/static/19e41f9fd7395eb6f1c816aa87182bb9/103b3/otherlinux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/start-stop-restart-infrastructure-agent/",
      "published_at": "2021-05-05T03:35:30Z",
      "updated_at": "2021-03-16T06:36:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent starts automatically after you run the installation script. However, there are situations where you may need to manually restart the agent (for example, after changing your agent configuration). Linux: Start, stop, restart, or check agent status For Linux, ensure you use the correct command for your init system. Select start, stop, restart, or status as appropriate: SystemD (Amazon Linux 2, SLES 12, CentOS 7 or higher, Debian 8 or higher, RHEL 7 or higher, Ubuntu 15.04 or higher): sudo systemctl <start|stop|restart|status> newrelic-infra Copy System V (Debian 7, SLES 11.4, RHEL 5): sudo /etc/init.d/newrelic-infra <start|stop|restart|status> Copy Upstart (Amazon Linux, RHEL 6, Ubuntu 14.04 or lower): sudo initctl <start|stop|restart|status> newrelic-infra Copy Windows: Start, stop, restart, or check agent status Important To start, stop, or restart the agent, you must run cmd.exe or PowerShell as Administrator. For Windows Server, you can use the Windows command prompt or PowerShell. Command prompt (cmd.exe) Start or stop the Windows agent: net <start|stop> newrelic-infra Copy Restart the Windows agent: net stop newrelic-infra ; net start newrelic-infra Copy Check the status of the Windows agent: sc query \"newrelic-infra\" | find \"STATE\" Copy PowerShell Start or stop the Windows agent: Stop-Service -Name \"newrelic-infra\" Start-Service -Name \"newrelic-infra\" Copy You can also use net start|stop newrelic-infra Restart the Windows agent: Restart-Service newrelic-infra Copy Check status of Windows agent: (Get-Service newrelic-infra).Status Copy Customize agent logs The infrastructure agent logs to a default location which depends on your platform. You can customize this location with the log_file setting. You can also generate verbose logs for troubleshooting. Determine your init system For Windows Server, the commands in this document use the Windows command prompt. For Linux, the infrastructure agent selects an init system appropriate for your distribution: Distribution SystemD System V Upstart Amazon Linux Amazon Linux 2 CentOS 7 CentOS 8 Debian 7 (\"Wheezy\") Debian 8 (\"Jessie\") Debian 9 (\"Stretch\") RHEL 5 RHEL 6 RHEL 7 RHEL 8 Ubuntu, 14.04 or lower Ubuntu, 16.04 or higher SLES 12 SLES 11 Configuration management tools To manage the infrastructure agent with your config management tool, see: Ansible configuration Chef configuration AWS Elastic Beanstalk configuration Puppet configuration",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.6046,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Start, stop, and restart the <em>infrastructure</em> <em>agent</em>",
        "sections": "Start, stop, and restart the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " Debian 7 (&quot;Wheezy&quot;) Debian 8 (&quot;Jessie&quot;) Debian 9 (&quot;Stretch&quot;) RHEL 5 RHEL 6 RHEL 7 RHEL 8 Ubuntu, 14.04 or lower Ubuntu, 16.04 or higher SLES 12 SLES 11 Configuration management tools To <em>manage</em> the <em>infrastructure</em> <em>agent</em> with <em>your</em> config management tool, see: Ansible configuration Chef configuration AWS Elastic Beanstalk configuration Puppet configuration"
      },
      "id": "603ec355e7b9d294092a0818"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-performance-overhead": [
    {
      "sections": [
        "Infrastructure agent behavior",
        "Agent service",
        "Agent startup",
        "Monitoring and resource caps",
        "Integration data",
        "Agent shutdown",
        "Maintenance",
        "Retry behavior",
        "Manage data reporting"
      ],
      "title": "Infrastructure agent behavior",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "704d0716fb7aa5a09d0db4a9fff12e53adb31758",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-behavior/",
      "published_at": "2021-05-05T03:35:31Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the infrastructure agent, you can monitor not only individual servers, but also understand how your service performs as a whole. The agent supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these agent versions exhibit a common set of behaviors. Agent service As of infrastructure agent v1.5.59, the agent bundles a binary named newrelic-infra-service. This binary can be managed by the OS service manager. At service startup time, this binary spawns (executes) the usual newrelic-infra process and supervises its child execution. Therefore agent service process should never be restarted, unless triggered via OS service manager. Agent startup During startup the agent will: Register a signal handler. Set the loggers. Load the configuration from file, environment variables, and call arguments. Register plugins for harvesting inventory, samplers, and integrations. StatsD integration with http_server_enabled\" Open an http port (by default, 8001) for receiving data. Startup duration before harvesting and sending data is usually less than six seconds. Monitoring and resource caps By default, the infrastructure agent runs in a single core. Every second it checks if there are events to send and, if there are, it sends them to the New Relic collector. Events that may be sent include: Default infrastructure events Events recorded by New Relic integrations. For descriptions of the default infrastructure events and their collection frequencies, see Infrastructure events. Integration data Integration monitoring is done by executing integration commands at given intervals (set in the config files) and reading their stout/err. The more integrations you enable, the greater the footprint of the agent. For more information, see the documentation for specific integrations. Agent shutdown When a shutdown signal is received, the agent stops all the registered plugins and integration processes. Maintenance The agent runs as a service. On installation, we set up all the service manager-required files, such as the systemD. service file. In case of a catastrophic failure, the service manager configuration will restart the agent. There are no automatic updates to agents. To install a new agent version: Linux: Manually install agent versions through the appropriate package manager (apt, yum, zypper). Windows: Manually download the msi package and install it with msiexec.exe. Retry behavior If a request made to the ingest service is unsuccessful, the payload is discarded; subsequent requests follow an exponential backoff pattern until one succeeds. For inventory, we store the deltas between system states in cache files. On failure, these deltas are not deleted but are reused on requests that follow. Manage data reporting For information about configuring reporting of data, see Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.66418,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> behavior",
        "sections": "<em>Infrastructure</em> <em>agent</em> behavior",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the <em>infrastructure</em> <em>agent</em>, you can monitor not only individual servers, but also understand how <em>your</em> service performs as a whole. The <em>agent</em> supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these <em>agent</em> versions exhibit a common set of behaviors. <em>Agent</em>"
      },
      "id": "603eb68428ccbc8576eba7a5"
    },
    {
      "sections": [
        "Agent message size"
      ],
      "title": "Agent message size",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "a762f70a367322dd6fa16c0825cbf3c3495b6b6c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/agent-message-size/",
      "published_at": "2021-05-05T03:36:57Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Since infrastructure agent version 1.0.989, the maximum message size is 1MB (10^6 bytes), and it can include events, metrics, and inventory integrations indistinctly. Previously, the maximum message size was 5MB, although inventory data was limited to 3MB.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.66418,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Agent</em> message size",
        "sections": "<em>Agent</em> message size",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Since <em>infrastructure</em> <em>agent</em> version 1.0.989, the maximum message size is 1MB (10^6 bytes), and it can include events, metrics, and inventory integrations indistinctly. Previously, the maximum message size was 5MB, although inventory data was limited to 3MB."
      },
      "id": "603ea87628ccbc6062eba74a"
    },
    {
      "sections": [
        "Troubleshoot a running infrastructure agent",
        "Linux newrelic-infra-ctl",
        "Important",
        "Windows newrelic-infra-ctl"
      ],
      "title": "Troubleshoot a running infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "e1f206feb686f819c00db0619a8534609fe19c53",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/troubleshoot-running-infrastructure-agent/",
      "published_at": "2021-05-05T03:36:57Z",
      "updated_at": "2021-03-16T07:33:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can troubleshoot our infrastructure agent using our newrelic-infra-ctl utility. This binary is also included in the installation package, in the same directory as the newrelic-infra default binary. Upon receiving the newrelic-infra-ctl command, the agent: Enables verbose logs for a period of five minutes, then reverts the log level to its previous setting. Logs all the agent config options. Logs all the integrations config options. Executes a health check for every loaded integration. (A health check is an immediate execution of the integration with extra logs and output validation.) Linux newrelic-infra-ctl In Linux systems, the troubleshooting binary is /usr/bin/newrelic-infra-ctl, available in both the package manager or the tarball assisted install methods. Important When running on Linux, newrelic-infra-ctl must be executed by either the root user or the same user running the newrelic-infra process. The newrelic-infra-ctl binary can automatically detect the agent process running in the host. It can also detect whether the agent is running inside a container. To change the default settings of newrelic-infra-ctl: To change... Execute pid newrelic-infra-ctl -pid 14580 cid (when using a containerized version of the agent) newrelic-infra-ctl -cid 8fddbcbb101c docker-api-version newrelic-infra-ctl -docker-api-version 1.24 Windows newrelic-infra-ctl In Windows, using the MSI installer, the troubleshooting binary is C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra-ctl.exe. Important In Windows, the agent process is always automatically detected. It does not depend on the pid or the cid.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.63385,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Troubleshoot a running <em>infrastructure</em> <em>agent</em>",
        "sections": "Troubleshoot a running <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "You can troubleshoot our <em>infrastructure</em> <em>agent</em> using our newrelic-infra-ctl utility. This binary is also included in the installation package, in the same directory as the newrelic-infra default binary. Upon receiving the newrelic-infra-ctl command, the <em>agent</em>: Enables verbose logs for a period"
      },
      "id": "603eb3dee7b9d22a5f2f736b"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/manage-your-agent/start-stop-restart-infrastructure-agent": [
    {
      "sections": [
        "Infrastructure agent behavior",
        "Agent service",
        "Agent startup",
        "Monitoring and resource caps",
        "Integration data",
        "Agent shutdown",
        "Maintenance",
        "Retry behavior",
        "Manage data reporting"
      ],
      "title": "Infrastructure agent behavior",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "704d0716fb7aa5a09d0db4a9fff12e53adb31758",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-behavior/",
      "published_at": "2021-05-05T03:35:31Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the infrastructure agent, you can monitor not only individual servers, but also understand how your service performs as a whole. The agent supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these agent versions exhibit a common set of behaviors. Agent service As of infrastructure agent v1.5.59, the agent bundles a binary named newrelic-infra-service. This binary can be managed by the OS service manager. At service startup time, this binary spawns (executes) the usual newrelic-infra process and supervises its child execution. Therefore agent service process should never be restarted, unless triggered via OS service manager. Agent startup During startup the agent will: Register a signal handler. Set the loggers. Load the configuration from file, environment variables, and call arguments. Register plugins for harvesting inventory, samplers, and integrations. StatsD integration with http_server_enabled\" Open an http port (by default, 8001) for receiving data. Startup duration before harvesting and sending data is usually less than six seconds. Monitoring and resource caps By default, the infrastructure agent runs in a single core. Every second it checks if there are events to send and, if there are, it sends them to the New Relic collector. Events that may be sent include: Default infrastructure events Events recorded by New Relic integrations. For descriptions of the default infrastructure events and their collection frequencies, see Infrastructure events. Integration data Integration monitoring is done by executing integration commands at given intervals (set in the config files) and reading their stout/err. The more integrations you enable, the greater the footprint of the agent. For more information, see the documentation for specific integrations. Agent shutdown When a shutdown signal is received, the agent stops all the registered plugins and integration processes. Maintenance The agent runs as a service. On installation, we set up all the service manager-required files, such as the systemD. service file. In case of a catastrophic failure, the service manager configuration will restart the agent. There are no automatic updates to agents. To install a new agent version: Linux: Manually install agent versions through the appropriate package manager (apt, yum, zypper). Windows: Manually download the msi package and install it with msiexec.exe. Retry behavior If a request made to the ingest service is unsuccessful, the payload is discarded; subsequent requests follow an exponential backoff pattern until one succeeds. For inventory, we store the deltas between system states in cache files. On failure, these deltas are not deleted but are reused on requests that follow. Manage data reporting For information about configuring reporting of data, see Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.66415,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> behavior",
        "sections": "<em>Infrastructure</em> <em>agent</em> behavior",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the <em>infrastructure</em> <em>agent</em>, you can monitor not only individual servers, but also understand how <em>your</em> service performs as a whole. The <em>agent</em> supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these <em>agent</em> versions exhibit a common set of behaviors. <em>Agent</em>"
      },
      "id": "603eb68428ccbc8576eba7a5"
    },
    {
      "sections": [
        "Agent message size"
      ],
      "title": "Agent message size",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "a762f70a367322dd6fa16c0825cbf3c3495b6b6c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/agent-message-size/",
      "published_at": "2021-05-05T03:36:57Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Since infrastructure agent version 1.0.989, the maximum message size is 1MB (10^6 bytes), and it can include events, metrics, and inventory integrations indistinctly. Previously, the maximum message size was 5MB, although inventory data was limited to 3MB.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.66415,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Agent</em> message size",
        "sections": "<em>Agent</em> message size",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Since <em>infrastructure</em> <em>agent</em> version 1.0.989, the maximum message size is 1MB (10^6 bytes), and it can include events, metrics, and inventory integrations indistinctly. Previously, the maximum message size was 5MB, although inventory data was limited to 3MB."
      },
      "id": "603ea87628ccbc6062eba74a"
    },
    {
      "sections": [
        "Troubleshoot a running infrastructure agent",
        "Linux newrelic-infra-ctl",
        "Important",
        "Windows newrelic-infra-ctl"
      ],
      "title": "Troubleshoot a running infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "e1f206feb686f819c00db0619a8534609fe19c53",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/troubleshoot-running-infrastructure-agent/",
      "published_at": "2021-05-05T03:36:57Z",
      "updated_at": "2021-03-16T07:33:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "You can troubleshoot our infrastructure agent using our newrelic-infra-ctl utility. This binary is also included in the installation package, in the same directory as the newrelic-infra default binary. Upon receiving the newrelic-infra-ctl command, the agent: Enables verbose logs for a period of five minutes, then reverts the log level to its previous setting. Logs all the agent config options. Logs all the integrations config options. Executes a health check for every loaded integration. (A health check is an immediate execution of the integration with extra logs and output validation.) Linux newrelic-infra-ctl In Linux systems, the troubleshooting binary is /usr/bin/newrelic-infra-ctl, available in both the package manager or the tarball assisted install methods. Important When running on Linux, newrelic-infra-ctl must be executed by either the root user or the same user running the newrelic-infra process. The newrelic-infra-ctl binary can automatically detect the agent process running in the host. It can also detect whether the agent is running inside a container. To change the default settings of newrelic-infra-ctl: To change... Execute pid newrelic-infra-ctl -pid 14580 cid (when using a containerized version of the agent) newrelic-infra-ctl -cid 8fddbcbb101c docker-api-version newrelic-infra-ctl -docker-api-version 1.24 Windows newrelic-infra-ctl In Windows, using the MSI installer, the troubleshooting binary is C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra-ctl.exe. Important In Windows, the agent process is always automatically detected. It does not depend on the pid or the cid.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.63385,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Troubleshoot a running <em>infrastructure</em> <em>agent</em>",
        "sections": "Troubleshoot a running <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "You can troubleshoot our <em>infrastructure</em> <em>agent</em> using our newrelic-infra-ctl utility. This binary is also included in the installation package, in the same directory as the newrelic-infra default binary. Upon receiving the newrelic-infra-ctl command, the <em>agent</em>: Enables verbose logs for a period"
      },
      "id": "603eb3dee7b9d22a5f2f736b"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/manage-your-agent/troubleshoot-running-infrastructure-agent": [
    {
      "sections": [
        "Infrastructure agent behavior",
        "Agent service",
        "Agent startup",
        "Monitoring and resource caps",
        "Integration data",
        "Agent shutdown",
        "Maintenance",
        "Retry behavior",
        "Manage data reporting"
      ],
      "title": "Infrastructure agent behavior",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "704d0716fb7aa5a09d0db4a9fff12e53adb31758",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/infrastructure-agent-behavior/",
      "published_at": "2021-05-05T03:35:31Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the infrastructure agent, you can monitor not only individual servers, but also understand how your service performs as a whole. The agent supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these agent versions exhibit a common set of behaviors. Agent service As of infrastructure agent v1.5.59, the agent bundles a binary named newrelic-infra-service. This binary can be managed by the OS service manager. At service startup time, this binary spawns (executes) the usual newrelic-infra process and supervises its child execution. Therefore agent service process should never be restarted, unless triggered via OS service manager. Agent startup During startup the agent will: Register a signal handler. Set the loggers. Load the configuration from file, environment variables, and call arguments. Register plugins for harvesting inventory, samplers, and integrations. StatsD integration with http_server_enabled\" Open an http port (by default, 8001) for receiving data. Startup duration before harvesting and sending data is usually less than six seconds. Monitoring and resource caps By default, the infrastructure agent runs in a single core. Every second it checks if there are events to send and, if there are, it sends them to the New Relic collector. Events that may be sent include: Default infrastructure events Events recorded by New Relic integrations. For descriptions of the default infrastructure events and their collection frequencies, see Infrastructure events. Integration data Integration monitoring is done by executing integration commands at given intervals (set in the config files) and reading their stout/err. The more integrations you enable, the greater the footprint of the agent. For more information, see the documentation for specific integrations. Agent shutdown When a shutdown signal is received, the agent stops all the registered plugins and integration processes. Maintenance The agent runs as a service. On installation, we set up all the service manager-required files, such as the systemD. service file. In case of a catastrophic failure, the service manager configuration will restart the agent. There are no automatic updates to agents. To install a new agent version: Linux: Manually install agent versions through the appropriate package manager (apt, yum, zypper). Windows: Manually download the msi package and install it with msiexec.exe. Retry behavior If a request made to the ingest service is unsuccessful, the payload is discarded; subsequent requests follow an exponential backoff pattern until one succeeds. For inventory, we store the deltas between system states in cache files. On failure, these deltas are not deleted but are reused on requests that follow. Manage data reporting For information about configuring reporting of data, see Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.66415,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> behavior",
        "sections": "<em>Infrastructure</em> <em>agent</em> behavior",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the <em>infrastructure</em> <em>agent</em>, you can monitor not only individual servers, but also understand how <em>your</em> service performs as a whole. The <em>agent</em> supports Amazon Linux, CentOS, Debian, RHEL, and Ubuntu as well as Windows Server. All of these <em>agent</em> versions exhibit a common set of behaviors. <em>Agent</em>"
      },
      "id": "603eb68428ccbc8576eba7a5"
    },
    {
      "sections": [
        "Agent message size"
      ],
      "title": "Agent message size",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "a762f70a367322dd6fa16c0825cbf3c3495b6b6c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/agent-message-size/",
      "published_at": "2021-05-05T03:36:57Z",
      "updated_at": "2021-03-16T08:32:22Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Since infrastructure agent version 1.0.989, the maximum message size is 1MB (10^6 bytes), and it can include events, metrics, and inventory integrations indistinctly. Previously, the maximum message size was 5MB, although inventory data was limited to 3MB.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.66415,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Agent</em> message size",
        "sections": "<em>Agent</em> message size",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Since <em>infrastructure</em> <em>agent</em> version 1.0.989, the maximum message size is 1MB (10^6 bytes), and it can include events, metrics, and inventory integrations indistinctly. Previously, the maximum message size was 5MB, although inventory data was limited to 3MB."
      },
      "id": "603ea87628ccbc6062eba74a"
    },
    {
      "sections": [
        "Start, stop, and restart the infrastructure agent",
        "Linux: Start, stop, restart, or check agent status",
        "Windows: Start, stop, restart, or check agent status",
        "Important",
        "Command prompt (cmd.exe)",
        "PowerShell",
        "Customize agent logs",
        "Determine your init system",
        "Configuration management tools"
      ],
      "title": "Start, stop, and restart the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Manage your agent"
      ],
      "external_id": "81bb8174fac415ee0ec94c51e123c32eba700d6e",
      "image": "https://docs.newrelic.com/static/19e41f9fd7395eb6f1c816aa87182bb9/103b3/otherlinux.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/manage-your-agent/start-stop-restart-infrastructure-agent/",
      "published_at": "2021-05-05T03:35:30Z",
      "updated_at": "2021-03-16T06:36:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent starts automatically after you run the installation script. However, there are situations where you may need to manually restart the agent (for example, after changing your agent configuration). Linux: Start, stop, restart, or check agent status For Linux, ensure you use the correct command for your init system. Select start, stop, restart, or status as appropriate: SystemD (Amazon Linux 2, SLES 12, CentOS 7 or higher, Debian 8 or higher, RHEL 7 or higher, Ubuntu 15.04 or higher): sudo systemctl <start|stop|restart|status> newrelic-infra Copy System V (Debian 7, SLES 11.4, RHEL 5): sudo /etc/init.d/newrelic-infra <start|stop|restart|status> Copy Upstart (Amazon Linux, RHEL 6, Ubuntu 14.04 or lower): sudo initctl <start|stop|restart|status> newrelic-infra Copy Windows: Start, stop, restart, or check agent status Important To start, stop, or restart the agent, you must run cmd.exe or PowerShell as Administrator. For Windows Server, you can use the Windows command prompt or PowerShell. Command prompt (cmd.exe) Start or stop the Windows agent: net <start|stop> newrelic-infra Copy Restart the Windows agent: net stop newrelic-infra ; net start newrelic-infra Copy Check the status of the Windows agent: sc query \"newrelic-infra\" | find \"STATE\" Copy PowerShell Start or stop the Windows agent: Stop-Service -Name \"newrelic-infra\" Start-Service -Name \"newrelic-infra\" Copy You can also use net start|stop newrelic-infra Restart the Windows agent: Restart-Service newrelic-infra Copy Check status of Windows agent: (Get-Service newrelic-infra).Status Copy Customize agent logs The infrastructure agent logs to a default location which depends on your platform. You can customize this location with the log_file setting. You can also generate verbose logs for troubleshooting. Determine your init system For Windows Server, the commands in this document use the Windows command prompt. For Linux, the infrastructure agent selects an init system appropriate for your distribution: Distribution SystemD System V Upstart Amazon Linux Amazon Linux 2 CentOS 7 CentOS 8 Debian 7 (\"Wheezy\") Debian 8 (\"Jessie\") Debian 9 (\"Stretch\") RHEL 5 RHEL 6 RHEL 7 RHEL 8 Ubuntu, 14.04 or lower Ubuntu, 16.04 or higher SLES 12 SLES 11 Configuration management tools To manage the infrastructure agent with your config management tool, see: Ansible configuration Chef configuration AWS Elastic Beanstalk configuration Puppet configuration",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 287.6046,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Start, stop, and restart the <em>infrastructure</em> <em>agent</em>",
        "sections": "Start, stop, and restart the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " Debian 7 (&quot;Wheezy&quot;) Debian 8 (&quot;Jessie&quot;) Debian 9 (&quot;Stretch&quot;) RHEL 5 RHEL 6 RHEL 7 RHEL 8 Ubuntu, 14.04 or lower Ubuntu, 16.04 or higher SLES 12 SLES 11 Configuration management tools To <em>manage</em> the <em>infrastructure</em> <em>agent</em> with <em>your</em> config management tool, see: Ansible configuration Chef configuration AWS Elastic Beanstalk configuration Puppet configuration"
      },
      "id": "603ec355e7b9d294092a0818"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/uninstall-infrastructure-agent": [
    {
      "sections": [
        "Uninstall infrastructure integrations",
        "Cloud integrations",
        "AWS",
        "Azure",
        "Google Cloud Platform (GCP)",
        "On-host integrations",
        "Apache",
        "Cassandra",
        "Kubernetes",
        "MySQL",
        "NGINX",
        "Redis",
        "StatsD",
        "Moving away from the integrations package",
        "Uninstall package"
      ],
      "title": "Uninstall infrastructure integrations",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Update or uninstall"
      ],
      "external_id": "1e9232193cbf71bdbe1a6c6d0374ed0d6b7e7b0f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/uninstall-infrastructure-integrations/",
      "published_at": "2021-05-05T03:38:21Z",
      "updated_at": "2021-03-16T08:33:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Uninstalling the infrastructure agent does not directly affect any of your infrastructure Integrations: if you uninstall the agent, your integrations will remain. Similarly, if you disable or uninstall your integrations, the infrastructure agent will remain. To uninstall any of your integrations, follow the procedure corresponding to the type of integration. Cloud integrations AWS You can disable infrastructure AWS integrations and still retain the connection between your AWS account and New Relic. We recommend not to disable your EC2 and EBS integrations because those add important metadata to your infrastructure data. If you want to... Do this Disable one or more AWS service integrations To disable services while keeping your AWS account linked to New Relic: From one.newrelic.com > Infrastructure, select AWS > Manage services. From your Edit AWS account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all AWS integrations To disconnect your AWS account completely from New Relic, you need to unlink your AWS account. This disables all New Relic integrations associated with that AWS account. Go to one.newrelic.com > Infrastructure > AWS > Manage services. From your Edit AWS account page, select Unlink this account. Save your changes. Sign in to AWS and select Services > IAM > Roles. Select the checkbox for the role you want to delete, then select Role Actions > Delete Role. Unlinking your AWS account will disable the trust relationship set up via your ARN. Azure If you want to... Do this Disable one or more Azure service integrations To disable services while keeping your Azure account linked to New Relic: From ione.newrelic.com > Infrastructure > Azure > Manage services. From your Edit Azure account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all Azure integrations To disconnect your Azure account completely from New Relic, you need to unlink your Azure account. This requires being either the user who registered the app or an administrator. This procedure will disable all New Relic integrations associated with that Azure account. Go to one.newrelic.com > Infrastructure > Azure > Manage services. From your Edit Azure account page, select Unlink this account. Save your changes. Sign in to Azure and go into All Services > Identity > App registrations, or go to Azure Active Directory service and select App registrations. Find the registered app (the recommended name is NewRelic-Integrations). To see the full list of available apps, select the dropdown menu beside the search field and select All apps. Select the app and, on the panel that opens, select Delete. Google Cloud Platform (GCP) If you want to... Do this Disable one or more GCP service integrations To disable services while keeping your GCP account linked to New Relic: From one.newrelic.com > Infrastructure > GCP > Manage services. From your Edit GCP account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all GCP integrations To disconnect your GCP account completely from New Relic, you need to unlink your GCP account. This disables all New Relic integrations associated with that GCP account. If you registered the GCP project using a User account, follow these steps. Go to one.newrelic.com > Infrastructure > GCP > Manage services. From your Edit GCP account page, select Unlink this account. Save your changes. If you registered the GCP project using a Service account, follow these steps. If you are deleting a custom role, be aware that this role may be used for other purposes besides New Relic access. Sign in to New Relic and go to Infrastructure > Integrations > Google Cloud Platform. For a standard (non-custom) user role, select Manage Services for the account you want to remove. Copy the value of User and save it. OR For a custom user role, go to IAM > admin > Roles, search for the role, select it, and select DELETE. You are now finished and can skip the remaining steps. Standard (non-custom) user role: Sign in to Google Cloud and select the correct project in the Select a project box. From the navigation menu, select IAM & admin > IAM. Search for and select the user value you saved, then select REMOVE. On-host integrations If you used the integrations package, see the integrations package instructions. Here are some examples: Apache Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-apache yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-apache zypper (SLES) sudo zypper -n remove nri-apache Cassandra Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-cassandra yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-cassandra zypper (SLES) sudo zypper -n remove nri-cassandra Kubernetes Each cluster will have a single node where kubectl is running. To uninstall the Kubernetes integration, use the following command on each of these nodes: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy MySQL Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-mysql yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-mysql zypper (SLES) sudo zypper -n remove nri-mysql NGINX Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-nginx yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-nginx zypper (SLES) sudo zypper -n remove nri-nginx Redis Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-redis yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-redis zypper (SLES) sudo zypper -n remove nri-redis StatsD cd /path/to/statsd npm uninstall @newrelic/statsd-infra-backend From the StatsD config.js, remove the \"@newrelic/statsd-infra-backend\" entry from the list of backends. Restart StatsD. Moving away from the integrations package While it is still possible to use the integrations package, we recommend removing it completely and working with integrations on an individual basis. The last integration package contains the following versions of the integrations: Apache 1.1.2 Cassandra 2.0.3 MySQL 1.1.5 Nginx 1.0.2 Redis 1.0.1 If you remove the integrations package and want to continue using the related on-host integrations, you will need to install them one by one. To uninstall the package and re-install your integrations: Remove the integrations package by following these instructions. The config files from the old integrations will not be deleted, so you won’t have to configure them again. Uninstall package Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove newrelic-infra-integrations sudo apt-get autoremove yum (Amazon Linux, CentOS, or RHEL) sudo yum remove newrelic-infra-integrations sudo yum autoremove zypper (SLES) sudo zypper -n remove newrelic-infra-integrations --clean-deps Copy Install your integrations one by one following these instructions. To replicate the integrations package, you will need to install all the available integrations again.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.30502,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Uninstall</em> <em>infrastructure</em> integrations",
        "sections": "<em>Uninstall</em> <em>infrastructure</em> integrations",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Uninstalling the <em>infrastructure</em> <em>agent</em> does not directly affect any of your <em>infrastructure</em> Integrations: if you <em>uninstall</em> the <em>agent</em>, your integrations will remain. Similarly, if you disable or <em>uninstall</em> your integrations, the <em>infrastructure</em> <em>agent</em> will remain. To <em>uninstall</em> any of your integrations"
      },
      "id": "603ea831196a67fc6fa83db5"
    },
    {
      "sections": [
        "Update the infrastructure agent",
        "Tip",
        "View the infrastructure agent version",
        "Update the agent for installs using the package manager",
        "Update using apt (Debian, Ubuntu)",
        "Update using yum (Amazon Linux, CentOS, RHEL)",
        "Update using Zypper (SLES)",
        "Update on Windows Server (32 bits)",
        "Update on Windows Server (64 bits)",
        "Update with config management tools",
        "Update the agent for assisted and manual tarball installs",
        "Important",
        "Update the containerized version of the agent",
        "Identify outdated agent versions from the UI"
      ],
      "title": "Update the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Update or uninstall"
      ],
      "external_id": "1ddaa00ceaf5936084d25f207f868d89fd0957f6",
      "image": "https://docs.newrelic.com/static/06b72c159cd2d6b502bb7cbab7a98e67/103b3/ebs.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/update-infrastructure-agent/",
      "published_at": "2021-05-05T03:36:57Z",
      "updated_at": "2021-03-13T03:15:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how to update the infrastructure agent to the latest version for Linux and Windows servers. Tip To install the infrastructure agent for the first time, see the installation documentation for Linux, Windows, or configuration management tools. To uninstall the infrastructure agent, see Uninstall the Infrastructure agent. View the infrastructure agent version The infrastructure agent does not update itself automatically. Check the infrastructure agent release notes to make sure you have the latest agent version. To view the current infrastructure agent version for a host, use any of these options: Go to one.newrelic.com > Infrastructure > Settings > Agents > Agent version. Go to one.newrelic.com > Infrastructure > Hosts > (select a host). Create a query for SystemSample. Update the agent for installs using the package manager If you used the default installation process, use your package manager to update the program and its dependencies to the latest version. Here are examples for some common systems: Update using apt ( Debian, Ubuntu) To manually update the infrastructure agent with apt-get: sudo apt-get update && sudo apt-get install --only-upgrade newrelic-infra -y Copy Update using yum ( Amazon Linux, CentOS, RHEL) To manually update the infrastructure agent with yum: sudo yum update newrelic-infra -y Copy After updating you may need to start the agent. Update using Zypper ( SLES) To manually update the infrastructure agent with Zypper: sudo zypper -n update newrelic-infra Copy After updating you may need to start the agent. Update on Windows Server (32 bits) To manually update the infrastructure agent on Windows Server: Download the latest .MSI installer image from https://download.newrelic.com/infrastructure_agent/windows/386/newrelic-infra-386.msi Run the install script. To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra-386.msi Copy After updating you may need to start the agent. Update on Windows Server (64 bits) To manually update the infrastructure agent on Windows Server: Download the latest .MSI installer image from https://download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi Run the install script. To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi Copy After updating you may need to start the agent. Update with config management tools To update the infrastructure agent using your configuration management tool, see the documentation for your tool: Configure with Ansible Configure with Chef Configure with AWS Elastic Beanstalk Configure with Puppet Update the agent for assisted and manual tarball installs Important Since there are are no automated scripts, old files may remain when you update. Be sure to manually remove outdated files. To update the agent, download the file again and follow the installation procedure for Linux (assisted or manual) or Windows (assisted or manual). This will overwrite your old installation. Update the containerized version of the agent Use the latest label to ensure that our Docker image is automatically updated. Identify outdated agent versions from the UI You can use the Infrastructure monitoring UI to search for outdated agent versions: Go to one.newrelic.com > Infrastructure > Inventory * * * * . In the search bar, type newrelic-infra. Select a group's dropdown to see the agent versions for that group. To manually check the infrastructure agent versions, you can log onto a server and run newrelic-infra --version, or the applicable command for your package manager.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 329.679,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Update</em> the <em>infrastructure</em> <em>agent</em>",
        "sections": "<em>Update</em> <em>the</em> <em>agent</em> for <em>installs</em> using <em>the</em> package manager",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Read on to learn how to <em>update</em> the <em>infrastructure</em> <em>agent</em> to the latest version for Linux and Windows servers. Tip To <em>install</em> the <em>infrastructure</em> <em>agent</em> for the first time, see the installation documentation for Linux, Windows, or configuration management tools. To <em>uninstall</em> the <em>infrastructure</em> <em>agent</em>"
      },
      "id": "60440e8fe7b9d252a2579a19"
    },
    {
      "sections": [
        "Zip assisted install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Zip assisted install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "fcadbeed626401863b6b16e5c52e9a472a5ac13e",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-04-16T02:59:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. Install the agent Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Once it's unpacked, access and edit the installation PowerShell script installer.ps1. Update your license key. Optional: Update any other parameters. Execute installer.ps1 with admin rights. Configure your installation Important Make sure any custom folder defined in the installation settings has permissions limitations properly defined. The infrastructure agent might execute any integration defined in the NRIA_PLUGIN_DIR directory with Administrator permissions. You can configure the following parameters during the assisted install for Windows: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: C:\\Program Files\\New Relic\\newrelic-infra Copy NRIA_APP_DATA_DIR This configures the data directory to store inventory and other agent files. Default: C:\\%ProgramData%\\New Relic\\newrelic-infra Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The New Relic license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy NRIA_OVERWRITE By default and for security reasons, Windows does not install a service if there's another service with the same name already installed. To bypass this check, make sure this setting NRIA_OVERWRITE is TRUE. Default: TRUE Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: C:\\Program Files\\NewRelic\\newrelic-infra\\inregrations.d Copy NRIA_SERVICE_NAME This provides the name for the Windows service. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.48088,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "sections": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. <em>Install</em> the <em>agent</em> Important As of version"
      },
      "id": "603ea7af196a67dab0a83d9d"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/uninstall-infrastructure-integrations": [
    {
      "sections": [
        "Uninstall the infrastructure agent",
        "Uninstall the Linux infrastructure agent",
        "Uninstall with apt (Debian, Ubuntu)",
        "Uninstall with yum (Amazon Linux, CentOS, RHEL)",
        "Uninstall with Zypper (SLES)",
        "Important",
        "Uninstall the Windows infrastructure agent",
        "Tip",
        "Uninstall using config management tools",
        "Optional: Purge remaining files"
      ],
      "title": "Uninstall the infrastructure agent ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Update or uninstall"
      ],
      "external_id": "d99fe6244698657f7f2e67bc6f6fcbfc9d8ffbf9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/uninstall-infrastructure-agent/",
      "published_at": "2021-05-05T03:36:57Z",
      "updated_at": "2021-03-16T08:33:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Your hosts are added automatically when you install the infrastructure agent for your Linux or Windows operating system, or update the agent. Similarly, your hosts disappear automatically when you uninstall the agent. You do not need to manually add or remove your hosts. Uninstalling the agent does not directly affect any of your integrations. To uninstall an integration, see Uninstall integrations. Uninstall the Linux infrastructure agent If you used the default install procedure for your infrastructure agent for Linux environments, use your package management tools to uninstall it. You do not need to stop the service before running the uninstall command. Uninstall with apt ( Debian, Ubuntu) Execute the following command as root: sudo apt-get remove newrelic-infra Copy Uninstall with yum ( Amazon Linux, CentOS, RHEL) Execute the following command as root: sudo yum remove newrelic-infra Copy Uninstall with Zypper ( SLES) Execute the following command as root: sudo zypper -n remove newrelic-infra Copy If you followed an assisted or manual installation process, you need to manually delete all the files created when you unpacked the agent file. Important Since there are are no automated scripts, old files may remain when you uninstall. Be sure to manually remove outdated files. Uninstall the Windows infrastructure agent Tip Requires Administrator rights in your Windows admin group. If you used the default install procedure for the infrastructure agent for Windows environments, to uninstall: Stop the infrastructure agent. From the Windows Control Panel, use the Add/Remove Programs and Features tool to uninstall the infrastructure agent. From the Windows Program Files, manually delete the New Relic folder to delete all files associated with the infrastructure agent for Windows. If you followed an assisted or manual installation process, you need to manually delete all the files created when you unpacked the agent file. Important Since there are are no automated scripts, old files may remain when you uninstall. Be sure to manually remove outdated files. Uninstall using config management tools To uninstall the infrastructure agent if you used a configuration management tool: Config management tools Uninstall New Relic infrastructure agent Ansible Set the 'agent_state' parameter to 'absent' Chef Set the 'agent_action' node to uninstall AWS Elastic Beanstalk Remove newrelic.config from .ebextensions, then deploy. Puppet Set the 'ensure' parameter to 'absent' Optional: Purge remaining files If you use standard package management tools for your selected platform, the uninstall process typically leaves configuration and other miscellaneous files. If you need to completely purge any of the remaining files after uninstalling the New Relic infrastructure agent, follow standard procedures for your operating system.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.30502,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Uninstall</em> the <em>infrastructure</em> <em>agent</em> ",
        "sections": "<em>Uninstall</em> the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Your hosts are added automatically when you <em>install</em> the <em>infrastructure</em> <em>agent</em> for your Linux or Windows operating system, or <em>update</em> the <em>agent</em>. Similarly, your hosts disappear automatically when you <em>uninstall</em> the <em>agent</em>. You do not need to manually add or remove your hosts. Uninstalling the <em>agent</em> does"
      },
      "id": "603ea0e964441f31114e885c"
    },
    {
      "sections": [
        "Update the infrastructure agent",
        "Tip",
        "View the infrastructure agent version",
        "Update the agent for installs using the package manager",
        "Update using apt (Debian, Ubuntu)",
        "Update using yum (Amazon Linux, CentOS, RHEL)",
        "Update using Zypper (SLES)",
        "Update on Windows Server (32 bits)",
        "Update on Windows Server (64 bits)",
        "Update with config management tools",
        "Update the agent for assisted and manual tarball installs",
        "Important",
        "Update the containerized version of the agent",
        "Identify outdated agent versions from the UI"
      ],
      "title": "Update the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Update or uninstall"
      ],
      "external_id": "1ddaa00ceaf5936084d25f207f868d89fd0957f6",
      "image": "https://docs.newrelic.com/static/06b72c159cd2d6b502bb7cbab7a98e67/103b3/ebs.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/update-infrastructure-agent/",
      "published_at": "2021-05-05T03:36:57Z",
      "updated_at": "2021-03-13T03:15:45Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Read on to learn how to update the infrastructure agent to the latest version for Linux and Windows servers. Tip To install the infrastructure agent for the first time, see the installation documentation for Linux, Windows, or configuration management tools. To uninstall the infrastructure agent, see Uninstall the Infrastructure agent. View the infrastructure agent version The infrastructure agent does not update itself automatically. Check the infrastructure agent release notes to make sure you have the latest agent version. To view the current infrastructure agent version for a host, use any of these options: Go to one.newrelic.com > Infrastructure > Settings > Agents > Agent version. Go to one.newrelic.com > Infrastructure > Hosts > (select a host). Create a query for SystemSample. Update the agent for installs using the package manager If you used the default installation process, use your package manager to update the program and its dependencies to the latest version. Here are examples for some common systems: Update using apt ( Debian, Ubuntu) To manually update the infrastructure agent with apt-get: sudo apt-get update && sudo apt-get install --only-upgrade newrelic-infra -y Copy Update using yum ( Amazon Linux, CentOS, RHEL) To manually update the infrastructure agent with yum: sudo yum update newrelic-infra -y Copy After updating you may need to start the agent. Update using Zypper ( SLES) To manually update the infrastructure agent with Zypper: sudo zypper -n update newrelic-infra Copy After updating you may need to start the agent. Update on Windows Server (32 bits) To manually update the infrastructure agent on Windows Server: Download the latest .MSI installer image from https://download.newrelic.com/infrastructure_agent/windows/386/newrelic-infra-386.msi Run the install script. To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra-386.msi Copy After updating you may need to start the agent. Update on Windows Server (64 bits) To manually update the infrastructure agent on Windows Server: Download the latest .MSI installer image from https://download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi Run the install script. To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi Copy After updating you may need to start the agent. Update with config management tools To update the infrastructure agent using your configuration management tool, see the documentation for your tool: Configure with Ansible Configure with Chef Configure with AWS Elastic Beanstalk Configure with Puppet Update the agent for assisted and manual tarball installs Important Since there are are no automated scripts, old files may remain when you update. Be sure to manually remove outdated files. To update the agent, download the file again and follow the installation procedure for Linux (assisted or manual) or Windows (assisted or manual). This will overwrite your old installation. Update the containerized version of the agent Use the latest label to ensure that our Docker image is automatically updated. Identify outdated agent versions from the UI You can use the Infrastructure monitoring UI to search for outdated agent versions: Go to one.newrelic.com > Infrastructure > Inventory * * * * . In the search bar, type newrelic-infra. Select a group's dropdown to see the agent versions for that group. To manually check the infrastructure agent versions, you can log onto a server and run newrelic-infra --version, or the applicable command for your package manager.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 329.679,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Update</em> the <em>infrastructure</em> <em>agent</em>",
        "sections": "<em>Update</em> <em>the</em> <em>agent</em> for <em>installs</em> using <em>the</em> package manager",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Read on to learn how to <em>update</em> the <em>infrastructure</em> <em>agent</em> to the latest version for Linux and Windows servers. Tip To <em>install</em> the <em>infrastructure</em> <em>agent</em> for the first time, see the installation documentation for Linux, Windows, or configuration management tools. To <em>uninstall</em> the <em>infrastructure</em> <em>agent</em>"
      },
      "id": "60440e8fe7b9d252a2579a19"
    },
    {
      "sections": [
        "Zip assisted install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Zip assisted install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "fcadbeed626401863b6b16e5c52e9a472a5ac13e",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-04-16T02:59:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. Install the agent Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Once it's unpacked, access and edit the installation PowerShell script installer.ps1. Update your license key. Optional: Update any other parameters. Execute installer.ps1 with admin rights. Configure your installation Important Make sure any custom folder defined in the installation settings has permissions limitations properly defined. The infrastructure agent might execute any integration defined in the NRIA_PLUGIN_DIR directory with Administrator permissions. You can configure the following parameters during the assisted install for Windows: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: C:\\Program Files\\New Relic\\newrelic-infra Copy NRIA_APP_DATA_DIR This configures the data directory to store inventory and other agent files. Default: C:\\%ProgramData%\\New Relic\\newrelic-infra Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The New Relic license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy NRIA_OVERWRITE By default and for security reasons, Windows does not install a service if there's another service with the same name already installed. To bypass this check, make sure this setting NRIA_OVERWRITE is TRUE. Default: TRUE Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: C:\\Program Files\\NewRelic\\newrelic-infra\\inregrations.d Copy NRIA_SERVICE_NAME This provides the name for the Windows service. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.48088,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "sections": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. <em>Install</em> the <em>agent</em> Important As of version"
      },
      "id": "603ea7af196a67dab0a83d9d"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/update-infrastructure-agent": [
    {
      "sections": [
        "Uninstall the infrastructure agent",
        "Uninstall the Linux infrastructure agent",
        "Uninstall with apt (Debian, Ubuntu)",
        "Uninstall with yum (Amazon Linux, CentOS, RHEL)",
        "Uninstall with Zypper (SLES)",
        "Important",
        "Uninstall the Windows infrastructure agent",
        "Tip",
        "Uninstall using config management tools",
        "Optional: Purge remaining files"
      ],
      "title": "Uninstall the infrastructure agent ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Update or uninstall"
      ],
      "external_id": "d99fe6244698657f7f2e67bc6f6fcbfc9d8ffbf9",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/uninstall-infrastructure-agent/",
      "published_at": "2021-05-05T03:36:57Z",
      "updated_at": "2021-03-16T08:33:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Your hosts are added automatically when you install the infrastructure agent for your Linux or Windows operating system, or update the agent. Similarly, your hosts disappear automatically when you uninstall the agent. You do not need to manually add or remove your hosts. Uninstalling the agent does not directly affect any of your integrations. To uninstall an integration, see Uninstall integrations. Uninstall the Linux infrastructure agent If you used the default install procedure for your infrastructure agent for Linux environments, use your package management tools to uninstall it. You do not need to stop the service before running the uninstall command. Uninstall with apt ( Debian, Ubuntu) Execute the following command as root: sudo apt-get remove newrelic-infra Copy Uninstall with yum ( Amazon Linux, CentOS, RHEL) Execute the following command as root: sudo yum remove newrelic-infra Copy Uninstall with Zypper ( SLES) Execute the following command as root: sudo zypper -n remove newrelic-infra Copy If you followed an assisted or manual installation process, you need to manually delete all the files created when you unpacked the agent file. Important Since there are are no automated scripts, old files may remain when you uninstall. Be sure to manually remove outdated files. Uninstall the Windows infrastructure agent Tip Requires Administrator rights in your Windows admin group. If you used the default install procedure for the infrastructure agent for Windows environments, to uninstall: Stop the infrastructure agent. From the Windows Control Panel, use the Add/Remove Programs and Features tool to uninstall the infrastructure agent. From the Windows Program Files, manually delete the New Relic folder to delete all files associated with the infrastructure agent for Windows. If you followed an assisted or manual installation process, you need to manually delete all the files created when you unpacked the agent file. Important Since there are are no automated scripts, old files may remain when you uninstall. Be sure to manually remove outdated files. Uninstall using config management tools To uninstall the infrastructure agent if you used a configuration management tool: Config management tools Uninstall New Relic infrastructure agent Ansible Set the 'agent_state' parameter to 'absent' Chef Set the 'agent_action' node to uninstall AWS Elastic Beanstalk Remove newrelic.config from .ebextensions, then deploy. Puppet Set the 'ensure' parameter to 'absent' Optional: Purge remaining files If you use standard package management tools for your selected platform, the uninstall process typically leaves configuration and other miscellaneous files. If you need to completely purge any of the remaining files after uninstalling the New Relic infrastructure agent, follow standard procedures for your operating system.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.30502,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Uninstall</em> the <em>infrastructure</em> <em>agent</em> ",
        "sections": "<em>Uninstall</em> the <em>infrastructure</em> <em>agent</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Your hosts are added automatically when you <em>install</em> the <em>infrastructure</em> <em>agent</em> for your Linux or Windows operating system, or <em>update</em> the <em>agent</em>. Similarly, your hosts disappear automatically when you <em>uninstall</em> the <em>agent</em>. You do not need to manually add or remove your hosts. Uninstalling the <em>agent</em> does"
      },
      "id": "603ea0e964441f31114e885c"
    },
    {
      "sections": [
        "Uninstall infrastructure integrations",
        "Cloud integrations",
        "AWS",
        "Azure",
        "Google Cloud Platform (GCP)",
        "On-host integrations",
        "Apache",
        "Cassandra",
        "Kubernetes",
        "MySQL",
        "NGINX",
        "Redis",
        "StatsD",
        "Moving away from the integrations package",
        "Uninstall package"
      ],
      "title": "Uninstall infrastructure integrations",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Update or uninstall"
      ],
      "external_id": "1e9232193cbf71bdbe1a6c6d0374ed0d6b7e7b0f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/update-or-uninstall/uninstall-infrastructure-integrations/",
      "published_at": "2021-05-05T03:38:21Z",
      "updated_at": "2021-03-16T08:33:55Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Uninstalling the infrastructure agent does not directly affect any of your infrastructure Integrations: if you uninstall the agent, your integrations will remain. Similarly, if you disable or uninstall your integrations, the infrastructure agent will remain. To uninstall any of your integrations, follow the procedure corresponding to the type of integration. Cloud integrations AWS You can disable infrastructure AWS integrations and still retain the connection between your AWS account and New Relic. We recommend not to disable your EC2 and EBS integrations because those add important metadata to your infrastructure data. If you want to... Do this Disable one or more AWS service integrations To disable services while keeping your AWS account linked to New Relic: From one.newrelic.com > Infrastructure, select AWS > Manage services. From your Edit AWS account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all AWS integrations To disconnect your AWS account completely from New Relic, you need to unlink your AWS account. This disables all New Relic integrations associated with that AWS account. Go to one.newrelic.com > Infrastructure > AWS > Manage services. From your Edit AWS account page, select Unlink this account. Save your changes. Sign in to AWS and select Services > IAM > Roles. Select the checkbox for the role you want to delete, then select Role Actions > Delete Role. Unlinking your AWS account will disable the trust relationship set up via your ARN. Azure If you want to... Do this Disable one or more Azure service integrations To disable services while keeping your Azure account linked to New Relic: From ione.newrelic.com > Infrastructure > Azure > Manage services. From your Edit Azure account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all Azure integrations To disconnect your Azure account completely from New Relic, you need to unlink your Azure account. This requires being either the user who registered the app or an administrator. This procedure will disable all New Relic integrations associated with that Azure account. Go to one.newrelic.com > Infrastructure > Azure > Manage services. From your Edit Azure account page, select Unlink this account. Save your changes. Sign in to Azure and go into All Services > Identity > App registrations, or go to Azure Active Directory service and select App registrations. Find the registered app (the recommended name is NewRelic-Integrations). To see the full list of available apps, select the dropdown menu beside the search field and select All apps. Select the app and, on the panel that opens, select Delete. Google Cloud Platform (GCP) If you want to... Do this Disable one or more GCP service integrations To disable services while keeping your GCP account linked to New Relic: From one.newrelic.com > Infrastructure > GCP > Manage services. From your Edit GCP account page, clear the checkbox for each active service you want to disable. Save your changes. Disable all GCP integrations To disconnect your GCP account completely from New Relic, you need to unlink your GCP account. This disables all New Relic integrations associated with that GCP account. If you registered the GCP project using a User account, follow these steps. Go to one.newrelic.com > Infrastructure > GCP > Manage services. From your Edit GCP account page, select Unlink this account. Save your changes. If you registered the GCP project using a Service account, follow these steps. If you are deleting a custom role, be aware that this role may be used for other purposes besides New Relic access. Sign in to New Relic and go to Infrastructure > Integrations > Google Cloud Platform. For a standard (non-custom) user role, select Manage Services for the account you want to remove. Copy the value of User and save it. OR For a custom user role, go to IAM > admin > Roles, search for the role, select it, and select DELETE. You are now finished and can skip the remaining steps. Standard (non-custom) user role: Sign in to Google Cloud and select the correct project in the Select a project box. From the navigation menu, select IAM & admin > IAM. Search for and select the user value you saved, then select REMOVE. On-host integrations If you used the integrations package, see the integrations package instructions. Here are some examples: Apache Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-apache yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-apache zypper (SLES) sudo zypper -n remove nri-apache Cassandra Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-cassandra yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-cassandra zypper (SLES) sudo zypper -n remove nri-cassandra Kubernetes Each cluster will have a single node where kubectl is running. To uninstall the Kubernetes integration, use the following command on each of these nodes: kubectl delete -f newrelic-infrastructure-k8s-latest.yaml Copy MySQL Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-mysql yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-mysql zypper (SLES) sudo zypper -n remove nri-mysql NGINX Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-nginx yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-nginx zypper (SLES) sudo zypper -n remove nri-nginx Redis Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove nri-redis yum (Amazon Linux, CentOS, or RHEL) sudo yum remove nri-redis zypper (SLES) sudo zypper -n remove nri-redis StatsD cd /path/to/statsd npm uninstall @newrelic/statsd-infra-backend From the StatsD config.js, remove the \"@newrelic/statsd-infra-backend\" entry from the list of backends. Restart StatsD. Moving away from the integrations package While it is still possible to use the integrations package, we recommend removing it completely and working with integrations on an individual basis. The last integration package contains the following versions of the integrations: Apache 1.1.2 Cassandra 2.0.3 MySQL 1.1.5 Nginx 1.0.2 Redis 1.0.1 If you remove the integrations package and want to continue using the related on-host integrations, you will need to install them one by one. To uninstall the package and re-install your integrations: Remove the integrations package by following these instructions. The config files from the old integrations will not be deleted, so you won’t have to configure them again. Uninstall package Package manager Uninstall instructions apt (Debian or Ubuntu) sudo apt-get remove newrelic-infra-integrations sudo apt-get autoremove yum (Amazon Linux, CentOS, or RHEL) sudo yum remove newrelic-infra-integrations sudo yum autoremove zypper (SLES) sudo zypper -n remove newrelic-infra-integrations --clean-deps Copy Install your integrations one by one following these instructions. To replicate the integrations package, you will need to install all the available integrations again.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 332.305,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Uninstall</em> <em>infrastructure</em> integrations",
        "sections": "<em>Uninstall</em> <em>infrastructure</em> integrations",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Uninstalling the <em>infrastructure</em> <em>agent</em> does not directly affect any of your <em>infrastructure</em> Integrations: if you <em>uninstall</em> the <em>agent</em>, your integrations will remain. Similarly, if you disable or <em>uninstall</em> your integrations, the <em>infrastructure</em> <em>agent</em> will remain. To <em>uninstall</em> any of your integrations"
      },
      "id": "603ea831196a67fc6fa83db5"
    },
    {
      "sections": [
        "Zip assisted install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Zip assisted install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "fcadbeed626401863b6b16e5c52e9a472a5ac13e",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-04-16T02:59:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. Install the agent Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Once it's unpacked, access and edit the installation PowerShell script installer.ps1. Update your license key. Optional: Update any other parameters. Execute installer.ps1 with admin rights. Configure your installation Important Make sure any custom folder defined in the installation settings has permissions limitations properly defined. The infrastructure agent might execute any integration defined in the NRIA_PLUGIN_DIR directory with Administrator permissions. You can configure the following parameters during the assisted install for Windows: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: C:\\Program Files\\New Relic\\newrelic-infra Copy NRIA_APP_DATA_DIR This configures the data directory to store inventory and other agent files. Default: C:\\%ProgramData%\\New Relic\\newrelic-infra Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The New Relic license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy NRIA_OVERWRITE By default and for security reasons, Windows does not install a service if there's another service with the same name already installed. To bypass this check, make sure this setting NRIA_OVERWRITE is TRUE. Default: TRUE Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: C:\\Program Files\\NewRelic\\newrelic-infra\\inregrations.d Copy NRIA_SERVICE_NAME This provides the name for the Windows service. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 195.48087,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "sections": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. <em>Install</em> the <em>agent</em> Important As of version"
      },
      "id": "603ea7af196a67dab0a83d9d"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/windows-installation/install-infrastructure-monitoring-agent-windows": [
    {
      "sections": [
        "Zip assisted install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Zip assisted install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "fcadbeed626401863b6b16e5c52e9a472a5ac13e",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-04-16T02:59:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. Install the agent Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Once it's unpacked, access and edit the installation PowerShell script installer.ps1. Update your license key. Optional: Update any other parameters. Execute installer.ps1 with admin rights. Configure your installation Important Make sure any custom folder defined in the installation settings has permissions limitations properly defined. The infrastructure agent might execute any integration defined in the NRIA_PLUGIN_DIR directory with Administrator permissions. You can configure the following parameters during the assisted install for Windows: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: C:\\Program Files\\New Relic\\newrelic-infra Copy NRIA_APP_DATA_DIR This configures the data directory to store inventory and other agent files. Default: C:\\%ProgramData%\\New Relic\\newrelic-infra Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The New Relic license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy NRIA_OVERWRITE By default and for security reasons, Windows does not install a service if there's another service with the same name already installed. To bypass this check, make sure this setting NRIA_OVERWRITE is TRUE. Default: TRUE Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: C:\\Program Files\\NewRelic\\newrelic-infra\\inregrations.d Copy NRIA_SERVICE_NAME This provides the name for the Windows service. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 338.53372,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "sections": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>, you can make the changes you need to the <em>installation</em> script we provide so you can adapt it to your environment. Before <em>installation</em>, make sure to check the compatibility and requirements. <em>Install</em> the <em>agent</em> Important As of version"
      },
      "id": "603ea7af196a67dab0a83d9d"
    },
    {
      "sections": [
        "Zip manual install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Install the service script",
        "Configuration file",
        "Changing the config file's location",
        "Configure the plugin directory",
        "Configure the agent directory",
        "Configure the log file",
        "What's next?"
      ],
      "title": "Zip manual install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "f7c89a92aefa26a400384c4334bcdc876dd07546",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-manual-install-infrastructure-agent-windows/",
      "published_at": "2021-05-05T03:34:17Z",
      "updated_at": "2021-03-16T08:33:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our custom installation process for the infrastructure agent for Windows allows you to tailor all aspects of the installation, and place files and folders wherever you want on your filesystem. This method gives you full control of the installation: you are responsible for placing the files in the correct folders, providing the correct configuration values, and ensuring the agent has all the right permissions. Before installation, check the compatibility and requirements. Install the agent To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Install the service script. Optionally, you can: Change the location of the configuration file. Configure the plugin directory. Configure the agent directory. Configure the log file. Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. Install the service script To proceed with the installation, you need to create the service. Check the file provided in the zip file for reference: C:\\Program Files\\New Relic\\newrelic-infra\\installer.ps1 Copy Configuration file The Infrastructure agent depends on a configuration file, usually named newrelic-infra.yml, placed in the same folder with the agent, to configure the agent's behavior. You can create a new config file based on config file template. For more information, see how to configure the agent. Changing the config file's location By default, the configuration file is located in C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml. To change the location of the configuration file: Execute the command regedit.exe. Browse to the folder Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\newrelic-infra\\ImagePath. Retrieve the ImagePath key. If the agent binary is on the default path, look for the key at C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.exe. Use the -config flag to add the new location of the configuration file to the key: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.exe -config c:\\config.yaml Copy Configure the plugin directory The Infrastructure agent allows you to install integrations that monitor and report data from popular services such as Kubernetes, AWS, MySQL, Redis, Kafka, etc. Each integration has its own configuration file, named by default integration-name-config.yml, placed in the predefined location C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d. On initialization, the agent loads the config file. To overwrite the predefined location of the integration configuration file, use one of the following methods: Set the location in the NRIA_PLUGIN_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the plugin_dir field. Pass it as a command line argument using -plugin_dir when you run the newrelic-infra binary. Configure the agent directory The agent requires its own defined directory to run the installed integrations, caching data (inventory), etc. The default location is C:\\Program Files\\New Relic\\newrelic-infra\\. The agent directory has the following structure and content: LICENSE: Text file containing the New Relic Infrastructure agent license. custom-integrations: Directory that stores the installed the custom integrations. newrelic-integrations: Directory that stores the New Relic official integrations. Additionally, the agent uses a different folder, app_data_dir, to store data. By default it points to C:\\ProgramData\\New Relic\\newrelic-infra\\. To overwrite the predefined location of the agent directory, use one of the following methods: Set the location in the NRIA_AGENT_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the agent_dir field. Pass it as a command line argument using -agent_dir when you run the newrelic-infra binary. Configure the log file By default the agent stores the log files in C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log. To overwrite the predefined location of the log file, use one of the following methods: Set the location in the NRIA_LOG_FILE environment variable. Set the custom path in the newrelic-infra.yml configuration file using the log_file field. Pass it as a command line argument using -log_file when you run the newrelic-infra binary. What's next? You may also want to: Add custom attributes to annotate your Infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic Infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.9696,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "sections": "Zip manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Our custom <em>installation</em> process for the <em>infrastructure</em> <em>agent</em> for <em>Windows</em> allows you to tailor all aspects of the <em>installation</em>, and place files and folders wherever you want on your filesystem. This method gives you full control of the <em>installation</em>: you are responsible for placing the files"
      },
      "id": "603ea57b196a678ad3a83dbf"
    },
    {
      "sections": [
        "Infrastructure agent configuration settings",
        "Important",
        "Agent variables",
        "license_key (REQUIRED)",
        "fedramp",
        "max_procs",
        "payload_compression_level",
        "startup_connection_retries",
        "startup_connection_retry_time",
        "startup_connection_timeout",
        "Cloud variables",
        "cloud_max_retry_count",
        "cloud_metadata_expiry_sec",
        "cloud_retry_backoff_sec",
        "disable_cloud_instance_id",
        "disable_cloud_metadata",
        "Debug variables",
        "trace",
        "Docker variables",
        "container_cache_metadata_limit",
        "docker_api_version",
        "File system variables",
        "custom_supported_file_systems",
        "file_devices_ignored",
        "Hostname variables",
        "display_name",
        "dns_hostname_resolution",
        "override_hostname",
        "override_hostname_short",
        "Installation variables",
        "agent_dir",
        "plugin_dir",
        "custom_plugin_installation_dir",
        "Integrations variables",
        "passthrough_environment",
        "entityname_integrations_v2_update",
        "http_server_enabled",
        "http_server_host",
        "http_server_port",
        "remove_entities_period",
        "Inventory variables",
        "offline_time_to_reset",
        "ignored_inventory",
        "Linux variables",
        "pid_file",
        "ignore_reclaimable",
        "Logging variables",
        "log_file",
        "log_format",
        "log_to_stdout",
        "verbose",
        "smart_verbose_mode_entry_limit",
        "Metrics variables",
        "custom_attributes",
        "Tip",
        "enable_process_metrics",
        "include_matching_metrics",
        "network_interface_filters",
        "Linux",
        "Windows",
        "disable_zero_mem_process_filter",
        "Plugins variables",
        "disable_all_plugins",
        "cloud_security_group_refresh_sec",
        "daemontools_interval_sec",
        "dpkg_interval_sec",
        "facter_interval_sec",
        "kernel_modules_refresh_sec",
        "network_interface_interval_sec",
        "rpm_interval_sec",
        "selinux_interval_sec",
        "sshd_config_refresh_sec",
        "supervisor_interval_sec",
        "sysctl_interval_sec",
        "systemd_interval_sec",
        "sysvinit_interval_sec",
        "upstart_interval_sec",
        "users_refresh_sec",
        "supervisor_rpc_sock",
        "facter_home_dir",
        "Proxy variables",
        "proxy",
        "ignore_system_proxy",
        "ca_bundle_dir",
        "ca_bundle_file",
        "proxy_validate_certificates",
        "proxy_config_plugin",
        "Samples variables",
        "metrics_network_sample_rate",
        "metrics_process_sample_rate",
        "metrics_storage_sample_rate",
        "metrics_system_sample_rate",
        "metrics_nfs_sample_rate",
        "detailed_nfs",
        "Security variables",
        "selinux_enable_semodule",
        "strip_command_line",
        "Windows variables",
        "windows_services_refresh_sec",
        "windows_updates_refresh_sec",
        "app_data_dir",
        "enable_win_update_plugin",
        "legacy_storage_sampler",
        "win_process_priority_class",
        "win_removable_drives",
        "What's next?"
      ],
      "title": "Infrastructure agent configuration settings ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Configuration"
      ],
      "external_id": "7d016d3f5af6ed1615904578f0ed8bce02aa335b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/configuration/infrastructure-agent-configuration-settings/",
      "published_at": "2021-05-05T02:35:20Z",
      "updated_at": "2021-03-30T19:18:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent has a large set of configuration settings to fine-tune its behavior. Here we: List all the configuration options (in both their YAML and the environment variable names). Explain what the settings do and when to use them. Give the variable type and default value (if any). List the minimum required agent version as applicable. For an example of how all these variables can be used, see our sample configuration template. Important license_key is the only required setting. Agent variables license_key (REQUIRED) Specifies the license key for your New Relic account. The agent uses this key to associate your server's metrics with your New Relic account. This setting is created as part of the standard installation process. YML option name Environment variable Type Default Version license_key NRIA_LICENSE_KEY string Example: license_key: 1234567890abcdefghijklmnopqrstuvwxyz1234 Copy fedramp Specifies whether Fedramp endpoints should be used. YML option name Environment variable Type Default Version fedramp NRIA_FEDRAMP boolean false 1.15.0 max_procs Specifies the number of logical processors available to the agent. Increasing this value helps to distribute the load between different cores. If set to -1, the agent will try to read the environment variable GOMAXPROCS. If this variable is not set, the default value will be the total number of cores available in the host. YML option name Environment variable Type Default Version max_procs NRIA_MAX_PROCS integer 1 1.0.1002 payload_compression_level Since version 1.0.804 or higher, data sent from the agent is compressed by default. To disable payload compression, set payload_compression_level to 0. Important Recommendation: Do not change this setting. YML option name Environment variable Type Default Version payload_compression_level NRIA_PAYLOAD_COMPRESSION_LEVEL integer 6 1.0.804 startup_connection_retries Number of times the agent will retry the request to check New Relic's platform availability at startup before throwing an error. If set to a negative value, the agent will keep checking the connection until it succeeds. YML option name Environment variable Type Default Version startup_connection_retries NRIA_STARTUP_CONNECTION_RETRIES integer 6 1.0.936 startup_connection_retry_time After a request has timed out, time the agent waits to retry a request to check New Relic's platform availability at startup. YML option name Environment variable Type Default Version startup_connection_retry_time NRIA_STARTUP_CONNECTION_RETRY_TIME string 5s 1.0.936 - 1.2.30 startup_connection_timeout Time the agent waits until a request to check New Relic's platform availability at startup is considered timed out. YML option name Environment variable Type Default Version startup_connection_timeout NRIA_STARTUP_CONNECTION_TIMEOUT string 10s 1.0.936 Cloud variables If the agent is running in a cloud instance, the agent will try to detect the cloud type and fetch metadata. cloud_max_retry_count Sets the number of times the agent retries to connect in case that cloud detection fails. If cloud detection fails during the initialization of the agent, the agent will retry after waiting for CloudRetryBackOffSec seconds. YML option name Environment variable Type Default Version cloud_max_retry_count NRIA_CLOUD_MAX_RETRY_COUNT integer 10 1.2.6 cloud_metadata_expiry_sec Sets the interval of time the agent will wait until discarding the metadata, in seconds. After this period metadata expires and the agent will fetch it again. YML option name Environment variable Type Default Version cloud_metadata_expiry_sec NRIA_CLOUD_METADATA_EXPIRY_SEC integer 300 1.2.6 cloud_retry_backoff_sec Sets the interval of time the agent waits between cloud detection retries in case that cloud detection failed, in seconds. If cloud detection fails during the initialization of the agent, it will retry for CloudMaxRetryCount times. YML option name Environment variable Type Default Version cloud_retry_backoff_sec NRIA_CLOUD_RETRY_BACKOFF_SEC integer 60 1.2.6 disable_cloud_instance_id Similar to DisableCloudMetadata, but it disables the collection of cloud metadata only for the host alias plugin. YML option name Environment variable Type Default Version disable_cloud_instance_id NRIA_DISABLE_CLOUD_INSTANCE_ID boolean false 1.0.220 disable_cloud_metadata Disables the collection of cloud metadata. YML option name Environment variable Type Default Version disable_cloud_metadata NRIA_DISABLE_CLOUD_METADATA boolean false 1.0.690 Debug variables trace Enables traces to be printed in the logs for the given features. Traces are additional log entries used for debugging purposes of specific features, they are only shown when the verbose log config option is enabled. Currently two features have traces defined: connect and attributes. YML option name Environment variable Type Default Version trace NRIA_TRACE [ ]string [] 1.0.1015 Example as a YAML attribute: trace: [connect, attributes] Copy Example as an environment variable: NRIA_TRACE='connect, attributes' Copy Docker variables container_cache_metadata_limit Time, in seconds, before the cached containers metadata expires and the agent needs to fetch them again. YML option name Environment variable Type Default Version container_cache_metadata_limit NRIA_CONTAINER_CACHE_METADATA_LIMIT integer 60 1.0.801 docker_api_version Specifies the Docker client API version. YML option name Environment variable Type Default Version docker_api_version NRIA_DOCKER_API_VERSION string 1.24 1.1.4 File system variables custom_supported_file_systems List of the types of file systems that the agent supports. This value needs to be a subset of the default list, and items that are not in the default list will be discarded. YML option name Environment variable Type Default Version custom_supported_file_systems NRIA_CUSTOM_SUPPORTED_FILESYSTEMS [ ]string Linux: [\"xfs\", \"btrfs\", \"ext\", \"ext2\", \"ext3\", \"ext4\", \"hfs\", \"vxfs\"] Windows: [\"NTFS\", \"ReFS\"] 1.0.220 file_devices_ignored List of storage devices to be ignored by the agent when gathering StorageSample data. YML option name Environment variable Type Default Version file_devices_ignored NRIA_FILE_DEVICES_IGNORED [ ]string 1.0.220 Example as a YAML attribute: file_devices_ignored: - sda1 - sda2 Copy Example as an environment variable: FILE_DEVICES_IGNORED=\"sda1,sda2\" Copy Hostname variables display_name Overrides the auto-generated hostname for reporting. This is useful when you have multiple hosts with the same name, since our infrastructure monitoring uses the hostname as the unique identifier for each host. Keep in mind this value is also used for the loopback address replacement on entity names. For more information, see our documentation on how entity name resolution works. YML option name Environment variable Type Default Version display_name NRIA_DISPLAY_NAME string empty 1.0.266 Example: display_name: teslaOne Copy dns_hostname_resolution When true, the full hostname is resolved by performing a reverse lookup of the host's address. Otherwise, it will be retrieved with the hostname command on Linux and from the TCP/IP parameters of the registry on Windows. YML option name Environment variable Type Default Version dns_hostname_resolution NRIA_DNS_HOSTNAME_RESOLUTION boolean true 1.2.6 override_hostname When set, this is the value that will be reported for the full hostname; otherwise, the agent will perform the normal lookup behavior. YML option name Environment variable Type Default Version override_hostname NRIA_OVERRIDE_HOSTNAME string 1.0.1015 Example: my.custom-hostname.co.org Copy override_hostname_short When set, this is the value that will be reported for the hostname; otherwise, the agent will perform the normal lookup behavior. YML option name Environment variable Type Default Version override_hostname_short NRIA_OVERRIDE_HOSTNAME_SHORT string 1.0.1015 Example: my.custom-hostname Copy Installation variables agent_dir Directory where the agent stores files for cache, inventory, integrations, etc. YML option name Environment variable Type Default Version agent_dir NRIA_AGENT_DIR string Linux: /var/db/newrelic-infra Windows: C:\\Program Files\\NewRelic\\newrelic-infra\\ 1.0.2 plugin_dir Directory containing the configuration files of the integrations. Each integration has its own configuration file, named by default <integration_name>-config.yml, placed in a predefined location from which the agent loads on initialization. YML option name Environment variable Type Default Version plugin_dir NRIA_PLUGIN_DIR string Linux: etc/newrelic-infra/integrations.d/ Windows: \\Program Files\\NewRelic\\newrelic-infra\\inregrations.d 1.0.2 Important With secrets management, you can configure on-host integrations with New Relic Infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. custom_plugin_installation_dir Specifies a custom path to install integrations, which allows to install them outside the agent_dir. It has priority when the agent is looking for installed integrations. YML option name Environment variable Type Default Version custom_plugin_installation_dir NRIA_CUSTOM_PLUGIN_INSTALLATION_DIR string Empty 1.0.2 Integrations variables passthrough_environment A list of environment variables that will be passed to all integrations. If an integration already has an existing configuration option with the same name, then the environment variable takes precedence. YML option name Environment variable Type Default Version passthrough_environment NRIA_PASSTHROUGH_ENVIRONMENT [ ]string Empty 1.0.719 Example as a YAML attribute (inside the agent's configuration file, located by default in /etc/newrelic-infra.yml): passthrough_environment: - HOST - PORT Copy Example as an environment variable: NRIA_PASSTHROUGH_ENVIRONMENT=\"HOST,PORT\" Copy entityname_integrations_v2_update The agent enables loopback-address replacement on the entity name (and therefore key) automatically for version 3 of the integration protocol. If you are using version 2 of the protocol and want this behavior, enable the entityname_integrations_v2_update option. YML option name Environment variable Type Default Version entityname_integrations_v2_update NRIA_ENTITYNAME_INTEGRATIONS_V2_UPDATE boolean false 1.2.15 http_server_enabled By setting this configuration parameter to true the agent will open an HTTP port (by default, 8001) to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_enabled NRIA_HTTP_SERVER_ENABLED boolean false 1.0.818 http_server_host By setting this value, the agent will start listening on the HTTPServerPort to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_host NRIA_HTTP_SERVER_HOST string localhost 1.0.818 http_server_port Sets the port for the http server to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_port NRIA_HTTP_SERVER_PORT integer 8001 1.0.818 remove_entities_period Starts the process of deleting entities that haven't reported information during this interval. Valid time units: s (seconds), m (minutes), and h (hour). YML option name Environment variable Type Default Version remove_entities_period NRIA_REMOVE_ENTITIES_PERIOD string 48h 1.0.859 Example: 1h Copy Inventory variables offline_time_to_reset If the cached inventory becomes older than this value (for example, because the agent is offline), the agent automatically removes and recreates the delta store. YML option name Environment variable Type Default Version offline_time_to_reset NRIA_OFFLINE_TIME_TO_RESET string 24h 1.0.888 ignored_inventory The list of inventory paths ignored by the agent. YML option name Environment variable Type Default Version ignored_inventory NRIA_IGNORED_INVENTORY string [ ] Empty list 1.0.336 Example as a YAML attribute: ignored_inventory: - files/config/stuff.bar - files/config/stuff.foo Copy Example as an environment variable: NRIA_IGNORED_INVENTORY=\"files/config/stuff.bar,files/config/stuff.foo\" Copy Linux variables pid_file Location on Linux where the pid file of the agent process is created. It is used at startup to ensure that no other instances of the agent are running. YML option name Environment variable Type Default Version pid_file NRIA_PID_FILE string /var/run/newrelic-infra/newrelic-infra.pid 1.0.2 ignore_reclaimable When true, formulation of the host virtual memory considers SReclaimable as available memory; otherwise SReclaimable will be considered part of the used memory. YML option name Environment variable Type Default Version ignore_reclaimable NRIA_IGNORE_RECLAIMABLE boolean false 1.2.6 Logging variables log_file Defines the file path for the logs. The default installation creates a log directory and it sets this filepath value in the log_file configuration option for you. This log directory is different for each OS, as shown below. Change this configuration option to customize the file path for the logs. YML option name Environment variable Type Default Version log_file NRIA_LOG_FILE string See below * Default paths: Linux: If not defined, it logs only in the standard output. Windows, agent version 1.0.752 or lower: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy Windows, agent version 1.0.775 to 1.0.944: C:\\%APPDATA%\\Roaming\\New Relic\\newrelic-infra\\newrelic-infra.log Copy Windows, agent version 1.0.944 or higher: C:\\%ProgramData%\\New Relic\\newrelic-infra\\newrelic-infra.log Copy If the directory can't be created: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy log_format Defines the log output format. Available values: text: Plain text output, one line per log entry. json: JSON-formatted output, one line per log entry. YML option name Environment variable Type Default Version log_format NRIA_LOG_FORMAT string text 1.4.9 log_to_stdout By default all logs are displayed in both standard output and a log file. To disable logs in the standard output, set this configuration option to false. YML option name Environment variable Type Default Version log_to_stdout NRIA_LOG_TO_STDOUT boolean true 1.0.703 verbose When verbose is set to 0, verbose logging is off, but the agent still creates logs. Set this to 1 to create verbose logs to use in troubleshooting the agent; set verbose to 2 ito use the smart verbose mode logging. Smart verbose mode logs the last smart_verbose_mode_entry_limit debug messages when an error is logged. Setting verbose to 3 enables forwarding the agent logs to New Relic Logs. Important Verbose logging can generate a lot of data very quickly. Run the agent in verbose mode only for as long as necessary to reproduce your issue, then set verbose: 0 and restart your agent to disable verbose logging. Alternatively, you can set verbose: 2, which will enable smart verbose mode. YML option name Environment variable Type Default Version verbose NRIA_VERBOSE Integer (0, 1, 2 or 3) 0 1.9.0 when using smart mode (2) 1.11.4 when forwarding the New Relic Logs (3) smart_verbose_mode_entry_limit smart_verbose_mode_entry_limit refers to the number of previous debug messages that will be logged when an error is logged. For example, if the limit is set to 5, debug logs will be cached in memory until an error is logged, at which point the previous 5 debug messages will also be logged Important This configuration option is only used when verbose is set to 2 (Smart Verbose Mode enabled). YML option name Environment variable Type Default Version smart_verbose_mode_entry_limit NRIA_SMART_VERBOSE_MODE_ENTRY_LIMIT Integer 1000 1.9.0 Metrics variables custom_attributes Custom attributes are key-value pairs (similar to tags in other tools) used to annotate the data from the Infrastructure agent. You can use this metadata to build filter sets, group your results, and annotate your data. For example, you might indicate a machine's environment (staging or production), the service a machine hosts (login service, for example), or the team responsible for that machine. Tip The agent collects many details about your environment as part of its default attributes, including Amazon Elastic Compute Cloud (Amazon EC2) tags. YML option name Environment variable Type custom_attributes NRIA_CUSTOM_ATTRIBUTES map [ string]interface { } Use a list of custom attributes to annotate the data from this agent instance. Separate keys and values with colons :, as in KEY: VALUE, and separate each key-value pair with a line break. Keys can be any valid YAML except slashes /. Values can be any YAML string, including spaces. Example as a YAML attribute: custom_attributes: environment: production service: login service team: alpha-team Copy Example as an environment variable: NRIA_CUSTOM_ATTRIBUTES='{\"customAttribute_1\":\"SOME_ATTRIBUTE\",\"customAttribute_2\": \"SOME_ATTRIBUTE_2\"}' Copy NRQL example filtering by custom attribute: FROM SystemSample SELECT * WHERE environment = 'production' Copy enable_process_metrics Important Requires infrastructure agent version 1.12.0 or higher. Accounts created before July 20, 2020 and/or infrastructure agents installed using the new Guided Install have this variable enabled by default. Enables the sending of process metrics to New Relic. By default, the infrastructure agent doesn't send data about the operating system's processes. The agent still collects such data, unless metrics_process_sample_rate is set to -1. To report metric data about all the operating system's processes, set enable_process_metrics to true. To disable, set to false. Sending all process data could increase the volume of data sent to New Relic. To fine-tune which processes you want to monitor, configure include_matching_metrics. By default, processes using low memory are excluded from being sampled. For more information, see disable-zero-mem-process-filter. YML option name Environment variable Type Default Version enable_process_metrics NRIA_ENABLE_PROCESS_METRICS boolean false include_matching_metrics Important Currently, this setting only applies to an operating system's processes metrics. You can control how much data is sent to New Relic by configuring include_matching_metrics, which allows you to restrict the transmission of metric data based on the values of metric attributes. You include metric data by defining literal or partial values for any of the attributes of the metric. For example, you can choose to send the host.process.cpuPercent of all processes whose process.name match the ^java regular expression. In this example, we include process metrics using executable files and names: include_matching_metrics: # You can combine attributes from different metrics process.name: - regex “^java” # Include all processes starting with \"java\" process.executable: - “/usr/bin/python2” # Include the Python 2.x executable - regex “\\\\System32\\\\svchost” # Include all svchost executables Copy If you need to include command line arguments in any of the values, set strip_command_line to false (the infrastructure agents removes CLI arguments by default to prevent secrets from leaking). To configure include_matching_metrics as an environment variable for the Kubernetes integration, add it in the manifest inside the env: object: env: - name: NRIA_INCLUDE_MATCHING_METRICS value: | process.name: - regex \"^java\" process.executable: - \"/usr/bin/python2\" - regex \"\\\\System32\\\\svchost\" Copy Default YML option name Environment variable Type Default Version include_matching_metrics NRIA_INCLUDE_MATCHING_METRICS metric.attribute: - regex \"pattern\" - \"string\" - \"string-with-wildcard * \" network_interface_filters You can use the network interface filters configuration to hide unused or uninteresting network interfaces from the infrastructure agent. This helps reduce resource usage, work, and noise in your data. Important Environment variables are not supported for this configuration setting. The configuration uses a simple pattern-matching mechanism that can look for interfaces that start with a specific sequence of letters or numbers following either pattern: {name}[other characters], where you specify the name using the prefix option [number]{name}[other characters], where you specify the name using the index-1 option New Relic infrastructure implements a curated default list of filters, available for both Linux and Windows, that you can modify. YML option name Environment variable Type Default Version network_interface_filters not supported map [ string] [ ]string 1.0.220 Linux Default network interface filters for Linux: Network interfaces that start with dummy, lo, vmnet, sit, tun, tap, or veth Network interfaces that contain tun or tap The following example (added to your configuration file) overrides the default filters. This will ignore network interfaces that start with dummy or lo , or contain tun preceded by a sequence of numbers, and followed by other characters: network_interface_filters: prefix: - dummy - lo index-1: - tun Copy Windows Default network interface filters for Windows: Network interfaces that start with Loop, isatap, or Local The following example (added to your configuration file) overrides the default filters. This will ignore network interfaces that start with Loop: network_interface_filters: prefix: - Loop Copy disable_zero_mem_process_filter The ZeroRSSFilter excludes processes that are not using memory from being sampled. Disable the filter so that the agent includes these processes in the ProcessSample. YML option name Environment variable Type Default Version disable_zero_mem_process_filter NRIA_DISABLE_ZERO_MEM_PROCESS_FILTER boolean false 1.0.832 Plugins variables Tip You can quickly disable all the variables by setting DisableAllPlugins to true, and turn on only those options you need. disable_all_plugins To disable all the plugins, set this option to true. YML option name Environment variable Type Default Version disable_all_plugins NRIA_DISABLE_ALL_PLUGINS boolean false 1.2.1 cloud_security_group_refresh_sec Sampling period for the CloudSecurityGroups plugin, in seconds. The minimum value is 30. To disable it, set it to -1. Important This plugin is activated only if the agent is running in an AWS instance. YML option name Environment variable Type Default Version cloud_security_group_refresh_sec NRIA_CLOUD_SECURITY_GROUP_REFRESH_SEC int64 60 1.0.692 daemontools_interval_sec Sampling period for the Daemontoolsplugin, in seconds. The minimum value is a 10. To disable it, set it to -1. YML option name Environment variable Type Default Version daemontools_interval_sec NRIA_DAEMONTOOLS_INTERVAL_SEC int64 15 1.0.316 dpkg_interval_sec Sampling period for the Dpkg plugin, in seconds. The minimum value is 30. To disable it, set it to -1. If the parameter is not explicitly set in the config file, it can be disabled by setting DisableAllPlugins to true. Important This is only activated in root or privileged running modes and on Debian-based distributions. YML option name Environment variable Type Default Version dpkg_interval_sec NRIA_DPKG_INTERVAL_SEC int64 30 1.0.316 facter_interval_sec Sampling period for the Facter plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version facter_interval_sec NRIA_FACTER_INTERVAL_SEC int64 30 1.0.316 kernel_modules_refresh_sec Sampling period for the CloudSecurityGroups plugin, in seconds. The minimum value is 10. To disable it, set it to -1. Important kernel_modules_refresh_sec is only activated in root or privileged running modes. YML option name Environment variable Type Default Version kernel_modules_refresh_sec NRIA_KERNEL_MODULES_REFRESH_SEC int64 10 1.0.755 network_interface_interval_sec Sampling period for the NetworkInterface plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version network_interface_interval_sec NRIA_NETWORK_INTERFACE_INTERVAL_SEC int64 60 1.0.329 rpm_interval_sec Sampling period for the Rpm plugin, in seconds. The minimum value is 30. To disable it, set it to -1. Important rpm_interval_sec is only activated when the agent runs in root or privileged modes for RedHat, RedHat AWS, or SUSE distributions. YML option name Environment variable Type Default Version rpm_interval_sec NRIA_RPM_INTERVAL_SEC int64 30 1.0.316 selinux_interval_sec Sampling period for the SELinux plugin, in seconds. The minimum value is 30. To disable it, set it to -1. This option is ignored if SelinuxEnableSemodule is set to false. For more information, see our troubleshooting doc on disabling the SELinux module. Important SELinux is only activated when the agent runs in root mode. YML option name Environment variable Type Default Version selinux_interval_sec NRIA_SELINUX_INTERVAL_SEC int64 30 1.0.316 sshd_config_refresh_sec Sampling period for the Sshd plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version sshd_config_refresh_sec NRIA_SSHD_CONFIG_REFRESH_SEC int64 15 1.0.755 supervisor_interval_sec Sampling period for the Supervisor plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version supervisor_interval_sec NRIA_SUPERVISOR_INTERVAL_SEC int64 15 1.0.316 sysctl_interval_sec Sampling period for the Sysctl plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version sysctl_interval_sec NRIA_SYSCTL_INTERVAL_SEC int64 60 1.0.316 systemd_interval_sec Sampling period for the Systemd plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version systemd_interval_sec NRIA_SYSTEMD_INTERVAL_SEC int64 30 1.0.316 sysvinit_interval_sec Sampling period for the sysv plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version sysvinit_interval_sec NRIA_SYSVINIT_INTERVAL_SEC int64 30 1.0.316 upstart_interval_sec Sampling period for the Upstart plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version upstart_interval_sec NRIA_UPSTART_INTERVAL_SEC int64 30 1.0.316 users_refresh_sec Sampling period for the Users plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version users_refresh_sec NRIA_USERS_REFRESH_SEC int64 30 1.0.755 supervisor_rpc_sock Location of the supervisor socket. YML option name Environment variable Type Default Version supervisor_rpc_sock NRIA_SUPERVISOR_RPC_SOCK string /var/run/supervisor.sock 1.0.2 facter_home_dir Sets the HOME environment variable for Puppet's Facter. If not defined, it defaults to the current user's home directory. YML option name Environment variable Type Default Version facter_home_dir NRIA_FACTER_HOME_DIR string 1.1.7 Proxy variables For infrastructure agent version 1.3.1 or higher, the precedence of the proxy configuration settings is: NRIA_PROXY proxy HTTP_PROXY HTTPS_PROXY proxy Your system may have firewall rules that require the agent to use a proxy to communicate with New Relic. If so, set the proxy URL in the form https://user:password@hostname:port. It can be HTTP or HTTPS. YML option name Environment variable Type Default Version proxy NRIA_PROXY string Empty 1.0.308 Example: https://proxy_user.access_10@proxy_01:1080 Copy ignore_system_proxy When set to true, the HTTPS_PROXY and HTTP_PROXY environment variables are ignored. This is useful when the agent needs to connect directly to the metrics collector and skip the existing system proxy. YML option name Environment variable Type Default Version ignore_system_proxy NRIA_IGNORE_SYSTEM_PROXY boolean false 1.0.1002 ca_bundle_dir If the HTTPS_PROXY option references to a proxy with self-signed certificates, this option specifies the path to the the directory where the proxy certificate is available. The certificates in the directory must end with the .pem extension. YML option name Environment variable Type Default Version ca_bundle_dir NRIA_CA_BUNDLE_DIR string 1.0.296 ca_bundle_file If the HTTPS_PROXY option references to a proxy with self-signed certificates, this option specifies the path to the certificate file. YML option name Environment variable Type Default Version ca_bundle_file NRIA_CA_BUNDLE_FILE string 1.0.296 proxy_validate_certificates If set to true, when the proxy is configured to use an HTTPS connection, it will only work: If the HTTPS proxy has certificates from a valid Certificate Authority. If the ca_bundle_file or ca_bundle_dir configuration properties contain the HTTPS proxy certificates. YML option name Environment variable Type Default Version proxy_validate_certificates NRIA_PROXY_VALIDATE_CERTIFICATES boolean false 1.3.0 proxy_config_plugin Sends the following proxy configuration information as inventory: HTTPS_PROXY HTTP_PROXY proxy ca_bundle_dir ca_bundle_file ignore_system_proxy proxy_validate_certificates YML option name Environment variable Type Default Version proxy_config_plugin NRIA_PROXY_CONFIG_PLUGIN boolean true 1.3.0 If you are having problems with proxy configuration, see Proxy troubleshooting. Samples variables metrics_network_sample_rate Sample rate of network samples, in seconds. Minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_network_sample_rate NRIA_METRICS_NETWORK_SAMPLE_RATE integer 10 1.0.308 metrics_process_sample_rate Sample rate of process samples, in seconds. Minimum value is 20. To disable process samples entirely, set metrics_process_sample_rate to -1. YML option name Environment variable Type Default Version metrics_process_sample_rate NRIA_METRICS_PROCESS_SAMPLE_RATE integer 20 1.0.308 metrics_storage_sample_rate Sample rate of storage samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_storage_sample_rate NRIA_METRICS_STORAGE_SAMPLE_RATE integer 5 1.0.308 metrics_system_sample_rate Sample rate of system samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_system_sample_rate NRIA_METRICS_SYSTEM_SAMPLE_RATE integer 5 1.0.308 metrics_nfs_sample_rate Sample rate of NFS storage samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_nfs_sample_rate NRIA_METRICS_NFS_SAMPLE_RATE integer 20 1.5.40 detailed_nfs Detailed NFS metrics. When enabled, the agent will provide a complete list of NFS metrics. YML option name Environment variable Type Default Version detailed_nfs NRIA_DETAILED_NFS boolean false 1.5.40 Security variables selinux_enable_semodule Get versions of policy modules installed using SEModule. If disabled, the SELinux plugin will only retrieve the status using SEStatus. YML option name Environment variable Type Default Version selinux_enable_semodule NRIA_SELINUX_ENABLE_SEMODULE boolean true 1.0.864 strip_command_line When true, the agent removes the command arguments from the commandLine attribute of the ProcessSample. Tip This is a security measure to prevent leaking sensitive information. YML option name Environment variable Type Default Version strip_command_line NRIA_STRIP_COMMAND_LINE boolean true 1.0.149 Windows variables windows_services_refresh_sec Sampling period for the Windows services plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version windows_services_refresh_sec NRIA_WINDOWS_SERVICES_REFRESH_SEC int64 30 1.0.755 windows_updates_refresh_sec Sampling period for the Windows updates plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version windows_updates_refresh_sec NRIA_WINDOWS_UPDATES_REFRESH_SEC int64 60 1.0.755 app_data_dir Defines the path to store data in a different path than the program files directory: %AppDir%/data: Used for storing the delta data %AppDir%/user_data: External directory for user-generated JSON files %AppDir%/newrelic-infra.log: If log file config option is not defined, then we use this directory path as default. YML option name Environment variable Type Default Version app_data_dir NRIA_APP_DATA_DIR string Windows: env(ProgramData)\\New Relic\\newrelic-infra Linux: Not applicable 1.0.755 enable_win_update_plugin Enables the Windows updates plugin, which retrieves the lists of hotfixes that are installed on the host. YML option name Environment variable Type Default Version enable_win_update_plugin NRIA_ENABLE_WIN_UPDATE_PLUGIN boolean false 1.0.274 legacy_storage_sampler If true, the agent will be forced to use Windows WMI (the agent's legacy method to grab metrics for Windows; for example, StorageSampler) and will disable the new method (which uses the PDH library). YML option name Environment variable Type Default Version legacy_storage_sampler NRIA_LEGACY_STORAGE_SAMPLER boolean 1.0.1051 win_process_priority_class This configuration option allows to increase the newrelic-infra.exe process priority to any of the following values: Normal Idle High RealTime BelowNormal AboveNormal YML option name Environment variable Type Default Version win_process_priority_class NRIA_WIN_PROCESS_PRIORITY_CLASS string 1.0.989 Example: Normal Copy win_removable_drives Enables the Windows agent to report drives A: and B: when they are mapped as removable drives. YML option name Environment variable Type Default Version win_removable_drives NRIA_WIN_REMOVABLE_DRIVES boolean true 1.3.1 What's next? You can also: Further understand the configuration of the agent. Create a configuration file using our template. See how you can manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.40857,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> configuration settings ",
        "sections": "<em>Infrastructure</em> <em>agent</em> configuration settings",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " management, you can configure on-host integrations with New Relic <em>Infrastructure</em>&#x27;s <em>agent</em> to use sensitive data (such as passwords) without having to write them as plain text into the integration&#x27;s configuration file. For more information, see Secrets management. custom_plugin_<em>installation</em>_dir Specifies"
      },
      "id": "603ea542196a67a38aa83dd8"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows": [
    {
      "sections": [
        "Zip manual install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Install the service script",
        "Configuration file",
        "Changing the config file's location",
        "Configure the plugin directory",
        "Configure the agent directory",
        "Configure the log file",
        "What's next?"
      ],
      "title": "Zip manual install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "f7c89a92aefa26a400384c4334bcdc876dd07546",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-manual-install-infrastructure-agent-windows/",
      "published_at": "2021-05-05T03:34:17Z",
      "updated_at": "2021-03-16T08:33:56Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Our custom installation process for the infrastructure agent for Windows allows you to tailor all aspects of the installation, and place files and folders wherever you want on your filesystem. This method gives you full control of the installation: you are responsible for placing the files in the correct folders, providing the correct configuration values, and ensuring the agent has all the right permissions. Before installation, check the compatibility and requirements. Install the agent To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Install the service script. Optionally, you can: Change the location of the configuration file. Configure the plugin directory. Configure the agent directory. Configure the log file. Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. Install the service script To proceed with the installation, you need to create the service. Check the file provided in the zip file for reference: C:\\Program Files\\New Relic\\newrelic-infra\\installer.ps1 Copy Configuration file The Infrastructure agent depends on a configuration file, usually named newrelic-infra.yml, placed in the same folder with the agent, to configure the agent's behavior. You can create a new config file based on config file template. For more information, see how to configure the agent. Changing the config file's location By default, the configuration file is located in C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml. To change the location of the configuration file: Execute the command regedit.exe. Browse to the folder Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\newrelic-infra\\ImagePath. Retrieve the ImagePath key. If the agent binary is on the default path, look for the key at C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.exe. Use the -config flag to add the new location of the configuration file to the key: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.exe -config c:\\config.yaml Copy Configure the plugin directory The Infrastructure agent allows you to install integrations that monitor and report data from popular services such as Kubernetes, AWS, MySQL, Redis, Kafka, etc. Each integration has its own configuration file, named by default integration-name-config.yml, placed in the predefined location C:\\Program Files\\New Relic\\newrelic-infra\\integrations.d. On initialization, the agent loads the config file. To overwrite the predefined location of the integration configuration file, use one of the following methods: Set the location in the NRIA_PLUGIN_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the plugin_dir field. Pass it as a command line argument using -plugin_dir when you run the newrelic-infra binary. Configure the agent directory The agent requires its own defined directory to run the installed integrations, caching data (inventory), etc. The default location is C:\\Program Files\\New Relic\\newrelic-infra\\. The agent directory has the following structure and content: LICENSE: Text file containing the New Relic Infrastructure agent license. custom-integrations: Directory that stores the installed the custom integrations. newrelic-integrations: Directory that stores the New Relic official integrations. Additionally, the agent uses a different folder, app_data_dir, to store data. By default it points to C:\\ProgramData\\New Relic\\newrelic-infra\\. To overwrite the predefined location of the agent directory, use one of the following methods: Set the location in the NRIA_AGENT_DIR environment variable. Set the custom path in the newrelic-infra.yml configuration file using the agent_dir field. Pass it as a command line argument using -agent_dir when you run the newrelic-infra binary. Configure the log file By default the agent stores the log files in C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log. To overwrite the predefined location of the log file, use one of the following methods: Set the location in the NRIA_LOG_FILE environment variable. Set the custom path in the newrelic-infra.yml configuration file using the log_file field. Pass it as a command line argument using -log_file when you run the newrelic-infra binary. What's next? You may also want to: Add custom attributes to annotate your Infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic Infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 290.96957,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "sections": "Zip manual <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "Our custom <em>installation</em> process for the <em>infrastructure</em> <em>agent</em> for <em>Windows</em> allows you to tailor all aspects of the <em>installation</em>, and place files and folders wherever you want on your filesystem. This method gives you full control of the <em>installation</em>: you are responsible for placing the files"
      },
      "id": "603ea57b196a678ad3a83dbf"
    },
    {
      "sections": [
        "Install the infrastructure monitoring agent for Windows",
        "Tip",
        "Install for Windows Server and Windows 10 using our wizard",
        "Step-by-step instructions",
        "PowerShell install",
        "32-bit Windows",
        "64-bit Windows",
        "Step-by-step install",
        "Important",
        "Scripted installation",
        "What's next?",
        "Install using zip files",
        "Caution",
        "Update the agent"
      ],
      "title": "Install the infrastructure monitoring agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "32766f16044664c9c7d66075801930ff53ca5c49",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/install-infrastructure-monitoring-agent-windows/",
      "published_at": "2021-05-05T03:28:38Z",
      "updated_at": "2021-03-13T01:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's infrastructure monitoring agent for Windows, you can monitor individual servers and also analyze how your service performs as a whole. The Windows agent can run on your own hardware or in cloud systems such as Amazon EC2 or Windows Azure. The infrastructure monitoring agent supports Windows Server and Windows 10. You can also install with Chef. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Install for Windows Server and Windows 10 using our wizard Before installation, be sure to review the requirements. Then, to install the infrastructure monitoring agent for Windows, you can use our launcher, or follow the instructions in this document to complete a basic installation. Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, follow our step-by-step tutorial, which follows. Step-by-step instructions To install the infrastructure monitoring agent, use our PowerShell script, or follow the step-by-step instructions: PowerShell install Review the agent requirements and supported operating systems. Open the PowerShell as administrator and run the following command: 32-bit Windows $LICENSE_KEY=\"YOUR_LICENSE_KEY\"; ` (New-Object System.Net.WebClient).DownloadFile(\"https://download.newrelic.com/infrastructure_agent/windows/386/newrelic-infra-386.msi\", \"$env:TEMP\\newrelic-infra.msi\"); ` msiexec.exe /qn /i \"$env:TEMP\\newrelic-infra.msi\" GENERATE_CONFIG=true LICENSE_KEY=\"$LICENSE_KEY\" | Out-Null; ` net start newrelic-infra Copy 64-bit Windows $LICENSE_KEY=\"YOUR_LICENSE_KEY\"; ` (New-Object System.Net.WebClient).DownloadFile(\"https://download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi\", \"$env:TEMP\\newrelic-infra.msi\"); ` msiexec.exe /qn /i \"$env:TEMP\\newrelic-infra.msi\" GENERATE_CONFIG=true LICENSE_KEY=\"$LICENSE_KEY\" | Out-Null; ` net start newrelic-infra Copy For a scripted installation, you can pass in configuration parameters. You must first add GENERATE_CONFIG=true and LICENSE_KEY=YOUR_LICENSE_KEY. You may then add these optional config settings: DISPLAY_NAME=YOUR_DISPLAY_NAME PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1','ATTRIBUTE_2':'VALUE_2'}\" The following example sets the license key and configures a proxy server for outbound communication, as well as adding one custom attribute: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi GENERATE_CONFIG=true LICENSE_KEY=YOUR_LICENSE_KEY PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1'}\" Copy Step-by-step install Review the infrastructure monitoring agent requirements and supported operating systems. Download the latest .MSI installer image from: 32-bit Windows https://download.newrelic.com/infrastructure_agent/windows/386/newrelic-infra-386.msi​ Copy 64-bit Windows https://download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi Copy Important Do not double-click the installer. This will not fully install the local agent and can result in permissions issues. In an admin account, run the install script using an absolute path. 32-bit Windows To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra-386.msi Copy 64-bit Windows To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi Copy Scripted installation For a scripted installation, you can also pass in configuration parameters. You must first add GENERATE_CONFIG=true and LICENSE_KEY=YOUR_LICENSE_KEY. You may then add these optional config settings: DISPLAY_NAME=YOUR_DISPLAY_NAME PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1','ATTRIBUTE_2':'VALUE_2'}\" The following example sets the license key and configures a proxy server for outbound communication, as well as adding one custom attribute: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi GENERATE_CONFIG=true LICENSE_KEY=YOUR_LICENSE_KEY PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1'}\" Copy Add your New Relic license key to the license_key attribute in newrelic-infra.yml, located in C:\\Program Files\\New Relic\\newrelic-infra\\. When finished, the contents of newrelic-infra.yml should resemble the following: license_key: YOUR_LICENSE_KEY Copy Start the newrelic-infra service. To start from the Windows command prompt, run: net start newrelic-infra Copy Wait a few minutes, then view your server in the Infrastructure UI. If no data appears after waiting a few minutes, follow the troubleshooting steps. Tip As of version 1.4.0, the infrastructure monitoring agent package includes newrelic-infra-ctl, which is used to help troubleshoot a running agent. We recommend adding it to PATH. What's next? The only required configuration option is the license_key setting, which is created as part of the installation procedures. You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Enable log forwarding. Add other New Relic infrastructure integrations to collect data from external services. Install using zip files For custom setup scenarios, you can install the infrastructure monitoring agent using our zip files in assisted or manual modes. This is especially useful when you need to adapt the default installation settings to your environment. Caution Installing the infrastructure monitoring agent using zip files is not supported. Update the agent To upgrade to the latest version, follow standard procedures to update the infrastructure monitoring agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 288.6331,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Windows</em>",
        "sections": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Windows</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". <em>Install</em> for <em>Windows</em> Server and <em>Windows</em> 10 using our wizard Before <em>installation</em>, be sure to review the requirements. Then, to <em>install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Windows</em>, you can use our launcher, or follow the instructions in this document to complete a basic <em>installation</em>. <em>Windows</em> If you don&#x27;t"
      },
      "id": "6044672464441f9bb6378ed9"
    },
    {
      "sections": [
        "Infrastructure agent configuration settings",
        "Important",
        "Agent variables",
        "license_key (REQUIRED)",
        "fedramp",
        "max_procs",
        "payload_compression_level",
        "startup_connection_retries",
        "startup_connection_retry_time",
        "startup_connection_timeout",
        "Cloud variables",
        "cloud_max_retry_count",
        "cloud_metadata_expiry_sec",
        "cloud_retry_backoff_sec",
        "disable_cloud_instance_id",
        "disable_cloud_metadata",
        "Debug variables",
        "trace",
        "Docker variables",
        "container_cache_metadata_limit",
        "docker_api_version",
        "File system variables",
        "custom_supported_file_systems",
        "file_devices_ignored",
        "Hostname variables",
        "display_name",
        "dns_hostname_resolution",
        "override_hostname",
        "override_hostname_short",
        "Installation variables",
        "agent_dir",
        "plugin_dir",
        "custom_plugin_installation_dir",
        "Integrations variables",
        "passthrough_environment",
        "entityname_integrations_v2_update",
        "http_server_enabled",
        "http_server_host",
        "http_server_port",
        "remove_entities_period",
        "Inventory variables",
        "offline_time_to_reset",
        "ignored_inventory",
        "Linux variables",
        "pid_file",
        "ignore_reclaimable",
        "Logging variables",
        "log_file",
        "log_format",
        "log_to_stdout",
        "verbose",
        "smart_verbose_mode_entry_limit",
        "Metrics variables",
        "custom_attributes",
        "Tip",
        "enable_process_metrics",
        "include_matching_metrics",
        "network_interface_filters",
        "Linux",
        "Windows",
        "disable_zero_mem_process_filter",
        "Plugins variables",
        "disable_all_plugins",
        "cloud_security_group_refresh_sec",
        "daemontools_interval_sec",
        "dpkg_interval_sec",
        "facter_interval_sec",
        "kernel_modules_refresh_sec",
        "network_interface_interval_sec",
        "rpm_interval_sec",
        "selinux_interval_sec",
        "sshd_config_refresh_sec",
        "supervisor_interval_sec",
        "sysctl_interval_sec",
        "systemd_interval_sec",
        "sysvinit_interval_sec",
        "upstart_interval_sec",
        "users_refresh_sec",
        "supervisor_rpc_sock",
        "facter_home_dir",
        "Proxy variables",
        "proxy",
        "ignore_system_proxy",
        "ca_bundle_dir",
        "ca_bundle_file",
        "proxy_validate_certificates",
        "proxy_config_plugin",
        "Samples variables",
        "metrics_network_sample_rate",
        "metrics_process_sample_rate",
        "metrics_storage_sample_rate",
        "metrics_system_sample_rate",
        "metrics_nfs_sample_rate",
        "detailed_nfs",
        "Security variables",
        "selinux_enable_semodule",
        "strip_command_line",
        "Windows variables",
        "windows_services_refresh_sec",
        "windows_updates_refresh_sec",
        "app_data_dir",
        "enable_win_update_plugin",
        "legacy_storage_sampler",
        "win_process_priority_class",
        "win_removable_drives",
        "What's next?"
      ],
      "title": "Infrastructure agent configuration settings ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Configuration"
      ],
      "external_id": "7d016d3f5af6ed1615904578f0ed8bce02aa335b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/configuration/infrastructure-agent-configuration-settings/",
      "published_at": "2021-05-05T02:35:20Z",
      "updated_at": "2021-03-30T19:18:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent has a large set of configuration settings to fine-tune its behavior. Here we: List all the configuration options (in both their YAML and the environment variable names). Explain what the settings do and when to use them. Give the variable type and default value (if any). List the minimum required agent version as applicable. For an example of how all these variables can be used, see our sample configuration template. Important license_key is the only required setting. Agent variables license_key (REQUIRED) Specifies the license key for your New Relic account. The agent uses this key to associate your server's metrics with your New Relic account. This setting is created as part of the standard installation process. YML option name Environment variable Type Default Version license_key NRIA_LICENSE_KEY string Example: license_key: 1234567890abcdefghijklmnopqrstuvwxyz1234 Copy fedramp Specifies whether Fedramp endpoints should be used. YML option name Environment variable Type Default Version fedramp NRIA_FEDRAMP boolean false 1.15.0 max_procs Specifies the number of logical processors available to the agent. Increasing this value helps to distribute the load between different cores. If set to -1, the agent will try to read the environment variable GOMAXPROCS. If this variable is not set, the default value will be the total number of cores available in the host. YML option name Environment variable Type Default Version max_procs NRIA_MAX_PROCS integer 1 1.0.1002 payload_compression_level Since version 1.0.804 or higher, data sent from the agent is compressed by default. To disable payload compression, set payload_compression_level to 0. Important Recommendation: Do not change this setting. YML option name Environment variable Type Default Version payload_compression_level NRIA_PAYLOAD_COMPRESSION_LEVEL integer 6 1.0.804 startup_connection_retries Number of times the agent will retry the request to check New Relic's platform availability at startup before throwing an error. If set to a negative value, the agent will keep checking the connection until it succeeds. YML option name Environment variable Type Default Version startup_connection_retries NRIA_STARTUP_CONNECTION_RETRIES integer 6 1.0.936 startup_connection_retry_time After a request has timed out, time the agent waits to retry a request to check New Relic's platform availability at startup. YML option name Environment variable Type Default Version startup_connection_retry_time NRIA_STARTUP_CONNECTION_RETRY_TIME string 5s 1.0.936 - 1.2.30 startup_connection_timeout Time the agent waits until a request to check New Relic's platform availability at startup is considered timed out. YML option name Environment variable Type Default Version startup_connection_timeout NRIA_STARTUP_CONNECTION_TIMEOUT string 10s 1.0.936 Cloud variables If the agent is running in a cloud instance, the agent will try to detect the cloud type and fetch metadata. cloud_max_retry_count Sets the number of times the agent retries to connect in case that cloud detection fails. If cloud detection fails during the initialization of the agent, the agent will retry after waiting for CloudRetryBackOffSec seconds. YML option name Environment variable Type Default Version cloud_max_retry_count NRIA_CLOUD_MAX_RETRY_COUNT integer 10 1.2.6 cloud_metadata_expiry_sec Sets the interval of time the agent will wait until discarding the metadata, in seconds. After this period metadata expires and the agent will fetch it again. YML option name Environment variable Type Default Version cloud_metadata_expiry_sec NRIA_CLOUD_METADATA_EXPIRY_SEC integer 300 1.2.6 cloud_retry_backoff_sec Sets the interval of time the agent waits between cloud detection retries in case that cloud detection failed, in seconds. If cloud detection fails during the initialization of the agent, it will retry for CloudMaxRetryCount times. YML option name Environment variable Type Default Version cloud_retry_backoff_sec NRIA_CLOUD_RETRY_BACKOFF_SEC integer 60 1.2.6 disable_cloud_instance_id Similar to DisableCloudMetadata, but it disables the collection of cloud metadata only for the host alias plugin. YML option name Environment variable Type Default Version disable_cloud_instance_id NRIA_DISABLE_CLOUD_INSTANCE_ID boolean false 1.0.220 disable_cloud_metadata Disables the collection of cloud metadata. YML option name Environment variable Type Default Version disable_cloud_metadata NRIA_DISABLE_CLOUD_METADATA boolean false 1.0.690 Debug variables trace Enables traces to be printed in the logs for the given features. Traces are additional log entries used for debugging purposes of specific features, they are only shown when the verbose log config option is enabled. Currently two features have traces defined: connect and attributes. YML option name Environment variable Type Default Version trace NRIA_TRACE [ ]string [] 1.0.1015 Example as a YAML attribute: trace: [connect, attributes] Copy Example as an environment variable: NRIA_TRACE='connect, attributes' Copy Docker variables container_cache_metadata_limit Time, in seconds, before the cached containers metadata expires and the agent needs to fetch them again. YML option name Environment variable Type Default Version container_cache_metadata_limit NRIA_CONTAINER_CACHE_METADATA_LIMIT integer 60 1.0.801 docker_api_version Specifies the Docker client API version. YML option name Environment variable Type Default Version docker_api_version NRIA_DOCKER_API_VERSION string 1.24 1.1.4 File system variables custom_supported_file_systems List of the types of file systems that the agent supports. This value needs to be a subset of the default list, and items that are not in the default list will be discarded. YML option name Environment variable Type Default Version custom_supported_file_systems NRIA_CUSTOM_SUPPORTED_FILESYSTEMS [ ]string Linux: [\"xfs\", \"btrfs\", \"ext\", \"ext2\", \"ext3\", \"ext4\", \"hfs\", \"vxfs\"] Windows: [\"NTFS\", \"ReFS\"] 1.0.220 file_devices_ignored List of storage devices to be ignored by the agent when gathering StorageSample data. YML option name Environment variable Type Default Version file_devices_ignored NRIA_FILE_DEVICES_IGNORED [ ]string 1.0.220 Example as a YAML attribute: file_devices_ignored: - sda1 - sda2 Copy Example as an environment variable: FILE_DEVICES_IGNORED=\"sda1,sda2\" Copy Hostname variables display_name Overrides the auto-generated hostname for reporting. This is useful when you have multiple hosts with the same name, since our infrastructure monitoring uses the hostname as the unique identifier for each host. Keep in mind this value is also used for the loopback address replacement on entity names. For more information, see our documentation on how entity name resolution works. YML option name Environment variable Type Default Version display_name NRIA_DISPLAY_NAME string empty 1.0.266 Example: display_name: teslaOne Copy dns_hostname_resolution When true, the full hostname is resolved by performing a reverse lookup of the host's address. Otherwise, it will be retrieved with the hostname command on Linux and from the TCP/IP parameters of the registry on Windows. YML option name Environment variable Type Default Version dns_hostname_resolution NRIA_DNS_HOSTNAME_RESOLUTION boolean true 1.2.6 override_hostname When set, this is the value that will be reported for the full hostname; otherwise, the agent will perform the normal lookup behavior. YML option name Environment variable Type Default Version override_hostname NRIA_OVERRIDE_HOSTNAME string 1.0.1015 Example: my.custom-hostname.co.org Copy override_hostname_short When set, this is the value that will be reported for the hostname; otherwise, the agent will perform the normal lookup behavior. YML option name Environment variable Type Default Version override_hostname_short NRIA_OVERRIDE_HOSTNAME_SHORT string 1.0.1015 Example: my.custom-hostname Copy Installation variables agent_dir Directory where the agent stores files for cache, inventory, integrations, etc. YML option name Environment variable Type Default Version agent_dir NRIA_AGENT_DIR string Linux: /var/db/newrelic-infra Windows: C:\\Program Files\\NewRelic\\newrelic-infra\\ 1.0.2 plugin_dir Directory containing the configuration files of the integrations. Each integration has its own configuration file, named by default <integration_name>-config.yml, placed in a predefined location from which the agent loads on initialization. YML option name Environment variable Type Default Version plugin_dir NRIA_PLUGIN_DIR string Linux: etc/newrelic-infra/integrations.d/ Windows: \\Program Files\\NewRelic\\newrelic-infra\\inregrations.d 1.0.2 Important With secrets management, you can configure on-host integrations with New Relic Infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. custom_plugin_installation_dir Specifies a custom path to install integrations, which allows to install them outside the agent_dir. It has priority when the agent is looking for installed integrations. YML option name Environment variable Type Default Version custom_plugin_installation_dir NRIA_CUSTOM_PLUGIN_INSTALLATION_DIR string Empty 1.0.2 Integrations variables passthrough_environment A list of environment variables that will be passed to all integrations. If an integration already has an existing configuration option with the same name, then the environment variable takes precedence. YML option name Environment variable Type Default Version passthrough_environment NRIA_PASSTHROUGH_ENVIRONMENT [ ]string Empty 1.0.719 Example as a YAML attribute (inside the agent's configuration file, located by default in /etc/newrelic-infra.yml): passthrough_environment: - HOST - PORT Copy Example as an environment variable: NRIA_PASSTHROUGH_ENVIRONMENT=\"HOST,PORT\" Copy entityname_integrations_v2_update The agent enables loopback-address replacement on the entity name (and therefore key) automatically for version 3 of the integration protocol. If you are using version 2 of the protocol and want this behavior, enable the entityname_integrations_v2_update option. YML option name Environment variable Type Default Version entityname_integrations_v2_update NRIA_ENTITYNAME_INTEGRATIONS_V2_UPDATE boolean false 1.2.15 http_server_enabled By setting this configuration parameter to true the agent will open an HTTP port (by default, 8001) to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_enabled NRIA_HTTP_SERVER_ENABLED boolean false 1.0.818 http_server_host By setting this value, the agent will start listening on the HTTPServerPort to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_host NRIA_HTTP_SERVER_HOST string localhost 1.0.818 http_server_port Sets the port for the http server to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_port NRIA_HTTP_SERVER_PORT integer 8001 1.0.818 remove_entities_period Starts the process of deleting entities that haven't reported information during this interval. Valid time units: s (seconds), m (minutes), and h (hour). YML option name Environment variable Type Default Version remove_entities_period NRIA_REMOVE_ENTITIES_PERIOD string 48h 1.0.859 Example: 1h Copy Inventory variables offline_time_to_reset If the cached inventory becomes older than this value (for example, because the agent is offline), the agent automatically removes and recreates the delta store. YML option name Environment variable Type Default Version offline_time_to_reset NRIA_OFFLINE_TIME_TO_RESET string 24h 1.0.888 ignored_inventory The list of inventory paths ignored by the agent. YML option name Environment variable Type Default Version ignored_inventory NRIA_IGNORED_INVENTORY string [ ] Empty list 1.0.336 Example as a YAML attribute: ignored_inventory: - files/config/stuff.bar - files/config/stuff.foo Copy Example as an environment variable: NRIA_IGNORED_INVENTORY=\"files/config/stuff.bar,files/config/stuff.foo\" Copy Linux variables pid_file Location on Linux where the pid file of the agent process is created. It is used at startup to ensure that no other instances of the agent are running. YML option name Environment variable Type Default Version pid_file NRIA_PID_FILE string /var/run/newrelic-infra/newrelic-infra.pid 1.0.2 ignore_reclaimable When true, formulation of the host virtual memory considers SReclaimable as available memory; otherwise SReclaimable will be considered part of the used memory. YML option name Environment variable Type Default Version ignore_reclaimable NRIA_IGNORE_RECLAIMABLE boolean false 1.2.6 Logging variables log_file Defines the file path for the logs. The default installation creates a log directory and it sets this filepath value in the log_file configuration option for you. This log directory is different for each OS, as shown below. Change this configuration option to customize the file path for the logs. YML option name Environment variable Type Default Version log_file NRIA_LOG_FILE string See below * Default paths: Linux: If not defined, it logs only in the standard output. Windows, agent version 1.0.752 or lower: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy Windows, agent version 1.0.775 to 1.0.944: C:\\%APPDATA%\\Roaming\\New Relic\\newrelic-infra\\newrelic-infra.log Copy Windows, agent version 1.0.944 or higher: C:\\%ProgramData%\\New Relic\\newrelic-infra\\newrelic-infra.log Copy If the directory can't be created: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy log_format Defines the log output format. Available values: text: Plain text output, one line per log entry. json: JSON-formatted output, one line per log entry. YML option name Environment variable Type Default Version log_format NRIA_LOG_FORMAT string text 1.4.9 log_to_stdout By default all logs are displayed in both standard output and a log file. To disable logs in the standard output, set this configuration option to false. YML option name Environment variable Type Default Version log_to_stdout NRIA_LOG_TO_STDOUT boolean true 1.0.703 verbose When verbose is set to 0, verbose logging is off, but the agent still creates logs. Set this to 1 to create verbose logs to use in troubleshooting the agent; set verbose to 2 ito use the smart verbose mode logging. Smart verbose mode logs the last smart_verbose_mode_entry_limit debug messages when an error is logged. Setting verbose to 3 enables forwarding the agent logs to New Relic Logs. Important Verbose logging can generate a lot of data very quickly. Run the agent in verbose mode only for as long as necessary to reproduce your issue, then set verbose: 0 and restart your agent to disable verbose logging. Alternatively, you can set verbose: 2, which will enable smart verbose mode. YML option name Environment variable Type Default Version verbose NRIA_VERBOSE Integer (0, 1, 2 or 3) 0 1.9.0 when using smart mode (2) 1.11.4 when forwarding the New Relic Logs (3) smart_verbose_mode_entry_limit smart_verbose_mode_entry_limit refers to the number of previous debug messages that will be logged when an error is logged. For example, if the limit is set to 5, debug logs will be cached in memory until an error is logged, at which point the previous 5 debug messages will also be logged Important This configuration option is only used when verbose is set to 2 (Smart Verbose Mode enabled). YML option name Environment variable Type Default Version smart_verbose_mode_entry_limit NRIA_SMART_VERBOSE_MODE_ENTRY_LIMIT Integer 1000 1.9.0 Metrics variables custom_attributes Custom attributes are key-value pairs (similar to tags in other tools) used to annotate the data from the Infrastructure agent. You can use this metadata to build filter sets, group your results, and annotate your data. For example, you might indicate a machine's environment (staging or production), the service a machine hosts (login service, for example), or the team responsible for that machine. Tip The agent collects many details about your environment as part of its default attributes, including Amazon Elastic Compute Cloud (Amazon EC2) tags. YML option name Environment variable Type custom_attributes NRIA_CUSTOM_ATTRIBUTES map [ string]interface { } Use a list of custom attributes to annotate the data from this agent instance. Separate keys and values with colons :, as in KEY: VALUE, and separate each key-value pair with a line break. Keys can be any valid YAML except slashes /. Values can be any YAML string, including spaces. Example as a YAML attribute: custom_attributes: environment: production service: login service team: alpha-team Copy Example as an environment variable: NRIA_CUSTOM_ATTRIBUTES='{\"customAttribute_1\":\"SOME_ATTRIBUTE\",\"customAttribute_2\": \"SOME_ATTRIBUTE_2\"}' Copy NRQL example filtering by custom attribute: FROM SystemSample SELECT * WHERE environment = 'production' Copy enable_process_metrics Important Requires infrastructure agent version 1.12.0 or higher. Accounts created before July 20, 2020 and/or infrastructure agents installed using the new Guided Install have this variable enabled by default. Enables the sending of process metrics to New Relic. By default, the infrastructure agent doesn't send data about the operating system's processes. The agent still collects such data, unless metrics_process_sample_rate is set to -1. To report metric data about all the operating system's processes, set enable_process_metrics to true. To disable, set to false. Sending all process data could increase the volume of data sent to New Relic. To fine-tune which processes you want to monitor, configure include_matching_metrics. By default, processes using low memory are excluded from being sampled. For more information, see disable-zero-mem-process-filter. YML option name Environment variable Type Default Version enable_process_metrics NRIA_ENABLE_PROCESS_METRICS boolean false include_matching_metrics Important Currently, this setting only applies to an operating system's processes metrics. You can control how much data is sent to New Relic by configuring include_matching_metrics, which allows you to restrict the transmission of metric data based on the values of metric attributes. You include metric data by defining literal or partial values for any of the attributes of the metric. For example, you can choose to send the host.process.cpuPercent of all processes whose process.name match the ^java regular expression. In this example, we include process metrics using executable files and names: include_matching_metrics: # You can combine attributes from different metrics process.name: - regex “^java” # Include all processes starting with \"java\" process.executable: - “/usr/bin/python2” # Include the Python 2.x executable - regex “\\\\System32\\\\svchost” # Include all svchost executables Copy If you need to include command line arguments in any of the values, set strip_command_line to false (the infrastructure agents removes CLI arguments by default to prevent secrets from leaking). To configure include_matching_metrics as an environment variable for the Kubernetes integration, add it in the manifest inside the env: object: env: - name: NRIA_INCLUDE_MATCHING_METRICS value: | process.name: - regex \"^java\" process.executable: - \"/usr/bin/python2\" - regex \"\\\\System32\\\\svchost\" Copy Default YML option name Environment variable Type Default Version include_matching_metrics NRIA_INCLUDE_MATCHING_METRICS metric.attribute: - regex \"pattern\" - \"string\" - \"string-with-wildcard * \" network_interface_filters You can use the network interface filters configuration to hide unused or uninteresting network interfaces from the infrastructure agent. This helps reduce resource usage, work, and noise in your data. Important Environment variables are not supported for this configuration setting. The configuration uses a simple pattern-matching mechanism that can look for interfaces that start with a specific sequence of letters or numbers following either pattern: {name}[other characters], where you specify the name using the prefix option [number]{name}[other characters], where you specify the name using the index-1 option New Relic infrastructure implements a curated default list of filters, available for both Linux and Windows, that you can modify. YML option name Environment variable Type Default Version network_interface_filters not supported map [ string] [ ]string 1.0.220 Linux Default network interface filters for Linux: Network interfaces that start with dummy, lo, vmnet, sit, tun, tap, or veth Network interfaces that contain tun or tap The following example (added to your configuration file) overrides the default filters. This will ignore network interfaces that start with dummy or lo , or contain tun preceded by a sequence of numbers, and followed by other characters: network_interface_filters: prefix: - dummy - lo index-1: - tun Copy Windows Default network interface filters for Windows: Network interfaces that start with Loop, isatap, or Local The following example (added to your configuration file) overrides the default filters. This will ignore network interfaces that start with Loop: network_interface_filters: prefix: - Loop Copy disable_zero_mem_process_filter The ZeroRSSFilter excludes processes that are not using memory from being sampled. Disable the filter so that the agent includes these processes in the ProcessSample. YML option name Environment variable Type Default Version disable_zero_mem_process_filter NRIA_DISABLE_ZERO_MEM_PROCESS_FILTER boolean false 1.0.832 Plugins variables Tip You can quickly disable all the variables by setting DisableAllPlugins to true, and turn on only those options you need. disable_all_plugins To disable all the plugins, set this option to true. YML option name Environment variable Type Default Version disable_all_plugins NRIA_DISABLE_ALL_PLUGINS boolean false 1.2.1 cloud_security_group_refresh_sec Sampling period for the CloudSecurityGroups plugin, in seconds. The minimum value is 30. To disable it, set it to -1. Important This plugin is activated only if the agent is running in an AWS instance. YML option name Environment variable Type Default Version cloud_security_group_refresh_sec NRIA_CLOUD_SECURITY_GROUP_REFRESH_SEC int64 60 1.0.692 daemontools_interval_sec Sampling period for the Daemontoolsplugin, in seconds. The minimum value is a 10. To disable it, set it to -1. YML option name Environment variable Type Default Version daemontools_interval_sec NRIA_DAEMONTOOLS_INTERVAL_SEC int64 15 1.0.316 dpkg_interval_sec Sampling period for the Dpkg plugin, in seconds. The minimum value is 30. To disable it, set it to -1. If the parameter is not explicitly set in the config file, it can be disabled by setting DisableAllPlugins to true. Important This is only activated in root or privileged running modes and on Debian-based distributions. YML option name Environment variable Type Default Version dpkg_interval_sec NRIA_DPKG_INTERVAL_SEC int64 30 1.0.316 facter_interval_sec Sampling period for the Facter plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version facter_interval_sec NRIA_FACTER_INTERVAL_SEC int64 30 1.0.316 kernel_modules_refresh_sec Sampling period for the CloudSecurityGroups plugin, in seconds. The minimum value is 10. To disable it, set it to -1. Important kernel_modules_refresh_sec is only activated in root or privileged running modes. YML option name Environment variable Type Default Version kernel_modules_refresh_sec NRIA_KERNEL_MODULES_REFRESH_SEC int64 10 1.0.755 network_interface_interval_sec Sampling period for the NetworkInterface plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version network_interface_interval_sec NRIA_NETWORK_INTERFACE_INTERVAL_SEC int64 60 1.0.329 rpm_interval_sec Sampling period for the Rpm plugin, in seconds. The minimum value is 30. To disable it, set it to -1. Important rpm_interval_sec is only activated when the agent runs in root or privileged modes for RedHat, RedHat AWS, or SUSE distributions. YML option name Environment variable Type Default Version rpm_interval_sec NRIA_RPM_INTERVAL_SEC int64 30 1.0.316 selinux_interval_sec Sampling period for the SELinux plugin, in seconds. The minimum value is 30. To disable it, set it to -1. This option is ignored if SelinuxEnableSemodule is set to false. For more information, see our troubleshooting doc on disabling the SELinux module. Important SELinux is only activated when the agent runs in root mode. YML option name Environment variable Type Default Version selinux_interval_sec NRIA_SELINUX_INTERVAL_SEC int64 30 1.0.316 sshd_config_refresh_sec Sampling period for the Sshd plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version sshd_config_refresh_sec NRIA_SSHD_CONFIG_REFRESH_SEC int64 15 1.0.755 supervisor_interval_sec Sampling period for the Supervisor plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version supervisor_interval_sec NRIA_SUPERVISOR_INTERVAL_SEC int64 15 1.0.316 sysctl_interval_sec Sampling period for the Sysctl plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version sysctl_interval_sec NRIA_SYSCTL_INTERVAL_SEC int64 60 1.0.316 systemd_interval_sec Sampling period for the Systemd plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version systemd_interval_sec NRIA_SYSTEMD_INTERVAL_SEC int64 30 1.0.316 sysvinit_interval_sec Sampling period for the sysv plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version sysvinit_interval_sec NRIA_SYSVINIT_INTERVAL_SEC int64 30 1.0.316 upstart_interval_sec Sampling period for the Upstart plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version upstart_interval_sec NRIA_UPSTART_INTERVAL_SEC int64 30 1.0.316 users_refresh_sec Sampling period for the Users plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version users_refresh_sec NRIA_USERS_REFRESH_SEC int64 30 1.0.755 supervisor_rpc_sock Location of the supervisor socket. YML option name Environment variable Type Default Version supervisor_rpc_sock NRIA_SUPERVISOR_RPC_SOCK string /var/run/supervisor.sock 1.0.2 facter_home_dir Sets the HOME environment variable for Puppet's Facter. If not defined, it defaults to the current user's home directory. YML option name Environment variable Type Default Version facter_home_dir NRIA_FACTER_HOME_DIR string 1.1.7 Proxy variables For infrastructure agent version 1.3.1 or higher, the precedence of the proxy configuration settings is: NRIA_PROXY proxy HTTP_PROXY HTTPS_PROXY proxy Your system may have firewall rules that require the agent to use a proxy to communicate with New Relic. If so, set the proxy URL in the form https://user:password@hostname:port. It can be HTTP or HTTPS. YML option name Environment variable Type Default Version proxy NRIA_PROXY string Empty 1.0.308 Example: https://proxy_user.access_10@proxy_01:1080 Copy ignore_system_proxy When set to true, the HTTPS_PROXY and HTTP_PROXY environment variables are ignored. This is useful when the agent needs to connect directly to the metrics collector and skip the existing system proxy. YML option name Environment variable Type Default Version ignore_system_proxy NRIA_IGNORE_SYSTEM_PROXY boolean false 1.0.1002 ca_bundle_dir If the HTTPS_PROXY option references to a proxy with self-signed certificates, this option specifies the path to the the directory where the proxy certificate is available. The certificates in the directory must end with the .pem extension. YML option name Environment variable Type Default Version ca_bundle_dir NRIA_CA_BUNDLE_DIR string 1.0.296 ca_bundle_file If the HTTPS_PROXY option references to a proxy with self-signed certificates, this option specifies the path to the certificate file. YML option name Environment variable Type Default Version ca_bundle_file NRIA_CA_BUNDLE_FILE string 1.0.296 proxy_validate_certificates If set to true, when the proxy is configured to use an HTTPS connection, it will only work: If the HTTPS proxy has certificates from a valid Certificate Authority. If the ca_bundle_file or ca_bundle_dir configuration properties contain the HTTPS proxy certificates. YML option name Environment variable Type Default Version proxy_validate_certificates NRIA_PROXY_VALIDATE_CERTIFICATES boolean false 1.3.0 proxy_config_plugin Sends the following proxy configuration information as inventory: HTTPS_PROXY HTTP_PROXY proxy ca_bundle_dir ca_bundle_file ignore_system_proxy proxy_validate_certificates YML option name Environment variable Type Default Version proxy_config_plugin NRIA_PROXY_CONFIG_PLUGIN boolean true 1.3.0 If you are having problems with proxy configuration, see Proxy troubleshooting. Samples variables metrics_network_sample_rate Sample rate of network samples, in seconds. Minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_network_sample_rate NRIA_METRICS_NETWORK_SAMPLE_RATE integer 10 1.0.308 metrics_process_sample_rate Sample rate of process samples, in seconds. Minimum value is 20. To disable process samples entirely, set metrics_process_sample_rate to -1. YML option name Environment variable Type Default Version metrics_process_sample_rate NRIA_METRICS_PROCESS_SAMPLE_RATE integer 20 1.0.308 metrics_storage_sample_rate Sample rate of storage samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_storage_sample_rate NRIA_METRICS_STORAGE_SAMPLE_RATE integer 5 1.0.308 metrics_system_sample_rate Sample rate of system samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_system_sample_rate NRIA_METRICS_SYSTEM_SAMPLE_RATE integer 5 1.0.308 metrics_nfs_sample_rate Sample rate of NFS storage samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_nfs_sample_rate NRIA_METRICS_NFS_SAMPLE_RATE integer 20 1.5.40 detailed_nfs Detailed NFS metrics. When enabled, the agent will provide a complete list of NFS metrics. YML option name Environment variable Type Default Version detailed_nfs NRIA_DETAILED_NFS boolean false 1.5.40 Security variables selinux_enable_semodule Get versions of policy modules installed using SEModule. If disabled, the SELinux plugin will only retrieve the status using SEStatus. YML option name Environment variable Type Default Version selinux_enable_semodule NRIA_SELINUX_ENABLE_SEMODULE boolean true 1.0.864 strip_command_line When true, the agent removes the command arguments from the commandLine attribute of the ProcessSample. Tip This is a security measure to prevent leaking sensitive information. YML option name Environment variable Type Default Version strip_command_line NRIA_STRIP_COMMAND_LINE boolean true 1.0.149 Windows variables windows_services_refresh_sec Sampling period for the Windows services plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version windows_services_refresh_sec NRIA_WINDOWS_SERVICES_REFRESH_SEC int64 30 1.0.755 windows_updates_refresh_sec Sampling period for the Windows updates plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version windows_updates_refresh_sec NRIA_WINDOWS_UPDATES_REFRESH_SEC int64 60 1.0.755 app_data_dir Defines the path to store data in a different path than the program files directory: %AppDir%/data: Used for storing the delta data %AppDir%/user_data: External directory for user-generated JSON files %AppDir%/newrelic-infra.log: If log file config option is not defined, then we use this directory path as default. YML option name Environment variable Type Default Version app_data_dir NRIA_APP_DATA_DIR string Windows: env(ProgramData)\\New Relic\\newrelic-infra Linux: Not applicable 1.0.755 enable_win_update_plugin Enables the Windows updates plugin, which retrieves the lists of hotfixes that are installed on the host. YML option name Environment variable Type Default Version enable_win_update_plugin NRIA_ENABLE_WIN_UPDATE_PLUGIN boolean false 1.0.274 legacy_storage_sampler If true, the agent will be forced to use Windows WMI (the agent's legacy method to grab metrics for Windows; for example, StorageSampler) and will disable the new method (which uses the PDH library). YML option name Environment variable Type Default Version legacy_storage_sampler NRIA_LEGACY_STORAGE_SAMPLER boolean 1.0.1051 win_process_priority_class This configuration option allows to increase the newrelic-infra.exe process priority to any of the following values: Normal Idle High RealTime BelowNormal AboveNormal YML option name Environment variable Type Default Version win_process_priority_class NRIA_WIN_PROCESS_PRIORITY_CLASS string 1.0.989 Example: Normal Copy win_removable_drives Enables the Windows agent to report drives A: and B: when they are mapped as removable drives. YML option name Environment variable Type Default Version win_removable_drives NRIA_WIN_REMOVABLE_DRIVES boolean true 1.3.1 What's next? You can also: Further understand the configuration of the agent. Create a configuration file using our template. See how you can manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.40857,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> configuration settings ",
        "sections": "<em>Infrastructure</em> <em>agent</em> configuration settings",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " management, you can configure on-host integrations with New Relic <em>Infrastructure</em>&#x27;s <em>agent</em> to use sensitive data (such as passwords) without having to write them as plain text into the integration&#x27;s configuration file. For more information, see Secrets management. custom_plugin_<em>installation</em>_dir Specifies"
      },
      "id": "603ea542196a67a38aa83dd8"
    }
  ],
  "/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-manual-install-infrastructure-agent-windows": [
    {
      "sections": [
        "Zip assisted install of the infrastructure agent for Windows",
        "Install the agent",
        "Important",
        "Configure your installation",
        "What's next?"
      ],
      "title": "Zip assisted install of the infrastructure agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "fcadbeed626401863b6b16e5c52e9a472a5ac13e",
      "image": "https://docs.newrelic.com/static/6ca3d6d18f535376b153baaece37fdcb/0abdd/Infra-windows-files.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/zip-assisted-install-infrastructure-agent-windows/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-04-16T02:59:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With the assisted install of the infrastructure agent for Windows, you can make the changes you need to the installation script we provide so you can adapt it to your environment. Before installation, make sure to check the compatibility and requirements. Install the agent Important As of version 1.4.0, the infrastructure agent package includes the additional newrelic-infra-ctl binary, which is used to help troubleshoot a running agent. Although this binary is not required to execute the agent, we recommend to add it in your path. See our docs for more information. To install the agent: Download the packaged agent file. Unpack the file. Make sure the file unpacks with the following structure: Once it's unpacked, access and edit the installation PowerShell script installer.ps1. Update your license key. Optional: Update any other parameters. Execute installer.ps1 with admin rights. Configure your installation Important Make sure any custom folder defined in the installation settings has permissions limitations properly defined. The infrastructure agent might execute any integration defined in the NRIA_PLUGIN_DIR directory with Administrator permissions. You can configure the following parameters during the assisted install for Windows: Variable Description NRIA_AGENT_DIR Required at agent startup. The agent home directory. Default: C:\\Program Files\\New Relic\\newrelic-infra Copy NRIA_APP_DATA_DIR This configures the data directory to store inventory and other agent files. Default: C:\\%ProgramData%\\New Relic\\newrelic-infra Copy NRIA_CONFIG_FILE Required at installation. The agent configuration file's location. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.yml Copy NRIA_LICENSE_KEY Only configuration option required at startup. The New Relic license key. NRIA_LOG_FILE Required at agent startup. The location where the agent will log. Default: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy NRIA_OVERWRITE By default and for security reasons, Windows does not install a service if there's another service with the same name already installed. To bypass this check, make sure this setting NRIA_OVERWRITE is TRUE. Default: TRUE Copy NRIA_PLUGIN_DIR Required at agent startup. The directory containing the configuration files of the integrations. Default: C:\\Program Files\\NewRelic\\newrelic-infra\\inregrations.d Copy NRIA_SERVICE_NAME This provides the name for the Windows service. What's next? You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Add other New Relic infrastructure integrations to collect data from external services. Manually start, stop, restart, or check the agent status.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 338.53363,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "sections": "Zip assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": "With the assisted <em>install</em> of the <em>infrastructure</em> <em>agent</em> for <em>Windows</em>, you can make the changes you need to the <em>installation</em> script we provide so you can adapt it to your environment. Before <em>installation</em>, make sure to check the compatibility and requirements. <em>Install</em> the <em>agent</em> Important As of version"
      },
      "id": "603ea7af196a67dab0a83d9d"
    },
    {
      "sections": [
        "Install the infrastructure monitoring agent for Windows",
        "Tip",
        "Install for Windows Server and Windows 10 using our wizard",
        "Step-by-step instructions",
        "PowerShell install",
        "32-bit Windows",
        "64-bit Windows",
        "Step-by-step install",
        "Important",
        "Scripted installation",
        "What's next?",
        "Install using zip files",
        "Caution",
        "Update the agent"
      ],
      "title": "Install the infrastructure monitoring agent for Windows",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Windows installation"
      ],
      "external_id": "32766f16044664c9c7d66075801930ff53ca5c49",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/windows-installation/install-infrastructure-monitoring-agent-windows/",
      "published_at": "2021-05-05T03:28:38Z",
      "updated_at": "2021-03-13T01:56:49Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic's infrastructure monitoring agent for Windows, you can monitor individual servers and also analyze how your service performs as a whole. The Windows agent can run on your own hardware or in cloud systems such as Amazon EC2 or Windows Azure. The infrastructure monitoring agent supports Windows Server and Windows 10. You can also install with Chef. Tip To use infrastructure monitoring and the rest of our observability platform, join the New Relic family! Sign up to create your free account in only a few seconds. Then ingest up to 100GB of data for free each month. Forever. Install for Windows Server and Windows 10 using our wizard Before installation, be sure to review the requirements. Then, to install the infrastructure monitoring agent for Windows, you can use our launcher, or follow the instructions in this document to complete a basic installation. Windows If you don't have a New Relic account yet, or prefer to follow the procedure manually, follow our step-by-step tutorial, which follows. Step-by-step instructions To install the infrastructure monitoring agent, use our PowerShell script, or follow the step-by-step instructions: PowerShell install Review the agent requirements and supported operating systems. Open the PowerShell as administrator and run the following command: 32-bit Windows $LICENSE_KEY=\"YOUR_LICENSE_KEY\"; ` (New-Object System.Net.WebClient).DownloadFile(\"https://download.newrelic.com/infrastructure_agent/windows/386/newrelic-infra-386.msi\", \"$env:TEMP\\newrelic-infra.msi\"); ` msiexec.exe /qn /i \"$env:TEMP\\newrelic-infra.msi\" GENERATE_CONFIG=true LICENSE_KEY=\"$LICENSE_KEY\" | Out-Null; ` net start newrelic-infra Copy 64-bit Windows $LICENSE_KEY=\"YOUR_LICENSE_KEY\"; ` (New-Object System.Net.WebClient).DownloadFile(\"https://download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi\", \"$env:TEMP\\newrelic-infra.msi\"); ` msiexec.exe /qn /i \"$env:TEMP\\newrelic-infra.msi\" GENERATE_CONFIG=true LICENSE_KEY=\"$LICENSE_KEY\" | Out-Null; ` net start newrelic-infra Copy For a scripted installation, you can pass in configuration parameters. You must first add GENERATE_CONFIG=true and LICENSE_KEY=YOUR_LICENSE_KEY. You may then add these optional config settings: DISPLAY_NAME=YOUR_DISPLAY_NAME PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1','ATTRIBUTE_2':'VALUE_2'}\" The following example sets the license key and configures a proxy server for outbound communication, as well as adding one custom attribute: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi GENERATE_CONFIG=true LICENSE_KEY=YOUR_LICENSE_KEY PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1'}\" Copy Step-by-step install Review the infrastructure monitoring agent requirements and supported operating systems. Download the latest .MSI installer image from: 32-bit Windows https://download.newrelic.com/infrastructure_agent/windows/386/newrelic-infra-386.msi​ Copy 64-bit Windows https://download.newrelic.com/infrastructure_agent/windows/newrelic-infra.msi Copy Important Do not double-click the installer. This will not fully install the local agent and can result in permissions issues. In an admin account, run the install script using an absolute path. 32-bit Windows To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra-386.msi Copy 64-bit Windows To install from the Windows command prompt, run: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi Copy Scripted installation For a scripted installation, you can also pass in configuration parameters. You must first add GENERATE_CONFIG=true and LICENSE_KEY=YOUR_LICENSE_KEY. You may then add these optional config settings: DISPLAY_NAME=YOUR_DISPLAY_NAME PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1','ATTRIBUTE_2':'VALUE_2'}\" The following example sets the license key and configures a proxy server for outbound communication, as well as adding one custom attribute: msiexec.exe /qn /i PATH\\TO\\newrelic-infra.msi GENERATE_CONFIG=true LICENSE_KEY=YOUR_LICENSE_KEY PROXY=http://YOUR_PROXY_SERVER:PROXY_PORT CUSTOM_ATTRIBUTES=\"{'ATTRIBUTE_1':'VALUE_1'}\" Copy Add your New Relic license key to the license_key attribute in newrelic-infra.yml, located in C:\\Program Files\\New Relic\\newrelic-infra\\. When finished, the contents of newrelic-infra.yml should resemble the following: license_key: YOUR_LICENSE_KEY Copy Start the newrelic-infra service. To start from the Windows command prompt, run: net start newrelic-infra Copy Wait a few minutes, then view your server in the Infrastructure UI. If no data appears after waiting a few minutes, follow the troubleshooting steps. Tip As of version 1.4.0, the infrastructure monitoring agent package includes newrelic-infra-ctl, which is used to help troubleshoot a running agent. We recommend adding it to PATH. What's next? The only required configuration option is the license_key setting, which is created as part of the installation procedures. You may also want to: Add custom attributes to annotate your infrastructure data. Connect your AWS account if your servers are hosted on Amazon EC2. Enable log forwarding. Add other New Relic infrastructure integrations to collect data from external services. Install using zip files For custom setup scenarios, you can install the infrastructure monitoring agent using our zip files in assisted or manual modes. This is especially useful when you need to adapt the default installation settings to your environment. Caution Installing the infrastructure monitoring agent using zip files is not supported. Update the agent To upgrade to the latest version, follow standard procedures to update the infrastructure monitoring agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 288.6331,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Windows</em>",
        "sections": "<em>Install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Windows</em>",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": ". <em>Install</em> for <em>Windows</em> Server and <em>Windows</em> 10 using our wizard Before <em>installation</em>, be sure to review the requirements. Then, to <em>install</em> the <em>infrastructure</em> monitoring <em>agent</em> for <em>Windows</em>, you can use our launcher, or follow the instructions in this document to complete a basic <em>installation</em>. <em>Windows</em> If you don&#x27;t"
      },
      "id": "6044672464441f9bb6378ed9"
    },
    {
      "sections": [
        "Infrastructure agent configuration settings",
        "Important",
        "Agent variables",
        "license_key (REQUIRED)",
        "fedramp",
        "max_procs",
        "payload_compression_level",
        "startup_connection_retries",
        "startup_connection_retry_time",
        "startup_connection_timeout",
        "Cloud variables",
        "cloud_max_retry_count",
        "cloud_metadata_expiry_sec",
        "cloud_retry_backoff_sec",
        "disable_cloud_instance_id",
        "disable_cloud_metadata",
        "Debug variables",
        "trace",
        "Docker variables",
        "container_cache_metadata_limit",
        "docker_api_version",
        "File system variables",
        "custom_supported_file_systems",
        "file_devices_ignored",
        "Hostname variables",
        "display_name",
        "dns_hostname_resolution",
        "override_hostname",
        "override_hostname_short",
        "Installation variables",
        "agent_dir",
        "plugin_dir",
        "custom_plugin_installation_dir",
        "Integrations variables",
        "passthrough_environment",
        "entityname_integrations_v2_update",
        "http_server_enabled",
        "http_server_host",
        "http_server_port",
        "remove_entities_period",
        "Inventory variables",
        "offline_time_to_reset",
        "ignored_inventory",
        "Linux variables",
        "pid_file",
        "ignore_reclaimable",
        "Logging variables",
        "log_file",
        "log_format",
        "log_to_stdout",
        "verbose",
        "smart_verbose_mode_entry_limit",
        "Metrics variables",
        "custom_attributes",
        "Tip",
        "enable_process_metrics",
        "include_matching_metrics",
        "network_interface_filters",
        "Linux",
        "Windows",
        "disable_zero_mem_process_filter",
        "Plugins variables",
        "disable_all_plugins",
        "cloud_security_group_refresh_sec",
        "daemontools_interval_sec",
        "dpkg_interval_sec",
        "facter_interval_sec",
        "kernel_modules_refresh_sec",
        "network_interface_interval_sec",
        "rpm_interval_sec",
        "selinux_interval_sec",
        "sshd_config_refresh_sec",
        "supervisor_interval_sec",
        "sysctl_interval_sec",
        "systemd_interval_sec",
        "sysvinit_interval_sec",
        "upstart_interval_sec",
        "users_refresh_sec",
        "supervisor_rpc_sock",
        "facter_home_dir",
        "Proxy variables",
        "proxy",
        "ignore_system_proxy",
        "ca_bundle_dir",
        "ca_bundle_file",
        "proxy_validate_certificates",
        "proxy_config_plugin",
        "Samples variables",
        "metrics_network_sample_rate",
        "metrics_process_sample_rate",
        "metrics_storage_sample_rate",
        "metrics_system_sample_rate",
        "metrics_nfs_sample_rate",
        "detailed_nfs",
        "Security variables",
        "selinux_enable_semodule",
        "strip_command_line",
        "Windows variables",
        "windows_services_refresh_sec",
        "windows_updates_refresh_sec",
        "app_data_dir",
        "enable_win_update_plugin",
        "legacy_storage_sampler",
        "win_process_priority_class",
        "win_removable_drives",
        "What's next?"
      ],
      "title": "Infrastructure agent configuration settings ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Install the infrastructure agent",
        "Configuration"
      ],
      "external_id": "7d016d3f5af6ed1615904578f0ed8bce02aa335b",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/install-infrastructure-agent/configuration/infrastructure-agent-configuration-settings/",
      "published_at": "2021-05-05T02:35:20Z",
      "updated_at": "2021-03-30T19:18:23Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The infrastructure agent has a large set of configuration settings to fine-tune its behavior. Here we: List all the configuration options (in both their YAML and the environment variable names). Explain what the settings do and when to use them. Give the variable type and default value (if any). List the minimum required agent version as applicable. For an example of how all these variables can be used, see our sample configuration template. Important license_key is the only required setting. Agent variables license_key (REQUIRED) Specifies the license key for your New Relic account. The agent uses this key to associate your server's metrics with your New Relic account. This setting is created as part of the standard installation process. YML option name Environment variable Type Default Version license_key NRIA_LICENSE_KEY string Example: license_key: 1234567890abcdefghijklmnopqrstuvwxyz1234 Copy fedramp Specifies whether Fedramp endpoints should be used. YML option name Environment variable Type Default Version fedramp NRIA_FEDRAMP boolean false 1.15.0 max_procs Specifies the number of logical processors available to the agent. Increasing this value helps to distribute the load between different cores. If set to -1, the agent will try to read the environment variable GOMAXPROCS. If this variable is not set, the default value will be the total number of cores available in the host. YML option name Environment variable Type Default Version max_procs NRIA_MAX_PROCS integer 1 1.0.1002 payload_compression_level Since version 1.0.804 or higher, data sent from the agent is compressed by default. To disable payload compression, set payload_compression_level to 0. Important Recommendation: Do not change this setting. YML option name Environment variable Type Default Version payload_compression_level NRIA_PAYLOAD_COMPRESSION_LEVEL integer 6 1.0.804 startup_connection_retries Number of times the agent will retry the request to check New Relic's platform availability at startup before throwing an error. If set to a negative value, the agent will keep checking the connection until it succeeds. YML option name Environment variable Type Default Version startup_connection_retries NRIA_STARTUP_CONNECTION_RETRIES integer 6 1.0.936 startup_connection_retry_time After a request has timed out, time the agent waits to retry a request to check New Relic's platform availability at startup. YML option name Environment variable Type Default Version startup_connection_retry_time NRIA_STARTUP_CONNECTION_RETRY_TIME string 5s 1.0.936 - 1.2.30 startup_connection_timeout Time the agent waits until a request to check New Relic's platform availability at startup is considered timed out. YML option name Environment variable Type Default Version startup_connection_timeout NRIA_STARTUP_CONNECTION_TIMEOUT string 10s 1.0.936 Cloud variables If the agent is running in a cloud instance, the agent will try to detect the cloud type and fetch metadata. cloud_max_retry_count Sets the number of times the agent retries to connect in case that cloud detection fails. If cloud detection fails during the initialization of the agent, the agent will retry after waiting for CloudRetryBackOffSec seconds. YML option name Environment variable Type Default Version cloud_max_retry_count NRIA_CLOUD_MAX_RETRY_COUNT integer 10 1.2.6 cloud_metadata_expiry_sec Sets the interval of time the agent will wait until discarding the metadata, in seconds. After this period metadata expires and the agent will fetch it again. YML option name Environment variable Type Default Version cloud_metadata_expiry_sec NRIA_CLOUD_METADATA_EXPIRY_SEC integer 300 1.2.6 cloud_retry_backoff_sec Sets the interval of time the agent waits between cloud detection retries in case that cloud detection failed, in seconds. If cloud detection fails during the initialization of the agent, it will retry for CloudMaxRetryCount times. YML option name Environment variable Type Default Version cloud_retry_backoff_sec NRIA_CLOUD_RETRY_BACKOFF_SEC integer 60 1.2.6 disable_cloud_instance_id Similar to DisableCloudMetadata, but it disables the collection of cloud metadata only for the host alias plugin. YML option name Environment variable Type Default Version disable_cloud_instance_id NRIA_DISABLE_CLOUD_INSTANCE_ID boolean false 1.0.220 disable_cloud_metadata Disables the collection of cloud metadata. YML option name Environment variable Type Default Version disable_cloud_metadata NRIA_DISABLE_CLOUD_METADATA boolean false 1.0.690 Debug variables trace Enables traces to be printed in the logs for the given features. Traces are additional log entries used for debugging purposes of specific features, they are only shown when the verbose log config option is enabled. Currently two features have traces defined: connect and attributes. YML option name Environment variable Type Default Version trace NRIA_TRACE [ ]string [] 1.0.1015 Example as a YAML attribute: trace: [connect, attributes] Copy Example as an environment variable: NRIA_TRACE='connect, attributes' Copy Docker variables container_cache_metadata_limit Time, in seconds, before the cached containers metadata expires and the agent needs to fetch them again. YML option name Environment variable Type Default Version container_cache_metadata_limit NRIA_CONTAINER_CACHE_METADATA_LIMIT integer 60 1.0.801 docker_api_version Specifies the Docker client API version. YML option name Environment variable Type Default Version docker_api_version NRIA_DOCKER_API_VERSION string 1.24 1.1.4 File system variables custom_supported_file_systems List of the types of file systems that the agent supports. This value needs to be a subset of the default list, and items that are not in the default list will be discarded. YML option name Environment variable Type Default Version custom_supported_file_systems NRIA_CUSTOM_SUPPORTED_FILESYSTEMS [ ]string Linux: [\"xfs\", \"btrfs\", \"ext\", \"ext2\", \"ext3\", \"ext4\", \"hfs\", \"vxfs\"] Windows: [\"NTFS\", \"ReFS\"] 1.0.220 file_devices_ignored List of storage devices to be ignored by the agent when gathering StorageSample data. YML option name Environment variable Type Default Version file_devices_ignored NRIA_FILE_DEVICES_IGNORED [ ]string 1.0.220 Example as a YAML attribute: file_devices_ignored: - sda1 - sda2 Copy Example as an environment variable: FILE_DEVICES_IGNORED=\"sda1,sda2\" Copy Hostname variables display_name Overrides the auto-generated hostname for reporting. This is useful when you have multiple hosts with the same name, since our infrastructure monitoring uses the hostname as the unique identifier for each host. Keep in mind this value is also used for the loopback address replacement on entity names. For more information, see our documentation on how entity name resolution works. YML option name Environment variable Type Default Version display_name NRIA_DISPLAY_NAME string empty 1.0.266 Example: display_name: teslaOne Copy dns_hostname_resolution When true, the full hostname is resolved by performing a reverse lookup of the host's address. Otherwise, it will be retrieved with the hostname command on Linux and from the TCP/IP parameters of the registry on Windows. YML option name Environment variable Type Default Version dns_hostname_resolution NRIA_DNS_HOSTNAME_RESOLUTION boolean true 1.2.6 override_hostname When set, this is the value that will be reported for the full hostname; otherwise, the agent will perform the normal lookup behavior. YML option name Environment variable Type Default Version override_hostname NRIA_OVERRIDE_HOSTNAME string 1.0.1015 Example: my.custom-hostname.co.org Copy override_hostname_short When set, this is the value that will be reported for the hostname; otherwise, the agent will perform the normal lookup behavior. YML option name Environment variable Type Default Version override_hostname_short NRIA_OVERRIDE_HOSTNAME_SHORT string 1.0.1015 Example: my.custom-hostname Copy Installation variables agent_dir Directory where the agent stores files for cache, inventory, integrations, etc. YML option name Environment variable Type Default Version agent_dir NRIA_AGENT_DIR string Linux: /var/db/newrelic-infra Windows: C:\\Program Files\\NewRelic\\newrelic-infra\\ 1.0.2 plugin_dir Directory containing the configuration files of the integrations. Each integration has its own configuration file, named by default <integration_name>-config.yml, placed in a predefined location from which the agent loads on initialization. YML option name Environment variable Type Default Version plugin_dir NRIA_PLUGIN_DIR string Linux: etc/newrelic-infra/integrations.d/ Windows: \\Program Files\\NewRelic\\newrelic-infra\\inregrations.d 1.0.2 Important With secrets management, you can configure on-host integrations with New Relic Infrastructure's agent to use sensitive data (such as passwords) without having to write them as plain text into the integration's configuration file. For more information, see Secrets management. custom_plugin_installation_dir Specifies a custom path to install integrations, which allows to install them outside the agent_dir. It has priority when the agent is looking for installed integrations. YML option name Environment variable Type Default Version custom_plugin_installation_dir NRIA_CUSTOM_PLUGIN_INSTALLATION_DIR string Empty 1.0.2 Integrations variables passthrough_environment A list of environment variables that will be passed to all integrations. If an integration already has an existing configuration option with the same name, then the environment variable takes precedence. YML option name Environment variable Type Default Version passthrough_environment NRIA_PASSTHROUGH_ENVIRONMENT [ ]string Empty 1.0.719 Example as a YAML attribute (inside the agent's configuration file, located by default in /etc/newrelic-infra.yml): passthrough_environment: - HOST - PORT Copy Example as an environment variable: NRIA_PASSTHROUGH_ENVIRONMENT=\"HOST,PORT\" Copy entityname_integrations_v2_update The agent enables loopback-address replacement on the entity name (and therefore key) automatically for version 3 of the integration protocol. If you are using version 2 of the protocol and want this behavior, enable the entityname_integrations_v2_update option. YML option name Environment variable Type Default Version entityname_integrations_v2_update NRIA_ENTITYNAME_INTEGRATIONS_V2_UPDATE boolean false 1.2.15 http_server_enabled By setting this configuration parameter to true the agent will open an HTTP port (by default, 8001) to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_enabled NRIA_HTTP_SERVER_ENABLED boolean false 1.0.818 http_server_host By setting this value, the agent will start listening on the HTTPServerPort to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_host NRIA_HTTP_SERVER_HOST string localhost 1.0.818 http_server_port Sets the port for the http server to receive data from the New Relic StatsD backend. YML option name Environment variable Type Default Version http_server_port NRIA_HTTP_SERVER_PORT integer 8001 1.0.818 remove_entities_period Starts the process of deleting entities that haven't reported information during this interval. Valid time units: s (seconds), m (minutes), and h (hour). YML option name Environment variable Type Default Version remove_entities_period NRIA_REMOVE_ENTITIES_PERIOD string 48h 1.0.859 Example: 1h Copy Inventory variables offline_time_to_reset If the cached inventory becomes older than this value (for example, because the agent is offline), the agent automatically removes and recreates the delta store. YML option name Environment variable Type Default Version offline_time_to_reset NRIA_OFFLINE_TIME_TO_RESET string 24h 1.0.888 ignored_inventory The list of inventory paths ignored by the agent. YML option name Environment variable Type Default Version ignored_inventory NRIA_IGNORED_INVENTORY string [ ] Empty list 1.0.336 Example as a YAML attribute: ignored_inventory: - files/config/stuff.bar - files/config/stuff.foo Copy Example as an environment variable: NRIA_IGNORED_INVENTORY=\"files/config/stuff.bar,files/config/stuff.foo\" Copy Linux variables pid_file Location on Linux where the pid file of the agent process is created. It is used at startup to ensure that no other instances of the agent are running. YML option name Environment variable Type Default Version pid_file NRIA_PID_FILE string /var/run/newrelic-infra/newrelic-infra.pid 1.0.2 ignore_reclaimable When true, formulation of the host virtual memory considers SReclaimable as available memory; otherwise SReclaimable will be considered part of the used memory. YML option name Environment variable Type Default Version ignore_reclaimable NRIA_IGNORE_RECLAIMABLE boolean false 1.2.6 Logging variables log_file Defines the file path for the logs. The default installation creates a log directory and it sets this filepath value in the log_file configuration option for you. This log directory is different for each OS, as shown below. Change this configuration option to customize the file path for the logs. YML option name Environment variable Type Default Version log_file NRIA_LOG_FILE string See below * Default paths: Linux: If not defined, it logs only in the standard output. Windows, agent version 1.0.752 or lower: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy Windows, agent version 1.0.775 to 1.0.944: C:\\%APPDATA%\\Roaming\\New Relic\\newrelic-infra\\newrelic-infra.log Copy Windows, agent version 1.0.944 or higher: C:\\%ProgramData%\\New Relic\\newrelic-infra\\newrelic-infra.log Copy If the directory can't be created: C:\\Program Files\\New Relic\\newrelic-infra\\newrelic-infra.log Copy log_format Defines the log output format. Available values: text: Plain text output, one line per log entry. json: JSON-formatted output, one line per log entry. YML option name Environment variable Type Default Version log_format NRIA_LOG_FORMAT string text 1.4.9 log_to_stdout By default all logs are displayed in both standard output and a log file. To disable logs in the standard output, set this configuration option to false. YML option name Environment variable Type Default Version log_to_stdout NRIA_LOG_TO_STDOUT boolean true 1.0.703 verbose When verbose is set to 0, verbose logging is off, but the agent still creates logs. Set this to 1 to create verbose logs to use in troubleshooting the agent; set verbose to 2 ito use the smart verbose mode logging. Smart verbose mode logs the last smart_verbose_mode_entry_limit debug messages when an error is logged. Setting verbose to 3 enables forwarding the agent logs to New Relic Logs. Important Verbose logging can generate a lot of data very quickly. Run the agent in verbose mode only for as long as necessary to reproduce your issue, then set verbose: 0 and restart your agent to disable verbose logging. Alternatively, you can set verbose: 2, which will enable smart verbose mode. YML option name Environment variable Type Default Version verbose NRIA_VERBOSE Integer (0, 1, 2 or 3) 0 1.9.0 when using smart mode (2) 1.11.4 when forwarding the New Relic Logs (3) smart_verbose_mode_entry_limit smart_verbose_mode_entry_limit refers to the number of previous debug messages that will be logged when an error is logged. For example, if the limit is set to 5, debug logs will be cached in memory until an error is logged, at which point the previous 5 debug messages will also be logged Important This configuration option is only used when verbose is set to 2 (Smart Verbose Mode enabled). YML option name Environment variable Type Default Version smart_verbose_mode_entry_limit NRIA_SMART_VERBOSE_MODE_ENTRY_LIMIT Integer 1000 1.9.0 Metrics variables custom_attributes Custom attributes are key-value pairs (similar to tags in other tools) used to annotate the data from the Infrastructure agent. You can use this metadata to build filter sets, group your results, and annotate your data. For example, you might indicate a machine's environment (staging or production), the service a machine hosts (login service, for example), or the team responsible for that machine. Tip The agent collects many details about your environment as part of its default attributes, including Amazon Elastic Compute Cloud (Amazon EC2) tags. YML option name Environment variable Type custom_attributes NRIA_CUSTOM_ATTRIBUTES map [ string]interface { } Use a list of custom attributes to annotate the data from this agent instance. Separate keys and values with colons :, as in KEY: VALUE, and separate each key-value pair with a line break. Keys can be any valid YAML except slashes /. Values can be any YAML string, including spaces. Example as a YAML attribute: custom_attributes: environment: production service: login service team: alpha-team Copy Example as an environment variable: NRIA_CUSTOM_ATTRIBUTES='{\"customAttribute_1\":\"SOME_ATTRIBUTE\",\"customAttribute_2\": \"SOME_ATTRIBUTE_2\"}' Copy NRQL example filtering by custom attribute: FROM SystemSample SELECT * WHERE environment = 'production' Copy enable_process_metrics Important Requires infrastructure agent version 1.12.0 or higher. Accounts created before July 20, 2020 and/or infrastructure agents installed using the new Guided Install have this variable enabled by default. Enables the sending of process metrics to New Relic. By default, the infrastructure agent doesn't send data about the operating system's processes. The agent still collects such data, unless metrics_process_sample_rate is set to -1. To report metric data about all the operating system's processes, set enable_process_metrics to true. To disable, set to false. Sending all process data could increase the volume of data sent to New Relic. To fine-tune which processes you want to monitor, configure include_matching_metrics. By default, processes using low memory are excluded from being sampled. For more information, see disable-zero-mem-process-filter. YML option name Environment variable Type Default Version enable_process_metrics NRIA_ENABLE_PROCESS_METRICS boolean false include_matching_metrics Important Currently, this setting only applies to an operating system's processes metrics. You can control how much data is sent to New Relic by configuring include_matching_metrics, which allows you to restrict the transmission of metric data based on the values of metric attributes. You include metric data by defining literal or partial values for any of the attributes of the metric. For example, you can choose to send the host.process.cpuPercent of all processes whose process.name match the ^java regular expression. In this example, we include process metrics using executable files and names: include_matching_metrics: # You can combine attributes from different metrics process.name: - regex “^java” # Include all processes starting with \"java\" process.executable: - “/usr/bin/python2” # Include the Python 2.x executable - regex “\\\\System32\\\\svchost” # Include all svchost executables Copy If you need to include command line arguments in any of the values, set strip_command_line to false (the infrastructure agents removes CLI arguments by default to prevent secrets from leaking). To configure include_matching_metrics as an environment variable for the Kubernetes integration, add it in the manifest inside the env: object: env: - name: NRIA_INCLUDE_MATCHING_METRICS value: | process.name: - regex \"^java\" process.executable: - \"/usr/bin/python2\" - regex \"\\\\System32\\\\svchost\" Copy Default YML option name Environment variable Type Default Version include_matching_metrics NRIA_INCLUDE_MATCHING_METRICS metric.attribute: - regex \"pattern\" - \"string\" - \"string-with-wildcard * \" network_interface_filters You can use the network interface filters configuration to hide unused or uninteresting network interfaces from the infrastructure agent. This helps reduce resource usage, work, and noise in your data. Important Environment variables are not supported for this configuration setting. The configuration uses a simple pattern-matching mechanism that can look for interfaces that start with a specific sequence of letters or numbers following either pattern: {name}[other characters], where you specify the name using the prefix option [number]{name}[other characters], where you specify the name using the index-1 option New Relic infrastructure implements a curated default list of filters, available for both Linux and Windows, that you can modify. YML option name Environment variable Type Default Version network_interface_filters not supported map [ string] [ ]string 1.0.220 Linux Default network interface filters for Linux: Network interfaces that start with dummy, lo, vmnet, sit, tun, tap, or veth Network interfaces that contain tun or tap The following example (added to your configuration file) overrides the default filters. This will ignore network interfaces that start with dummy or lo , or contain tun preceded by a sequence of numbers, and followed by other characters: network_interface_filters: prefix: - dummy - lo index-1: - tun Copy Windows Default network interface filters for Windows: Network interfaces that start with Loop, isatap, or Local The following example (added to your configuration file) overrides the default filters. This will ignore network interfaces that start with Loop: network_interface_filters: prefix: - Loop Copy disable_zero_mem_process_filter The ZeroRSSFilter excludes processes that are not using memory from being sampled. Disable the filter so that the agent includes these processes in the ProcessSample. YML option name Environment variable Type Default Version disable_zero_mem_process_filter NRIA_DISABLE_ZERO_MEM_PROCESS_FILTER boolean false 1.0.832 Plugins variables Tip You can quickly disable all the variables by setting DisableAllPlugins to true, and turn on only those options you need. disable_all_plugins To disable all the plugins, set this option to true. YML option name Environment variable Type Default Version disable_all_plugins NRIA_DISABLE_ALL_PLUGINS boolean false 1.2.1 cloud_security_group_refresh_sec Sampling period for the CloudSecurityGroups plugin, in seconds. The minimum value is 30. To disable it, set it to -1. Important This plugin is activated only if the agent is running in an AWS instance. YML option name Environment variable Type Default Version cloud_security_group_refresh_sec NRIA_CLOUD_SECURITY_GROUP_REFRESH_SEC int64 60 1.0.692 daemontools_interval_sec Sampling period for the Daemontoolsplugin, in seconds. The minimum value is a 10. To disable it, set it to -1. YML option name Environment variable Type Default Version daemontools_interval_sec NRIA_DAEMONTOOLS_INTERVAL_SEC int64 15 1.0.316 dpkg_interval_sec Sampling period for the Dpkg plugin, in seconds. The minimum value is 30. To disable it, set it to -1. If the parameter is not explicitly set in the config file, it can be disabled by setting DisableAllPlugins to true. Important This is only activated in root or privileged running modes and on Debian-based distributions. YML option name Environment variable Type Default Version dpkg_interval_sec NRIA_DPKG_INTERVAL_SEC int64 30 1.0.316 facter_interval_sec Sampling period for the Facter plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version facter_interval_sec NRIA_FACTER_INTERVAL_SEC int64 30 1.0.316 kernel_modules_refresh_sec Sampling period for the CloudSecurityGroups plugin, in seconds. The minimum value is 10. To disable it, set it to -1. Important kernel_modules_refresh_sec is only activated in root or privileged running modes. YML option name Environment variable Type Default Version kernel_modules_refresh_sec NRIA_KERNEL_MODULES_REFRESH_SEC int64 10 1.0.755 network_interface_interval_sec Sampling period for the NetworkInterface plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version network_interface_interval_sec NRIA_NETWORK_INTERFACE_INTERVAL_SEC int64 60 1.0.329 rpm_interval_sec Sampling period for the Rpm plugin, in seconds. The minimum value is 30. To disable it, set it to -1. Important rpm_interval_sec is only activated when the agent runs in root or privileged modes for RedHat, RedHat AWS, or SUSE distributions. YML option name Environment variable Type Default Version rpm_interval_sec NRIA_RPM_INTERVAL_SEC int64 30 1.0.316 selinux_interval_sec Sampling period for the SELinux plugin, in seconds. The minimum value is 30. To disable it, set it to -1. This option is ignored if SelinuxEnableSemodule is set to false. For more information, see our troubleshooting doc on disabling the SELinux module. Important SELinux is only activated when the agent runs in root mode. YML option name Environment variable Type Default Version selinux_interval_sec NRIA_SELINUX_INTERVAL_SEC int64 30 1.0.316 sshd_config_refresh_sec Sampling period for the Sshd plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version sshd_config_refresh_sec NRIA_SSHD_CONFIG_REFRESH_SEC int64 15 1.0.755 supervisor_interval_sec Sampling period for the Supervisor plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version supervisor_interval_sec NRIA_SUPERVISOR_INTERVAL_SEC int64 15 1.0.316 sysctl_interval_sec Sampling period for the Sysctl plugin, in seconds. The minimum value is 30. To disable it, set it to -1. YML option name Environment variable Type Default Version sysctl_interval_sec NRIA_SYSCTL_INTERVAL_SEC int64 60 1.0.316 systemd_interval_sec Sampling period for the Systemd plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version systemd_interval_sec NRIA_SYSTEMD_INTERVAL_SEC int64 30 1.0.316 sysvinit_interval_sec Sampling period for the sysv plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version sysvinit_interval_sec NRIA_SYSVINIT_INTERVAL_SEC int64 30 1.0.316 upstart_interval_sec Sampling period for the Upstart plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version upstart_interval_sec NRIA_UPSTART_INTERVAL_SEC int64 30 1.0.316 users_refresh_sec Sampling period for the Users plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version users_refresh_sec NRIA_USERS_REFRESH_SEC int64 30 1.0.755 supervisor_rpc_sock Location of the supervisor socket. YML option name Environment variable Type Default Version supervisor_rpc_sock NRIA_SUPERVISOR_RPC_SOCK string /var/run/supervisor.sock 1.0.2 facter_home_dir Sets the HOME environment variable for Puppet's Facter. If not defined, it defaults to the current user's home directory. YML option name Environment variable Type Default Version facter_home_dir NRIA_FACTER_HOME_DIR string 1.1.7 Proxy variables For infrastructure agent version 1.3.1 or higher, the precedence of the proxy configuration settings is: NRIA_PROXY proxy HTTP_PROXY HTTPS_PROXY proxy Your system may have firewall rules that require the agent to use a proxy to communicate with New Relic. If so, set the proxy URL in the form https://user:password@hostname:port. It can be HTTP or HTTPS. YML option name Environment variable Type Default Version proxy NRIA_PROXY string Empty 1.0.308 Example: https://proxy_user.access_10@proxy_01:1080 Copy ignore_system_proxy When set to true, the HTTPS_PROXY and HTTP_PROXY environment variables are ignored. This is useful when the agent needs to connect directly to the metrics collector and skip the existing system proxy. YML option name Environment variable Type Default Version ignore_system_proxy NRIA_IGNORE_SYSTEM_PROXY boolean false 1.0.1002 ca_bundle_dir If the HTTPS_PROXY option references to a proxy with self-signed certificates, this option specifies the path to the the directory where the proxy certificate is available. The certificates in the directory must end with the .pem extension. YML option name Environment variable Type Default Version ca_bundle_dir NRIA_CA_BUNDLE_DIR string 1.0.296 ca_bundle_file If the HTTPS_PROXY option references to a proxy with self-signed certificates, this option specifies the path to the certificate file. YML option name Environment variable Type Default Version ca_bundle_file NRIA_CA_BUNDLE_FILE string 1.0.296 proxy_validate_certificates If set to true, when the proxy is configured to use an HTTPS connection, it will only work: If the HTTPS proxy has certificates from a valid Certificate Authority. If the ca_bundle_file or ca_bundle_dir configuration properties contain the HTTPS proxy certificates. YML option name Environment variable Type Default Version proxy_validate_certificates NRIA_PROXY_VALIDATE_CERTIFICATES boolean false 1.3.0 proxy_config_plugin Sends the following proxy configuration information as inventory: HTTPS_PROXY HTTP_PROXY proxy ca_bundle_dir ca_bundle_file ignore_system_proxy proxy_validate_certificates YML option name Environment variable Type Default Version proxy_config_plugin NRIA_PROXY_CONFIG_PLUGIN boolean true 1.3.0 If you are having problems with proxy configuration, see Proxy troubleshooting. Samples variables metrics_network_sample_rate Sample rate of network samples, in seconds. Minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_network_sample_rate NRIA_METRICS_NETWORK_SAMPLE_RATE integer 10 1.0.308 metrics_process_sample_rate Sample rate of process samples, in seconds. Minimum value is 20. To disable process samples entirely, set metrics_process_sample_rate to -1. YML option name Environment variable Type Default Version metrics_process_sample_rate NRIA_METRICS_PROCESS_SAMPLE_RATE integer 20 1.0.308 metrics_storage_sample_rate Sample rate of storage samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_storage_sample_rate NRIA_METRICS_STORAGE_SAMPLE_RATE integer 5 1.0.308 metrics_system_sample_rate Sample rate of system samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_system_sample_rate NRIA_METRICS_SYSTEM_SAMPLE_RATE integer 5 1.0.308 metrics_nfs_sample_rate Sample rate of NFS storage samples, in seconds. Minimum value is 5. To disable it, set it to -1. YML option name Environment variable Type Default Version metrics_nfs_sample_rate NRIA_METRICS_NFS_SAMPLE_RATE integer 20 1.5.40 detailed_nfs Detailed NFS metrics. When enabled, the agent will provide a complete list of NFS metrics. YML option name Environment variable Type Default Version detailed_nfs NRIA_DETAILED_NFS boolean false 1.5.40 Security variables selinux_enable_semodule Get versions of policy modules installed using SEModule. If disabled, the SELinux plugin will only retrieve the status using SEStatus. YML option name Environment variable Type Default Version selinux_enable_semodule NRIA_SELINUX_ENABLE_SEMODULE boolean true 1.0.864 strip_command_line When true, the agent removes the command arguments from the commandLine attribute of the ProcessSample. Tip This is a security measure to prevent leaking sensitive information. YML option name Environment variable Type Default Version strip_command_line NRIA_STRIP_COMMAND_LINE boolean true 1.0.149 Windows variables windows_services_refresh_sec Sampling period for the Windows services plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version windows_services_refresh_sec NRIA_WINDOWS_SERVICES_REFRESH_SEC int64 30 1.0.755 windows_updates_refresh_sec Sampling period for the Windows updates plugin, in seconds. The minimum value is 10. To disable it, set it to -1. YML option name Environment variable Type Default Version windows_updates_refresh_sec NRIA_WINDOWS_UPDATES_REFRESH_SEC int64 60 1.0.755 app_data_dir Defines the path to store data in a different path than the program files directory: %AppDir%/data: Used for storing the delta data %AppDir%/user_data: External directory for user-generated JSON files %AppDir%/newrelic-infra.log: If log file config option is not defined, then we use this directory path as default. YML option name Environment variable Type Default Version app_data_dir NRIA_APP_DATA_DIR string Windows: env(ProgramData)\\New Relic\\newrelic-infra Linux: Not applicable 1.0.755 enable_win_update_plugin Enables the Windows updates plugin, which retrieves the lists of hotfixes that are installed on the host. YML option name Environment variable Type Default Version enable_win_update_plugin NRIA_ENABLE_WIN_UPDATE_PLUGIN boolean false 1.0.274 legacy_storage_sampler If true, the agent will be forced to use Windows WMI (the agent's legacy method to grab metrics for Windows; for example, StorageSampler) and will disable the new method (which uses the PDH library). YML option name Environment variable Type Default Version legacy_storage_sampler NRIA_LEGACY_STORAGE_SAMPLER boolean 1.0.1051 win_process_priority_class This configuration option allows to increase the newrelic-infra.exe process priority to any of the following values: Normal Idle High RealTime BelowNormal AboveNormal YML option name Environment variable Type Default Version win_process_priority_class NRIA_WIN_PROCESS_PRIORITY_CLASS string 1.0.989 Example: Normal Copy win_removable_drives Enables the Windows agent to report drives A: and B: when they are mapped as removable drives. YML option name Environment variable Type Default Version win_removable_drives NRIA_WIN_REMOVABLE_DRIVES boolean true 1.3.1 What's next? You can also: Further understand the configuration of the agent. Create a configuration file using our template. See how you can manage the agent.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 182.40855,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>agent</em> configuration settings ",
        "sections": "<em>Infrastructure</em> <em>agent</em> configuration settings",
        "tags": "<em>Install</em> the <em>infrastructure</em> <em>agent</em>",
        "body": " management, you can configure on-host integrations with New Relic <em>Infrastructure</em>&#x27;s <em>agent</em> to use sensitive data (such as passwords) without having to write them as plain text into the integration&#x27;s configuration file. For more information, see Secrets management. custom_plugin_<em>installation</em>_dir Specifies"
      },
      "id": "603ea542196a67a38aa83dd8"
    }
  ],
  "/docs/infrastructure/manage-your-data/data-instrumentation/apm-data-infrastructure-monitoring": [
    {
      "sections": [
        "Default infrastructure monitoring data",
        "Important",
        "Infrastructure events",
        "Supported Linux storage systems",
        "Supported Windows storage systems",
        "Query infrastructure data",
        "Manage data",
        "Add custom attributes",
        "Common Amazon EC2 attributes",
        "awsRegion",
        "awsAvailabilityZone",
        "ec2InstanceType",
        "ec2InstanceId",
        "ec2AmiId",
        "ec2SubnetId",
        "ec2VpcId",
        "Other Amazon EC2 attributes"
      ],
      "title": "Default infrastructure monitoring data ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "01647189a48892103f4dc6abe07ce29d5fc13f0d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/default-infrastructure-monitoring-data/",
      "published_at": "2021-05-05T03:38:21Z",
      "updated_at": "2021-03-30T08:36:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important As of April 12, 2021, we are upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. New Relic's infrastructure monitoring agent collects and displays data using six primary events, each with associated attributes that represent assorted metrics and metadata. Understanding infrastructure data can help you: Better understand our infrastructure monitoring UI. Manage your infrastructure data. Create better host filter sets. Run better queries of your data. Set up better monitoring solutions using custom attributes. Infrastructure events The following are events reported by default by the infrastructure agent and some infrastructure integrations. The attributes attached to these events are the metadata and metrics used to create our infrastructure UI visualizations. You can also create custom queries and charts of this data. If you're using integrations, see that integration's doc for more on reported data. For common AWS attributes, see AWS data. Select an event name in the following table to see its attributes. Event Description SystemSample SystemSample contains data describing the current overall state of the entire server, including CPU, memory, disk, and network. We take a snapshot of this data every 5 seconds and package it into a SystemSample event, which is then sent to New Relic. This data appears in the Hosts UI page. ProcessSample ProcessSample gathers detailed resource usage information from programs running on a single system. We take a snapshot of this data every 20 seconds for every active process and package it into a ProcessSample event, which is then sent to New Relic. This data appears on the Processes UI page. Important Process metrics are not sent to New Relic by default for accounts created after July 20, 2020. Enable process metrics to get this data into the Infrastructure monitoring UI. StorageSample StorageSample represents a single storage device associated with a server. Each sample gathers descriptive information about the device, the type of file system it uses, and its current usage and capacity. We take a snapshot of this data every 20 seconds for each mounted file system and package it into a StorageSample event, which is then sent to New Relic. This data appears on the Storage UI page. Important If your server uses disks with file systems other than the supported file systems in the following table, StorageSample events will not be generated for those disks. Supported Linux storage systems Supported Linux storage file systems: xfs vxfs btrfs ext ext2 ext3 ext4 hfs Supported Windows storage systems Supported Windows storage file systems: NTFS ReFS (version 1.0.976 and higher) NetworkSample NetworkSample captures the descriptive and state information for each network device associated with a server. It includes the device's interface and address information, as well as current usage data. We take a snapshot of this data every 10 seconds for each attached network interface and package it into a NetworkSample event, which is then sent to New Relic. This data appears on the Network UI page. ContainerSample ContainerSample collects the descriptive and state information for each Docker container. It includes the container's ID, name, image, image name, as well metrics about CPU, memory and networking. We take a snapshot of this data every 15 seconds for each container and package it into a ContainerSample event, which is then sent to New Relic. This data appears on the Containers UI page. For more information, see Docker monitoring. InfrastructureEvent InfrastructureEvent describes changes (deltas) that occur in a system's live state. When an inventory or system state is added, removed, or changed, New Relic will produce an InfrastructureEvent that logs that activity. This data appears on the Events UI page. To learn about infrastructure integration data, see the documentation for a specific integration. If an AWS integration is enabled, your infrastructure events may also have AWS attributes attached. Query infrastructure data You can query your infrastructure data to troubleshoot a problem or create a chart, or to understand what data is available. For example, to see what data is attached to ProcessSample, you would run this NRQL query: SELECT * FROM ProcessSample Copy You can also query infrastructure using dimensional metrics. Manage data For tips on managing data ingest and reporting, see Manage infrastructure data. Add custom attributes You can create custom attributes in the infrastructure agent's YAML file. Use this metadata to: Create infrastructure filter sets Populate the Group by menu Annotate your infrastructure data Common Amazon EC2 attributes If you connect your Amazon Elastic Compute Cloud (EC2) account to our infrastructure monitoring, we report data from your Amazon EC2 instances. Amazon EC2-related attributes are common attributes that can be used in any event. These attributes are drawn from the EC2 API. No CloudWatch information is collected. These attributes and their values are subject to change if Amazon changes the data they expose. awsRegion The region (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. awsAvailabilityZone The availability zone (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceType The Amazon Web Services instance type, displayed in AWS-specific codes. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceId The Amazon Web Services instance's unique identifying number for the server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2AmiId The Amazon Machine Image (AMI) identification number of the image used by Amazon Web Services to bootstrap the Amazon EC2 instance. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2SubnetId The networking sub-net identifier on which the server is connected. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2VpcId The Virtual Private Cloud identifier (if any) for this server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. Other Amazon EC2 attributes If Amazon Web Services changes the metadata they make available to New Relic, other attributes and values collected also may be available.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.75812,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Default <em>infrastructure</em> monitoring <em>data</em> ",
        "sections": "Default <em>infrastructure</em> monitoring <em>data</em>",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": ". <em>Manage</em> <em>your</em> <em>infrastructure</em> <em>data</em>. Create better host filter sets. Run better queries of <em>your</em> <em>data</em>. Set up better monitoring solutions using custom attributes. <em>Infrastructure</em> events The following are events reported by default by the <em>infrastructure</em> agent and some <em>infrastructure</em> integrations"
      },
      "id": "6043edcd28ccbcfa8a2c6086"
    },
    {
      "sections": [
        "On-host integrations metrics",
        "New Relic Integrations Metrics"
      ],
      "title": "On-host integrations metrics",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "fe96c0c4950380504b1a33c3ad861bcb17507cba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics/",
      "published_at": "2021-05-05T03:38:23Z",
      "updated_at": "2021-03-16T15:57:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Integrations Metrics The following table contains the metrics we collect for our infrastructure integrations. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent cpuPercent Agent host.cpuStealPercent cpuStealPercent Agent host.cpuSystemPercent cpuSystemPercent Agent host.cpuUserPercent cpuUserPercent Agent host.disk.avgQueueLen avgQueueLen Agent host.disk.avgReadQueueLen avgReadQueueLen Agent host.disk.avgWriteQueueLen avgWriteQueueLen Agent host.disk.currentQueueLen currentQueueLen Agent host.disk.freeBytes diskFreeBytes Agent host.disk.freePercent diskFreePercent Agent host.disk.inodesFree inodesFree Agent host.disk.inodesTotal inodesTotal Agent host.disk.inodesUsed inodesUsed Agent host.disk.inodesUsedPercent inodesUsedPercent Agent host.disk.readBytesPerSecond readBytesPerSecond Agent host.disk.readIoPerSecond readIoPerSecond Agent host.disk.readUtilizationPercent readUtilizationPercent Agent host.disk.readWriteBytesPerSecond readWriteBytesPerSecond Agent host.disk.totalBytes diskTotalBytes Agent host.disk.totalUtilizationPercent totalUtilizationPercent Agent host.disk.usedBytes diskUsedBytes Agent host.disk.usedPercent diskUsedPercent Agent host.disk.writeBytesPerSecond writeBytesPerSecond Agent host.disk.writeIoPerSecond writeIoPerSecond Agent host.disk.writeUtilizationPercent writeUtilizationPercent Agent host.diskFreeBytes diskFreeBytes Agent host.diskFreePercent diskFreePercent Agent host.diskReadsPerSecond diskReadsPerSecond Agent host.diskReadUtilizationPercent diskReadUtilizationPercent Agent host.diskTotalBytes diskTotalBytes Agent host.diskUsedBytes diskUsedBytes Agent host.diskUsedPercent diskUsedPercent Agent host.diskUtilizationPercent diskUtilizationPercent Agent host.diskWritesPerSecond diskWritesPerSecond Agent host.diskWriteUtilizationPercent diskWriteUtilizationPercent Agent host.loadAverageFifteenMinute loadAverageFifteenMinute Agent host.loadAverageFiveMinute loadAverageFiveMinute Agent host.loadAverageOneMinute loadAverageOneMinute Agent host.memoryFreeBytes memoryFreeBytes Agent host.memoryFreePercent memoryFreePercent Agent host.memoryTotalBytes memoryTotalBytes Agent host.memoryUsedBytes memoryUsedBytes Agent host.memoryUsedPercent memoryUsedPercent Agent host.net.receiveBytesPerSecond receiveBytesPerSecond Agent host.net.receiveDroppedPerSecond receiveDroppedPerSecond Agent host.net.receiveErrorsPerSecond receiveErrorsPerSecond Agent host.net.receivePacketsPerSecond receivePacketsPerSecond Agent host.net.transmitBytesPerSecond transmitBytesPerSecond Agent host.net.transmitDroppedPerSecond transmitDroppedPerSecond Agent host.net.transmitErrorsPerSecond transmitErrorsPerSecond Agent host.net.transmitPacketsPerSecond transmitPacketsPerSecond Agent host.process.cpuPercent cpuPercent Agent host.process.cpuSystemPercent cpuSystemPercent Agent host.process.cpuUserPercent cpuUserPercent Agent host.process.fileDescriptorCount fileDescriptorCount Agent host.process.ioReadBytesPerSecond ioReadBytesPerSecond Agent host.process.ioReadCountPerSecond ioReadCountPerSecond Agent host.process.ioTotalReadBytes ioTotalReadBytes Agent host.process.ioTotalReadCount ioTotalReadCount Agent host.process.ioTotalWriteBytes ioTotalWriteBytes Agent host.process.ioTotalWriteCount ioTotalWriteCount Agent host.process.ioWriteBytesPerSecond ioWriteBytesPerSecond Agent host.process.ioWriteCountPerSecond ioWriteCountPerSecond Agent host.process.memoryResidentSizeBytes memoryResidentSizeBytes Agent host.process.memoryVirtualSizeBytes memoryVirtualSizeBytes Agent host.process.threadCount threadCount Agent host.swapFreeBytes swapFreeBytes Agent host.swapTotalBytes swapTotalBytes Agent host.swapUsedBytes swapUsedBytes Apache apache.server.busyWorkers server.busyWorkers Apache apache.server.idleWorkers server.idleWorkers Apache apache.server.net.bytesPerSecond net.bytesPerSecond Apache apache.server.net.requestsPerSecond net.requestsPerSecond Apache apache.server.scoreboard.closingWorkers server.scoreboard.closingWorkers Apache apache.server.scoreboard.dnsLookupWorkers server.scoreboard.dnsLookupWorkers Apache apache.server.scoreboard.finishingWorkers server.scoreboard.finishingWorkers Apache apache.server.scoreboard.idleCleanupWorkers server.scoreboard.idleCleanupWorkers Apache apache.server.scoreboard.keepAliveWorkers server.scoreboard.keepAliveWorkers Apache apache.server.scoreboard.loggingWorkers server.scoreboard.loggingWorkers Apache apache.server.scoreboard.readingWorkers server.scoreboard.readingWorkers Apache apache.server.scoreboard.startingWorkers server.scoreboard.startingWorkers Apache apache.server.scoreboard.totalWorkers server.scoreboard.totalWorkers Apache apache.server.scoreboard.writingWorkers server.scoreboard.writingWorkers Cassandra cassandra.node.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.node.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.node.client.connectedNativeClients client.connectedNativeClients Cassandra cassandra.node.commitLogCompletedTasksPerSecond db.commitLogCompletedTasksPerSecond Cassandra cassandra.node.commitLogPendingTasks db.commitLogPendindTasks Cassandra cassandra.node.commitLogTotalSizeBytes db.commitLogTotalSizeBytes Cassandra cassandra.node.droppedBatchRemoveMessagesPerSecond db.droppedBatchRemoveMessagesPerSecond Cassandra cassandra.node.droppedBatchStoreMessagesPerSecond db.droppedBatchStoreMessagesPerSecond Cassandra cassandra.node.droppedCounterMutationMessagesPerSecond db.droppedCounterMutationMessagesPerSecond Cassandra cassandra.node.droppedHintMessagesPerSecond db.droppedHintMessagesPerSecond Cassandra cassandra.node.droppedMutationMessagesPerSecond db.droppedMutationMessagesPerSecond Cassandra cassandra.node.droppedPagedRangeMessagesPerSecond db.droppedPagedRangeMessagesPerSecond Cassandra cassandra.node.droppedRangeSliceMessagesPerSecond db.droppedRangeSliceMessagesPerSecond Cassandra cassandra.node.droppedReadMessagesPerSecond db.droppedReadMessagesPerSecond Cassandra cassandra.node.droppedReadRepairMessagesPerSecond db.droppedReadRepairMessagesPerSecond Cassandra cassandra.node.droppedRequestResponseMessagesPerSecond db.droppedRequestResponseMessagesPerSecond Cassandra cassandra.node.droppedTraceMessagesPerSecond db.droppedTraceMessagesPerSecond Cassandra cassandra.node.keyCacheCapacityBytes db.keyCacheCapacityBytes Cassandra cassandra.node.keyCacheHitRate db.keyCacheHitRate Cassandra cassandra.node.keyCacheHitsPerSecond db.keyCacheHitsPerSecond Cassandra cassandra.node.keyCacheRequestsPerSecond db.keyCacheRequestsPerSecond Cassandra cassandra.node.keyCacheSizeBytes db.keyCacheSizeBytes Cassandra cassandra.node.liveSsTableCount db.liveSSTableCount Cassandra cassandra.node.loadBytes db.loadBytes Cassandra cassandra.node.query.casReadRequestsPerSecond query.CASReadRequestsPerSecond Cassandra cassandra.node.query.casWriteRequestsPerSecond query.CASWriteRequestsPerSecond Cassandra cassandra.node.query.rangeSliceRequestsPerSecond query.rangeSliceRequestsPerSecond Cassandra cassandra.node.query.rangeSliceTimeoutsPerSecond query.rangeSliceTimeoutsPerSecond Cassandra cassandra.node.query.rangeSliceUnavailablesPerSecond query.rangeSliceUnavailablesPerSecond Cassandra cassandra.node.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.node.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.node.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.node.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.node.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.node.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.node.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.node.query.readTimeoutsPerSecond query.readTimeoutsPerSecond Cassandra cassandra.node.query.readUnavailablesPerSecond query.readUnavailablesPerSecond Cassandra cassandra.node.query.viewWriteRequestsPerSecond query.viewWriteRequestsPerSecond Cassandra cassandra.node.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.node.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.node.query.writeTimeoutsPerSecond query.writeTimeoutsPerSecond Cassandra cassandra.node.query.writeUnavailablesPerSecond query.writeUnavailablesPerSecond Cassandra cassandra.node.rowCacheCapacityBytes db.rowCacheCapacityBytes Cassandra cassandra.node.rowCacheHitRate db.rowCacheHitRate Cassandra cassandra.node.rowCacheHitsPerSecond db.rowCacheHitsPerSecond Cassandra cassandra.node.rowCacheRequestsPerSecond db.rowCacheRequestsPerSecond Cassandra cassandra.node.rowCacheSizeBytes db.rowCacheSizeBytes Cassandra cassandra.node.storage.exceptionCount storage.exceptionCount Cassandra cassandra.node.threadPool.antiEntropyStage.activeTasks db.threadpool.internalAntiEntropyStageActiveTasks Cassandra cassandra.node.threadPool.antiEntropyStage.completedTasks db.threadpool.internalAntiEntropyStageCompletedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.currentlyBlockedTasks db.threadpool.internalAntiEntropyStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.pendingTasks db.threadpool.internalAntiEntropyStagePendingTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.activeTasks db.threadpool.internalCacheCleanupExecutorActiveTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.completedTasks db.threadpool.internalCacheCleanupExecutorCompletedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.currentlyBlockedTasks db.threadpool.internalCacheCleanupExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.pendingTasks db.threadpool.internalCacheCleanupExecutorPendingTasks Cassandra cassandra.node.threadPool.compactionExecutor.activeTasks db.threadpool.internalCompactionExecutorActiveTasks Cassandra cassandra.node.threadPool.compactionExecutor.completedTasks db.threadpool.internalCompactionExecutorCompletedTasks Cassandra cassandra.node.threadPool.compactionExecutor.currentlyBlockedTasks db.threadpool.internalCompactionExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.compactionExecutor.pendingTasks db.threadpool.internalCompactionExecutorPendingTasks Cassandra cassandra.node.threadPool.counterMutationStage.activeTasks db.threadpool.requestCounterMutationStageActiveTasks Cassandra cassandra.node.threadPool.counterMutationStage.completedTasks db.threadpool.requestCounterMutationStageCompletedTasks Cassandra cassandra.node.threadPool.counterMutationStage.currentlyBlockedTasks db.threadpool.requestCounterMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.counterMutationStage.pendingTasks db.threadpool.requestCounterMutationStagePendingTasks Cassandra cassandra.node.threadPool.gossipStage.activeTasks db.threadpool.internalGossipStageActiveTasks Cassandra cassandra.node.threadPool.gossipStage.completedTasks db.threadpool.internalGossipStageCompletedTasks Cassandra cassandra.node.threadPool.gossipStage.currentlyBlockedTasks db.threadpool.internalGossipStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.gossipStage.pendingTasks db.threadpool.internalGossipStagePendingTasks Cassandra cassandra.node.threadPool.hintsDispatcher.activeTasks db.threadpool.internalHintsDispatcherActiveTasks Cassandra cassandra.node.threadPool.hintsDispatcher.completedTasks db.threadpool.internalHintsDispatcherCompletedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.currentlyBlockedTasks db.threadpool.internalHintsDispatcherCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.pendingTasks db.threadpool.internalHintsDispatcherPendingTasks Cassandra cassandra.node.threadPool.internalResponseStage.activeTasks db.threadpool.internalInternalResponseStageActiveTasks Cassandra cassandra.node.threadPool.internalResponseStage.completedTasks db.threadpool.internalInternalResponseStageCompletedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pCurrentlyBlockedTasks db.threadpool.internalInternalResponseStagePCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pendingTasks db.threadpool.internalInternalResponseStagePendingTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.activeTasks db.threadpool.internalMemtableFlushWriterActiveTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.completedTasks db.threadpool.internalMemtableFlushWriterCompletedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.currentlyBlockedTasks db.threadpool.internalMemtableFlushWriterCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.pendingTasks db.threadpool.internalMemtableFlushWriterPendingTasks Cassandra cassandra.node.threadPool.memtablePostFlush.activeTasks db.threadpool.internalMemtablePostFlushActiveTasks Cassandra cassandra.node.threadPool.memtablePostFlush.completedTasks db.threadpool.internalMemtablePostFlushCompletedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.currentlyBlockedTasks db.threadpool.internalMemtablePostFlushCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.pendingTasks db.threadpool.internalMemtablePostFlushPendingTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.activeTasks db.threadpool.internalMemtableReclaimMemoryActiveTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.completedTasks db.threadpool.internalMemtableReclaimMemoryCompletedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.currentlyBlockedTasks db.threadpool.internalMemtableReclaimMemoryCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.pendingTasks db.threadpool.internalMemtableReclaimMemoryPendingTasks Cassandra cassandra.node.threadPool.migrationStage.activeTasks db.threadpool.internalMigrationStageActiveTasks Cassandra cassandra.node.threadPool.migrationStage.completedTasks db.threadpool.internalMigrationStageCompletedTasks Cassandra cassandra.node.threadPool.migrationStage.currentlyBlockedTasks db.threadpool.internalMigrationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.migrationStage.pendingTasks db.threadpool.internalMigrationStagePendingTasks Cassandra cassandra.node.threadPool.miscStage.activeTasks db.threadpool.internalMiscStageActiveTasks Cassandra cassandra.node.threadPool.miscStage.completedTasks db.threadpool.internalMiscStageCompletedTasks Cassandra cassandra.node.threadPool.miscStage.currentlyBlockedTasks db.threadpool.internalMiscStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.miscStage.pendingTasks db.threadpool.internalMiscStagePendingTasks Cassandra cassandra.node.threadPool.mutationStage.activeTasks db.threadpool.requestMutationStageActiveTasks Cassandra cassandra.node.threadPool.mutationStage.completedTasks db.threadpool.requestMutationStageCompletedTasks Cassandra cassandra.node.threadPool.mutationStage.currentlyBlockedTasks db.threadpool.requestMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.mutationStage.pendingTasks db.threadpool.requestMutationStagePendingTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.activeTasks db.threadpool.internalPendingRangeCalculatorActiveTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.completedTasks db.threadpool.internalPendingRangeCalculatorCompletedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.currentlyBlockedTasks db.threadpool.internalPendingRangeCalculatorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.pendingTasks db.threadpool.internalPendingRangeCalculatorPendingTasks Cassandra cassandra.node.threadPool.readRepairStage.activeTasks db.threadpool.requestReadRepairStageActiveTasks Cassandra cassandra.node.threadPool.readRepairStage.completedTasks db.threadpool.requestReadRepairStageCompletedTasks Cassandra cassandra.node.threadPool.readRepairStage.currentlyBlockedTasks db.threadpool.requestReadRepairStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readRepairStage.pendingTasks db.threadpool.requestReadRepairStagePendingTasks Cassandra cassandra.node.threadPool.readStage.activeTasks db.threadpool.requestReadStageActiveTasks Cassandra cassandra.node.threadPool.readStage.completedTasks db.threadpool.requestReadStageCompletedTasks Cassandra cassandra.node.threadPool.readStage.currentlyBlockedTasks db.threadpool.requestReadStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readStage.pendingTasks db.threadpool.requestReadStagePendingTasks Cassandra cassandra.node.threadPool.requestResponseStage.activeTasks db.threadpool.requestRequestResponseStageActiveTasks Cassandra cassandra.node.threadPool.requestResponseStage.completedTasks db.threadpool.requestRequestResponseStageCompletedTasks Cassandra cassandra.node.threadPool.requestResponseStage.currentlyBlockedTasks db.threadpool.requestRequestResponseStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.requestResponseStage.pendingTasks db.threadpool.requestRequestResponseStagePendingTasks Cassandra cassandra.node.threadPool.sampler.activeTasks db.threadpool.internalSamplerActiveTasks Cassandra cassandra.node.threadPool.sampler.completedTasks db.threadpool.internalSamplerCompletedTasks Cassandra cassandra.node.threadPool.sampler.currentlyBlockedTasks db.threadpool.internalSamplerCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.sampler.pendingTasks db.threadpool.internalSamplerPendingTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.activeTasks db.threadpool.internalSecondaryIndexManagementActiveTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.completedTasks db.threadpool.internalSecondaryIndexManagementCompletedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.currentlyBlockedTasks db.threadpool.internalSecondaryIndexManagementCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.pendingTasks db.threadpool.internalSecondaryIndexManagementPendingTasks Cassandra cassandra.node.threadPool.validationExecutor.activeTasks db.threadpool.internalValidationExecutorActiveTasks Cassandra cassandra.node.threadPool.validationExecutor.completedTasks db.threadpool.internalValidationExecutorCompletedTasks Cassandra cassandra.node.threadPool.validationExecutor.currentlyBlockedTasks db.threadpool.internalValidationExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.validationExecutor.pendingTasks db.threadpool.internalValidationExecutorPendingTasks Cassandra cassandra.node.threadPool.viewMutationStage.activeTasks db.threadpool.requestViewMutationStageActiveTasks Cassandra cassandra.node.threadPool.viewMutationStage.completedTasks db.threadpool.requestViewMutationStageCompletedTasks Cassandra cassandra.node.threadPool.viewMutationStage.currentlyBlockedTasks db.threadpool.requestViewMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.viewMutationStage.pendingTasks db.threadpool.requestViewMutationStagePendingTasks Cassandra cassandra.node.totalHintsInProgress db.totalHintsInProgress Cassandra cassandra.node.totalHintsPerSecond db.totalHintsPerSecond Cassandra cassandra.columnFamily.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.columnFamily.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.columnFamily.bloomFilterFalseRatio db.bloomFilterFalseRatio Cassandra cassandra.columnFamily.liveDiskSpaceUsedBytes db.liveDiskSpaceUsedBytes Cassandra cassandra.columnFamily.liveSsTableCount db.liveSSTableCount Cassandra cassandra.columnFamily.maxRowSize db.maxRowSize Cassandra cassandra.columnFamily.meanRowSize db.meanRowSize Cassandra cassandra.columnFamily.memtableLiveDataSize db.memtableLiveDataSize Cassandra cassandra.columnFamily.minRowSize db.minRowSize Cassandra cassandra.columnFamily.pendingCompactions db.pendingCompactions Cassandra cassandra.columnFamily.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.columnFamily.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.columnFamily.speculativeRetries db.speculativeRetries Cassandra cassandra.columnFamily.ssTablesPerRead50ThPercentileMilliseconds db.SSTablesPerRead50thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead75ThPercentileMilliseconds db.SSTablesPerRead75thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead95ThPercentileMilliseconds db.SSTablesPerRead95thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead98ThPercentileMilliseconds db.SSTablesPerRead98thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead999ThPercentileMilliseconds db.SSTablesPerRead999thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead99ThPercentileMilliseconds db.SSTablesPerRead99thPercentileMilliseconds Cassandra cassandra.columnFamily.tombstoneScannedHistogram50ThPercentile db.tombstoneScannedHistogram50thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram75ThPercentile db.tombstoneScannedHistogram75thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram95ThPercentile db.tombstoneScannedHistogram95thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram98ThPercentile db.tombstoneScannedHistogram98thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram999ThPercentile db.tombstoneScannedHistogram999thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram99ThPercentile db.tombstoneScannedHistogram99thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogramCount db.tombstoneScannedHistogramCount Consul consul.datacenter.catalog.criticalNodes catalog.criticalNodes Consul consul.datacenter.catalog.passingNodes catalog.passingNodes Consul consul.datacenter.catalog.registeredNodes catalog.registeredNodes Consul consul.datacenter.catalog.upNodes catalog.upNodes Consul consul.datacenter.catalog.warningNodes catalog.warningNodes Consul consul.datacenter.cluster.flaps cluster.flaps Consul consul.datacenter.cluster.suspects cluster.suspects Consul consul.datacenter.raft.commitTime raft.commitTimes Consul consul.datacenter.raft.commitTimeAvgInMilliseconds raft.commitTimeAvgInMilliseconds Consul consul.datacenter.raft.commitTimeMaxInMilliseconds raft.commitTimeMaxInMilliseconds Consul consul.datacenter.raft.completedLeaderElections raft.completedLeaderElections Consul consul.datacenter.raft.initiatedLeaderElections raft.initiatedLeaderElections Consul consul.datacenter.raft.lastContactAvgInMilliseconds raft.lastContactAvgInMilliseconds Consul consul.datacenter.raft.lastContactMaxInMilliseconds raft.lastContactMaxInMilliseconds Consul consul.datacenter.raft.lastContacts raft.lastContacts Consul consul.datacenter.raft.logDispatchAvgInMilliseconds raft.logDispatchAvgInMilliseconds Consul consul.datacenter.raft.logDispatches raft.logDispatches Consul consul.datacenter.raft.logDispatchMaxInMilliseconds raft.logDispatchMaxInMilliseconds Consul consul.datacenter.raft.txns raft.txns Consul consul.agent.aclCacheHitPerSecond agent.aclCacheHit Consul consul.agent.aclCacheMissPerSecond agent.aclCacheMiss Consul consul.agent.client.rpcFailed client.rpcFailed Consul consul.agent.client.rpcLoad client.rpcLoad Consul consul.agent.kvStores agent.kvStoress Consul consul.agent.kvStoresAvgInMilliseconds agent.kvStoresAvgInMilliseconds Consul consul.agent.kvStoresMaxInMilliseconds agent.kvStoresMaxInMilliseconds Consul consul.agent.net.agent.maxLatencyInMilliseconds net.agent.maxLatencyInMilliseconds Consul consul.agent.net.medianLatencyInMilliseconds net.agent.medianLatencyInMilliseconds Consul consul.agent.net.minLatencyInMilliseconds net.agent.minLatencyInMilliseconds Consul consul.agent.net.p25LatencyInMilliseconds net.agent.p25LatencyInMilliseconds Consul consul.agent.net.p75LatencyInMilliseconds net.agent.p75LatencyInMilliseconds Consul consul.agent.net.p90LatencyInMilliseconds net.agent.p90LatencyInMilliseconds Consul consul.agent.net.p95LatencyInMilliseconds net.agent.p95LatencyInMilliseconds Consul consul.agent.net.p99LatencyInMilliseconds net.agent.p99LatencyInMilliseconds Consul consul.agent.peers agent.peers Consul consul.agent.runtime.allocations runtime.allocations Consul consul.agent.runtime.allocationsInBytes runtime.allocationsInBytes Consul consul.agent.runtime.frees runtime.frees Consul consul.agent.runtime.gcCycles runtime.gcCycles Consul consul.agent.runtime.gcPauseInMilliseconds runtime.gcPauseInMilliseconds Consul consul.agent.runtime.goroutines runtime.goroutines Consul consul.agent.runtime.heapObjects runtime.heapObjects Consul consul.agent.runtime.virtualAddressSpaceInBytes runtime.virtualAddressSpaceInBytes Consul consul.agent.staleQueries agent.staleQueries Consul consul.agent.txnAvgInMilliseconds agent.txnAvgInMilliseconds Consul consul.agent.txnMaxInMilliseconds agent.txnMaxInMilliseconds Consul consul.agent.txns agent.txns Couchbase couchbase.bucket.activeItemsEnteringDiskQueuePerSecond bucket.activeItemsEnteringDiskQueuePerSecond Couchbase couchbase.bucket.activeItemsInMemory bucket.activeItemsInMemory Couchbase couchbase.bucket.activeResidentItemsRatio bucket.activeResidentItemsRatio Couchbase couchbase.bucket.averageDiskCommitTimeInMilliseconds bucket.averageDiskCommitTimeInMilliseconds Couchbase couchbase.bucket.averageDiskUpdateTimeInMilliseconds bucket.averageDiskUpdateTimeInMilliseconds Couchbase couchbase.bucket.cacheMisses bucket.cacheMisses Couchbase couchbase.bucket.cacheMissRatio bucket.cacheMissRatio Couchbase couchbase.bucket.casHits bucket.casHits Couchbase couchbase.bucket.casMisses bucket.casMisses Couchbase couchbase.bucket.couchDocsFragmentationPercent bucket.couchDocsFragmentationPercent Couchbase couchbase.bucket.currentConnections bucket.currentConnections Couchbase couchbase.bucket.dataUsedInBytes bucket.dataUsedInBytes Couchbase couchbase.bucket.decrementHitsPerSecond bucket.decrementHitsPerSecond Couchbase couchbase.bucket.decrementMissesPerSecond bucket.decrementMissesPerSecond Couchbase couchbase.bucket.deleteHitsPerSecond bucket.deleteHitsPerSecond Couchbase couchbase.bucket.deleteMissesPerSecond bucket.deleteMissesPerSecond Couchbase couchbase.bucket.diskCreateOperationsPerSecond bucket.diskCreateOperationsPerSecond Couchbase couchbase.bucket.diskFetchesPerSecond bucket.diskFetchesPerSecond Couchbase couchbase.bucket.diskReadsPerSecond bucket.diskReadsPerSecond Couchbase couchbase.bucket.diskUpdateOperationsPerSecond bucket.diskUpdateOperationsPerSecond Couchbase couchbase.bucket.diskUsedInBytes bucket.diskUsedInBytes Couchbase couchbase.bucket.diskWriteQueue bucket.diskWriteQueue Couchbase couchbase.bucket.drainedItemsInQueue bucket.drainedItemsInQueue Couchbase couchbase.bucket.drainedItemsOnDiskQueue bucket.drainedItemsOnDiskQueue Couchbase couchbase.bucket.drainedPendingItemsInQueue bucket.drainedPendingItemsInQueue Couchbase couchbase.bucket.ejectionsPerSecond bucket.ejectionsPerSecond Couchbase couchbase.bucket.evictionsPerSecond bucket.evictionsPerSecond Couchbase couchbase.bucket.getHitsPerSecond bucket.getHitsPerSecond Couchbase couchbase.bucket.getMissesPerSecond bucket.getMissesPerSecond Couchbase couchbase.bucket.hitRatio bucket.hitRatio Couchbase couchbase.bucket.incrementHitsPerSecond bucket.incrementHitsPerSecond Couchbase couchbase.bucket.incrementMissesPerSecond bucket.incrementMissesPerSecond Couchbase couchbase.bucket.itemCount bucket.itemCount Couchbase couchbase.bucket.itemsBeingWritten bucket.itemsBeingWritten Couchbase couchbase.bucket.itemsEjectedFromMemoryToDisk bucket.itemsEjectedFromMemoryToDisk Couchbase couchbase.bucket.itemsOnDiskQueue bucket.itemsOnDiskQueue Couchbase couchbase.bucket.itemsQueuedForStorage bucket.itemsQueuedForStorage Couchbase couchbase.bucket.maximumMemoryUsage bucket.maximumMemoryUsage Couchbase couchbase.bucket.memoryHighWaterMarkInBytes bucket.memoryHighWaterMarkInBytes Couchbase couchbase.bucket.memoryLowWaterMarkInBytes bucket.memoryLowWaterMarkInBytes Couchbase couchbase.bucket.memoryUsedInBytes bucket.memoryUsedInBytes Couchbase couchbase.bucket.metadataInRamInBytes bucket.metadataInRAMInBytes Couchbase couchbase.bucket.missesPerSecond bucket.missesPerSecond Couchbase couchbase.bucket.outOfMemoryErrorsPerSecond bucket.outOfMemoryErrorsPerSecond Couchbase couchbase.bucket.overheadInBytes bucket.overheadInBytes Couchbase couchbase.bucket.pendingItemsInDiskQueue bucket.pendingItemsInDiskQueue Couchbase couchbase.bucket.pendingResidentItemsRatio bucket.pendingResidentItemsRatio Couchbase couchbase.bucket.quotaUtilization bucket.quotaUtilization Couchbase couchbase.bucket.readOperationsPerSecond bucket.readOperationsPerSecond Couchbase couchbase.bucket.readRatePerSecond bucket.readRatePerSecond Couchbase couchbase.bucket.recoverableOutOfMemoryCount bucket.recoverableOutOfMemoryCount Couchbase couchbase.bucket.replicaIndex bucket.replicaIndex Couchbase couchbase.bucket.replicaNumber bucket.replicaNumber Couchbase couchbase.bucket.replicaResidentItemsRatio bucket.replicaResidentItemsRatio Couchbase couchbase.bucket.residentItemsRatio bucket.residentItemsRatio Couchbase couchbase.bucket.temporaryOutOfMemoryErrorsPerSecond bucket.temporaryOutOfMemoryErrorsPerSecond Couchbase couchbase.bucket.threadsNumber bucket.threadsNumber Couchbase couchbase.bucket.totalItems bucket.totalItems Couchbase couchbase.bucket.totalOperationsPerSecond bucket.totalOperationsPerSecond Couchbase couchbase.bucket.viewFragmentationPercent bucket.viewFragmentationPercent Couchbase couchbase.bucket.writeOperationsPerSecond bucket.writeOperationsPerSecond Couchbase couchbase.bucket.writeRatePerSecond bucket.writeRatePerSecond Couchbase couchbase.cluster.autoFailoverCount cluster.autoFailoverCount Couchbase couchbase.cluster.autoFailoverEnabled cluster.autoFailoverEnabled Couchbase couchbase.cluster.databaseFragmentationThreshold cluster.databaseFragmentationThreshold Couchbase couchbase.cluster.diskFreeInBytes cluster.diskFreeInBytes Couchbase couchbase.cluster.diskQuotaTotalInBytes cluster.diskQuotaTotalInBytes Couchbase couchbase.cluster.diskTotalInBytes cluster.diskTotalInBytes Couchbase couchbase.cluster.diskUsedByDataInBytes cluster.diskUsedByDataInBytes Couchbase couchbase.cluster.diskUsedInBytes cluster.diskUsedInBytes Couchbase couchbase.cluster.indexFragmentationThreshold cluster.indexFragmentationThreshold Couchbase couchbase.cluster.maximumBucketCount cluster.maximumBucketCount Couchbase couchbase.cluster.memoryQuotaTotalInBytes cluster.memoryQuotaTotalInBytes Couchbase couchbase.cluster.memoryQuotaTotalPerNodeInBytes cluster.memoryQuotaTotalPerNodeInBytes Couchbase couchbase.cluster.memoryQuotaUsedInBytes cluster.memoryQuotaUsedInBytes Couchbase couchbase.cluster.memoryQuotaUsedPerNodeInBytes cluster.memoryQuotaUsedPerNodeInBytes Couchbase couchbase.cluster.memoryTotalInBytes cluster.memoryTotalInBytes Couchbase couchbase.cluster.memoryUsedByDataInBytes cluster.memoryUsedByDataInBytes Couchbase couchbase.cluster.memoryUsedInBytes cluster.memoryUsedInBytes Couchbase couchbase.cluster.viewFragmentationThreshold cluster.viewFragmentationThreshold Couchbase couchbase.node.backgroundFetches node.backgroundFetches Couchbase couchbase.node.cmdGet node.cmdGet Couchbase couchbase.node.couchDocsActualDiskSizeInBytes node.couchDocsActualDiskSizeInBytes Couchbase couchbase.node.couchDocsDataSizeInBytes node.couchDocsDataSizeInBytes Couchbase couchbase.node.couchSpatialDataSizeInBytes node.couchSpatialDataSizeInBytes Couchbase couchbase.node.couchSpatialDiskSizeInBytes node.couchSpatialDiskSizeInBytes Couchbase couchbase.node.couchViewsActualDiskSizeInBytes node.couchViewsActualDiskSizeInBytes Couchbase couchbase.node.couchViewsDataSizeInBytes node.couchViewsDataSizeInBytes Couchbase couchbase.node.cpuUtilization node.cpuUtilization Couchbase couchbase.node.currentItems node.currentItems Couchbase couchbase.node.currentItemsTotal node.currentItemsTotal Couchbase couchbase.node.getHits node.getHits Couchbase couchbase.node.memoryFreeInBytes node.memoryFreeInBytes Couchbase couchbase.node.memoryTotalInBytes node.memoryTotalInBytes Couchbase couchbase.node.memoryUsedInBytes node.memoryUsedInBytes Couchbase couchbase.node.ops node.ops Couchbase couchbase.node.swapTotalInBytes node.swapTotalInBytes Couchbase couchbase.node.swapUsedInBytes node.swapUsedInBytes Couchbase couchbase.node.uptimeInMilliseconds node.uptimeInMilliseconds Couchbase couchbase.node.vbucketActiveNonResidentItems node.vbucketActiveNonResidentItems Couchbase couchbase.node.vbucketInMemoryItems node.vbucketInMemoryItems Couchbase couchbase.queryengine.activeRequests queryengine.activeRequests Couchbase couchbase.queryengine.averageRequestTimeInMilliseconds queryengine.averageRequestTimeInMilliseconds Couchbase couchbase.queryengine.completedLimit queryengine.completedLimit Couchbase couchbase.queryengine.completedRequests queryengine.completedRequests Couchbase couchbase.queryengine.completedThresholdInMilliseconds queryengine.completedThresholdInMilliseconds Couchbase couchbase.queryengine.cores queryengine.cores Couchbase couchbase.queryengine.garbageCollectionNumber queryengine.garbageCollectionNumber Couchbase couchbase.queryengine.garbageCollectionPaused queryengine.garbageCollectionPaused Couchbase couchbase.queryengine.garbageCollectionTimePausedInMilliseconds queryengine.garbageCollectionTimePausedInMilliseconds Couchbase couchbase.queryengine.medianRequestTimeInMilliseconds queryengine.medianRequestTimeInMilliseconds Couchbase couchbase.queryengine.preparedStatementUtilization queryengine.preparedStatementUtilization Couchbase couchbase.queryengine.requestsLast15MinutesPerSecond queryengine.requestsLast15MinutesPerSecond Couchbase couchbase.queryengine.requestsLast1MinutesPerSecond queryengine.requestsLast1MinutesPerSecond Couchbase couchbase.queryengine.requestsLast5MinutesPerSecond queryengine.requestsLast5MinutesPerSecond Couchbase couchbase.queryengine.requestTime80thPercentileInMilliseconds queryengine.requestTime80thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime95thPercentileInMilliseconds queryengine.requestTime95thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime99thPercentileInMilliseconds queryengine.requestTime99thPercentileInMilliseconds Couchbase couchbase.queryengine.systemCpuUtilization queryengine.systemCPUUtilization Couchbase couchbase.queryengine.systemMemoryInBytes queryengine.systemMemoryInBytes Couchbase couchbase.queryengine.totalMemoryInBytes queryengine.totalMemoryInBytes Couchbase couchbase.queryengine.totalThreads queryengine.totalThreads Couchbase couchbase.queryengine.uptimeInMilliseconds queryengine.uptimeInMilliseconds Couchbase couchbase.queryengine.usedMemoryInBytes queryengine.usedMemoryInBytes Couchbase couchbase.queryengine.userCpuUtilization queryengine.userCPUUtilization Docker docker.container.cpuKernelPercent cpuKernelPercent Docker docker.container.cpuLimitCores cpuLimitCores Docker docker.container.cpuPercent cpuPercent Docker docker.container.cpuThrottlePeriods cpuThrottlePeriods Docker docker.container.cpuThrottleTimeMs cpuThrottleTimeMs Docker docker.container.cpuUsedCores cpuUsedCores Docker docker.container.cpuUsedCoresPercent cpuUsedCoresPercent Docker docker.container.cpuUserPercent cpuUserPercent Docker docker.container.ioReadBytesPerSecond ioReadBytesPerSecond Docker docker.container.ioReadCountPerSecond ioReadCountPerSecond Docker docker.container.ioTotalBytes ioTotalBytes Docker docker.container.ioTotalReadBytes ioTotalReadBytes Docker docker.container.ioTotalReadCount ioTotalReadCount Docker docker.container.ioTotalWriteBytes ioTotalWriteBytes Docker docker.container.ioTotalWriteCount ioTotalWriteCount Docker docker.container.ioWriteBytesPerSecond ioWriteBytesPerSecond Docker docker.container.ioWriteCountPerSecond ioWriteCountPerSecond Docker docker.container.memoryCacheBytes memoryCacheBytes Docker docker.container.memoryResidentSizeBytes memoryResidentSizeBytes Docker docker.container.memorySizeLimitBytes memorySizeLimitBytes Docker docker.container.memoryUsageBytes memoryUsageBytes Docker docker.container.memoryUsageLimitPercent memoryUsageLimitPercent Docker docker.container.networkRxBytes networkRxBytes Docker docker.container.networkRxBytesPerSecond networkRxBytesPerSecond Docker docker.container.networkRxDropped networkRxDropped Docker docker.container.networkRxDroppedPerSecond networkRxDroppedPerSecond Docker docker.container.networkRxErrors networkRxErrors Docker docker.container.networkRxErrorsPerSecond networkRxErrorsPerSecond Docker docker.container.networkRxPackets networkRxPackets Docker docker.container.networkRxPacketsPerSecond networkRxPacketsPerSecond Docker docker.container.networkTxBytes networkTxBytes Docker docker.container.networkTxBytesPerSecond networkTxBytesPerSecond Docker docker.container.networkTxDropped networkTxDropped Docker docker.container.networkTxDroppedPerSecond networkTxDroppedPerSecond Docker docker.container.networkTxErrors networkTxErrors Docker docker.container.networkTxErrorsPerSecond networkTxErrorsPerSecond Docker docker.container.networkTxPackets networkTxPackets Docker docker.container.networkTxPacketsPerSecond networkTxPacketsPerSecond Docker docker.container.pids pids Docker docker.container.processCount processCount Docker docker.container.processCountLimit processCountLimit Docker docker.container.restartCount restartCount Docker docker.container.threadCount threadCount Docker docker.container.threadCountLimit threadCountLimit ElasticSearch elasticsearch.cluster.dataNodes cluster.dataNodes ElasticSearch elasticsearch.cluster.nodes cluster.nodes ElasticSearch elasticsearch.cluster.shards.active shards.active ElasticSearch elasticsearch.cluster.shards.initializing shards.initializing ElasticSearch elasticsearch.cluster.shards.primaryActive shards.primaryActive ElasticSearch elasticsearch.cluster.shards.relocating shards.relocating ElasticSearch elasticsearch.cluster.shards.unassigned shards.unassigned ElasticSearch elasticsearch.cluster.tempData temp-data ElasticSearch elasticsearch.index.docs index.docs ElasticSearch elasticsearch.index.docsDeleted index.docsDeleted ElasticSearch elasticsearch.index.primaryShards index.primaryShards ElasticSearch elasticsearch.index.primaryStoreSizeInBytes index.primaryStoreSizeInBytes ElasticSearch elasticsearch.index.replicaShards index.replicaShards ElasticSearch elasticsearch.index.rollup.docsCount primaries.docsnumber ElasticSearch elasticsearch.index.rollup.docsDeleted primaries.docsDeleted ElasticSearch elasticsearch.index.rollup.flushTotal primaries.flushesTotal ElasticSearch elasticsearch.index.rollup.flushTotalTimeInMilliseconds primaries.flushTotalTimeInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsExist primaries.get.documentsExist ElasticSearch elasticsearch.index.rollup.get.documentsExistInMilliseconds primaries.get.documentsExistInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsMissing primaries.get.documentsMissing ElasticSearch elasticsearch.index.rollup.get.documentsMissingInMilliseconds primaries.get.documentsMissingInMilliseconds ElasticSearch elasticsearch.index.rollup.get.requests primaries.get.requests ElasticSearch elasticsearch.index.rollup.get.requestsCurrent primaries.get.requestsCurrent ElasticSearch elasticsearch.index.rollup.get.requestsInMilliseconds primaries.get.requestsInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeleted primaries.index.docsCurrentlyDeleted ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeletedInMilliseconds primaries.index.docsCurrentlyDeletedInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexing primaries.index.docsCurrentlyIndexing ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexingInMilliseconds primaries.index.docsCurrentlyIndexingInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsDeleted primaries.index.docsDeleted ElasticSearch elasticsearch.index.rollup.index.docsTotal primaries.index.docsTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotal primaries.indexRefreshesTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotalInMilliseconds primaries.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.merges.current primaries.merges.current ElasticSearch elasticsearch.index.rollup.merges.docsSegmentsCurrentlyMerged primaries.merges.docsSegmentsCurrentlyMerged ElasticSearch elasticsearch.index.rollup.merges.docsTotal primaries.merges.docsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsCurrentlyMergedInBytes primaries.merges.segmentsCurrentlyMergedInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotal primaries.merges.segmentsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInBytes primaries.merges.segmentsTotalInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInMilliseconds primaries.merges.segmentsTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesInMilliseconds primaries.queriesInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesTotal primaries.queriesTotal ElasticSearch elasticsearch.index.rollup.queryActive primaries.queryActive ElasticSearch elasticsearch.index.rollup.queryFetches primaries.queryFetches ElasticSearch elasticsearch.index.rollup.queryFetchesInMilliseconds primaries.queryFetchesInMilliseconds ElasticSearch elasticsearch.index.rollup.queryFetchesTotal primaries.queryFetchesTotal ElasticSearch elasticsearch.index.rollup.sizeInBytes primaries.sizeInBytes ElasticSearch elasticsearch.index.storeSizeInBytes index.storeSizeInBytes ElasticSearch elasticsearch.node.activeSearches activeSearches ElasticSearch elasticsearch.node.activeSearchesInMilliseconds activeSearchesInMilliseconds ElasticSearch elasticsearch.node.breakers.estimatedSizeFieldDataCircuitBreakerInBytes breakers.estimatedSizeFieldDataCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeParentCircuitBreakerInBytes breakers.estimatedSizeParentCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeRequestCircuitBreakerInBytes breakers.estimatedSizeRequestCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.fieldDataCircuitBreakerTripped breakers.fieldDataCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.parentCircuitBreakerTripped breakers.parentCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.requestCircuitBreakerTripped breakers.requestCircuitBreakerTripped ElasticSearch elasticsearch.node.flush.indexRefreshesTotal flush.indexRefreshesTotal ElasticSearch elasticsearch.node.flush.indexRefreshesTotalInMilliseconds flush.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.node.fs.bytesAvailableJvmInBytes fs.bytesAvailableJVMInBytes ElasticSearch elasticsearch.node.fs.dataRead fs.bytesReadsInBytes ElasticSearch elasticsearch.node.fs.dataWritten fs.writesInBytes ElasticSearch elasticsearch.node.fs.ioOperations fs.iOOperations ElasticSearch elasticsearch.node.fs.readOperations fs.reads ElasticSearch elasticsearch.node.fs.totalSizeInBytes fs.totalSizeInBytes ElasticSearch elasticsearch.node.fs.unallocatedBytes fs.unallocatedBytesInBYtes ElasticSearch elasticsearch.node.fs.writeOperations fs.writeOperations ElasticSearch elasticsearch.node.get.currentRequestsRunning get.currentRequestsRunning ElasticSearch elasticsearch.node.get.requestsDocumentExists get.requestsDocumentExists ElasticSearch elasticsearch.node.get.requestsDocumentExistsInMilliseconds get.requestsDocumentExistsInMilliseconds ElasticSearch elasticsearch.node.get.requestsDocumentMissing get.requestsDocumentMissing ElasticSearch elasticsearch.node.get.requestsDocumentMissingInMilliseconds get.requestsDocumentMissingInMilliseconds ElasticSearch elasticsearch.node.get.timeGetRequestsInMilliseconds get.timeGetRequestsInMilliseconds ElasticSearch elasticsearch.node.get.totalGetRequests get.totalGetRequests ElasticSearch elasticsearch.node.http.currentOpenConnections http.currentOpenConnections ElasticSearch elasticsearch.node.http.openedConnections http.openedConnections ElasticSearch elasticsearch.node.index.indexingOperationsFailed indices.indexingOperationsFailed ElasticSearch elasticsearch.node.index.indexingWaitedThrottlingInMilliseconds indices.indexingWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.memoryQueryCacheInBytes indices.memoryQueryCacheInBytes ElasticSearch elasticsearch.node.index.numberIndices indices.numberIndices ElasticSearch elasticsearch.node.index.queryCacheEvictions indices.queryCacheEvictions ElasticSearch elasticsearch.node.index.queryCacheHits indices.queryCacheHits ElasticSearch elasticsearch.node.index.queryCacheMisses indices.queryCacheMisses ElasticSearch elasticsearch.node.index.recoveryOngoingShardSource indices.recoveryOngoingShardSource ElasticSearch elasticsearch.node.index.recoveryOngoingShardTarget indices.recoveryOngoingShardTarget ElasticSearch elasticsearch.node.index.recoveryWaitedThrottlingInMilliseconds indices.recoveryWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.requestCacheEvictions indices.requestCacheEvictions ElasticSearch elasticsearch.node.index.requestCacheHits indices.requestCacheHits ElasticSearch elasticsearch.node.index.requestCacheMemoryInBytes indices.requestCacheMemoryInBytes ElasticSearch elasticsearch.node.index.requestCacheMisses indices.requestCacheMisses ElasticSearch elasticsearch.node.index.segmentsIndexShard indices.segmentsIndexShard ElasticSearch elasticsearch.node.index.segmentsMemoryUsedDocValuesInBytes indices.segmentsMemoryUsedDocValuesInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedFixedBitSetInBytes indices.segmentsMemoryUsedFixedBitSetInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexSegmentsInBytes indices.segmentsMemoryUsedIndexSegmentsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexWriterInBytes indices.segmentsMemoryUsedIndexWriterInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedNormsInBytes indices.segmentsMemoryUsedNormsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedSegmentVersionMapInBytes indices.segmentsMemoryUsedSegmentVersionMapInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedStoredFieldsInBytes indices.segmentsMemoryUsedStoredFieldsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermsInBytes indices.segmentsMemoryUsedTermsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermVectorsInBytes indices.segmentsMemoryUsedTermVectorsInBytes ElasticSearch elasticsearch.node.index.translogOperations indices.translogOperations ElasticSearch elasticsearch.node.index.translogOperationsInBytes indices.translogOperationsInBytes ElasticSearch elasticsearch.node.indexing.docsCurrentlyDeleted indexing.docsCurrentlyDeleted ElasticSearch elasticsearch.node.indexing.documentsCurrentlyIndexing indexing.documentsCurrentlyIndexing ElasticSearch elasticsearch.node.indexing.documentsIndexed indexing.documentsIndexed ElasticSearch elasticsearch.node.indexing.timeDeletingDocumentsInMilliseconds indexing.timeDeletingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.timeIndexingDocumentsInMilliseconds indexing.timeIndexingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.totalDocumentsDeleted indexing.totalDocumentsDeleted ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjects jvm.gc.majorCollectionsOldGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjects jvm.gc.majorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjects jvm.gc.minorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.mem.heapCommittedInBytes jvm.mem.heapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.heapMaxInBytes jvm.mem.heapMaxInBytes ElasticSearch elasticsearch.node.jvm.mem.heapUsed jvm.mem.heapUsed ElasticSearch elasticsearch.node.jvm.mem.heapUsedInBytes jvm.mem.heapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.maxOldGenerationHeapInBytes jvm.mem.maxOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.maxSurvivorSpaceInBytes jvm.mem.maxSurvivorSpaceInBYtes ElasticSearch elasticsearch.node.jvm.mem.maxYoungGenerationHeapInBytes jvm.mem.maxYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapCommittedInBytes jvm.mem.nonHeapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapUsedInBytes jvm.mem.nonHeapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.usedOldGenerationHeapInBytes jvm.mem.usedOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.usedSurvivorSpaceInBytes jvm.mem.usedSurvivorSpaceInBytes ElasticSearch elasticsearch.node.jvm.mem.usedYoungGenerationHeapInBytes jvm.mem.usedYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.threadsActive jvm.ThreadsActive ElasticSearch elasticsearch.node.jvm.threadsPeak jvm.ThreadsPeak ElasticSearch elasticsearch.node.merges.currentActive merges.currentActive ElasticSearch elasticsearch.node.merges.docsSegmentMerges merges.docsSegmentMerges ElasticSearch elasticsearch.node.merges.docsSegmentsMerging merges.docsSegmentsMerging ElasticSearch elasticsearch.node.merges.mergedSegmentsInBytes merges.mergedSegmentsInBytes ElasticSearch elasticsearch.node.merges.segmentMerges merges.segmentMerges ElasticSearch elasticsearch.node.merges.sizeSegmentsMergingInBytes merges.sizeSegmentsMergingInBytes ElasticSearch elasticsearch.node.merges.totalSegmentMergingInMilliseconds merges.totalSegmentMergingInMilliseconds ElasticSearch elasticsearch.node.openFd openFD ElasticSearch elasticsearch.node.queriesTotal queriesTotal ElasticSearch elasticsearch.node.refresh.total refresh.total ElasticSearch elasticsearch.node.refresh.totalInMilliseconds refresh.totalInMilliseconds ElasticSearch elasticsearch.node.searchFetchCurrentlyRunning searchFetchCurrentlyRunning ElasticSearch elasticsearch.node.searchFetches searchFetches ElasticSearch elasticsearch.node.sizeStoreInBytes sizeStoreInBytes ElasticSearch elasticsearch.node.threadpool.activeFetchShardStarted threadpool.activeFetchShardStarted ElasticSearch elasticsearch.node.threadpool.bulkActive threadpool.bulkActive ElasticSearch elasticsearch.node.threadpool.bulkQueue threadpool.bulkQueue ElasticSearch elasticsearch.node.threadpool.bulkRejected threadpool.bulkRejected ElasticSearch elasticsearch.node.threadpool.bulkThreads threadpool.bulkThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStartedQueue threadpool.fetchShardStartedQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStartedRejected threadpool.fetchShardStartedRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStartedThreads threadpool.fetchShardStartedThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStoreActive threadpool.fetchShardStoreActive ElasticSearch elasticsearch.node.threadpool.fetchShardStoreQueue threadpool.fetchShardStoreQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStoreRejected threadpool.fetchShardStoreRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStoreThreads threadpool.fetchShardStoreThreads ElasticSearch elasticsearch.node.threadpool.flushActive threadpool.flushActive ElasticSearch elasticsearch.node.threadpool.flushQueue threadpool.flushQueue ElasticSearch elasticsearch.node.threadpool.flushRejected threadpool.flushRejected ElasticSearch elasticsearch.node.threadpool.flushThreads threadpool.flushThreads ElasticSearch elasticsearch.node.threadpool.forceMergeActive threadpool.forceMergeActive ElasticSearch elasticsearch.node.threadpool.forceMergeQueue threadpool.forceMergeQueue ElasticSearch elasticsearch.node.threadpool.forceMergeRejected threadpool.forceMergeRejected ElasticSearch elasticsearch.node.threadpool.forceMergeThreads threadpool.forceMergeThreads ElasticSearch elasticsearch.node.threadpool.genericActive threadpool.genericActive ElasticSearch elasticsearch.node.threadpool.genericQueue threadpool.genericQueue ElasticSearch elasticsearch.node.threadpool.genericRejected threadpool.genericRejected ElasticSearch elasticsearch.node.threadpool.genericThreads threadpool.genericThreads ElasticSearch elasticsearch.node.threadpool.getActive threadpool.getActive ElasticSearch elasticsearch.node.threadpool.getQueue threadpool.getQueue ElasticSearch elasticsearch.node.threadpool.getRejected threadpool.getRejected ElasticSearch elasticsearch.node.threadpool.getThreads threadpool.getThreads ElasticSearch elasticsearch.node.threadpool.indexActive threadpool.indexActive ElasticSearch elasticsearch.node.threadpool.indexQueue threadpool.indexQueue ElasticSearch elasticsearch.node.threadpool.indexRejected threadpool.indexRejected ElasticSearch elasticsearch.node.threadpool.indexThreads threadpool.indexThreads ElasticSearch elasticsearch.node.threadpool.listenerActive threadpool.listenerActive ElasticSearch elasticsearch.node.threadpool.listenerQueue threadpool.listenerQueue ElasticSearch elasticsearch.node.threadpool.listenerRejected threadpool.listenerRejected ElasticSearch elasticsearch.node.threadpool.listenerThreads threadpool.listenerThreads ElasticSearch elasticsearch.node.threadpool.managementActive threadpool.managementActive ElasticSearch elasticsearch.node.threadpool.managementQueue threadpool.managementQueue ElasticSearch elasticsearch.node.threadpool.managementRejected threadpool.managementRejected ElasticSearch elasticsearch.node.threadpool.managementThreads threadpool.managementThreads ElasticSearch elasticsearch.node.threadpool.refreshActive threadpool.refreshActive ElasticSearch elasticsearch.node.threadpool.refreshQueue threadpool.refreshQueue ElasticSearch elasticsearch.node.threadpool.refreshRejected threadpool.refreshRejected ElasticSearch elasticsearch.node.threadpool.refreshThreads threadpool.refreshThreads ElasticSearch elasticsearch.node.threadpool.searchActive threadpool.searchActive ElasticSearch elasticsearch.node.threadpool.searchQueue threadpool.searchQueue ElasticSearch elasticsearch.node.threadpool.searchRejected threadpool.searchRejected ElasticSearch elasticsearch.node.threadpool.searchThreads threadpool.searchThreads ElasticSearch elasticsearch.node.threadpool.snapshotActive threadpool.snapshotActive ElasticSearch elasticsearch.node.threadpool.snapshotQueue threadpool.snapshotQueue ElasticSearch elasticsearch.node.threadpool.snapshotRejected threadpool.snapshotRejected ElasticSearch elasticsearch.node.threadpool.snapshotThreads threadpool.snapshotThreads ElasticSearch elasticsearch.node.transport.connectionsOpened transport.connectionsOpened ElasticSearch elasticsearch.node.transport.packetsReceived transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes F5 f5.node.availabilityState node.availabilityState F5 f5.node.connections node.connections F5 f5.node.connectionsPerSecond node.connectionsPerSecond F5 f5.node.enabled node.enabled F5 f5.node.inDataInBytesPerSecond node.inDataInBytesPerSecond F5 f5.node.monitorStatus node.monitorStatus F5 f5.node.outDataInBytesPerSecond node.outDataInBytesPerSecond F5 f5.node.packetsReceivedPerSecond node.packetsReceivedPerSecond F5 f5.node.packetsSentPerSecond node.packetsSentPerSecond F5 f5.node.requestsPerSecond node.requestsPerSecond F5 f5.node.sessions node.sessions F5 f5.node.sessionStatus node.sessionStatus F5 f5.poolMember.availabilityState member.availabilityState F5 f5.poolMember.connections member.connections F5 f5.poolMember.enabled member.enabled F5 f5.poolMember.inDataInBytesPerSecond member.inDataInBytesPerSecond F5 f5.poolMember.monitorStatus member.monitorStatus F5 f5.poolMember.outDataInBytesPerSecond member.outDataInBytesPerSecond F5 f5.poolMember.packetsReceivedPerSecond member.packetsReceivedPerSecond F5 f5.poolMember.packetsSentPerSecond member.packetsSentPerSecond F5 f5.poolMember.requestsPerSecond member.requestsPerSecond F5 f5.poolMember.sessions member.sessions F5 f5.poolMember.sessionStatus member.sessionStatus F5 f5.pool.activeMembers pool.activeMembers F5 f5.pool.availabilityState pool.availabilityState F5 f5.pool.connections pool.connections F5 f5.pool.connqAgeEdm pool.connqAgeEdm F5 f5.pool.connqAgeEma pool.connqAgeEma F5 f5.pool.connqAgeHead pool.connqAgeHead F5 f5.pool.connqAgeMax pool.connqAgeMax F5 f5.pool.connqAllAgeEdm pool.connqAllAgeEdm F5 f5.pool.connqAllAgeEma pool.connqAllAgeEma F5 f5.pool.connqAllAgeHead pool.connqAllAgeHead F5 f5.pool.connqAllAgeMax pool.connqAllAgeMax F5 f5.pool.connqAllDepth pool.connqAllDepth F5 f5.pool.connqDepth pool.connqDepth F5 f5.pool.currentConnections pool.currentConnections F5 f5.pool.enabled pool.enabled F5 f5.pool.inDataInBytesPerSecond pool.inDataInBytesPerSecond F5 f5.pool.minActiveMembers pool.minActiveMembers F5 f5.pool.outDataInBytesPerSecond pool.outDataInBytesPerSecond F5 f5.pool.packetsReceivedPerSecond pool.packetsReceivedPerSecond F5 f5.pool.packetsSentPerSecond pool.packetsSentPerSecond F5 f5.pool.requestsPerSecond pool.requestsPerSecond F5 f5.pool.sessions pool.sessions F5 f5.system.cpuIdleTicksPerSecond system.cpuIdleTicksPerSecond F5 f5.system.cpuIdleUtilization system.cpuIdleUtilization F5 f5.system.cpuInterruptRequestUtilization system.cpuInterruptRequestUtilization F5 f5.system.cpuIoWaitUtilization system.cpuIOWaitUtilization F5 f5.system.cpuNiceLevelUtilization system.cpuNiceLevelUtilization F5 f5.system.cpuSoftInterruptRequestUtilization system.cpuSoftInterruptRequestUtilization F5 f5.system.cpuStolenUtilization system.cpuStolenUtilization F5 f5.system.cpuSystemTicksPerSecond system.cpuSystemTicksPerSecond F5 f5.system.cpuSystemUtilization system.cpuSystemUtilization F5 f5.system.cpuUserTicksPerSecond system.cpuUserTicksPerSecond F5 f5.system.cpuUserUtilization system.cpuUserUtilization F5 f5.system.memoryFreeInBytes system.memoryFreeInBytes F5 f5.system.memoryTotalInBytes system.memoryTotalInBytes F5 f5.system.memoryUsedInBytes system.memoryUsedInBytes F5 f5.system.otherMemoryFreeInBytes system.otherMemoryFreeInBytes F5 f5.system.otherMemoryTotalInBytes system.otherMemoryTotalInBytes F5 f5.system.otherMemoryUsedInBytes system.otherMemoryUsedInBytes F5 f5.system.swapFreeInBytes system.swapFreeInBytes F5 f5.system.swapTotalInBytes system.swapTotalInBytes F5 f5.system.swapUsedInBytes system.swapUsedInBytes F5 f5.system.tmmMemoryFreeInBytes system.tmmMemoryFreeInBytes F5 f5.system.tmmMemoryTotalInBytes system.tmmMemoryTotalInBytes F5 f5.system.tmmMemoryUsedInBytes system.tmmMemoryUsedInBytes F5 f5.virtualserver.availabilityState virtualserver.availabilityState F5 f5.virtualserver.clientsideConnectionsPerSecond virtualserver.clientsideConnectionsPerSecond F5 f5.virtualserver.connections virtualserver.connections F5 f5.virtualserver.csMaxConnDur virtualserver.csMaxConnDur F5 f5.virtualserver.csMeanConnDur virtualserver.csMeanConnDur F5 f5.virtualserver.csMinConnDur virtualserver.csMinConnDur F5 f5.virtualserver.enabled virtualserver.enabled F5 f5.virtualserver.ephemeralBytesInPerSecond virtualserver.ephemeralBytesInPerSecond F5 f5.virtualserver.ephemeralBytesOutPerSecond virtualserver.ephemeralBytesOutPerSecond F5 f5.virtualserver.ephemeralConnectionsPerSecond virtualserver.ephemeralConnectionsPerSecond F5 f5.virtualserver.ephemeralCurrentConnections virtualserver.ephemeralCurrentConnections F5 f5.virtualserver.ephemeralEvictedConnectionsPerSecond virtualserver.ephemeralEvictedConnectionsPerSecond F5 f5.virtualserver.ephemeralMaxConnections virtualserver.ephemeralMaxConnections F5 f5.virtualserver.ephemeralPacketsReceivedPerSecond virtualserver.ephemeralPacketsReceivedPerSecond F5 f5.virtualserver.ephemeralPacketsSentPerSecond virtualserver.ephemeralPacketsSentPerSecond F5 f5.virtualserver.ephemeralSlowKilledPerSecond virtualserver.ephemeralSlowKilledPerSecond F5 f5.virtualserver.evictedConnsPerSecond virtualserver.evictedConnsPerSecond F5 f5.virtualserver.inDataInBytesPerSecond virtualserver.inDataInBytesPerSecond F5 f5.virtualserver.outDataInBytesPerSecond virtualserver.outDataInBytesPerSecond F5 f5.virtualserver.packetsReceivedPerSecond virtualserver.packetsReceivedPerSecond F5 f5.virtualserver.packetsSentPerSecond virtualserver.packetsSentPerSecond F5 f5.virtualserver.requestsPerSecond virtualserver.requestsPerSecond F5 f5.virtualserver.slowKilledPerSecond virtualserver.slowKilledPerSecond F5 f5.virtualserver.usageRatio virtualserver.usageRatio HAProxy haproxy.backend.activeServers backend.activeServers HAProxy haproxy.backend.averageConnectTimeInSeconds backend.averageConnectTimeInSeconds HAProxy haproxy.backend.averageQueueTimeInSeconds backend.averageQueueTimeInSeconds HAProxy haproxy.backend.averageResponseTimeInSeconds backend.averageResponseTimeInSeconds HAProxy haproxy.backend.averageTotalSessionTimeInSeconds backend.averageTotalSessionTimeInSeconds HAProxy haproxy.backend.backupServers backend.backupServers HAProxy haproxy.backend.bytesInPerSecond backend.bytesInPerSecond HAProxy haproxy.backend.bytesOutPerSecond backend.bytesOutPerSecond HAProxy haproxy.backend.bytesThatBypassedCompressorPerSecond backend.bytesThatBypassedCompressorPerSecond HAProxy haproxy.backend.connectingRequestErrorsPerSecond backend.connectingRequestErrorsPerSecond HAProxy haproxy.backend.connectionRetriesPerSecond backend.connectionRetriesPerSecond HAProxy haproxy.backend.currentQueuedRequestsWithoutServer backend.currentQueuedRequestsWithoutServer HAProxy haproxy.backend.currentSessions backend.currentSessions HAProxy haproxy.backend.dataTransfersAbortedByClientPerSecond backend.dataTransfersAbortedByClientPerSecond HAProxy haproxy.backend.dataTransfersAbortedByServerPerSecond backend.dataTransfersAbortedByServerPerSecond HAProxy haproxy.backend.downtimeInSeconds backend.downtimeInSeconds HAProxy haproxy.backend.http100ResponsesPerSecond backend.http100ResponsesPerSecond HAProxy haproxy.backend.http200ResponsesPerSecond backend.http200ResponsesPerSecond HAProxy haproxy.backend.http300ResponsesPerSecond backend.http300ResponsesPerSecond HAProxy haproxy.backend.http400ResponsesPerSecond backend.http400ResponsesPerSecond HAProxy haproxy.backend.http500ResponsesPerSecond backend.http500ResponsesPerSecond HAProxy haproxy.backend.httpOtherResponsesPerSecond backend.httpOtherResponsesPerSecond HAProxy haproxy.backend.httpRequestsPerSecond backend.httpRequestsPerSecond HAProxy haproxy.backend.httpResponseBytesEmittedByCompressorPerSecond backend.httpResponseBytesEmittedByCompressorPerSecond HAProxy haproxy.backend.httpResponseBytesFedToCompressorPerSecond backend.httpResponseBytesFedToCompressorPerSecond HAProxy haproxy.backend.httpResponsesCompressedPerSecond backend.httpResponsesCompressedPerSecond HAProxy haproxy.backend.interceptedRequestsPerSecond backend.interceptedRequestsPerSecond HAProxy haproxy.backend.maxQueuedRequestsWithoutServer backend.maxQueuedRequestsWithoutServer HAProxy haproxy.backend.maxSessions backend.maxSessions HAProxy haproxy.backend.maxSessionsPerSecond backend.maxSessionsPerSecond HAProxy haproxy.backend.requestRedispatchPerSecond backend.requestRedispatchPerSecond HAProxy haproxy.backend.requestsDenied.securityConcernsPerSecond backend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.backend.responseErrorsPerSecond backend.responseErrorsPerSecond HAProxy haproxy.backend.responsesDenied.securityConcernsPerSecond backend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.backend.serverSelectedPerSecond backend.serverSelectedPerSecond HAProxy haproxy.backend.sessionsPerSecond backend.sessionsPerSecond HAProxy haproxy.backend.timeSinceLastSessionAssignedInSeconds backend.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.backend.timeSinceLastUpDownTransitionInSeconds backend.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.backend.totalWeight backend.totalWeight HAProxy haproxy.backend.type backend.type HAProxy haproxy.backend.upToDownTransitionsPerSecond backend.upToDownTransitionsPerSecond HAProxy haproxy.frontend.bytesInPerSecond frontend.bytesInPerSecond HAProxy haproxy.frontend.bytesOutPerSecond frontend.bytesOutPerSecond HAProxy haproxy.frontend.connectionsPerSecond frontend.connectionsPerSecond HAProxy haproxy.frontend.currentSessions frontend.currentSessions HAProxy haproxy.frontend.http100ResponsesPerSecond frontend.http100ResponsesPerSecond HAProxy haproxy.frontend.http200ResponsesPerSecond frontend.http200ResponsesPerSecond HAProxy haproxy.frontend.http300ResponsesPerSecond frontend.http300ResponsesPerSecond HAProxy haproxy.frontend.http400ResponsesPerSecond frontend.http400ResponsesPerSecond HAProxy haproxy.frontend.http500ResponsesPerSecond frontend.http500ResponsesPerSecond HAProxy haproxy.frontend.httpOtherResponsesPerSecond frontend.httpOtherResponsesPerSecond HAProxy haproxy.frontend.httpRequests.maxPerSecond frontend.httpRequests.maxPerSecond HAProxy haproxy.frontend.httpRequestsPerSecond frontend.httpRequestsPerSecond HAProxy haproxy.frontend.interceptedRequestsPerSecond frontend.interceptedRequestsPerSecond HAProxy haproxy.frontend.maxConnectionsPerSecond frontend.maxConnectionsPerSecond HAProxy haproxy.frontend.maxSessions frontend.maxSessions HAProxy haproxy.frontend.maxSessionsPerSecond frontend.maxSessionsPerSecond HAProxy haproxy.frontend.requestErrorsPerSecond frontend.requestErrorsPerSecond HAProxy haproxy.frontend.requestsDenied.securityConcernsPerSecond frontend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestConnectionRulesPerSecond frontend.requestsDenied.tcpRequestConnectionRulesPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestSessionRulesPerSecond frontend.requestsDenied.tcpRequestSessionRulesPerSecond HAProxy haproxy.frontend.responsesDenied.securityConcernsPerSecond frontend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.frontend.sessionsPerSecond frontend.sessionsPerSecond HAProxy haproxy.server.averageConnectTimeInSeconds server.averageConnectTimeInSeconds HAProxy haproxy.server.averageQueueTimeInSeconds server.averageQueueTimeInSeconds HAProxy haproxy.server.averageResponseTimeInSeconds server.averageResponseTimeInSeconds HAProxy haproxy.server.averageTotalSessionTimeInSeconds server.averageTotalSessionTimeInSeconds HAProxy haproxy.server.bytesInPerSecond server.bytesInPerSecond HAProxy haproxy.server.bytesOutPerSecond server.bytesOutPerSecond HAProxy haproxy.server.connectingRequestErrorsPerSecond server.connectingRequestErrorsPerSecond HAProxy haproxy.server.connectionRetriesPerSecond server.connectionRetriesPerSecond HAProxy haproxy.server.currentQueuedRequestsWithoutServer server.currentQueuedRequestsWithoutServer HAProxy haproxy.server.currentSessions server.currentSessions HAProxy haproxy.server.dataTransfersAbortedByClientPerSecond server.dataTransfersAbortedByClientPerSecond HAProxy haproxy.server.dataTransfersAbortedByServerPerSecond server.dataTransfersAbortedByServerPerSecond HAProxy haproxy.server.downtimeInSeconds server.downtimeInSeconds HAProxy haproxy.server.failedChecksPerSecond server.failedChecksPerSecond HAProxy haproxy.server.healthCheckDurationInMilliseconds server.healthCheckDurationInMilliseconds HAProxy haproxy.server.http100ResponsesPerSecond server.http100ResponsesPerSecond HAProxy haproxy.server.http200ResponsesPerSecond server.http200ResponsesPerSecond HAProxy haproxy.server.http300ResponsesPerSecond server.http300ResponsesPerSecond HAProxy haproxy.server.http400ResponsesPerSecond server.http400ResponsesPerSecond HAProxy haproxy.server.http500ResponsesPerSecond server.http500ResponsesPerSecond HAProxy haproxy.server.httpOtherResponsesPerSecond server.httpOtherResponsesPerSecond HAProxy haproxy.server.isActive server.isActive HAProxy haproxy.server.isBackup server.isBackup HAProxy haproxy.server.maxQueuedRequestsWithoutServer server.maxQueuedRequestsWithoutServer HAProxy haproxy.server.maxSessions server.maxSessions HAProxy haproxy.server.maxSessionsPerSecond server.maxSessionsPerSecond HAProxy haproxy.server.requestRedispatchPerSecond server.requestRedispatchPerSecond HAProxy haproxy.server.requestsDenied.securityConcernsPerSecond server.requestsDenied.securityConcernsPerSecond HAProxy haproxy.server.responseErrorsPerSecond server.responseErrorsPerSecond HAProxy haproxy.server.responsesDenied.securityConcernsPerSecond server.responsesDenied.securityConcernsPerSecond HAProxy haproxy.server.serverSelectedPerSecond server.serverSelectedPerSecond HAProxy haproxy.server.serverWeight server.serverWeight HAProxy haproxy.server.sessionsPerSecond server.sessionsPerSecond HAProxy haproxy.server.throttlePercentage server.throttlePercentage HAProxy haproxy.server.timeSinceLastSessionAssignedInSeconds server.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.server.timeSinceLastUpDownTransitionInSeconds server.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.server.type server.type HAProxy haproxy.server.upToDownTransitionsPerSecond server.upToDownTransitionsPerSecond Kafka kafka.broker.bytesWrittenToTopicPerSecond broker.bytesWrittenToTopicPerSecond Kafka kafka.broker.consumer.requestsExpiredPerSecond consumer.requestsExpiredPerSecond Kafka kafka.broker.follower.requestExpirationPerSecond follower.requestExpirationPerSecond Kafka kafka.broker.ioInPerSecond broker.IOInPerSecond Kafka kafka.broker.ioOutPerSecond broker.IOOutPerSecond Kafka kafka.broker.logFlushPerSecond broker.logFlushPerSecond Kafka kafka.broker.messagesInPerSecond broker.messagesInPerSecond Kafka kafka.broker.net.bytesRejectedPerSecond net.bytesRejectedPerSecond Kafka kafka.broker.replication.isrExpandsPerSecond replication.isrExpandsPerSecond Kafka kafka.broker.replication.isrShrinksPerSecond replication.isrShrinksPerSecond Kafka kafka.broker.replication.leaderElectionPerSecond replication.leaderElectionPerSecond Kafka kafka.broker.replication.uncleanLeaderElectionPerSecond replication.uncleanLeaderElectionPerSecond Kafka kafka.broker.replication.unreplicatedPartitions replication.unreplicatedPartitions Kafka kafka.broker.request.avgTimeFetch request.avgTimeFetch Kafka kafka.broker.request.avgTimeMetadata request.avgTimeMetadata Kafka kafka.broker.request.avgTimeMetadata99Percentile request.avgTimeMetadata99Percentile Kafka kafka.broker.request.avgTimeOffset request.avgTimeOffset Kafka kafka.broker.request.avgTimeOffset99Percentile request.avgTimeOffset99Percentile Kafka kafka.broker.request.avgTimeProduceRequest request.avgTimeProduceRequest Kafka kafka.broker.request.avgTimeUpdateMetadata request.avgTimeUpdateMetadata Kafka kafka.broker.request.avgTimeUpdateMetadata99Percentile request.avgTimeUpdateMetadata99Percentile Kafka kafka.broker.request.clientFetchesFailedPerSecond request.clientFetchesFailedPerSecond Kafka kafka.broker.request.fetchConsumerRequestsPerSecond request.fetchConsumerRequestsPerSecond Kafka kafka.broker.request.fetchFollowerRequestsPerSecond request.fetchFollowerRequestsPerSecond Kafka kafka.broker.request.fetchTime99Percentile request.fetchTime99Percentile Kafka kafka.broker.request.handlerIdle request.handlerIdle Kafka kafka.broker.request.listGroupsRequestsPerSecond request.listGroupsRequestsPerSecond Kafka kafka.broker.request.metadataRequestsPerSecond request.metadataRequestsPerSecond Kafka kafka.broker.request.offsetCommitRequestsPerSecond request.offsetCommitRequestsPerSecond Kafka kafka.broker.request.produceRequestsFailedPerSecond request.produceRequestsFailedPerSecond Kafka kafka.broker.request.produceRequestsPerSecond request.produceRequestsPerSecond Kafka kafka.broker.request.produceTime99Percentile request.produceTime99Percentile Kafka kafka.broker.topic.diskSize topic.diskSize Kafka kafka.topic.bytesInPerSec topic.BytesInPerSec Kafka kafka.topic.bytesOutPerSec topic.BytesOutPerSec Kafka kafka.topic.messagesInPerSec topic.MessagesInPerSec Kafka kafka.topic.partitionsWithNonPreferredLeader topic.partitionsWithNonPreferredLeader Kafka kafka.topic.respondsToMetadataRequests topic.respondsToMetadataRequests Kafka kafka.topic.retentionBytesOrTime topic.retentionBytesOrTime Kafka kafka.topic.underReplicatedPartitions topic.underReplicatedPartitions Kafka kafka.producer.ageMetadataUsedInMilliseconds producer.ageMetadataUsedInMilliseconds Kafka kafka.producer.availableBufferInBytes producer.availableBufferInBytes Kafka kafka.producer.avgBytesSentPerRequestInBytes producer.avgBytesSentPerRequestInBytes Kafka kafka.producer.avgCompressionRateRecordBatches producer.avgCompressionRateRecordBatches Kafka kafka.producer.avgRecordAccumulatorsInMilliseconds producer.avgRecordAccumulatorsInMilliseconds Kafka kafka.producer.avgRecordSizeInBytes producer.avgRecordSizeInBytes Kafka kafka.producer.avgRecordsSentPerSecond producer.avgRecordsSentPerSecond Kafka kafka.producer.avgRecordsSentPerTopicPerSecond producer.avgRecordsSentPerTopicPerSecond Kafka kafka.producer.avgRequestLatency producer.avgRequestLatencyPerSecond Kafka kafka.producer.avgThrottleTime producer.avgThrottleTime Kafka kafka.producer.bufferMemoryAvailableInBytes producer.bufferMemoryAvailableInBytes Kafka kafka.producer.bufferpoolWaitTime producer.bufferpoolWaitTime Kafka kafka.producer.bytesOutPerSecond producer.bytesOutPerSecond Kafka kafka.producer.compressionRateRecordBatches producer.compressionRateRecordBatches Kafka kafka.producer.ioWaitTime producer.ioWaitTime Kafka kafka.producer.maxBytesSentPerRequestInBytes producer.maxBytesSentPerRequestInBytes Kafka kafka.producer.maxRecordSizeInBytes producer.maxRecordSizeInBytes Kafka kafka.producer.maxRequestLatencyInMilliseconds producer.maxRequestLatencyInMilliseconds Kafka kafka.producer.maxThrottleTime producer.maxThrottleTime Kafka kafka.producer.requestPerSecond producer.requestPerSecond Kafka kafka.producer.requestsWaitingResponse producer.requestsWaitingResponse Kafka kafka.producer.responsePerSecond producer.responsePerSecond Kafka kafka.producer.threadsWaiting producer.threadsWaiting Kafka kafka.consumer.avgFetchSizeInBytes consumer.avgFetchSizeInBytes Kafka kafka.consumer.avgRecordConsumedPerTopic consumer.avgRecordConsumedPerTopic Kafka kafka.consumer.avgRecordConsumedPerTopicPerSecond consumer.avgRecordConsumedPerTopicPerSecond Kafka kafka.consumer.bytesInPerSecond consumer.bytesInPerSecond Kafka kafka.consumer.fetchPerSecond consumer.fetchPerSecond Kafka kafka.consumer.hwm consumer.hwm Kafka kafka.consumer.lag consumer.lag Kafka kafka.consumer.maxFetchSizeInBytes consumer.maxFetchSizeInBytes Kafka kafka.consumer.maxLag consumer.maxLag Kafka kafka.consumer.messageConsumptionPerSecond consumer.messageConsumptionPerSecond Kafka kafka.consumer.offset consumer.offset Kafka kafka.consumer.totalLag consumer.totalLag Kafka kafka.consumerGroup.maxLag consumerGroup.maxLag Kafka kafka.consumerGroup.totalLag consumerGroup.totalLag Kubernetes k8s.apiserver.goGoroutines goGoroutines Kubernetes k8s.apiserver.goThreads goThreads Kubernetes k8s.apiserver.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.apiserver.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.controllermanager.goGoroutines goGoroutines Kubernetes k8s.controllermanager.goThreads goThreads Kubernetes k8s.controllermanager.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.controllermanager.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.controllermanager.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.goGoroutines goGoroutines Kubernetes k8s.etcd.goThreads goThreads Kubernetes k8s.etcd.mvccDbTotalSizeInBytes etcdMvccDbTotalSizeInBytes Kubernetes k8s.etcd.networkClientGrpcReceivedBytesRate etcdNetworkClientGrpcReceivedBytesRate Kubernetes k8s.etcd.networkClientGrpcSentBytesRate etcdNetworkClientGrpcSentBytesRate Kubernetes k8s.etcd.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.etcd.process.maxFds processMaxFds Kubernetes k8s.etcd.process.openFds processOpenFds Kubernetes k8s.etcd.process.processFdsUtilization processFdsUtilization Kubernetes k8s.etcd.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.serverHasLeader etcdServerHasLeader Kubernetes k8s.etcd.serverLeaderChangesSeenDelta etcdServerLeaderChangesSeenDelta Kubernetes k8s.etcd.serverProposalsAppliedDelta etcdServerProposalsAppliedDelta Kubernetes k8s.etcd.serverProposalsAppliedRate etcdServerProposalsAppliedRate Kubernetes k8s.etcd.serverProposalsCommittedDelta etcdServerProposalsCommittedDelta Kubernetes k8s.etcd.serverProposalsCommittedRate etcdServerProposalsCommittedRate Kubernetes k8s.etcd.serverProposalsFailedDelta etcdServerProposalsFailedDelta Kubernetes k8s.etcd.serverProposalsFailedRate etcdServerProposalsFailedRate Kubernetes k8s.etcd.serverProposalsPending etcdServerProposalsPending Kubernetes k8s.scheduler.goGoroutines goGoroutines Kubernetes k8s.scheduler.goThreads goThreads Kubernetes k8s.scheduler.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.scheduler.podPreemptionVictims schedulerPodPreemptionVictims Kubernetes k8s.scheduler.preemptionAttemptsDelta schedulerPreemptionAttemptsDelta Kubernetes k8s.scheduler.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.scheduler.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.container.cpuCfsPeriodsDelta containerCpuCfsPeriodsDelta Kubernetes k8s.container.cpuCfsPeriodsTotal containerCpuCfsPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledPeriodsDelta containerCpuCfsThrottledPeriodsDelta Kubernetes k8s.container.cpuCfsThrottledPeriodsTotal containerCpuCfsThrottledPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledSecondsDelta containerCpuCfsThrottledSecondsDelta Kubernetes k8s.container.cpuCfsThrottledSecondsTotal containerCpuCfsThrottledSecondsTotal Kubernetes k8s.container.cpuCoresUtilization cpuCoresUtilization Kubernetes k8s.container.cpuLimitCores cpuLimitCores Kubernetes k8s.container.cpuRequestedCores cpuRequestedCores Kubernetes k8s.container.cpuUsedCores cpuUsedCores Kubernetes k8s.container.fsAvailableBytes fsAvailableBytes Kubernetes k8s.container.fsCapacityBytes fsCapacityBytes Kubernetes k8s.container.fsInodes fsInodes Kubernetes k8s.container.fsInodesFree fsInodesFree Kubernetes k8s.container.fsInodesUsed fsInodesUsed Kubernetes k8s.container.fsUsedBytes fsUsedBytes Kubernetes k8s.container.fsUsedPercent fsUsedPercent Kubernetes k8s.container.isReady isReady Kubernetes k8s.container.memoryLimitBytes memoryLimitBytes Kubernetes k8s.container.memoryMappedFileBytes containerMemoryMappedFileBytes Kubernetes k8s.container.memoryRequestedBytes memoryRequestedBytes Kubernetes k8s.container.memoryUsedBytes memoryUsedBytes Kubernetes k8s.container.memoryUtilization memoryUtilization Kubernetes k8s.container.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.container.requestedCpuCoresUtilization requestedCpuCoresUtilization Kubernetes k8s.container.requestedMemoryUtilization requestedMemoryUtilization Kubernetes k8s.container.restartCount restartCount Kubernetes k8s.daemonset.createdAt createdAt Kubernetes k8s.daemonset.metadataGeneration metadataGeneration Kubernetes k8s.daemonset.podsAvailable podsAvailable Kubernetes k8s.daemonset.podsDesired podsDesired Kubernetes k8s.daemonset.podsMisscheduled podsMisscheduled Kubernetes k8s.daemonset.podsReady podsReady Kubernetes k8s.daemonset.podsScheduled podsScheduled Kubernetes k8s.daemonset.podsUnavailable podsUnavailable Kubernetes k8s.daemonset.podsUpdatedScheduled podsUpdatedScheduled Kubernetes k8s.deployment.createdAt createdAt Kubernetes k8s.deployment.podsAvailable podsAvailable Kubernetes k8s.deployment.podsDesired podsDesired Kubernetes k8s.deployment.podsMaxUnavailable podsMaxUnavailable Kubernetes k8s.deployment.podsTotal podsTotal Kubernetes k8s.deployment.podsUnavailable podsUnavailable Kubernetes k8s.deployment.podsUpdated podsUpdated Kubernetes k8s.endpoint.addressAvailable addressAvailable Kubernetes k8s.endpoint.addressNotReady addressNotReady Kubernetes k8s.endpoint.createdAt createdAt Kubernetes k8s.namespace.createdAt createdAt Kubernetes k8s.node.allocatableAttachableVolumes* allocatableAttachableVolumes* Kubernetes k8s.node.allocatableCpuCores allocatableCpuCores Kubernetes k8s.node.allocatableCpuCoresUtilization allocatableCpuCoresUtilization Kubernetes k8s.node.allocatableEphemeralStorageBytes allocatableEphemeralStorageBytes Kubernetes k8s.node.allocatableHugepages* allocatableHugepages* Kubernetes k8s.node.allocatableMemoryBytes allocatableMemoryBytes Kubernetes k8s.node.allocatableMemoryUtilization allocatableMemoryUtilization Kubernetes k8s.node.allocatablePods allocatablePods Kubernetes k8s.node.capacityAttachableVolumes* capacityAttachableVolumes* Kubernetes k8s.node.capacityCpuCores capacityCpuCores Kubernetes k8s.node.capacityEphemeralStorageBytes capacityEphemeralStorageBytes Kubernetes k8s.node.capacityHugepages* capacityHugepages* Kubernetes k8s.node.capacityMemoryBytes capacityMemoryBytes Kubernetes k8s.node.capacityPods capacityPods Kubernetes k8s.node.cpuUsedCoreMilliseconds cpuUsedCoreMilliseconds Kubernetes k8s.node.cpuUsedCores cpuUsedCores Kubernetes k8s.node.fsAvailableBytes fsAvailableBytes Kubernetes k8s.node.fsCapacityBytes fsCapacityBytes Kubernetes k8s.node.fsCapacityUtilization fsCapacityUtilization Kubernetes k8s.node.fsInodes fsInodes Kubernetes k8s.node.fsInodesFree fsInodesFree Kubernetes k8s.node.fsInodesUsed fsInodesUsed Kubernetes k8s.node.fsUsedBytes fsUsedBytes Kubernetes k8s.node.memoryAvailableBytes memoryAvailableBytes Kubernetes k8s.node.memoryMajorPageFaultsPerSecond memoryMajorPageFaultsPerSecond Kubernetes k8s.node.memoryPageFaults memoryPageFaults Kubernetes k8s.node.memoryRssBytes memoryRssBytes Kubernetes k8s.node.memoryUsedBytes memoryUsedBytes Kubernetes k8s.node.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.node.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.node.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.node.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.node.runtimeAvailableBytes runtimeAvailableBytes Kubernetes k8s.node.runtimeCapacityBytes runtimeCapacityBytes Kubernetes k8s.node.runtimeInodes runtimeInodes Kubernetes k8s.node.runtimeInodesFree runtimeInodesFree Kubernetes k8s.node.runtimeInodesUsed runtimeInodesUsed Kubernetes k8s.node.runtimeUsedBytes runtimeUsedBytes Kubernetes k8s.pod.createdAt createdAt Kubernetes k8s.pod.isReady isReady Kubernetes k8s.pod.isScheduled isScheduled Kubernetes k8s.pod.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.pod.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.pod.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.pod.startTime startTime Kubernetes k8s.replicaset.createdAt createdAt Kubernetes k8s.replicaset.observedGeneration observedGeneration Kubernetes k8s.replicaset.podsDesired podsDesired Kubernetes k8s.replicaset.podsFullyLabeled podsFullyLabeled Kubernetes k8s.replicaset.podsMissing podsMissing Kubernetes k8s.replicaset.podsReady podsReady Kubernetes k8s.replicaset.podsTotal podsTotal Kubernetes k8s.service.createdAt createdAt Kubernetes k8s.statefulset.createdAt createdAt Kubernetes k8s.statefulset.currentRevision currentRevision Kubernetes k8s.statefulset.metadataGeneration metadataGeneration Kubernetes k8s.statefulset.observedGeneration observedGeneration Kubernetes k8s.statefulset.podsCurrent podsCurrent Kubernetes k8s.statefulset.podsDesired podsDesired Kubernetes k8s.statefulset.podsReady podsReady Kubernetes k8s.statefulset.podsTotal podsTotal Kubernetes k8s.statefulset.podsUpdated podsUpdated Kubernetes k8s.statefulset.updateRevision updateRevision Kubernetes k8s.volume.fsAvailableBytes fsAvailableBytes Kubernetes k8s.volume.fsCapacityBytes fsCapacityBytes Kubernetes k8s.volume.fsInodes fsInodes Kubernetes k8s.volume.fsInodesFree fsInodesFree Kubernetes k8s.volume.fsInodesUsed fsInodesUsed Kubernetes k8s.volume.fsUsedBytes fsUsedBytes Kubernetes k8s.volume.fsUsedPercent fsUsedPercent Memcached memcached.server.activeSlabs activeSlabs Memcached memcached.server.avgItemSizeInBytes avgItemSizeInBytes Memcached memcached.server.bytesReadServerPerSecond bytesReadServerPerSecond Memcached memcached.server.bytesUsedServerInBytes bytesUsedServerInBytes Memcached memcached.server.bytesWrittenServerPerSecond bytesWrittenServerPerSecond Memcached memcached.server.casHitRatePerSecond casHitRatePerSecond Memcached memcached.server.casMissRatePerSecond casMissRatePerSecond Memcached memcached.server.casWrongRatePerSecond casWrongRatePerSecond Memcached memcached.server.cmdFlushRatePerSecond cmdFlushRatePerSecond Memcached memcached.server.cmdGetRatePerSecond cmdGetRatePerSecond Memcached memcached.server.cmdSetRatePerSecond cmdSetRatePerSecond Memcached memcached.server.connectionRateServerPerSecond connectionRateServerPerSecond Memcached memcached.server.connectionStructuresAllocated connectionStructuresAllocated Memcached memcached.server.currentItemsStoredServer currentItemsStoredServer Memcached memcached.server.deleteCmdNoneRemovedPerSecond deleteCmdNoneRemovedPerSecond Memcached memcached.server.deleteCmdRemovedPerSecond deleteCmdRemovedPerSecond Memcached memcached.server.evictionsPerSecond evictionsPerSecond Memcached memcached.server.getHitPercent getHitPercent Memcached memcached.server.getHitPerSecond getHitPerSecond Memcached memcached.server.getMissPerSecond getMissPerSecond Memcached memcached.server.itemsStoredPerSecond itemsStoredPerSecond Memcached memcached.server.limitBytesStorage limitBytesStorage Memcached memcached.server.limitMaxBytes limitMaxBytes Memcached memcached.server.maxConnectionLimitPerSecond serverMaxConnectionLimitPerSecond Memcached memcached.server.memAllocatedSlabsInBytes memAllocatedSlabsInBytes Memcached memcached.server.openConnectionsServer openConnectionsServer Memcached memcached.server.pointerSize pointerSize Memcached memcached.server.rusageSystem usageRate Memcached memcached.server.rusageUser executionTime Memcached memcached.server.storingItemsPercentMemory storingItemsPercentMemory Memcached memcached.server.threads threads Memcached memcached.server.uptimeInMilliseconds uptimeInMilliseconds Memcached memcached.slab.activeItemsBumpedPerSecond activeItemsBumpedPerSecond Memcached memcached.slab.casBadValPerSecond casBadValPerSecond Memcached memcached.slab.casModifiedSlabPerSecond casModifiedSlabPerSecond Memcached memcached.slab.chunkSizeInBytes chunkSizeInBytes Memcached memcached.slab.chunksPerPage chunksPerPage Memcached memcached.slab.cmdSetRateSlabPerSecond cmdSetRateSlabPerSecond Memcached memcached.slab.decrsModifySlabPerSecond decrsModifySlabPerSecond Memcached memcached.slab.deleteRateSlabPerSecond deleteRateSlabPerSecond Memcached memcached.slab.entriesReclaimedPerSecond entriesReclaimedPerSecond Memcached memcached.slab.evictionsBeforeExpirationPerSecond evictionsBeforeExpirationPerSecond Memcached memcached.slab.evictionsBeforeExplicitExpirationPerSecond evictionsBeforeExplicitExpirationPerSecond Memcached memcached.slab.expiredItemsReclaimedPerSecond expiredItemsReclaimedPerSecond Memcached memcached.slab.freedChunks freedChunks Memcached memcached.slab.freedChunksEnd freedChunksEnd Memcached memcached.slab.getHitRateSlabPerSecond getHitRateSlabPerSecond Memcached memcached.slab.incrsModifySlabPerSecond incrsModifySlabPerSecond Memcached memcached.slab.itemsCold itemsCold Memcached memcached.slab.itemsColdPerSecond itemsColdPerSecond Memcached memcached.slab.itemsDirectReclaimedPerSecond itemsDirectReclaimedPerSecond Memcached memcached.slab.itemsFreedCrawlerPerSecond itemsFreedCrawlerPerSecond Memcached memcached.slab.itemsHot itemsHot Memcached memcached.slab.itemsOldestInMilliseconds itemsOldestInMilliseconds Memcached memcached.slab.itemsRefcountLockedPerSecond itemsRefcountLockedPerSecond Memcached memcached.slab.itemsSlabClass itemsSlabClass Memcached memcached.slab.itemsTimeSinceEvictionInMilliseconds itemsTimeSinceEvictionInMilliseconds Memcached memcached.slab.itemsWarm itemsWarm Memcached memcached.slab.itemsWarmPerSecond itemsWarmPerSecond Memcached memcached.slab.memRequestedSlabInBytesPerSecond memRequestedSlabInBytesPerSecond Memcached memcached.slab.outOfMemoryPerSecond outOfMemoryPerSecond Memcached memcached.slab.selfHealedSlabPerSecond selfHealedSlabPerSecond Memcached memcached.slab.totalChunksSlab totalChunksSlab Memcached memcached.slab.totalPagesSlab totalPagesSlab Memcached memcached.slab.touchHitSlabPerSecond touchHitSlabPerSecond Memcached memcached.slab.usedChunksItems usedChunksItems Memcached memcached.slab.usedChunksPerSecond usedChunksPerSecond Memcached memcached.slab.validItemsEvictedPerSecond validItemsEvictedPerSecond MongoDB mongo.index.accesses collection.indexAccesses MongoDB mongo.index.sizeInBytes collection.indexSizeInBytes MongoDB mongo.collection.avgObjSizeInBytes collection.avgObjSizeInBytes MongoDB mongo.collection.capped collection.capped MongoDB mongo.collection.count collection.count MongoDB mongo.collection.max collection.max MongoDB mongo.collection.maxSizeInBytes collection.maxSizeInBytes MongoDB mongo.collection.nindexes collection.nindexes MongoDB mongo.collection.sizeInBytes collection.sizeInBytes MongoDB mongo.collection.storageSizeInBytes collection.storageSizeInBytes MongoDB mongo.configServer.asserts.messagesPerSecond asserts.messagesPerSecond MongoDB mongo.configServer.asserts.regularPerSecond asserts.regularPerSecond MongoDB mongo.configServer.asserts.rolloversPerSecond asserts.rolloversPerSecond MongoDB mongo.configServer.asserts.userPerSecond asserts.userPerSecond MongoDB mongo.configServer.asserts.warningPerSecond asserts.warningPerSecond MongoDB mongo.configServer.commands.countFailedPerSecond commands.countFailedPerSecond MongoDB mongo.configServer.commands.countPerSecond commands.countPerSecond MongoDB mongo.configServer.commands.createIndexesFailedPerSecond commands.createIndexesFailedPerSecond MongoDB mongo.configServer.commands.createIndexesPerSecond commands.createIndexesPerSecond MongoDB mongo.configServer.commands.deleteFailedPerSecond commands.deleteFailedPerSecond MongoDB mongo.configServer.commands.deletePerSecond commands.deletePerSecond MongoDB mongo.configServer.commands.evalFailedPerSecond commands.evalFailedPerSecond MongoDB mongo.configServer.commands.evalPerSecond commands.evalPerSecond MongoDB mongo.configServer.commands.findAndModifyFailedPerSecond commands.findAndModifyFailedPerSecond MongoDB mongo.configServer.commands.findAndModifyPerSecond commands.findAndModifyPerSecond MongoDB mongo.configServer.commands.insertFailedPerSecond commands.insertFailedPerSecond MongoDB mongo.configServer.commands.insertPerSecond commands.insertPerSecond MongoDB mongo.configServer.commands.updateFailedPerSecond commands.updateFailedPerSecond MongoDB mongo.configServer.commands.updatePerSecond commands.updatePerSecond MongoDB mongo.configServer.connections.available connections.available MongoDB mongo.configServer.connections.current connections.current MongoDB mongo.configServer.connections.totalCreated connections.totalCreated MongoDB mongo.configServer.cursor.openNoTimeout cursor.openNoTimeout MongoDB mongo.configServer.cursor.openPinned cursor.openPinned MongoDB mongo.configServer.cursor.openTotal cursor.openTotal MongoDB mongo.configServer.cursor.timedOutPerSecond cursor.timedOutPerSecond MongoDB mongo.configServer.document.deletedPerSecond document.deletedPerSecond MongoDB mongo.configServer.document.insertedPerSecond document.insertedPerSecond MongoDB mongo.configServer.document.returnedPerSecond document.returnedPerSecond MongoDB mongo.configServer.document.updatedPerSecond document.updatedPerSecond MongoDB mongo.configServer.dur.commits dur.commits MongoDB mongo.configServer.dur.commitsInWriteLock dur.commitsInWriteLock MongoDB mongo.configServer.dur.compression dur.compression MongoDB mongo.configServer.dur.earlyCommits dur.earlyCommits MongoDB mongo.configServer.dur.preparingInMilliseconds dur.preparingInMilliseconds MongoDB mongo.configServer.dur.remappingInMilliseconds dur.remappingInMilliseconds MongoDB mongo.configServer.dur.timeCollectedCommitsInMilliseconds dur.timeCollectedCommitsInMilliseconds MongoDB mongo.configServer.dur.writingDataFilesInMilliseconds dur.writingDataFilesInMilliseconds MongoDB mongo.configServer.dur.writingJournalInMilliseconds dur.writingJournalInMilliseconds MongoDB mongo.configServer.flush.averageInMilliseconds flush.averageInMilliseconds MongoDB mongo.configServer.flush.flushesDisk flush.flushesDisk MongoDB mongo.configServer.flush.lastInMilliseconds flush.lastInMilliseconds MongoDB mongo.configServer.flush.totalInMilliseconds flush.totalInMilliseconds MongoDB mongo.configServer.getlasterror.wtimeMillisPerSecond getlasterror.wtimeMillisPerSecond MongoDB mongo.configServer.getlasterror.wtimeoutsPerSecond getlasterror.wtimeoutsPerSecond MongoDB mongo.configServer.globallock.activeClientsReaders globallock.activeClientsReaders MongoDB mongo.configServer.globallock.activeClientsTotal globallock.activeClientsTotal MongoDB mongo.configServer.globallock.activeClientsWriters globallock.activeClientsWriters MongoDB mongo.configServer.globallock.currentQueueReaders globallock.currentQueueReaders MongoDB mongo.configServer.globallock.currentQueueTotal globallock.currentQueueTotal MongoDB mongo.configServer.globallock.currentQueueWriters globallock.currentQueueWriters MongoDB mongo.configServer.globallock.totalTime globallock.totaltime MongoDB mongo.configServer.locks.collectionAcquireExclusive locks.collectionAcquireExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentExclusive locks.collectionAcquireIntentExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentShared locks.collectionAcquireIntentShared MongoDB mongo.configServer.locks.collectionAcquireWaitCountExclusive locks.collectionAcquireWaitCountExclusive MongoDB mongo.configServer.locks.collectionTimeAcquiringMicrosExclusive locks.collectionTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseAcquireExclusive locks.databaseAcquireExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentExclusive locks.databaseAcquireIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentShared locks.databaseAcquireIntentShared MongoDB mongo.configServer.locks.databaseAcquireShared locks.databaseAcquireShared MongoDB mongo.configServer.locks.databaseAcquireWaitExclusive locks.databaseAcquireWaitExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentExclusive locks.databaseAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentShared locks.databaseAcquireWaitIntentShared MongoDB mongo.configServer.locks.databaseAcquireWaitShared locks.databaseAcquireWaitShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosExclusive locks.databaseTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentExclusive locks.databaseTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentShared locks.databaseTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosShared locks.databaseTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.globalAcquireExclusive locks.globalAcquireExclusive MongoDB mongo.configServer.locks.globalAcquireIntentExclusive locks.globalAcquireIntentExclusive MongoDB mongo.configServer.locks.globalAcquireIntentShared locks.globalAcquireIntentShared MongoDB mongo.configServer.locks.globalAcquireShared locks.globalAcquireShared MongoDB mongo.configServer.locks.globalAcquireWaitExclusive locks.globalAcquireWaitExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentExclusive locks.globalAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentShared locks.globalAcquireWaitIntentShared MongoDB mongo.configServer.locks.globalAcquireWaitShared locks.globalAcquireWaitShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosExclusive locks.globalTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentExclusive locks.globalTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentShared locks.globalTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosShared locks.globalTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.metadataAcquireExclusive locks.metadataAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireExclusive locks.oplogAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentExclusive locks.oplogAcquireIntentExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentShared locks.oplogAcquireIntentShared MongoDB mongo",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.94492,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "New Relic Integrations Metrics The following table contains the metrics we collect for our <em>infrastructure</em> integrations. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent"
      },
      "id": "603e8a8a64441f69a34e8841"
    },
    {
      "sections": [
        "Manage infrastructure data reporting"
      ],
      "title": "Manage infrastructure data reporting",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "7cd87ba8f7686e9233f4171021607d499bf6bc72",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/manage-infrastructure-data-reporting/",
      "published_at": "2021-05-05T03:38:21Z",
      "updated_at": "2021-03-16T07:33:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you use the infrastructure agent or on-host integrations that report data via the infrastructure agent, there are several ways to configure data reporting. Here are two common options for managing data reporting: Enable/disable process metrics Select specific attributes to report For other agent configuration options, see Configuration. For our infrastructure integrations, you can also change the frequency of data reporting: For on-host integrations: use a specific integration's interval configuration setting. For cloud integrations (AWS, Azure, Google Cloud): edit the polling frequency. For more about New Relic data management in general, see Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.77257,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Manage</em> <em>infrastructure</em> <em>data</em> reporting",
        "sections": "<em>Manage</em> <em>infrastructure</em> <em>data</em> reporting",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": " configuration options, see Configuration. For our <em>infrastructure</em> integrations, you can also change the frequency of <em>data</em> reporting: For on-host integrations: use a specific integration&#x27;s interval configuration setting. For cloud integrations (AWS, Azure, Google Cloud): edit the polling frequency. For more about New Relic <em>data</em> management in general, see <em>Manage</em> <em>data</em>."
      },
      "id": "603e775a196a67f470a83de4"
    }
  ],
  "/docs/infrastructure/manage-your-data/data-instrumentation/default-infrastructure-monitoring-data": [
    {
      "sections": [
        "On-host integrations metrics",
        "New Relic Integrations Metrics"
      ],
      "title": "On-host integrations metrics",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "fe96c0c4950380504b1a33c3ad861bcb17507cba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics/",
      "published_at": "2021-05-05T03:38:23Z",
      "updated_at": "2021-03-16T15:57:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Integrations Metrics The following table contains the metrics we collect for our infrastructure integrations. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent cpuPercent Agent host.cpuStealPercent cpuStealPercent Agent host.cpuSystemPercent cpuSystemPercent Agent host.cpuUserPercent cpuUserPercent Agent host.disk.avgQueueLen avgQueueLen Agent host.disk.avgReadQueueLen avgReadQueueLen Agent host.disk.avgWriteQueueLen avgWriteQueueLen Agent host.disk.currentQueueLen currentQueueLen Agent host.disk.freeBytes diskFreeBytes Agent host.disk.freePercent diskFreePercent Agent host.disk.inodesFree inodesFree Agent host.disk.inodesTotal inodesTotal Agent host.disk.inodesUsed inodesUsed Agent host.disk.inodesUsedPercent inodesUsedPercent Agent host.disk.readBytesPerSecond readBytesPerSecond Agent host.disk.readIoPerSecond readIoPerSecond Agent host.disk.readUtilizationPercent readUtilizationPercent Agent host.disk.readWriteBytesPerSecond readWriteBytesPerSecond Agent host.disk.totalBytes diskTotalBytes Agent host.disk.totalUtilizationPercent totalUtilizationPercent Agent host.disk.usedBytes diskUsedBytes Agent host.disk.usedPercent diskUsedPercent Agent host.disk.writeBytesPerSecond writeBytesPerSecond Agent host.disk.writeIoPerSecond writeIoPerSecond Agent host.disk.writeUtilizationPercent writeUtilizationPercent Agent host.diskFreeBytes diskFreeBytes Agent host.diskFreePercent diskFreePercent Agent host.diskReadsPerSecond diskReadsPerSecond Agent host.diskReadUtilizationPercent diskReadUtilizationPercent Agent host.diskTotalBytes diskTotalBytes Agent host.diskUsedBytes diskUsedBytes Agent host.diskUsedPercent diskUsedPercent Agent host.diskUtilizationPercent diskUtilizationPercent Agent host.diskWritesPerSecond diskWritesPerSecond Agent host.diskWriteUtilizationPercent diskWriteUtilizationPercent Agent host.loadAverageFifteenMinute loadAverageFifteenMinute Agent host.loadAverageFiveMinute loadAverageFiveMinute Agent host.loadAverageOneMinute loadAverageOneMinute Agent host.memoryFreeBytes memoryFreeBytes Agent host.memoryFreePercent memoryFreePercent Agent host.memoryTotalBytes memoryTotalBytes Agent host.memoryUsedBytes memoryUsedBytes Agent host.memoryUsedPercent memoryUsedPercent Agent host.net.receiveBytesPerSecond receiveBytesPerSecond Agent host.net.receiveDroppedPerSecond receiveDroppedPerSecond Agent host.net.receiveErrorsPerSecond receiveErrorsPerSecond Agent host.net.receivePacketsPerSecond receivePacketsPerSecond Agent host.net.transmitBytesPerSecond transmitBytesPerSecond Agent host.net.transmitDroppedPerSecond transmitDroppedPerSecond Agent host.net.transmitErrorsPerSecond transmitErrorsPerSecond Agent host.net.transmitPacketsPerSecond transmitPacketsPerSecond Agent host.process.cpuPercent cpuPercent Agent host.process.cpuSystemPercent cpuSystemPercent Agent host.process.cpuUserPercent cpuUserPercent Agent host.process.fileDescriptorCount fileDescriptorCount Agent host.process.ioReadBytesPerSecond ioReadBytesPerSecond Agent host.process.ioReadCountPerSecond ioReadCountPerSecond Agent host.process.ioTotalReadBytes ioTotalReadBytes Agent host.process.ioTotalReadCount ioTotalReadCount Agent host.process.ioTotalWriteBytes ioTotalWriteBytes Agent host.process.ioTotalWriteCount ioTotalWriteCount Agent host.process.ioWriteBytesPerSecond ioWriteBytesPerSecond Agent host.process.ioWriteCountPerSecond ioWriteCountPerSecond Agent host.process.memoryResidentSizeBytes memoryResidentSizeBytes Agent host.process.memoryVirtualSizeBytes memoryVirtualSizeBytes Agent host.process.threadCount threadCount Agent host.swapFreeBytes swapFreeBytes Agent host.swapTotalBytes swapTotalBytes Agent host.swapUsedBytes swapUsedBytes Apache apache.server.busyWorkers server.busyWorkers Apache apache.server.idleWorkers server.idleWorkers Apache apache.server.net.bytesPerSecond net.bytesPerSecond Apache apache.server.net.requestsPerSecond net.requestsPerSecond Apache apache.server.scoreboard.closingWorkers server.scoreboard.closingWorkers Apache apache.server.scoreboard.dnsLookupWorkers server.scoreboard.dnsLookupWorkers Apache apache.server.scoreboard.finishingWorkers server.scoreboard.finishingWorkers Apache apache.server.scoreboard.idleCleanupWorkers server.scoreboard.idleCleanupWorkers Apache apache.server.scoreboard.keepAliveWorkers server.scoreboard.keepAliveWorkers Apache apache.server.scoreboard.loggingWorkers server.scoreboard.loggingWorkers Apache apache.server.scoreboard.readingWorkers server.scoreboard.readingWorkers Apache apache.server.scoreboard.startingWorkers server.scoreboard.startingWorkers Apache apache.server.scoreboard.totalWorkers server.scoreboard.totalWorkers Apache apache.server.scoreboard.writingWorkers server.scoreboard.writingWorkers Cassandra cassandra.node.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.node.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.node.client.connectedNativeClients client.connectedNativeClients Cassandra cassandra.node.commitLogCompletedTasksPerSecond db.commitLogCompletedTasksPerSecond Cassandra cassandra.node.commitLogPendingTasks db.commitLogPendindTasks Cassandra cassandra.node.commitLogTotalSizeBytes db.commitLogTotalSizeBytes Cassandra cassandra.node.droppedBatchRemoveMessagesPerSecond db.droppedBatchRemoveMessagesPerSecond Cassandra cassandra.node.droppedBatchStoreMessagesPerSecond db.droppedBatchStoreMessagesPerSecond Cassandra cassandra.node.droppedCounterMutationMessagesPerSecond db.droppedCounterMutationMessagesPerSecond Cassandra cassandra.node.droppedHintMessagesPerSecond db.droppedHintMessagesPerSecond Cassandra cassandra.node.droppedMutationMessagesPerSecond db.droppedMutationMessagesPerSecond Cassandra cassandra.node.droppedPagedRangeMessagesPerSecond db.droppedPagedRangeMessagesPerSecond Cassandra cassandra.node.droppedRangeSliceMessagesPerSecond db.droppedRangeSliceMessagesPerSecond Cassandra cassandra.node.droppedReadMessagesPerSecond db.droppedReadMessagesPerSecond Cassandra cassandra.node.droppedReadRepairMessagesPerSecond db.droppedReadRepairMessagesPerSecond Cassandra cassandra.node.droppedRequestResponseMessagesPerSecond db.droppedRequestResponseMessagesPerSecond Cassandra cassandra.node.droppedTraceMessagesPerSecond db.droppedTraceMessagesPerSecond Cassandra cassandra.node.keyCacheCapacityBytes db.keyCacheCapacityBytes Cassandra cassandra.node.keyCacheHitRate db.keyCacheHitRate Cassandra cassandra.node.keyCacheHitsPerSecond db.keyCacheHitsPerSecond Cassandra cassandra.node.keyCacheRequestsPerSecond db.keyCacheRequestsPerSecond Cassandra cassandra.node.keyCacheSizeBytes db.keyCacheSizeBytes Cassandra cassandra.node.liveSsTableCount db.liveSSTableCount Cassandra cassandra.node.loadBytes db.loadBytes Cassandra cassandra.node.query.casReadRequestsPerSecond query.CASReadRequestsPerSecond Cassandra cassandra.node.query.casWriteRequestsPerSecond query.CASWriteRequestsPerSecond Cassandra cassandra.node.query.rangeSliceRequestsPerSecond query.rangeSliceRequestsPerSecond Cassandra cassandra.node.query.rangeSliceTimeoutsPerSecond query.rangeSliceTimeoutsPerSecond Cassandra cassandra.node.query.rangeSliceUnavailablesPerSecond query.rangeSliceUnavailablesPerSecond Cassandra cassandra.node.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.node.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.node.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.node.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.node.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.node.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.node.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.node.query.readTimeoutsPerSecond query.readTimeoutsPerSecond Cassandra cassandra.node.query.readUnavailablesPerSecond query.readUnavailablesPerSecond Cassandra cassandra.node.query.viewWriteRequestsPerSecond query.viewWriteRequestsPerSecond Cassandra cassandra.node.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.node.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.node.query.writeTimeoutsPerSecond query.writeTimeoutsPerSecond Cassandra cassandra.node.query.writeUnavailablesPerSecond query.writeUnavailablesPerSecond Cassandra cassandra.node.rowCacheCapacityBytes db.rowCacheCapacityBytes Cassandra cassandra.node.rowCacheHitRate db.rowCacheHitRate Cassandra cassandra.node.rowCacheHitsPerSecond db.rowCacheHitsPerSecond Cassandra cassandra.node.rowCacheRequestsPerSecond db.rowCacheRequestsPerSecond Cassandra cassandra.node.rowCacheSizeBytes db.rowCacheSizeBytes Cassandra cassandra.node.storage.exceptionCount storage.exceptionCount Cassandra cassandra.node.threadPool.antiEntropyStage.activeTasks db.threadpool.internalAntiEntropyStageActiveTasks Cassandra cassandra.node.threadPool.antiEntropyStage.completedTasks db.threadpool.internalAntiEntropyStageCompletedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.currentlyBlockedTasks db.threadpool.internalAntiEntropyStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.pendingTasks db.threadpool.internalAntiEntropyStagePendingTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.activeTasks db.threadpool.internalCacheCleanupExecutorActiveTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.completedTasks db.threadpool.internalCacheCleanupExecutorCompletedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.currentlyBlockedTasks db.threadpool.internalCacheCleanupExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.pendingTasks db.threadpool.internalCacheCleanupExecutorPendingTasks Cassandra cassandra.node.threadPool.compactionExecutor.activeTasks db.threadpool.internalCompactionExecutorActiveTasks Cassandra cassandra.node.threadPool.compactionExecutor.completedTasks db.threadpool.internalCompactionExecutorCompletedTasks Cassandra cassandra.node.threadPool.compactionExecutor.currentlyBlockedTasks db.threadpool.internalCompactionExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.compactionExecutor.pendingTasks db.threadpool.internalCompactionExecutorPendingTasks Cassandra cassandra.node.threadPool.counterMutationStage.activeTasks db.threadpool.requestCounterMutationStageActiveTasks Cassandra cassandra.node.threadPool.counterMutationStage.completedTasks db.threadpool.requestCounterMutationStageCompletedTasks Cassandra cassandra.node.threadPool.counterMutationStage.currentlyBlockedTasks db.threadpool.requestCounterMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.counterMutationStage.pendingTasks db.threadpool.requestCounterMutationStagePendingTasks Cassandra cassandra.node.threadPool.gossipStage.activeTasks db.threadpool.internalGossipStageActiveTasks Cassandra cassandra.node.threadPool.gossipStage.completedTasks db.threadpool.internalGossipStageCompletedTasks Cassandra cassandra.node.threadPool.gossipStage.currentlyBlockedTasks db.threadpool.internalGossipStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.gossipStage.pendingTasks db.threadpool.internalGossipStagePendingTasks Cassandra cassandra.node.threadPool.hintsDispatcher.activeTasks db.threadpool.internalHintsDispatcherActiveTasks Cassandra cassandra.node.threadPool.hintsDispatcher.completedTasks db.threadpool.internalHintsDispatcherCompletedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.currentlyBlockedTasks db.threadpool.internalHintsDispatcherCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.pendingTasks db.threadpool.internalHintsDispatcherPendingTasks Cassandra cassandra.node.threadPool.internalResponseStage.activeTasks db.threadpool.internalInternalResponseStageActiveTasks Cassandra cassandra.node.threadPool.internalResponseStage.completedTasks db.threadpool.internalInternalResponseStageCompletedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pCurrentlyBlockedTasks db.threadpool.internalInternalResponseStagePCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pendingTasks db.threadpool.internalInternalResponseStagePendingTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.activeTasks db.threadpool.internalMemtableFlushWriterActiveTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.completedTasks db.threadpool.internalMemtableFlushWriterCompletedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.currentlyBlockedTasks db.threadpool.internalMemtableFlushWriterCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.pendingTasks db.threadpool.internalMemtableFlushWriterPendingTasks Cassandra cassandra.node.threadPool.memtablePostFlush.activeTasks db.threadpool.internalMemtablePostFlushActiveTasks Cassandra cassandra.node.threadPool.memtablePostFlush.completedTasks db.threadpool.internalMemtablePostFlushCompletedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.currentlyBlockedTasks db.threadpool.internalMemtablePostFlushCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.pendingTasks db.threadpool.internalMemtablePostFlushPendingTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.activeTasks db.threadpool.internalMemtableReclaimMemoryActiveTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.completedTasks db.threadpool.internalMemtableReclaimMemoryCompletedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.currentlyBlockedTasks db.threadpool.internalMemtableReclaimMemoryCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.pendingTasks db.threadpool.internalMemtableReclaimMemoryPendingTasks Cassandra cassandra.node.threadPool.migrationStage.activeTasks db.threadpool.internalMigrationStageActiveTasks Cassandra cassandra.node.threadPool.migrationStage.completedTasks db.threadpool.internalMigrationStageCompletedTasks Cassandra cassandra.node.threadPool.migrationStage.currentlyBlockedTasks db.threadpool.internalMigrationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.migrationStage.pendingTasks db.threadpool.internalMigrationStagePendingTasks Cassandra cassandra.node.threadPool.miscStage.activeTasks db.threadpool.internalMiscStageActiveTasks Cassandra cassandra.node.threadPool.miscStage.completedTasks db.threadpool.internalMiscStageCompletedTasks Cassandra cassandra.node.threadPool.miscStage.currentlyBlockedTasks db.threadpool.internalMiscStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.miscStage.pendingTasks db.threadpool.internalMiscStagePendingTasks Cassandra cassandra.node.threadPool.mutationStage.activeTasks db.threadpool.requestMutationStageActiveTasks Cassandra cassandra.node.threadPool.mutationStage.completedTasks db.threadpool.requestMutationStageCompletedTasks Cassandra cassandra.node.threadPool.mutationStage.currentlyBlockedTasks db.threadpool.requestMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.mutationStage.pendingTasks db.threadpool.requestMutationStagePendingTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.activeTasks db.threadpool.internalPendingRangeCalculatorActiveTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.completedTasks db.threadpool.internalPendingRangeCalculatorCompletedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.currentlyBlockedTasks db.threadpool.internalPendingRangeCalculatorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.pendingTasks db.threadpool.internalPendingRangeCalculatorPendingTasks Cassandra cassandra.node.threadPool.readRepairStage.activeTasks db.threadpool.requestReadRepairStageActiveTasks Cassandra cassandra.node.threadPool.readRepairStage.completedTasks db.threadpool.requestReadRepairStageCompletedTasks Cassandra cassandra.node.threadPool.readRepairStage.currentlyBlockedTasks db.threadpool.requestReadRepairStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readRepairStage.pendingTasks db.threadpool.requestReadRepairStagePendingTasks Cassandra cassandra.node.threadPool.readStage.activeTasks db.threadpool.requestReadStageActiveTasks Cassandra cassandra.node.threadPool.readStage.completedTasks db.threadpool.requestReadStageCompletedTasks Cassandra cassandra.node.threadPool.readStage.currentlyBlockedTasks db.threadpool.requestReadStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readStage.pendingTasks db.threadpool.requestReadStagePendingTasks Cassandra cassandra.node.threadPool.requestResponseStage.activeTasks db.threadpool.requestRequestResponseStageActiveTasks Cassandra cassandra.node.threadPool.requestResponseStage.completedTasks db.threadpool.requestRequestResponseStageCompletedTasks Cassandra cassandra.node.threadPool.requestResponseStage.currentlyBlockedTasks db.threadpool.requestRequestResponseStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.requestResponseStage.pendingTasks db.threadpool.requestRequestResponseStagePendingTasks Cassandra cassandra.node.threadPool.sampler.activeTasks db.threadpool.internalSamplerActiveTasks Cassandra cassandra.node.threadPool.sampler.completedTasks db.threadpool.internalSamplerCompletedTasks Cassandra cassandra.node.threadPool.sampler.currentlyBlockedTasks db.threadpool.internalSamplerCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.sampler.pendingTasks db.threadpool.internalSamplerPendingTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.activeTasks db.threadpool.internalSecondaryIndexManagementActiveTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.completedTasks db.threadpool.internalSecondaryIndexManagementCompletedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.currentlyBlockedTasks db.threadpool.internalSecondaryIndexManagementCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.pendingTasks db.threadpool.internalSecondaryIndexManagementPendingTasks Cassandra cassandra.node.threadPool.validationExecutor.activeTasks db.threadpool.internalValidationExecutorActiveTasks Cassandra cassandra.node.threadPool.validationExecutor.completedTasks db.threadpool.internalValidationExecutorCompletedTasks Cassandra cassandra.node.threadPool.validationExecutor.currentlyBlockedTasks db.threadpool.internalValidationExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.validationExecutor.pendingTasks db.threadpool.internalValidationExecutorPendingTasks Cassandra cassandra.node.threadPool.viewMutationStage.activeTasks db.threadpool.requestViewMutationStageActiveTasks Cassandra cassandra.node.threadPool.viewMutationStage.completedTasks db.threadpool.requestViewMutationStageCompletedTasks Cassandra cassandra.node.threadPool.viewMutationStage.currentlyBlockedTasks db.threadpool.requestViewMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.viewMutationStage.pendingTasks db.threadpool.requestViewMutationStagePendingTasks Cassandra cassandra.node.totalHintsInProgress db.totalHintsInProgress Cassandra cassandra.node.totalHintsPerSecond db.totalHintsPerSecond Cassandra cassandra.columnFamily.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.columnFamily.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.columnFamily.bloomFilterFalseRatio db.bloomFilterFalseRatio Cassandra cassandra.columnFamily.liveDiskSpaceUsedBytes db.liveDiskSpaceUsedBytes Cassandra cassandra.columnFamily.liveSsTableCount db.liveSSTableCount Cassandra cassandra.columnFamily.maxRowSize db.maxRowSize Cassandra cassandra.columnFamily.meanRowSize db.meanRowSize Cassandra cassandra.columnFamily.memtableLiveDataSize db.memtableLiveDataSize Cassandra cassandra.columnFamily.minRowSize db.minRowSize Cassandra cassandra.columnFamily.pendingCompactions db.pendingCompactions Cassandra cassandra.columnFamily.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.columnFamily.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.columnFamily.speculativeRetries db.speculativeRetries Cassandra cassandra.columnFamily.ssTablesPerRead50ThPercentileMilliseconds db.SSTablesPerRead50thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead75ThPercentileMilliseconds db.SSTablesPerRead75thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead95ThPercentileMilliseconds db.SSTablesPerRead95thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead98ThPercentileMilliseconds db.SSTablesPerRead98thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead999ThPercentileMilliseconds db.SSTablesPerRead999thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead99ThPercentileMilliseconds db.SSTablesPerRead99thPercentileMilliseconds Cassandra cassandra.columnFamily.tombstoneScannedHistogram50ThPercentile db.tombstoneScannedHistogram50thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram75ThPercentile db.tombstoneScannedHistogram75thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram95ThPercentile db.tombstoneScannedHistogram95thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram98ThPercentile db.tombstoneScannedHistogram98thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram999ThPercentile db.tombstoneScannedHistogram999thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram99ThPercentile db.tombstoneScannedHistogram99thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogramCount db.tombstoneScannedHistogramCount Consul consul.datacenter.catalog.criticalNodes catalog.criticalNodes Consul consul.datacenter.catalog.passingNodes catalog.passingNodes Consul consul.datacenter.catalog.registeredNodes catalog.registeredNodes Consul consul.datacenter.catalog.upNodes catalog.upNodes Consul consul.datacenter.catalog.warningNodes catalog.warningNodes Consul consul.datacenter.cluster.flaps cluster.flaps Consul consul.datacenter.cluster.suspects cluster.suspects Consul consul.datacenter.raft.commitTime raft.commitTimes Consul consul.datacenter.raft.commitTimeAvgInMilliseconds raft.commitTimeAvgInMilliseconds Consul consul.datacenter.raft.commitTimeMaxInMilliseconds raft.commitTimeMaxInMilliseconds Consul consul.datacenter.raft.completedLeaderElections raft.completedLeaderElections Consul consul.datacenter.raft.initiatedLeaderElections raft.initiatedLeaderElections Consul consul.datacenter.raft.lastContactAvgInMilliseconds raft.lastContactAvgInMilliseconds Consul consul.datacenter.raft.lastContactMaxInMilliseconds raft.lastContactMaxInMilliseconds Consul consul.datacenter.raft.lastContacts raft.lastContacts Consul consul.datacenter.raft.logDispatchAvgInMilliseconds raft.logDispatchAvgInMilliseconds Consul consul.datacenter.raft.logDispatches raft.logDispatches Consul consul.datacenter.raft.logDispatchMaxInMilliseconds raft.logDispatchMaxInMilliseconds Consul consul.datacenter.raft.txns raft.txns Consul consul.agent.aclCacheHitPerSecond agent.aclCacheHit Consul consul.agent.aclCacheMissPerSecond agent.aclCacheMiss Consul consul.agent.client.rpcFailed client.rpcFailed Consul consul.agent.client.rpcLoad client.rpcLoad Consul consul.agent.kvStores agent.kvStoress Consul consul.agent.kvStoresAvgInMilliseconds agent.kvStoresAvgInMilliseconds Consul consul.agent.kvStoresMaxInMilliseconds agent.kvStoresMaxInMilliseconds Consul consul.agent.net.agent.maxLatencyInMilliseconds net.agent.maxLatencyInMilliseconds Consul consul.agent.net.medianLatencyInMilliseconds net.agent.medianLatencyInMilliseconds Consul consul.agent.net.minLatencyInMilliseconds net.agent.minLatencyInMilliseconds Consul consul.agent.net.p25LatencyInMilliseconds net.agent.p25LatencyInMilliseconds Consul consul.agent.net.p75LatencyInMilliseconds net.agent.p75LatencyInMilliseconds Consul consul.agent.net.p90LatencyInMilliseconds net.agent.p90LatencyInMilliseconds Consul consul.agent.net.p95LatencyInMilliseconds net.agent.p95LatencyInMilliseconds Consul consul.agent.net.p99LatencyInMilliseconds net.agent.p99LatencyInMilliseconds Consul consul.agent.peers agent.peers Consul consul.agent.runtime.allocations runtime.allocations Consul consul.agent.runtime.allocationsInBytes runtime.allocationsInBytes Consul consul.agent.runtime.frees runtime.frees Consul consul.agent.runtime.gcCycles runtime.gcCycles Consul consul.agent.runtime.gcPauseInMilliseconds runtime.gcPauseInMilliseconds Consul consul.agent.runtime.goroutines runtime.goroutines Consul consul.agent.runtime.heapObjects runtime.heapObjects Consul consul.agent.runtime.virtualAddressSpaceInBytes runtime.virtualAddressSpaceInBytes Consul consul.agent.staleQueries agent.staleQueries Consul consul.agent.txnAvgInMilliseconds agent.txnAvgInMilliseconds Consul consul.agent.txnMaxInMilliseconds agent.txnMaxInMilliseconds Consul consul.agent.txns agent.txns Couchbase couchbase.bucket.activeItemsEnteringDiskQueuePerSecond bucket.activeItemsEnteringDiskQueuePerSecond Couchbase couchbase.bucket.activeItemsInMemory bucket.activeItemsInMemory Couchbase couchbase.bucket.activeResidentItemsRatio bucket.activeResidentItemsRatio Couchbase couchbase.bucket.averageDiskCommitTimeInMilliseconds bucket.averageDiskCommitTimeInMilliseconds Couchbase couchbase.bucket.averageDiskUpdateTimeInMilliseconds bucket.averageDiskUpdateTimeInMilliseconds Couchbase couchbase.bucket.cacheMisses bucket.cacheMisses Couchbase couchbase.bucket.cacheMissRatio bucket.cacheMissRatio Couchbase couchbase.bucket.casHits bucket.casHits Couchbase couchbase.bucket.casMisses bucket.casMisses Couchbase couchbase.bucket.couchDocsFragmentationPercent bucket.couchDocsFragmentationPercent Couchbase couchbase.bucket.currentConnections bucket.currentConnections Couchbase couchbase.bucket.dataUsedInBytes bucket.dataUsedInBytes Couchbase couchbase.bucket.decrementHitsPerSecond bucket.decrementHitsPerSecond Couchbase couchbase.bucket.decrementMissesPerSecond bucket.decrementMissesPerSecond Couchbase couchbase.bucket.deleteHitsPerSecond bucket.deleteHitsPerSecond Couchbase couchbase.bucket.deleteMissesPerSecond bucket.deleteMissesPerSecond Couchbase couchbase.bucket.diskCreateOperationsPerSecond bucket.diskCreateOperationsPerSecond Couchbase couchbase.bucket.diskFetchesPerSecond bucket.diskFetchesPerSecond Couchbase couchbase.bucket.diskReadsPerSecond bucket.diskReadsPerSecond Couchbase couchbase.bucket.diskUpdateOperationsPerSecond bucket.diskUpdateOperationsPerSecond Couchbase couchbase.bucket.diskUsedInBytes bucket.diskUsedInBytes Couchbase couchbase.bucket.diskWriteQueue bucket.diskWriteQueue Couchbase couchbase.bucket.drainedItemsInQueue bucket.drainedItemsInQueue Couchbase couchbase.bucket.drainedItemsOnDiskQueue bucket.drainedItemsOnDiskQueue Couchbase couchbase.bucket.drainedPendingItemsInQueue bucket.drainedPendingItemsInQueue Couchbase couchbase.bucket.ejectionsPerSecond bucket.ejectionsPerSecond Couchbase couchbase.bucket.evictionsPerSecond bucket.evictionsPerSecond Couchbase couchbase.bucket.getHitsPerSecond bucket.getHitsPerSecond Couchbase couchbase.bucket.getMissesPerSecond bucket.getMissesPerSecond Couchbase couchbase.bucket.hitRatio bucket.hitRatio Couchbase couchbase.bucket.incrementHitsPerSecond bucket.incrementHitsPerSecond Couchbase couchbase.bucket.incrementMissesPerSecond bucket.incrementMissesPerSecond Couchbase couchbase.bucket.itemCount bucket.itemCount Couchbase couchbase.bucket.itemsBeingWritten bucket.itemsBeingWritten Couchbase couchbase.bucket.itemsEjectedFromMemoryToDisk bucket.itemsEjectedFromMemoryToDisk Couchbase couchbase.bucket.itemsOnDiskQueue bucket.itemsOnDiskQueue Couchbase couchbase.bucket.itemsQueuedForStorage bucket.itemsQueuedForStorage Couchbase couchbase.bucket.maximumMemoryUsage bucket.maximumMemoryUsage Couchbase couchbase.bucket.memoryHighWaterMarkInBytes bucket.memoryHighWaterMarkInBytes Couchbase couchbase.bucket.memoryLowWaterMarkInBytes bucket.memoryLowWaterMarkInBytes Couchbase couchbase.bucket.memoryUsedInBytes bucket.memoryUsedInBytes Couchbase couchbase.bucket.metadataInRamInBytes bucket.metadataInRAMInBytes Couchbase couchbase.bucket.missesPerSecond bucket.missesPerSecond Couchbase couchbase.bucket.outOfMemoryErrorsPerSecond bucket.outOfMemoryErrorsPerSecond Couchbase couchbase.bucket.overheadInBytes bucket.overheadInBytes Couchbase couchbase.bucket.pendingItemsInDiskQueue bucket.pendingItemsInDiskQueue Couchbase couchbase.bucket.pendingResidentItemsRatio bucket.pendingResidentItemsRatio Couchbase couchbase.bucket.quotaUtilization bucket.quotaUtilization Couchbase couchbase.bucket.readOperationsPerSecond bucket.readOperationsPerSecond Couchbase couchbase.bucket.readRatePerSecond bucket.readRatePerSecond Couchbase couchbase.bucket.recoverableOutOfMemoryCount bucket.recoverableOutOfMemoryCount Couchbase couchbase.bucket.replicaIndex bucket.replicaIndex Couchbase couchbase.bucket.replicaNumber bucket.replicaNumber Couchbase couchbase.bucket.replicaResidentItemsRatio bucket.replicaResidentItemsRatio Couchbase couchbase.bucket.residentItemsRatio bucket.residentItemsRatio Couchbase couchbase.bucket.temporaryOutOfMemoryErrorsPerSecond bucket.temporaryOutOfMemoryErrorsPerSecond Couchbase couchbase.bucket.threadsNumber bucket.threadsNumber Couchbase couchbase.bucket.totalItems bucket.totalItems Couchbase couchbase.bucket.totalOperationsPerSecond bucket.totalOperationsPerSecond Couchbase couchbase.bucket.viewFragmentationPercent bucket.viewFragmentationPercent Couchbase couchbase.bucket.writeOperationsPerSecond bucket.writeOperationsPerSecond Couchbase couchbase.bucket.writeRatePerSecond bucket.writeRatePerSecond Couchbase couchbase.cluster.autoFailoverCount cluster.autoFailoverCount Couchbase couchbase.cluster.autoFailoverEnabled cluster.autoFailoverEnabled Couchbase couchbase.cluster.databaseFragmentationThreshold cluster.databaseFragmentationThreshold Couchbase couchbase.cluster.diskFreeInBytes cluster.diskFreeInBytes Couchbase couchbase.cluster.diskQuotaTotalInBytes cluster.diskQuotaTotalInBytes Couchbase couchbase.cluster.diskTotalInBytes cluster.diskTotalInBytes Couchbase couchbase.cluster.diskUsedByDataInBytes cluster.diskUsedByDataInBytes Couchbase couchbase.cluster.diskUsedInBytes cluster.diskUsedInBytes Couchbase couchbase.cluster.indexFragmentationThreshold cluster.indexFragmentationThreshold Couchbase couchbase.cluster.maximumBucketCount cluster.maximumBucketCount Couchbase couchbase.cluster.memoryQuotaTotalInBytes cluster.memoryQuotaTotalInBytes Couchbase couchbase.cluster.memoryQuotaTotalPerNodeInBytes cluster.memoryQuotaTotalPerNodeInBytes Couchbase couchbase.cluster.memoryQuotaUsedInBytes cluster.memoryQuotaUsedInBytes Couchbase couchbase.cluster.memoryQuotaUsedPerNodeInBytes cluster.memoryQuotaUsedPerNodeInBytes Couchbase couchbase.cluster.memoryTotalInBytes cluster.memoryTotalInBytes Couchbase couchbase.cluster.memoryUsedByDataInBytes cluster.memoryUsedByDataInBytes Couchbase couchbase.cluster.memoryUsedInBytes cluster.memoryUsedInBytes Couchbase couchbase.cluster.viewFragmentationThreshold cluster.viewFragmentationThreshold Couchbase couchbase.node.backgroundFetches node.backgroundFetches Couchbase couchbase.node.cmdGet node.cmdGet Couchbase couchbase.node.couchDocsActualDiskSizeInBytes node.couchDocsActualDiskSizeInBytes Couchbase couchbase.node.couchDocsDataSizeInBytes node.couchDocsDataSizeInBytes Couchbase couchbase.node.couchSpatialDataSizeInBytes node.couchSpatialDataSizeInBytes Couchbase couchbase.node.couchSpatialDiskSizeInBytes node.couchSpatialDiskSizeInBytes Couchbase couchbase.node.couchViewsActualDiskSizeInBytes node.couchViewsActualDiskSizeInBytes Couchbase couchbase.node.couchViewsDataSizeInBytes node.couchViewsDataSizeInBytes Couchbase couchbase.node.cpuUtilization node.cpuUtilization Couchbase couchbase.node.currentItems node.currentItems Couchbase couchbase.node.currentItemsTotal node.currentItemsTotal Couchbase couchbase.node.getHits node.getHits Couchbase couchbase.node.memoryFreeInBytes node.memoryFreeInBytes Couchbase couchbase.node.memoryTotalInBytes node.memoryTotalInBytes Couchbase couchbase.node.memoryUsedInBytes node.memoryUsedInBytes Couchbase couchbase.node.ops node.ops Couchbase couchbase.node.swapTotalInBytes node.swapTotalInBytes Couchbase couchbase.node.swapUsedInBytes node.swapUsedInBytes Couchbase couchbase.node.uptimeInMilliseconds node.uptimeInMilliseconds Couchbase couchbase.node.vbucketActiveNonResidentItems node.vbucketActiveNonResidentItems Couchbase couchbase.node.vbucketInMemoryItems node.vbucketInMemoryItems Couchbase couchbase.queryengine.activeRequests queryengine.activeRequests Couchbase couchbase.queryengine.averageRequestTimeInMilliseconds queryengine.averageRequestTimeInMilliseconds Couchbase couchbase.queryengine.completedLimit queryengine.completedLimit Couchbase couchbase.queryengine.completedRequests queryengine.completedRequests Couchbase couchbase.queryengine.completedThresholdInMilliseconds queryengine.completedThresholdInMilliseconds Couchbase couchbase.queryengine.cores queryengine.cores Couchbase couchbase.queryengine.garbageCollectionNumber queryengine.garbageCollectionNumber Couchbase couchbase.queryengine.garbageCollectionPaused queryengine.garbageCollectionPaused Couchbase couchbase.queryengine.garbageCollectionTimePausedInMilliseconds queryengine.garbageCollectionTimePausedInMilliseconds Couchbase couchbase.queryengine.medianRequestTimeInMilliseconds queryengine.medianRequestTimeInMilliseconds Couchbase couchbase.queryengine.preparedStatementUtilization queryengine.preparedStatementUtilization Couchbase couchbase.queryengine.requestsLast15MinutesPerSecond queryengine.requestsLast15MinutesPerSecond Couchbase couchbase.queryengine.requestsLast1MinutesPerSecond queryengine.requestsLast1MinutesPerSecond Couchbase couchbase.queryengine.requestsLast5MinutesPerSecond queryengine.requestsLast5MinutesPerSecond Couchbase couchbase.queryengine.requestTime80thPercentileInMilliseconds queryengine.requestTime80thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime95thPercentileInMilliseconds queryengine.requestTime95thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime99thPercentileInMilliseconds queryengine.requestTime99thPercentileInMilliseconds Couchbase couchbase.queryengine.systemCpuUtilization queryengine.systemCPUUtilization Couchbase couchbase.queryengine.systemMemoryInBytes queryengine.systemMemoryInBytes Couchbase couchbase.queryengine.totalMemoryInBytes queryengine.totalMemoryInBytes Couchbase couchbase.queryengine.totalThreads queryengine.totalThreads Couchbase couchbase.queryengine.uptimeInMilliseconds queryengine.uptimeInMilliseconds Couchbase couchbase.queryengine.usedMemoryInBytes queryengine.usedMemoryInBytes Couchbase couchbase.queryengine.userCpuUtilization queryengine.userCPUUtilization Docker docker.container.cpuKernelPercent cpuKernelPercent Docker docker.container.cpuLimitCores cpuLimitCores Docker docker.container.cpuPercent cpuPercent Docker docker.container.cpuThrottlePeriods cpuThrottlePeriods Docker docker.container.cpuThrottleTimeMs cpuThrottleTimeMs Docker docker.container.cpuUsedCores cpuUsedCores Docker docker.container.cpuUsedCoresPercent cpuUsedCoresPercent Docker docker.container.cpuUserPercent cpuUserPercent Docker docker.container.ioReadBytesPerSecond ioReadBytesPerSecond Docker docker.container.ioReadCountPerSecond ioReadCountPerSecond Docker docker.container.ioTotalBytes ioTotalBytes Docker docker.container.ioTotalReadBytes ioTotalReadBytes Docker docker.container.ioTotalReadCount ioTotalReadCount Docker docker.container.ioTotalWriteBytes ioTotalWriteBytes Docker docker.container.ioTotalWriteCount ioTotalWriteCount Docker docker.container.ioWriteBytesPerSecond ioWriteBytesPerSecond Docker docker.container.ioWriteCountPerSecond ioWriteCountPerSecond Docker docker.container.memoryCacheBytes memoryCacheBytes Docker docker.container.memoryResidentSizeBytes memoryResidentSizeBytes Docker docker.container.memorySizeLimitBytes memorySizeLimitBytes Docker docker.container.memoryUsageBytes memoryUsageBytes Docker docker.container.memoryUsageLimitPercent memoryUsageLimitPercent Docker docker.container.networkRxBytes networkRxBytes Docker docker.container.networkRxBytesPerSecond networkRxBytesPerSecond Docker docker.container.networkRxDropped networkRxDropped Docker docker.container.networkRxDroppedPerSecond networkRxDroppedPerSecond Docker docker.container.networkRxErrors networkRxErrors Docker docker.container.networkRxErrorsPerSecond networkRxErrorsPerSecond Docker docker.container.networkRxPackets networkRxPackets Docker docker.container.networkRxPacketsPerSecond networkRxPacketsPerSecond Docker docker.container.networkTxBytes networkTxBytes Docker docker.container.networkTxBytesPerSecond networkTxBytesPerSecond Docker docker.container.networkTxDropped networkTxDropped Docker docker.container.networkTxDroppedPerSecond networkTxDroppedPerSecond Docker docker.container.networkTxErrors networkTxErrors Docker docker.container.networkTxErrorsPerSecond networkTxErrorsPerSecond Docker docker.container.networkTxPackets networkTxPackets Docker docker.container.networkTxPacketsPerSecond networkTxPacketsPerSecond Docker docker.container.pids pids Docker docker.container.processCount processCount Docker docker.container.processCountLimit processCountLimit Docker docker.container.restartCount restartCount Docker docker.container.threadCount threadCount Docker docker.container.threadCountLimit threadCountLimit ElasticSearch elasticsearch.cluster.dataNodes cluster.dataNodes ElasticSearch elasticsearch.cluster.nodes cluster.nodes ElasticSearch elasticsearch.cluster.shards.active shards.active ElasticSearch elasticsearch.cluster.shards.initializing shards.initializing ElasticSearch elasticsearch.cluster.shards.primaryActive shards.primaryActive ElasticSearch elasticsearch.cluster.shards.relocating shards.relocating ElasticSearch elasticsearch.cluster.shards.unassigned shards.unassigned ElasticSearch elasticsearch.cluster.tempData temp-data ElasticSearch elasticsearch.index.docs index.docs ElasticSearch elasticsearch.index.docsDeleted index.docsDeleted ElasticSearch elasticsearch.index.primaryShards index.primaryShards ElasticSearch elasticsearch.index.primaryStoreSizeInBytes index.primaryStoreSizeInBytes ElasticSearch elasticsearch.index.replicaShards index.replicaShards ElasticSearch elasticsearch.index.rollup.docsCount primaries.docsnumber ElasticSearch elasticsearch.index.rollup.docsDeleted primaries.docsDeleted ElasticSearch elasticsearch.index.rollup.flushTotal primaries.flushesTotal ElasticSearch elasticsearch.index.rollup.flushTotalTimeInMilliseconds primaries.flushTotalTimeInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsExist primaries.get.documentsExist ElasticSearch elasticsearch.index.rollup.get.documentsExistInMilliseconds primaries.get.documentsExistInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsMissing primaries.get.documentsMissing ElasticSearch elasticsearch.index.rollup.get.documentsMissingInMilliseconds primaries.get.documentsMissingInMilliseconds ElasticSearch elasticsearch.index.rollup.get.requests primaries.get.requests ElasticSearch elasticsearch.index.rollup.get.requestsCurrent primaries.get.requestsCurrent ElasticSearch elasticsearch.index.rollup.get.requestsInMilliseconds primaries.get.requestsInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeleted primaries.index.docsCurrentlyDeleted ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeletedInMilliseconds primaries.index.docsCurrentlyDeletedInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexing primaries.index.docsCurrentlyIndexing ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexingInMilliseconds primaries.index.docsCurrentlyIndexingInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsDeleted primaries.index.docsDeleted ElasticSearch elasticsearch.index.rollup.index.docsTotal primaries.index.docsTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotal primaries.indexRefreshesTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotalInMilliseconds primaries.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.merges.current primaries.merges.current ElasticSearch elasticsearch.index.rollup.merges.docsSegmentsCurrentlyMerged primaries.merges.docsSegmentsCurrentlyMerged ElasticSearch elasticsearch.index.rollup.merges.docsTotal primaries.merges.docsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsCurrentlyMergedInBytes primaries.merges.segmentsCurrentlyMergedInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotal primaries.merges.segmentsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInBytes primaries.merges.segmentsTotalInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInMilliseconds primaries.merges.segmentsTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesInMilliseconds primaries.queriesInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesTotal primaries.queriesTotal ElasticSearch elasticsearch.index.rollup.queryActive primaries.queryActive ElasticSearch elasticsearch.index.rollup.queryFetches primaries.queryFetches ElasticSearch elasticsearch.index.rollup.queryFetchesInMilliseconds primaries.queryFetchesInMilliseconds ElasticSearch elasticsearch.index.rollup.queryFetchesTotal primaries.queryFetchesTotal ElasticSearch elasticsearch.index.rollup.sizeInBytes primaries.sizeInBytes ElasticSearch elasticsearch.index.storeSizeInBytes index.storeSizeInBytes ElasticSearch elasticsearch.node.activeSearches activeSearches ElasticSearch elasticsearch.node.activeSearchesInMilliseconds activeSearchesInMilliseconds ElasticSearch elasticsearch.node.breakers.estimatedSizeFieldDataCircuitBreakerInBytes breakers.estimatedSizeFieldDataCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeParentCircuitBreakerInBytes breakers.estimatedSizeParentCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeRequestCircuitBreakerInBytes breakers.estimatedSizeRequestCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.fieldDataCircuitBreakerTripped breakers.fieldDataCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.parentCircuitBreakerTripped breakers.parentCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.requestCircuitBreakerTripped breakers.requestCircuitBreakerTripped ElasticSearch elasticsearch.node.flush.indexRefreshesTotal flush.indexRefreshesTotal ElasticSearch elasticsearch.node.flush.indexRefreshesTotalInMilliseconds flush.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.node.fs.bytesAvailableJvmInBytes fs.bytesAvailableJVMInBytes ElasticSearch elasticsearch.node.fs.dataRead fs.bytesReadsInBytes ElasticSearch elasticsearch.node.fs.dataWritten fs.writesInBytes ElasticSearch elasticsearch.node.fs.ioOperations fs.iOOperations ElasticSearch elasticsearch.node.fs.readOperations fs.reads ElasticSearch elasticsearch.node.fs.totalSizeInBytes fs.totalSizeInBytes ElasticSearch elasticsearch.node.fs.unallocatedBytes fs.unallocatedBytesInBYtes ElasticSearch elasticsearch.node.fs.writeOperations fs.writeOperations ElasticSearch elasticsearch.node.get.currentRequestsRunning get.currentRequestsRunning ElasticSearch elasticsearch.node.get.requestsDocumentExists get.requestsDocumentExists ElasticSearch elasticsearch.node.get.requestsDocumentExistsInMilliseconds get.requestsDocumentExistsInMilliseconds ElasticSearch elasticsearch.node.get.requestsDocumentMissing get.requestsDocumentMissing ElasticSearch elasticsearch.node.get.requestsDocumentMissingInMilliseconds get.requestsDocumentMissingInMilliseconds ElasticSearch elasticsearch.node.get.timeGetRequestsInMilliseconds get.timeGetRequestsInMilliseconds ElasticSearch elasticsearch.node.get.totalGetRequests get.totalGetRequests ElasticSearch elasticsearch.node.http.currentOpenConnections http.currentOpenConnections ElasticSearch elasticsearch.node.http.openedConnections http.openedConnections ElasticSearch elasticsearch.node.index.indexingOperationsFailed indices.indexingOperationsFailed ElasticSearch elasticsearch.node.index.indexingWaitedThrottlingInMilliseconds indices.indexingWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.memoryQueryCacheInBytes indices.memoryQueryCacheInBytes ElasticSearch elasticsearch.node.index.numberIndices indices.numberIndices ElasticSearch elasticsearch.node.index.queryCacheEvictions indices.queryCacheEvictions ElasticSearch elasticsearch.node.index.queryCacheHits indices.queryCacheHits ElasticSearch elasticsearch.node.index.queryCacheMisses indices.queryCacheMisses ElasticSearch elasticsearch.node.index.recoveryOngoingShardSource indices.recoveryOngoingShardSource ElasticSearch elasticsearch.node.index.recoveryOngoingShardTarget indices.recoveryOngoingShardTarget ElasticSearch elasticsearch.node.index.recoveryWaitedThrottlingInMilliseconds indices.recoveryWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.requestCacheEvictions indices.requestCacheEvictions ElasticSearch elasticsearch.node.index.requestCacheHits indices.requestCacheHits ElasticSearch elasticsearch.node.index.requestCacheMemoryInBytes indices.requestCacheMemoryInBytes ElasticSearch elasticsearch.node.index.requestCacheMisses indices.requestCacheMisses ElasticSearch elasticsearch.node.index.segmentsIndexShard indices.segmentsIndexShard ElasticSearch elasticsearch.node.index.segmentsMemoryUsedDocValuesInBytes indices.segmentsMemoryUsedDocValuesInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedFixedBitSetInBytes indices.segmentsMemoryUsedFixedBitSetInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexSegmentsInBytes indices.segmentsMemoryUsedIndexSegmentsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexWriterInBytes indices.segmentsMemoryUsedIndexWriterInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedNormsInBytes indices.segmentsMemoryUsedNormsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedSegmentVersionMapInBytes indices.segmentsMemoryUsedSegmentVersionMapInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedStoredFieldsInBytes indices.segmentsMemoryUsedStoredFieldsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermsInBytes indices.segmentsMemoryUsedTermsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermVectorsInBytes indices.segmentsMemoryUsedTermVectorsInBytes ElasticSearch elasticsearch.node.index.translogOperations indices.translogOperations ElasticSearch elasticsearch.node.index.translogOperationsInBytes indices.translogOperationsInBytes ElasticSearch elasticsearch.node.indexing.docsCurrentlyDeleted indexing.docsCurrentlyDeleted ElasticSearch elasticsearch.node.indexing.documentsCurrentlyIndexing indexing.documentsCurrentlyIndexing ElasticSearch elasticsearch.node.indexing.documentsIndexed indexing.documentsIndexed ElasticSearch elasticsearch.node.indexing.timeDeletingDocumentsInMilliseconds indexing.timeDeletingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.timeIndexingDocumentsInMilliseconds indexing.timeIndexingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.totalDocumentsDeleted indexing.totalDocumentsDeleted ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjects jvm.gc.majorCollectionsOldGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjects jvm.gc.majorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjects jvm.gc.minorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.mem.heapCommittedInBytes jvm.mem.heapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.heapMaxInBytes jvm.mem.heapMaxInBytes ElasticSearch elasticsearch.node.jvm.mem.heapUsed jvm.mem.heapUsed ElasticSearch elasticsearch.node.jvm.mem.heapUsedInBytes jvm.mem.heapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.maxOldGenerationHeapInBytes jvm.mem.maxOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.maxSurvivorSpaceInBytes jvm.mem.maxSurvivorSpaceInBYtes ElasticSearch elasticsearch.node.jvm.mem.maxYoungGenerationHeapInBytes jvm.mem.maxYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapCommittedInBytes jvm.mem.nonHeapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapUsedInBytes jvm.mem.nonHeapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.usedOldGenerationHeapInBytes jvm.mem.usedOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.usedSurvivorSpaceInBytes jvm.mem.usedSurvivorSpaceInBytes ElasticSearch elasticsearch.node.jvm.mem.usedYoungGenerationHeapInBytes jvm.mem.usedYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.threadsActive jvm.ThreadsActive ElasticSearch elasticsearch.node.jvm.threadsPeak jvm.ThreadsPeak ElasticSearch elasticsearch.node.merges.currentActive merges.currentActive ElasticSearch elasticsearch.node.merges.docsSegmentMerges merges.docsSegmentMerges ElasticSearch elasticsearch.node.merges.docsSegmentsMerging merges.docsSegmentsMerging ElasticSearch elasticsearch.node.merges.mergedSegmentsInBytes merges.mergedSegmentsInBytes ElasticSearch elasticsearch.node.merges.segmentMerges merges.segmentMerges ElasticSearch elasticsearch.node.merges.sizeSegmentsMergingInBytes merges.sizeSegmentsMergingInBytes ElasticSearch elasticsearch.node.merges.totalSegmentMergingInMilliseconds merges.totalSegmentMergingInMilliseconds ElasticSearch elasticsearch.node.openFd openFD ElasticSearch elasticsearch.node.queriesTotal queriesTotal ElasticSearch elasticsearch.node.refresh.total refresh.total ElasticSearch elasticsearch.node.refresh.totalInMilliseconds refresh.totalInMilliseconds ElasticSearch elasticsearch.node.searchFetchCurrentlyRunning searchFetchCurrentlyRunning ElasticSearch elasticsearch.node.searchFetches searchFetches ElasticSearch elasticsearch.node.sizeStoreInBytes sizeStoreInBytes ElasticSearch elasticsearch.node.threadpool.activeFetchShardStarted threadpool.activeFetchShardStarted ElasticSearch elasticsearch.node.threadpool.bulkActive threadpool.bulkActive ElasticSearch elasticsearch.node.threadpool.bulkQueue threadpool.bulkQueue ElasticSearch elasticsearch.node.threadpool.bulkRejected threadpool.bulkRejected ElasticSearch elasticsearch.node.threadpool.bulkThreads threadpool.bulkThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStartedQueue threadpool.fetchShardStartedQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStartedRejected threadpool.fetchShardStartedRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStartedThreads threadpool.fetchShardStartedThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStoreActive threadpool.fetchShardStoreActive ElasticSearch elasticsearch.node.threadpool.fetchShardStoreQueue threadpool.fetchShardStoreQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStoreRejected threadpool.fetchShardStoreRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStoreThreads threadpool.fetchShardStoreThreads ElasticSearch elasticsearch.node.threadpool.flushActive threadpool.flushActive ElasticSearch elasticsearch.node.threadpool.flushQueue threadpool.flushQueue ElasticSearch elasticsearch.node.threadpool.flushRejected threadpool.flushRejected ElasticSearch elasticsearch.node.threadpool.flushThreads threadpool.flushThreads ElasticSearch elasticsearch.node.threadpool.forceMergeActive threadpool.forceMergeActive ElasticSearch elasticsearch.node.threadpool.forceMergeQueue threadpool.forceMergeQueue ElasticSearch elasticsearch.node.threadpool.forceMergeRejected threadpool.forceMergeRejected ElasticSearch elasticsearch.node.threadpool.forceMergeThreads threadpool.forceMergeThreads ElasticSearch elasticsearch.node.threadpool.genericActive threadpool.genericActive ElasticSearch elasticsearch.node.threadpool.genericQueue threadpool.genericQueue ElasticSearch elasticsearch.node.threadpool.genericRejected threadpool.genericRejected ElasticSearch elasticsearch.node.threadpool.genericThreads threadpool.genericThreads ElasticSearch elasticsearch.node.threadpool.getActive threadpool.getActive ElasticSearch elasticsearch.node.threadpool.getQueue threadpool.getQueue ElasticSearch elasticsearch.node.threadpool.getRejected threadpool.getRejected ElasticSearch elasticsearch.node.threadpool.getThreads threadpool.getThreads ElasticSearch elasticsearch.node.threadpool.indexActive threadpool.indexActive ElasticSearch elasticsearch.node.threadpool.indexQueue threadpool.indexQueue ElasticSearch elasticsearch.node.threadpool.indexRejected threadpool.indexRejected ElasticSearch elasticsearch.node.threadpool.indexThreads threadpool.indexThreads ElasticSearch elasticsearch.node.threadpool.listenerActive threadpool.listenerActive ElasticSearch elasticsearch.node.threadpool.listenerQueue threadpool.listenerQueue ElasticSearch elasticsearch.node.threadpool.listenerRejected threadpool.listenerRejected ElasticSearch elasticsearch.node.threadpool.listenerThreads threadpool.listenerThreads ElasticSearch elasticsearch.node.threadpool.managementActive threadpool.managementActive ElasticSearch elasticsearch.node.threadpool.managementQueue threadpool.managementQueue ElasticSearch elasticsearch.node.threadpool.managementRejected threadpool.managementRejected ElasticSearch elasticsearch.node.threadpool.managementThreads threadpool.managementThreads ElasticSearch elasticsearch.node.threadpool.refreshActive threadpool.refreshActive ElasticSearch elasticsearch.node.threadpool.refreshQueue threadpool.refreshQueue ElasticSearch elasticsearch.node.threadpool.refreshRejected threadpool.refreshRejected ElasticSearch elasticsearch.node.threadpool.refreshThreads threadpool.refreshThreads ElasticSearch elasticsearch.node.threadpool.searchActive threadpool.searchActive ElasticSearch elasticsearch.node.threadpool.searchQueue threadpool.searchQueue ElasticSearch elasticsearch.node.threadpool.searchRejected threadpool.searchRejected ElasticSearch elasticsearch.node.threadpool.searchThreads threadpool.searchThreads ElasticSearch elasticsearch.node.threadpool.snapshotActive threadpool.snapshotActive ElasticSearch elasticsearch.node.threadpool.snapshotQueue threadpool.snapshotQueue ElasticSearch elasticsearch.node.threadpool.snapshotRejected threadpool.snapshotRejected ElasticSearch elasticsearch.node.threadpool.snapshotThreads threadpool.snapshotThreads ElasticSearch elasticsearch.node.transport.connectionsOpened transport.connectionsOpened ElasticSearch elasticsearch.node.transport.packetsReceived transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes F5 f5.node.availabilityState node.availabilityState F5 f5.node.connections node.connections F5 f5.node.connectionsPerSecond node.connectionsPerSecond F5 f5.node.enabled node.enabled F5 f5.node.inDataInBytesPerSecond node.inDataInBytesPerSecond F5 f5.node.monitorStatus node.monitorStatus F5 f5.node.outDataInBytesPerSecond node.outDataInBytesPerSecond F5 f5.node.packetsReceivedPerSecond node.packetsReceivedPerSecond F5 f5.node.packetsSentPerSecond node.packetsSentPerSecond F5 f5.node.requestsPerSecond node.requestsPerSecond F5 f5.node.sessions node.sessions F5 f5.node.sessionStatus node.sessionStatus F5 f5.poolMember.availabilityState member.availabilityState F5 f5.poolMember.connections member.connections F5 f5.poolMember.enabled member.enabled F5 f5.poolMember.inDataInBytesPerSecond member.inDataInBytesPerSecond F5 f5.poolMember.monitorStatus member.monitorStatus F5 f5.poolMember.outDataInBytesPerSecond member.outDataInBytesPerSecond F5 f5.poolMember.packetsReceivedPerSecond member.packetsReceivedPerSecond F5 f5.poolMember.packetsSentPerSecond member.packetsSentPerSecond F5 f5.poolMember.requestsPerSecond member.requestsPerSecond F5 f5.poolMember.sessions member.sessions F5 f5.poolMember.sessionStatus member.sessionStatus F5 f5.pool.activeMembers pool.activeMembers F5 f5.pool.availabilityState pool.availabilityState F5 f5.pool.connections pool.connections F5 f5.pool.connqAgeEdm pool.connqAgeEdm F5 f5.pool.connqAgeEma pool.connqAgeEma F5 f5.pool.connqAgeHead pool.connqAgeHead F5 f5.pool.connqAgeMax pool.connqAgeMax F5 f5.pool.connqAllAgeEdm pool.connqAllAgeEdm F5 f5.pool.connqAllAgeEma pool.connqAllAgeEma F5 f5.pool.connqAllAgeHead pool.connqAllAgeHead F5 f5.pool.connqAllAgeMax pool.connqAllAgeMax F5 f5.pool.connqAllDepth pool.connqAllDepth F5 f5.pool.connqDepth pool.connqDepth F5 f5.pool.currentConnections pool.currentConnections F5 f5.pool.enabled pool.enabled F5 f5.pool.inDataInBytesPerSecond pool.inDataInBytesPerSecond F5 f5.pool.minActiveMembers pool.minActiveMembers F5 f5.pool.outDataInBytesPerSecond pool.outDataInBytesPerSecond F5 f5.pool.packetsReceivedPerSecond pool.packetsReceivedPerSecond F5 f5.pool.packetsSentPerSecond pool.packetsSentPerSecond F5 f5.pool.requestsPerSecond pool.requestsPerSecond F5 f5.pool.sessions pool.sessions F5 f5.system.cpuIdleTicksPerSecond system.cpuIdleTicksPerSecond F5 f5.system.cpuIdleUtilization system.cpuIdleUtilization F5 f5.system.cpuInterruptRequestUtilization system.cpuInterruptRequestUtilization F5 f5.system.cpuIoWaitUtilization system.cpuIOWaitUtilization F5 f5.system.cpuNiceLevelUtilization system.cpuNiceLevelUtilization F5 f5.system.cpuSoftInterruptRequestUtilization system.cpuSoftInterruptRequestUtilization F5 f5.system.cpuStolenUtilization system.cpuStolenUtilization F5 f5.system.cpuSystemTicksPerSecond system.cpuSystemTicksPerSecond F5 f5.system.cpuSystemUtilization system.cpuSystemUtilization F5 f5.system.cpuUserTicksPerSecond system.cpuUserTicksPerSecond F5 f5.system.cpuUserUtilization system.cpuUserUtilization F5 f5.system.memoryFreeInBytes system.memoryFreeInBytes F5 f5.system.memoryTotalInBytes system.memoryTotalInBytes F5 f5.system.memoryUsedInBytes system.memoryUsedInBytes F5 f5.system.otherMemoryFreeInBytes system.otherMemoryFreeInBytes F5 f5.system.otherMemoryTotalInBytes system.otherMemoryTotalInBytes F5 f5.system.otherMemoryUsedInBytes system.otherMemoryUsedInBytes F5 f5.system.swapFreeInBytes system.swapFreeInBytes F5 f5.system.swapTotalInBytes system.swapTotalInBytes F5 f5.system.swapUsedInBytes system.swapUsedInBytes F5 f5.system.tmmMemoryFreeInBytes system.tmmMemoryFreeInBytes F5 f5.system.tmmMemoryTotalInBytes system.tmmMemoryTotalInBytes F5 f5.system.tmmMemoryUsedInBytes system.tmmMemoryUsedInBytes F5 f5.virtualserver.availabilityState virtualserver.availabilityState F5 f5.virtualserver.clientsideConnectionsPerSecond virtualserver.clientsideConnectionsPerSecond F5 f5.virtualserver.connections virtualserver.connections F5 f5.virtualserver.csMaxConnDur virtualserver.csMaxConnDur F5 f5.virtualserver.csMeanConnDur virtualserver.csMeanConnDur F5 f5.virtualserver.csMinConnDur virtualserver.csMinConnDur F5 f5.virtualserver.enabled virtualserver.enabled F5 f5.virtualserver.ephemeralBytesInPerSecond virtualserver.ephemeralBytesInPerSecond F5 f5.virtualserver.ephemeralBytesOutPerSecond virtualserver.ephemeralBytesOutPerSecond F5 f5.virtualserver.ephemeralConnectionsPerSecond virtualserver.ephemeralConnectionsPerSecond F5 f5.virtualserver.ephemeralCurrentConnections virtualserver.ephemeralCurrentConnections F5 f5.virtualserver.ephemeralEvictedConnectionsPerSecond virtualserver.ephemeralEvictedConnectionsPerSecond F5 f5.virtualserver.ephemeralMaxConnections virtualserver.ephemeralMaxConnections F5 f5.virtualserver.ephemeralPacketsReceivedPerSecond virtualserver.ephemeralPacketsReceivedPerSecond F5 f5.virtualserver.ephemeralPacketsSentPerSecond virtualserver.ephemeralPacketsSentPerSecond F5 f5.virtualserver.ephemeralSlowKilledPerSecond virtualserver.ephemeralSlowKilledPerSecond F5 f5.virtualserver.evictedConnsPerSecond virtualserver.evictedConnsPerSecond F5 f5.virtualserver.inDataInBytesPerSecond virtualserver.inDataInBytesPerSecond F5 f5.virtualserver.outDataInBytesPerSecond virtualserver.outDataInBytesPerSecond F5 f5.virtualserver.packetsReceivedPerSecond virtualserver.packetsReceivedPerSecond F5 f5.virtualserver.packetsSentPerSecond virtualserver.packetsSentPerSecond F5 f5.virtualserver.requestsPerSecond virtualserver.requestsPerSecond F5 f5.virtualserver.slowKilledPerSecond virtualserver.slowKilledPerSecond F5 f5.virtualserver.usageRatio virtualserver.usageRatio HAProxy haproxy.backend.activeServers backend.activeServers HAProxy haproxy.backend.averageConnectTimeInSeconds backend.averageConnectTimeInSeconds HAProxy haproxy.backend.averageQueueTimeInSeconds backend.averageQueueTimeInSeconds HAProxy haproxy.backend.averageResponseTimeInSeconds backend.averageResponseTimeInSeconds HAProxy haproxy.backend.averageTotalSessionTimeInSeconds backend.averageTotalSessionTimeInSeconds HAProxy haproxy.backend.backupServers backend.backupServers HAProxy haproxy.backend.bytesInPerSecond backend.bytesInPerSecond HAProxy haproxy.backend.bytesOutPerSecond backend.bytesOutPerSecond HAProxy haproxy.backend.bytesThatBypassedCompressorPerSecond backend.bytesThatBypassedCompressorPerSecond HAProxy haproxy.backend.connectingRequestErrorsPerSecond backend.connectingRequestErrorsPerSecond HAProxy haproxy.backend.connectionRetriesPerSecond backend.connectionRetriesPerSecond HAProxy haproxy.backend.currentQueuedRequestsWithoutServer backend.currentQueuedRequestsWithoutServer HAProxy haproxy.backend.currentSessions backend.currentSessions HAProxy haproxy.backend.dataTransfersAbortedByClientPerSecond backend.dataTransfersAbortedByClientPerSecond HAProxy haproxy.backend.dataTransfersAbortedByServerPerSecond backend.dataTransfersAbortedByServerPerSecond HAProxy haproxy.backend.downtimeInSeconds backend.downtimeInSeconds HAProxy haproxy.backend.http100ResponsesPerSecond backend.http100ResponsesPerSecond HAProxy haproxy.backend.http200ResponsesPerSecond backend.http200ResponsesPerSecond HAProxy haproxy.backend.http300ResponsesPerSecond backend.http300ResponsesPerSecond HAProxy haproxy.backend.http400ResponsesPerSecond backend.http400ResponsesPerSecond HAProxy haproxy.backend.http500ResponsesPerSecond backend.http500ResponsesPerSecond HAProxy haproxy.backend.httpOtherResponsesPerSecond backend.httpOtherResponsesPerSecond HAProxy haproxy.backend.httpRequestsPerSecond backend.httpRequestsPerSecond HAProxy haproxy.backend.httpResponseBytesEmittedByCompressorPerSecond backend.httpResponseBytesEmittedByCompressorPerSecond HAProxy haproxy.backend.httpResponseBytesFedToCompressorPerSecond backend.httpResponseBytesFedToCompressorPerSecond HAProxy haproxy.backend.httpResponsesCompressedPerSecond backend.httpResponsesCompressedPerSecond HAProxy haproxy.backend.interceptedRequestsPerSecond backend.interceptedRequestsPerSecond HAProxy haproxy.backend.maxQueuedRequestsWithoutServer backend.maxQueuedRequestsWithoutServer HAProxy haproxy.backend.maxSessions backend.maxSessions HAProxy haproxy.backend.maxSessionsPerSecond backend.maxSessionsPerSecond HAProxy haproxy.backend.requestRedispatchPerSecond backend.requestRedispatchPerSecond HAProxy haproxy.backend.requestsDenied.securityConcernsPerSecond backend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.backend.responseErrorsPerSecond backend.responseErrorsPerSecond HAProxy haproxy.backend.responsesDenied.securityConcernsPerSecond backend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.backend.serverSelectedPerSecond backend.serverSelectedPerSecond HAProxy haproxy.backend.sessionsPerSecond backend.sessionsPerSecond HAProxy haproxy.backend.timeSinceLastSessionAssignedInSeconds backend.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.backend.timeSinceLastUpDownTransitionInSeconds backend.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.backend.totalWeight backend.totalWeight HAProxy haproxy.backend.type backend.type HAProxy haproxy.backend.upToDownTransitionsPerSecond backend.upToDownTransitionsPerSecond HAProxy haproxy.frontend.bytesInPerSecond frontend.bytesInPerSecond HAProxy haproxy.frontend.bytesOutPerSecond frontend.bytesOutPerSecond HAProxy haproxy.frontend.connectionsPerSecond frontend.connectionsPerSecond HAProxy haproxy.frontend.currentSessions frontend.currentSessions HAProxy haproxy.frontend.http100ResponsesPerSecond frontend.http100ResponsesPerSecond HAProxy haproxy.frontend.http200ResponsesPerSecond frontend.http200ResponsesPerSecond HAProxy haproxy.frontend.http300ResponsesPerSecond frontend.http300ResponsesPerSecond HAProxy haproxy.frontend.http400ResponsesPerSecond frontend.http400ResponsesPerSecond HAProxy haproxy.frontend.http500ResponsesPerSecond frontend.http500ResponsesPerSecond HAProxy haproxy.frontend.httpOtherResponsesPerSecond frontend.httpOtherResponsesPerSecond HAProxy haproxy.frontend.httpRequests.maxPerSecond frontend.httpRequests.maxPerSecond HAProxy haproxy.frontend.httpRequestsPerSecond frontend.httpRequestsPerSecond HAProxy haproxy.frontend.interceptedRequestsPerSecond frontend.interceptedRequestsPerSecond HAProxy haproxy.frontend.maxConnectionsPerSecond frontend.maxConnectionsPerSecond HAProxy haproxy.frontend.maxSessions frontend.maxSessions HAProxy haproxy.frontend.maxSessionsPerSecond frontend.maxSessionsPerSecond HAProxy haproxy.frontend.requestErrorsPerSecond frontend.requestErrorsPerSecond HAProxy haproxy.frontend.requestsDenied.securityConcernsPerSecond frontend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestConnectionRulesPerSecond frontend.requestsDenied.tcpRequestConnectionRulesPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestSessionRulesPerSecond frontend.requestsDenied.tcpRequestSessionRulesPerSecond HAProxy haproxy.frontend.responsesDenied.securityConcernsPerSecond frontend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.frontend.sessionsPerSecond frontend.sessionsPerSecond HAProxy haproxy.server.averageConnectTimeInSeconds server.averageConnectTimeInSeconds HAProxy haproxy.server.averageQueueTimeInSeconds server.averageQueueTimeInSeconds HAProxy haproxy.server.averageResponseTimeInSeconds server.averageResponseTimeInSeconds HAProxy haproxy.server.averageTotalSessionTimeInSeconds server.averageTotalSessionTimeInSeconds HAProxy haproxy.server.bytesInPerSecond server.bytesInPerSecond HAProxy haproxy.server.bytesOutPerSecond server.bytesOutPerSecond HAProxy haproxy.server.connectingRequestErrorsPerSecond server.connectingRequestErrorsPerSecond HAProxy haproxy.server.connectionRetriesPerSecond server.connectionRetriesPerSecond HAProxy haproxy.server.currentQueuedRequestsWithoutServer server.currentQueuedRequestsWithoutServer HAProxy haproxy.server.currentSessions server.currentSessions HAProxy haproxy.server.dataTransfersAbortedByClientPerSecond server.dataTransfersAbortedByClientPerSecond HAProxy haproxy.server.dataTransfersAbortedByServerPerSecond server.dataTransfersAbortedByServerPerSecond HAProxy haproxy.server.downtimeInSeconds server.downtimeInSeconds HAProxy haproxy.server.failedChecksPerSecond server.failedChecksPerSecond HAProxy haproxy.server.healthCheckDurationInMilliseconds server.healthCheckDurationInMilliseconds HAProxy haproxy.server.http100ResponsesPerSecond server.http100ResponsesPerSecond HAProxy haproxy.server.http200ResponsesPerSecond server.http200ResponsesPerSecond HAProxy haproxy.server.http300ResponsesPerSecond server.http300ResponsesPerSecond HAProxy haproxy.server.http400ResponsesPerSecond server.http400ResponsesPerSecond HAProxy haproxy.server.http500ResponsesPerSecond server.http500ResponsesPerSecond HAProxy haproxy.server.httpOtherResponsesPerSecond server.httpOtherResponsesPerSecond HAProxy haproxy.server.isActive server.isActive HAProxy haproxy.server.isBackup server.isBackup HAProxy haproxy.server.maxQueuedRequestsWithoutServer server.maxQueuedRequestsWithoutServer HAProxy haproxy.server.maxSessions server.maxSessions HAProxy haproxy.server.maxSessionsPerSecond server.maxSessionsPerSecond HAProxy haproxy.server.requestRedispatchPerSecond server.requestRedispatchPerSecond HAProxy haproxy.server.requestsDenied.securityConcernsPerSecond server.requestsDenied.securityConcernsPerSecond HAProxy haproxy.server.responseErrorsPerSecond server.responseErrorsPerSecond HAProxy haproxy.server.responsesDenied.securityConcernsPerSecond server.responsesDenied.securityConcernsPerSecond HAProxy haproxy.server.serverSelectedPerSecond server.serverSelectedPerSecond HAProxy haproxy.server.serverWeight server.serverWeight HAProxy haproxy.server.sessionsPerSecond server.sessionsPerSecond HAProxy haproxy.server.throttlePercentage server.throttlePercentage HAProxy haproxy.server.timeSinceLastSessionAssignedInSeconds server.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.server.timeSinceLastUpDownTransitionInSeconds server.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.server.type server.type HAProxy haproxy.server.upToDownTransitionsPerSecond server.upToDownTransitionsPerSecond Kafka kafka.broker.bytesWrittenToTopicPerSecond broker.bytesWrittenToTopicPerSecond Kafka kafka.broker.consumer.requestsExpiredPerSecond consumer.requestsExpiredPerSecond Kafka kafka.broker.follower.requestExpirationPerSecond follower.requestExpirationPerSecond Kafka kafka.broker.ioInPerSecond broker.IOInPerSecond Kafka kafka.broker.ioOutPerSecond broker.IOOutPerSecond Kafka kafka.broker.logFlushPerSecond broker.logFlushPerSecond Kafka kafka.broker.messagesInPerSecond broker.messagesInPerSecond Kafka kafka.broker.net.bytesRejectedPerSecond net.bytesRejectedPerSecond Kafka kafka.broker.replication.isrExpandsPerSecond replication.isrExpandsPerSecond Kafka kafka.broker.replication.isrShrinksPerSecond replication.isrShrinksPerSecond Kafka kafka.broker.replication.leaderElectionPerSecond replication.leaderElectionPerSecond Kafka kafka.broker.replication.uncleanLeaderElectionPerSecond replication.uncleanLeaderElectionPerSecond Kafka kafka.broker.replication.unreplicatedPartitions replication.unreplicatedPartitions Kafka kafka.broker.request.avgTimeFetch request.avgTimeFetch Kafka kafka.broker.request.avgTimeMetadata request.avgTimeMetadata Kafka kafka.broker.request.avgTimeMetadata99Percentile request.avgTimeMetadata99Percentile Kafka kafka.broker.request.avgTimeOffset request.avgTimeOffset Kafka kafka.broker.request.avgTimeOffset99Percentile request.avgTimeOffset99Percentile Kafka kafka.broker.request.avgTimeProduceRequest request.avgTimeProduceRequest Kafka kafka.broker.request.avgTimeUpdateMetadata request.avgTimeUpdateMetadata Kafka kafka.broker.request.avgTimeUpdateMetadata99Percentile request.avgTimeUpdateMetadata99Percentile Kafka kafka.broker.request.clientFetchesFailedPerSecond request.clientFetchesFailedPerSecond Kafka kafka.broker.request.fetchConsumerRequestsPerSecond request.fetchConsumerRequestsPerSecond Kafka kafka.broker.request.fetchFollowerRequestsPerSecond request.fetchFollowerRequestsPerSecond Kafka kafka.broker.request.fetchTime99Percentile request.fetchTime99Percentile Kafka kafka.broker.request.handlerIdle request.handlerIdle Kafka kafka.broker.request.listGroupsRequestsPerSecond request.listGroupsRequestsPerSecond Kafka kafka.broker.request.metadataRequestsPerSecond request.metadataRequestsPerSecond Kafka kafka.broker.request.offsetCommitRequestsPerSecond request.offsetCommitRequestsPerSecond Kafka kafka.broker.request.produceRequestsFailedPerSecond request.produceRequestsFailedPerSecond Kafka kafka.broker.request.produceRequestsPerSecond request.produceRequestsPerSecond Kafka kafka.broker.request.produceTime99Percentile request.produceTime99Percentile Kafka kafka.broker.topic.diskSize topic.diskSize Kafka kafka.topic.bytesInPerSec topic.BytesInPerSec Kafka kafka.topic.bytesOutPerSec topic.BytesOutPerSec Kafka kafka.topic.messagesInPerSec topic.MessagesInPerSec Kafka kafka.topic.partitionsWithNonPreferredLeader topic.partitionsWithNonPreferredLeader Kafka kafka.topic.respondsToMetadataRequests topic.respondsToMetadataRequests Kafka kafka.topic.retentionBytesOrTime topic.retentionBytesOrTime Kafka kafka.topic.underReplicatedPartitions topic.underReplicatedPartitions Kafka kafka.producer.ageMetadataUsedInMilliseconds producer.ageMetadataUsedInMilliseconds Kafka kafka.producer.availableBufferInBytes producer.availableBufferInBytes Kafka kafka.producer.avgBytesSentPerRequestInBytes producer.avgBytesSentPerRequestInBytes Kafka kafka.producer.avgCompressionRateRecordBatches producer.avgCompressionRateRecordBatches Kafka kafka.producer.avgRecordAccumulatorsInMilliseconds producer.avgRecordAccumulatorsInMilliseconds Kafka kafka.producer.avgRecordSizeInBytes producer.avgRecordSizeInBytes Kafka kafka.producer.avgRecordsSentPerSecond producer.avgRecordsSentPerSecond Kafka kafka.producer.avgRecordsSentPerTopicPerSecond producer.avgRecordsSentPerTopicPerSecond Kafka kafka.producer.avgRequestLatency producer.avgRequestLatencyPerSecond Kafka kafka.producer.avgThrottleTime producer.avgThrottleTime Kafka kafka.producer.bufferMemoryAvailableInBytes producer.bufferMemoryAvailableInBytes Kafka kafka.producer.bufferpoolWaitTime producer.bufferpoolWaitTime Kafka kafka.producer.bytesOutPerSecond producer.bytesOutPerSecond Kafka kafka.producer.compressionRateRecordBatches producer.compressionRateRecordBatches Kafka kafka.producer.ioWaitTime producer.ioWaitTime Kafka kafka.producer.maxBytesSentPerRequestInBytes producer.maxBytesSentPerRequestInBytes Kafka kafka.producer.maxRecordSizeInBytes producer.maxRecordSizeInBytes Kafka kafka.producer.maxRequestLatencyInMilliseconds producer.maxRequestLatencyInMilliseconds Kafka kafka.producer.maxThrottleTime producer.maxThrottleTime Kafka kafka.producer.requestPerSecond producer.requestPerSecond Kafka kafka.producer.requestsWaitingResponse producer.requestsWaitingResponse Kafka kafka.producer.responsePerSecond producer.responsePerSecond Kafka kafka.producer.threadsWaiting producer.threadsWaiting Kafka kafka.consumer.avgFetchSizeInBytes consumer.avgFetchSizeInBytes Kafka kafka.consumer.avgRecordConsumedPerTopic consumer.avgRecordConsumedPerTopic Kafka kafka.consumer.avgRecordConsumedPerTopicPerSecond consumer.avgRecordConsumedPerTopicPerSecond Kafka kafka.consumer.bytesInPerSecond consumer.bytesInPerSecond Kafka kafka.consumer.fetchPerSecond consumer.fetchPerSecond Kafka kafka.consumer.hwm consumer.hwm Kafka kafka.consumer.lag consumer.lag Kafka kafka.consumer.maxFetchSizeInBytes consumer.maxFetchSizeInBytes Kafka kafka.consumer.maxLag consumer.maxLag Kafka kafka.consumer.messageConsumptionPerSecond consumer.messageConsumptionPerSecond Kafka kafka.consumer.offset consumer.offset Kafka kafka.consumer.totalLag consumer.totalLag Kafka kafka.consumerGroup.maxLag consumerGroup.maxLag Kafka kafka.consumerGroup.totalLag consumerGroup.totalLag Kubernetes k8s.apiserver.goGoroutines goGoroutines Kubernetes k8s.apiserver.goThreads goThreads Kubernetes k8s.apiserver.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.apiserver.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.controllermanager.goGoroutines goGoroutines Kubernetes k8s.controllermanager.goThreads goThreads Kubernetes k8s.controllermanager.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.controllermanager.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.controllermanager.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.goGoroutines goGoroutines Kubernetes k8s.etcd.goThreads goThreads Kubernetes k8s.etcd.mvccDbTotalSizeInBytes etcdMvccDbTotalSizeInBytes Kubernetes k8s.etcd.networkClientGrpcReceivedBytesRate etcdNetworkClientGrpcReceivedBytesRate Kubernetes k8s.etcd.networkClientGrpcSentBytesRate etcdNetworkClientGrpcSentBytesRate Kubernetes k8s.etcd.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.etcd.process.maxFds processMaxFds Kubernetes k8s.etcd.process.openFds processOpenFds Kubernetes k8s.etcd.process.processFdsUtilization processFdsUtilization Kubernetes k8s.etcd.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.serverHasLeader etcdServerHasLeader Kubernetes k8s.etcd.serverLeaderChangesSeenDelta etcdServerLeaderChangesSeenDelta Kubernetes k8s.etcd.serverProposalsAppliedDelta etcdServerProposalsAppliedDelta Kubernetes k8s.etcd.serverProposalsAppliedRate etcdServerProposalsAppliedRate Kubernetes k8s.etcd.serverProposalsCommittedDelta etcdServerProposalsCommittedDelta Kubernetes k8s.etcd.serverProposalsCommittedRate etcdServerProposalsCommittedRate Kubernetes k8s.etcd.serverProposalsFailedDelta etcdServerProposalsFailedDelta Kubernetes k8s.etcd.serverProposalsFailedRate etcdServerProposalsFailedRate Kubernetes k8s.etcd.serverProposalsPending etcdServerProposalsPending Kubernetes k8s.scheduler.goGoroutines goGoroutines Kubernetes k8s.scheduler.goThreads goThreads Kubernetes k8s.scheduler.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.scheduler.podPreemptionVictims schedulerPodPreemptionVictims Kubernetes k8s.scheduler.preemptionAttemptsDelta schedulerPreemptionAttemptsDelta Kubernetes k8s.scheduler.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.scheduler.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.container.cpuCfsPeriodsDelta containerCpuCfsPeriodsDelta Kubernetes k8s.container.cpuCfsPeriodsTotal containerCpuCfsPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledPeriodsDelta containerCpuCfsThrottledPeriodsDelta Kubernetes k8s.container.cpuCfsThrottledPeriodsTotal containerCpuCfsThrottledPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledSecondsDelta containerCpuCfsThrottledSecondsDelta Kubernetes k8s.container.cpuCfsThrottledSecondsTotal containerCpuCfsThrottledSecondsTotal Kubernetes k8s.container.cpuCoresUtilization cpuCoresUtilization Kubernetes k8s.container.cpuLimitCores cpuLimitCores Kubernetes k8s.container.cpuRequestedCores cpuRequestedCores Kubernetes k8s.container.cpuUsedCores cpuUsedCores Kubernetes k8s.container.fsAvailableBytes fsAvailableBytes Kubernetes k8s.container.fsCapacityBytes fsCapacityBytes Kubernetes k8s.container.fsInodes fsInodes Kubernetes k8s.container.fsInodesFree fsInodesFree Kubernetes k8s.container.fsInodesUsed fsInodesUsed Kubernetes k8s.container.fsUsedBytes fsUsedBytes Kubernetes k8s.container.fsUsedPercent fsUsedPercent Kubernetes k8s.container.isReady isReady Kubernetes k8s.container.memoryLimitBytes memoryLimitBytes Kubernetes k8s.container.memoryMappedFileBytes containerMemoryMappedFileBytes Kubernetes k8s.container.memoryRequestedBytes memoryRequestedBytes Kubernetes k8s.container.memoryUsedBytes memoryUsedBytes Kubernetes k8s.container.memoryUtilization memoryUtilization Kubernetes k8s.container.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.container.requestedCpuCoresUtilization requestedCpuCoresUtilization Kubernetes k8s.container.requestedMemoryUtilization requestedMemoryUtilization Kubernetes k8s.container.restartCount restartCount Kubernetes k8s.daemonset.createdAt createdAt Kubernetes k8s.daemonset.metadataGeneration metadataGeneration Kubernetes k8s.daemonset.podsAvailable podsAvailable Kubernetes k8s.daemonset.podsDesired podsDesired Kubernetes k8s.daemonset.podsMisscheduled podsMisscheduled Kubernetes k8s.daemonset.podsReady podsReady Kubernetes k8s.daemonset.podsScheduled podsScheduled Kubernetes k8s.daemonset.podsUnavailable podsUnavailable Kubernetes k8s.daemonset.podsUpdatedScheduled podsUpdatedScheduled Kubernetes k8s.deployment.createdAt createdAt Kubernetes k8s.deployment.podsAvailable podsAvailable Kubernetes k8s.deployment.podsDesired podsDesired Kubernetes k8s.deployment.podsMaxUnavailable podsMaxUnavailable Kubernetes k8s.deployment.podsTotal podsTotal Kubernetes k8s.deployment.podsUnavailable podsUnavailable Kubernetes k8s.deployment.podsUpdated podsUpdated Kubernetes k8s.endpoint.addressAvailable addressAvailable Kubernetes k8s.endpoint.addressNotReady addressNotReady Kubernetes k8s.endpoint.createdAt createdAt Kubernetes k8s.namespace.createdAt createdAt Kubernetes k8s.node.allocatableAttachableVolumes* allocatableAttachableVolumes* Kubernetes k8s.node.allocatableCpuCores allocatableCpuCores Kubernetes k8s.node.allocatableCpuCoresUtilization allocatableCpuCoresUtilization Kubernetes k8s.node.allocatableEphemeralStorageBytes allocatableEphemeralStorageBytes Kubernetes k8s.node.allocatableHugepages* allocatableHugepages* Kubernetes k8s.node.allocatableMemoryBytes allocatableMemoryBytes Kubernetes k8s.node.allocatableMemoryUtilization allocatableMemoryUtilization Kubernetes k8s.node.allocatablePods allocatablePods Kubernetes k8s.node.capacityAttachableVolumes* capacityAttachableVolumes* Kubernetes k8s.node.capacityCpuCores capacityCpuCores Kubernetes k8s.node.capacityEphemeralStorageBytes capacityEphemeralStorageBytes Kubernetes k8s.node.capacityHugepages* capacityHugepages* Kubernetes k8s.node.capacityMemoryBytes capacityMemoryBytes Kubernetes k8s.node.capacityPods capacityPods Kubernetes k8s.node.cpuUsedCoreMilliseconds cpuUsedCoreMilliseconds Kubernetes k8s.node.cpuUsedCores cpuUsedCores Kubernetes k8s.node.fsAvailableBytes fsAvailableBytes Kubernetes k8s.node.fsCapacityBytes fsCapacityBytes Kubernetes k8s.node.fsCapacityUtilization fsCapacityUtilization Kubernetes k8s.node.fsInodes fsInodes Kubernetes k8s.node.fsInodesFree fsInodesFree Kubernetes k8s.node.fsInodesUsed fsInodesUsed Kubernetes k8s.node.fsUsedBytes fsUsedBytes Kubernetes k8s.node.memoryAvailableBytes memoryAvailableBytes Kubernetes k8s.node.memoryMajorPageFaultsPerSecond memoryMajorPageFaultsPerSecond Kubernetes k8s.node.memoryPageFaults memoryPageFaults Kubernetes k8s.node.memoryRssBytes memoryRssBytes Kubernetes k8s.node.memoryUsedBytes memoryUsedBytes Kubernetes k8s.node.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.node.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.node.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.node.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.node.runtimeAvailableBytes runtimeAvailableBytes Kubernetes k8s.node.runtimeCapacityBytes runtimeCapacityBytes Kubernetes k8s.node.runtimeInodes runtimeInodes Kubernetes k8s.node.runtimeInodesFree runtimeInodesFree Kubernetes k8s.node.runtimeInodesUsed runtimeInodesUsed Kubernetes k8s.node.runtimeUsedBytes runtimeUsedBytes Kubernetes k8s.pod.createdAt createdAt Kubernetes k8s.pod.isReady isReady Kubernetes k8s.pod.isScheduled isScheduled Kubernetes k8s.pod.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.pod.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.pod.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.pod.startTime startTime Kubernetes k8s.replicaset.createdAt createdAt Kubernetes k8s.replicaset.observedGeneration observedGeneration Kubernetes k8s.replicaset.podsDesired podsDesired Kubernetes k8s.replicaset.podsFullyLabeled podsFullyLabeled Kubernetes k8s.replicaset.podsMissing podsMissing Kubernetes k8s.replicaset.podsReady podsReady Kubernetes k8s.replicaset.podsTotal podsTotal Kubernetes k8s.service.createdAt createdAt Kubernetes k8s.statefulset.createdAt createdAt Kubernetes k8s.statefulset.currentRevision currentRevision Kubernetes k8s.statefulset.metadataGeneration metadataGeneration Kubernetes k8s.statefulset.observedGeneration observedGeneration Kubernetes k8s.statefulset.podsCurrent podsCurrent Kubernetes k8s.statefulset.podsDesired podsDesired Kubernetes k8s.statefulset.podsReady podsReady Kubernetes k8s.statefulset.podsTotal podsTotal Kubernetes k8s.statefulset.podsUpdated podsUpdated Kubernetes k8s.statefulset.updateRevision updateRevision Kubernetes k8s.volume.fsAvailableBytes fsAvailableBytes Kubernetes k8s.volume.fsCapacityBytes fsCapacityBytes Kubernetes k8s.volume.fsInodes fsInodes Kubernetes k8s.volume.fsInodesFree fsInodesFree Kubernetes k8s.volume.fsInodesUsed fsInodesUsed Kubernetes k8s.volume.fsUsedBytes fsUsedBytes Kubernetes k8s.volume.fsUsedPercent fsUsedPercent Memcached memcached.server.activeSlabs activeSlabs Memcached memcached.server.avgItemSizeInBytes avgItemSizeInBytes Memcached memcached.server.bytesReadServerPerSecond bytesReadServerPerSecond Memcached memcached.server.bytesUsedServerInBytes bytesUsedServerInBytes Memcached memcached.server.bytesWrittenServerPerSecond bytesWrittenServerPerSecond Memcached memcached.server.casHitRatePerSecond casHitRatePerSecond Memcached memcached.server.casMissRatePerSecond casMissRatePerSecond Memcached memcached.server.casWrongRatePerSecond casWrongRatePerSecond Memcached memcached.server.cmdFlushRatePerSecond cmdFlushRatePerSecond Memcached memcached.server.cmdGetRatePerSecond cmdGetRatePerSecond Memcached memcached.server.cmdSetRatePerSecond cmdSetRatePerSecond Memcached memcached.server.connectionRateServerPerSecond connectionRateServerPerSecond Memcached memcached.server.connectionStructuresAllocated connectionStructuresAllocated Memcached memcached.server.currentItemsStoredServer currentItemsStoredServer Memcached memcached.server.deleteCmdNoneRemovedPerSecond deleteCmdNoneRemovedPerSecond Memcached memcached.server.deleteCmdRemovedPerSecond deleteCmdRemovedPerSecond Memcached memcached.server.evictionsPerSecond evictionsPerSecond Memcached memcached.server.getHitPercent getHitPercent Memcached memcached.server.getHitPerSecond getHitPerSecond Memcached memcached.server.getMissPerSecond getMissPerSecond Memcached memcached.server.itemsStoredPerSecond itemsStoredPerSecond Memcached memcached.server.limitBytesStorage limitBytesStorage Memcached memcached.server.limitMaxBytes limitMaxBytes Memcached memcached.server.maxConnectionLimitPerSecond serverMaxConnectionLimitPerSecond Memcached memcached.server.memAllocatedSlabsInBytes memAllocatedSlabsInBytes Memcached memcached.server.openConnectionsServer openConnectionsServer Memcached memcached.server.pointerSize pointerSize Memcached memcached.server.rusageSystem usageRate Memcached memcached.server.rusageUser executionTime Memcached memcached.server.storingItemsPercentMemory storingItemsPercentMemory Memcached memcached.server.threads threads Memcached memcached.server.uptimeInMilliseconds uptimeInMilliseconds Memcached memcached.slab.activeItemsBumpedPerSecond activeItemsBumpedPerSecond Memcached memcached.slab.casBadValPerSecond casBadValPerSecond Memcached memcached.slab.casModifiedSlabPerSecond casModifiedSlabPerSecond Memcached memcached.slab.chunkSizeInBytes chunkSizeInBytes Memcached memcached.slab.chunksPerPage chunksPerPage Memcached memcached.slab.cmdSetRateSlabPerSecond cmdSetRateSlabPerSecond Memcached memcached.slab.decrsModifySlabPerSecond decrsModifySlabPerSecond Memcached memcached.slab.deleteRateSlabPerSecond deleteRateSlabPerSecond Memcached memcached.slab.entriesReclaimedPerSecond entriesReclaimedPerSecond Memcached memcached.slab.evictionsBeforeExpirationPerSecond evictionsBeforeExpirationPerSecond Memcached memcached.slab.evictionsBeforeExplicitExpirationPerSecond evictionsBeforeExplicitExpirationPerSecond Memcached memcached.slab.expiredItemsReclaimedPerSecond expiredItemsReclaimedPerSecond Memcached memcached.slab.freedChunks freedChunks Memcached memcached.slab.freedChunksEnd freedChunksEnd Memcached memcached.slab.getHitRateSlabPerSecond getHitRateSlabPerSecond Memcached memcached.slab.incrsModifySlabPerSecond incrsModifySlabPerSecond Memcached memcached.slab.itemsCold itemsCold Memcached memcached.slab.itemsColdPerSecond itemsColdPerSecond Memcached memcached.slab.itemsDirectReclaimedPerSecond itemsDirectReclaimedPerSecond Memcached memcached.slab.itemsFreedCrawlerPerSecond itemsFreedCrawlerPerSecond Memcached memcached.slab.itemsHot itemsHot Memcached memcached.slab.itemsOldestInMilliseconds itemsOldestInMilliseconds Memcached memcached.slab.itemsRefcountLockedPerSecond itemsRefcountLockedPerSecond Memcached memcached.slab.itemsSlabClass itemsSlabClass Memcached memcached.slab.itemsTimeSinceEvictionInMilliseconds itemsTimeSinceEvictionInMilliseconds Memcached memcached.slab.itemsWarm itemsWarm Memcached memcached.slab.itemsWarmPerSecond itemsWarmPerSecond Memcached memcached.slab.memRequestedSlabInBytesPerSecond memRequestedSlabInBytesPerSecond Memcached memcached.slab.outOfMemoryPerSecond outOfMemoryPerSecond Memcached memcached.slab.selfHealedSlabPerSecond selfHealedSlabPerSecond Memcached memcached.slab.totalChunksSlab totalChunksSlab Memcached memcached.slab.totalPagesSlab totalPagesSlab Memcached memcached.slab.touchHitSlabPerSecond touchHitSlabPerSecond Memcached memcached.slab.usedChunksItems usedChunksItems Memcached memcached.slab.usedChunksPerSecond usedChunksPerSecond Memcached memcached.slab.validItemsEvictedPerSecond validItemsEvictedPerSecond MongoDB mongo.index.accesses collection.indexAccesses MongoDB mongo.index.sizeInBytes collection.indexSizeInBytes MongoDB mongo.collection.avgObjSizeInBytes collection.avgObjSizeInBytes MongoDB mongo.collection.capped collection.capped MongoDB mongo.collection.count collection.count MongoDB mongo.collection.max collection.max MongoDB mongo.collection.maxSizeInBytes collection.maxSizeInBytes MongoDB mongo.collection.nindexes collection.nindexes MongoDB mongo.collection.sizeInBytes collection.sizeInBytes MongoDB mongo.collection.storageSizeInBytes collection.storageSizeInBytes MongoDB mongo.configServer.asserts.messagesPerSecond asserts.messagesPerSecond MongoDB mongo.configServer.asserts.regularPerSecond asserts.regularPerSecond MongoDB mongo.configServer.asserts.rolloversPerSecond asserts.rolloversPerSecond MongoDB mongo.configServer.asserts.userPerSecond asserts.userPerSecond MongoDB mongo.configServer.asserts.warningPerSecond asserts.warningPerSecond MongoDB mongo.configServer.commands.countFailedPerSecond commands.countFailedPerSecond MongoDB mongo.configServer.commands.countPerSecond commands.countPerSecond MongoDB mongo.configServer.commands.createIndexesFailedPerSecond commands.createIndexesFailedPerSecond MongoDB mongo.configServer.commands.createIndexesPerSecond commands.createIndexesPerSecond MongoDB mongo.configServer.commands.deleteFailedPerSecond commands.deleteFailedPerSecond MongoDB mongo.configServer.commands.deletePerSecond commands.deletePerSecond MongoDB mongo.configServer.commands.evalFailedPerSecond commands.evalFailedPerSecond MongoDB mongo.configServer.commands.evalPerSecond commands.evalPerSecond MongoDB mongo.configServer.commands.findAndModifyFailedPerSecond commands.findAndModifyFailedPerSecond MongoDB mongo.configServer.commands.findAndModifyPerSecond commands.findAndModifyPerSecond MongoDB mongo.configServer.commands.insertFailedPerSecond commands.insertFailedPerSecond MongoDB mongo.configServer.commands.insertPerSecond commands.insertPerSecond MongoDB mongo.configServer.commands.updateFailedPerSecond commands.updateFailedPerSecond MongoDB mongo.configServer.commands.updatePerSecond commands.updatePerSecond MongoDB mongo.configServer.connections.available connections.available MongoDB mongo.configServer.connections.current connections.current MongoDB mongo.configServer.connections.totalCreated connections.totalCreated MongoDB mongo.configServer.cursor.openNoTimeout cursor.openNoTimeout MongoDB mongo.configServer.cursor.openPinned cursor.openPinned MongoDB mongo.configServer.cursor.openTotal cursor.openTotal MongoDB mongo.configServer.cursor.timedOutPerSecond cursor.timedOutPerSecond MongoDB mongo.configServer.document.deletedPerSecond document.deletedPerSecond MongoDB mongo.configServer.document.insertedPerSecond document.insertedPerSecond MongoDB mongo.configServer.document.returnedPerSecond document.returnedPerSecond MongoDB mongo.configServer.document.updatedPerSecond document.updatedPerSecond MongoDB mongo.configServer.dur.commits dur.commits MongoDB mongo.configServer.dur.commitsInWriteLock dur.commitsInWriteLock MongoDB mongo.configServer.dur.compression dur.compression MongoDB mongo.configServer.dur.earlyCommits dur.earlyCommits MongoDB mongo.configServer.dur.preparingInMilliseconds dur.preparingInMilliseconds MongoDB mongo.configServer.dur.remappingInMilliseconds dur.remappingInMilliseconds MongoDB mongo.configServer.dur.timeCollectedCommitsInMilliseconds dur.timeCollectedCommitsInMilliseconds MongoDB mongo.configServer.dur.writingDataFilesInMilliseconds dur.writingDataFilesInMilliseconds MongoDB mongo.configServer.dur.writingJournalInMilliseconds dur.writingJournalInMilliseconds MongoDB mongo.configServer.flush.averageInMilliseconds flush.averageInMilliseconds MongoDB mongo.configServer.flush.flushesDisk flush.flushesDisk MongoDB mongo.configServer.flush.lastInMilliseconds flush.lastInMilliseconds MongoDB mongo.configServer.flush.totalInMilliseconds flush.totalInMilliseconds MongoDB mongo.configServer.getlasterror.wtimeMillisPerSecond getlasterror.wtimeMillisPerSecond MongoDB mongo.configServer.getlasterror.wtimeoutsPerSecond getlasterror.wtimeoutsPerSecond MongoDB mongo.configServer.globallock.activeClientsReaders globallock.activeClientsReaders MongoDB mongo.configServer.globallock.activeClientsTotal globallock.activeClientsTotal MongoDB mongo.configServer.globallock.activeClientsWriters globallock.activeClientsWriters MongoDB mongo.configServer.globallock.currentQueueReaders globallock.currentQueueReaders MongoDB mongo.configServer.globallock.currentQueueTotal globallock.currentQueueTotal MongoDB mongo.configServer.globallock.currentQueueWriters globallock.currentQueueWriters MongoDB mongo.configServer.globallock.totalTime globallock.totaltime MongoDB mongo.configServer.locks.collectionAcquireExclusive locks.collectionAcquireExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentExclusive locks.collectionAcquireIntentExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentShared locks.collectionAcquireIntentShared MongoDB mongo.configServer.locks.collectionAcquireWaitCountExclusive locks.collectionAcquireWaitCountExclusive MongoDB mongo.configServer.locks.collectionTimeAcquiringMicrosExclusive locks.collectionTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseAcquireExclusive locks.databaseAcquireExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentExclusive locks.databaseAcquireIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentShared locks.databaseAcquireIntentShared MongoDB mongo.configServer.locks.databaseAcquireShared locks.databaseAcquireShared MongoDB mongo.configServer.locks.databaseAcquireWaitExclusive locks.databaseAcquireWaitExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentExclusive locks.databaseAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentShared locks.databaseAcquireWaitIntentShared MongoDB mongo.configServer.locks.databaseAcquireWaitShared locks.databaseAcquireWaitShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosExclusive locks.databaseTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentExclusive locks.databaseTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentShared locks.databaseTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosShared locks.databaseTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.globalAcquireExclusive locks.globalAcquireExclusive MongoDB mongo.configServer.locks.globalAcquireIntentExclusive locks.globalAcquireIntentExclusive MongoDB mongo.configServer.locks.globalAcquireIntentShared locks.globalAcquireIntentShared MongoDB mongo.configServer.locks.globalAcquireShared locks.globalAcquireShared MongoDB mongo.configServer.locks.globalAcquireWaitExclusive locks.globalAcquireWaitExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentExclusive locks.globalAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentShared locks.globalAcquireWaitIntentShared MongoDB mongo.configServer.locks.globalAcquireWaitShared locks.globalAcquireWaitShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosExclusive locks.globalTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentExclusive locks.globalTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentShared locks.globalTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosShared locks.globalTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.metadataAcquireExclusive locks.metadataAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireExclusive locks.oplogAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentExclusive locks.oplogAcquireIntentExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentShared locks.oplogAcquireIntentShared MongoDB mongo",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.94492,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "New Relic Integrations Metrics The following table contains the metrics we collect for our <em>infrastructure</em> integrations. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent"
      },
      "id": "603e8a8a64441f69a34e8841"
    },
    {
      "sections": [
        "APM data in infrastructure monitoring",
        "How to integrate APM and infrastructure data",
        "View APM charts",
        "Filter by application data",
        "Tip",
        "Switch between infrastructure and APM",
        "APM data in Inventory and Events",
        "View host data in APM",
        "Troubleshoot missing APM data"
      ],
      "title": "APM data in infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "ac221ae748f8f2eb5a0ab7373853c5ea78974e41",
      "image": "https://docs.newrelic.com/static/4ab30e9528ae8a5121a1691143f80d44/ff42b/Infrastructure-APM-application-data-chart.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/apm-data-infrastructure-monitoring/",
      "published_at": "2021-05-05T03:39:35Z",
      "updated_at": "2021-03-16T07:33:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The integration of APM and infrastructure data lets you see your APM data and infrastructure data side by side so you can find the root cause of problems more quickly. Let's look at how the APM-Infrastructure integration works and how to make use of the shared data. The main ways to find and use APM data in infrastructure monitoring are: View APM charts on Infrastructure monitoring UI pages Filter hosts by application data Switch between Infrastructure and APM Examine APM data in Inventory and Events pages Infrastructure data appears in APM in the host table on the APM Summary page. How to integrate APM and infrastructure data For APM and infrastructure data to be integrated, all of the following must be true: The APM agent and the infrastructure agent must be installed on the same host. Both agents must use the same New Relic license key. They must use the same hostname. If the integration is not working, see Troubleshooting the APM-Infrastructure integration. View APM charts When your APM and infrastructure data is linked, you have access to APM data charts on these Infrastructure monitoring UI pages: Hosts, Network, Storage, and Processes. To switch to different charts: select the dropdown beside a chart's name and choose a new chart. Application-related charts will be near the top. one.newrelic.com > Infrastructure > Hosts: If your APM and Infrastructure data is linked, the charts in Infrastructure monitoring can be changed to show your application data. Filter by application data When your APM and infrastructure data is linked, you can filter displayed host data using Applications: From the host filter, select Applications. Select the application you want to filter on. Tip On the Hosts page, you can also filter by selecting items in the Applications column. Switch between infrastructure and APM When your APM and infrastructure accounts are linked, you can switch over from infrastructure to APM and vice versa for the same selected time range. You can switch from infrastructure to APM from these locations: From the host filter Applications menu On the Hosts page, when selecting applications in the Applications table column. You can switch from APM to infrastructure from the host table on the APM Summary page. APM data in Inventory and Events When your APM and infrastructure data is linked, you can view and filter on application data on the Infrastructure monitoring UI's Inventory page and the Events page. View host data in APM When your APM and infrastructure data is linked, you have more available host data in APM. The APM Summary page contains a table with data about your app's hosts and instances, including: Apdex Response time Throughput Error rate CPU usage Memory You can toggle between a table view or breakout metric details for the individual hosts by selecting View table or Break out each metric by host. For more information on host data on the APM Summary page, see host details. Troubleshoot missing APM data APM/Infrastructure integration should happen automatically if you have both the APM agent and the infrastructure agent installed on the same host(s) and they use the same New Relic license key and have the same hostname set. If you do not see APM data in infrastructure monitoring, see Troubleshooting.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.77258,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM <em>data</em> in <em>infrastructure</em> monitoring",
        "sections": "APM <em>data</em> in <em>infrastructure</em> monitoring",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "The integration of APM and <em>infrastructure</em> <em>data</em> lets you see <em>your</em> APM <em>data</em> and <em>infrastructure</em> <em>data</em> side by side so you can find the root cause of problems more quickly. Let&#x27;s look at how the APM-<em>Infrastructure</em> integration works and how to make use of the shared <em>data</em>. The main ways to find and use"
      },
      "id": "603e88b2e7b9d246932a07f6"
    },
    {
      "sections": [
        "Manage infrastructure data reporting"
      ],
      "title": "Manage infrastructure data reporting",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "7cd87ba8f7686e9233f4171021607d499bf6bc72",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/manage-infrastructure-data-reporting/",
      "published_at": "2021-05-05T03:38:21Z",
      "updated_at": "2021-03-16T07:33:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you use the infrastructure agent or on-host integrations that report data via the infrastructure agent, there are several ways to configure data reporting. Here are two common options for managing data reporting: Enable/disable process metrics Select specific attributes to report For other agent configuration options, see Configuration. For our infrastructure integrations, you can also change the frequency of data reporting: For on-host integrations: use a specific integration's interval configuration setting. For cloud integrations (AWS, Azure, Google Cloud): edit the polling frequency. For more about New Relic data management in general, see Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.77257,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Manage</em> <em>infrastructure</em> <em>data</em> reporting",
        "sections": "<em>Manage</em> <em>infrastructure</em> <em>data</em> reporting",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": " configuration options, see Configuration. For our <em>infrastructure</em> integrations, you can also change the frequency of <em>data</em> reporting: For on-host integrations: use a specific integration&#x27;s interval configuration setting. For cloud integrations (AWS, Azure, Google Cloud): edit the polling frequency. For more about New Relic <em>data</em> management in general, see <em>Manage</em> <em>data</em>."
      },
      "id": "603e775a196a67f470a83de4"
    }
  ],
  "/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics": [
    {
      "sections": [
        "Default infrastructure monitoring data",
        "Important",
        "Infrastructure events",
        "Supported Linux storage systems",
        "Supported Windows storage systems",
        "Query infrastructure data",
        "Manage data",
        "Add custom attributes",
        "Common Amazon EC2 attributes",
        "awsRegion",
        "awsAvailabilityZone",
        "ec2InstanceType",
        "ec2InstanceId",
        "ec2AmiId",
        "ec2SubnetId",
        "ec2VpcId",
        "Other Amazon EC2 attributes"
      ],
      "title": "Default infrastructure monitoring data ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "01647189a48892103f4dc6abe07ce29d5fc13f0d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/default-infrastructure-monitoring-data/",
      "published_at": "2021-05-05T03:38:21Z",
      "updated_at": "2021-03-30T08:36:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important As of April 12, 2021, we are upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. New Relic's infrastructure monitoring agent collects and displays data using six primary events, each with associated attributes that represent assorted metrics and metadata. Understanding infrastructure data can help you: Better understand our infrastructure monitoring UI. Manage your infrastructure data. Create better host filter sets. Run better queries of your data. Set up better monitoring solutions using custom attributes. Infrastructure events The following are events reported by default by the infrastructure agent and some infrastructure integrations. The attributes attached to these events are the metadata and metrics used to create our infrastructure UI visualizations. You can also create custom queries and charts of this data. If you're using integrations, see that integration's doc for more on reported data. For common AWS attributes, see AWS data. Select an event name in the following table to see its attributes. Event Description SystemSample SystemSample contains data describing the current overall state of the entire server, including CPU, memory, disk, and network. We take a snapshot of this data every 5 seconds and package it into a SystemSample event, which is then sent to New Relic. This data appears in the Hosts UI page. ProcessSample ProcessSample gathers detailed resource usage information from programs running on a single system. We take a snapshot of this data every 20 seconds for every active process and package it into a ProcessSample event, which is then sent to New Relic. This data appears on the Processes UI page. Important Process metrics are not sent to New Relic by default for accounts created after July 20, 2020. Enable process metrics to get this data into the Infrastructure monitoring UI. StorageSample StorageSample represents a single storage device associated with a server. Each sample gathers descriptive information about the device, the type of file system it uses, and its current usage and capacity. We take a snapshot of this data every 20 seconds for each mounted file system and package it into a StorageSample event, which is then sent to New Relic. This data appears on the Storage UI page. Important If your server uses disks with file systems other than the supported file systems in the following table, StorageSample events will not be generated for those disks. Supported Linux storage systems Supported Linux storage file systems: xfs vxfs btrfs ext ext2 ext3 ext4 hfs Supported Windows storage systems Supported Windows storage file systems: NTFS ReFS (version 1.0.976 and higher) NetworkSample NetworkSample captures the descriptive and state information for each network device associated with a server. It includes the device's interface and address information, as well as current usage data. We take a snapshot of this data every 10 seconds for each attached network interface and package it into a NetworkSample event, which is then sent to New Relic. This data appears on the Network UI page. ContainerSample ContainerSample collects the descriptive and state information for each Docker container. It includes the container's ID, name, image, image name, as well metrics about CPU, memory and networking. We take a snapshot of this data every 15 seconds for each container and package it into a ContainerSample event, which is then sent to New Relic. This data appears on the Containers UI page. For more information, see Docker monitoring. InfrastructureEvent InfrastructureEvent describes changes (deltas) that occur in a system's live state. When an inventory or system state is added, removed, or changed, New Relic will produce an InfrastructureEvent that logs that activity. This data appears on the Events UI page. To learn about infrastructure integration data, see the documentation for a specific integration. If an AWS integration is enabled, your infrastructure events may also have AWS attributes attached. Query infrastructure data You can query your infrastructure data to troubleshoot a problem or create a chart, or to understand what data is available. For example, to see what data is attached to ProcessSample, you would run this NRQL query: SELECT * FROM ProcessSample Copy You can also query infrastructure using dimensional metrics. Manage data For tips on managing data ingest and reporting, see Manage infrastructure data. Add custom attributes You can create custom attributes in the infrastructure agent's YAML file. Use this metadata to: Create infrastructure filter sets Populate the Group by menu Annotate your infrastructure data Common Amazon EC2 attributes If you connect your Amazon Elastic Compute Cloud (EC2) account to our infrastructure monitoring, we report data from your Amazon EC2 instances. Amazon EC2-related attributes are common attributes that can be used in any event. These attributes are drawn from the EC2 API. No CloudWatch information is collected. These attributes and their values are subject to change if Amazon changes the data they expose. awsRegion The region (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. awsAvailabilityZone The availability zone (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceType The Amazon Web Services instance type, displayed in AWS-specific codes. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceId The Amazon Web Services instance's unique identifying number for the server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2AmiId The Amazon Machine Image (AMI) identification number of the image used by Amazon Web Services to bootstrap the Amazon EC2 instance. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2SubnetId The networking sub-net identifier on which the server is connected. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2VpcId The Virtual Private Cloud identifier (if any) for this server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. Other Amazon EC2 attributes If Amazon Web Services changes the metadata they make available to New Relic, other attributes and values collected also may be available.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.7581,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Default <em>infrastructure</em> monitoring <em>data</em> ",
        "sections": "Default <em>infrastructure</em> monitoring <em>data</em>",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": ". <em>Manage</em> <em>your</em> <em>infrastructure</em> <em>data</em>. Create better host filter sets. Run better queries of <em>your</em> <em>data</em>. Set up better monitoring solutions using custom attributes. <em>Infrastructure</em> events The following are events reported by default by the <em>infrastructure</em> agent and some <em>infrastructure</em> integrations"
      },
      "id": "6043edcd28ccbcfa8a2c6086"
    },
    {
      "sections": [
        "APM data in infrastructure monitoring",
        "How to integrate APM and infrastructure data",
        "View APM charts",
        "Filter by application data",
        "Tip",
        "Switch between infrastructure and APM",
        "APM data in Inventory and Events",
        "View host data in APM",
        "Troubleshoot missing APM data"
      ],
      "title": "APM data in infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "ac221ae748f8f2eb5a0ab7373853c5ea78974e41",
      "image": "https://docs.newrelic.com/static/4ab30e9528ae8a5121a1691143f80d44/ff42b/Infrastructure-APM-application-data-chart.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/apm-data-infrastructure-monitoring/",
      "published_at": "2021-05-05T03:39:35Z",
      "updated_at": "2021-03-16T07:33:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The integration of APM and infrastructure data lets you see your APM data and infrastructure data side by side so you can find the root cause of problems more quickly. Let's look at how the APM-Infrastructure integration works and how to make use of the shared data. The main ways to find and use APM data in infrastructure monitoring are: View APM charts on Infrastructure monitoring UI pages Filter hosts by application data Switch between Infrastructure and APM Examine APM data in Inventory and Events pages Infrastructure data appears in APM in the host table on the APM Summary page. How to integrate APM and infrastructure data For APM and infrastructure data to be integrated, all of the following must be true: The APM agent and the infrastructure agent must be installed on the same host. Both agents must use the same New Relic license key. They must use the same hostname. If the integration is not working, see Troubleshooting the APM-Infrastructure integration. View APM charts When your APM and infrastructure data is linked, you have access to APM data charts on these Infrastructure monitoring UI pages: Hosts, Network, Storage, and Processes. To switch to different charts: select the dropdown beside a chart's name and choose a new chart. Application-related charts will be near the top. one.newrelic.com > Infrastructure > Hosts: If your APM and Infrastructure data is linked, the charts in Infrastructure monitoring can be changed to show your application data. Filter by application data When your APM and infrastructure data is linked, you can filter displayed host data using Applications: From the host filter, select Applications. Select the application you want to filter on. Tip On the Hosts page, you can also filter by selecting items in the Applications column. Switch between infrastructure and APM When your APM and infrastructure accounts are linked, you can switch over from infrastructure to APM and vice versa for the same selected time range. You can switch from infrastructure to APM from these locations: From the host filter Applications menu On the Hosts page, when selecting applications in the Applications table column. You can switch from APM to infrastructure from the host table on the APM Summary page. APM data in Inventory and Events When your APM and infrastructure data is linked, you can view and filter on application data on the Infrastructure monitoring UI's Inventory page and the Events page. View host data in APM When your APM and infrastructure data is linked, you have more available host data in APM. The APM Summary page contains a table with data about your app's hosts and instances, including: Apdex Response time Throughput Error rate CPU usage Memory You can toggle between a table view or breakout metric details for the individual hosts by selecting View table or Break out each metric by host. For more information on host data on the APM Summary page, see host details. Troubleshoot missing APM data APM/Infrastructure integration should happen automatically if you have both the APM agent and the infrastructure agent installed on the same host(s) and they use the same New Relic license key and have the same hostname set. If you do not see APM data in infrastructure monitoring, see Troubleshooting.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.77258,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM <em>data</em> in <em>infrastructure</em> monitoring",
        "sections": "APM <em>data</em> in <em>infrastructure</em> monitoring",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "The integration of APM and <em>infrastructure</em> <em>data</em> lets you see <em>your</em> APM <em>data</em> and <em>infrastructure</em> <em>data</em> side by side so you can find the root cause of problems more quickly. Let&#x27;s look at how the APM-<em>Infrastructure</em> integration works and how to make use of the shared <em>data</em>. The main ways to find and use"
      },
      "id": "603e88b2e7b9d246932a07f6"
    },
    {
      "sections": [
        "Manage infrastructure data reporting"
      ],
      "title": "Manage infrastructure data reporting",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "7cd87ba8f7686e9233f4171021607d499bf6bc72",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/manage-infrastructure-data-reporting/",
      "published_at": "2021-05-05T03:38:21Z",
      "updated_at": "2021-03-16T07:33:34Z",
      "document_type": "page",
      "popularity": 1,
      "body": "If you use the infrastructure agent or on-host integrations that report data via the infrastructure agent, there are several ways to configure data reporting. Here are two common options for managing data reporting: Enable/disable process metrics Select specific attributes to report For other agent configuration options, see Configuration. For our infrastructure integrations, you can also change the frequency of data reporting: For on-host integrations: use a specific integration's interval configuration setting. For cloud integrations (AWS, Azure, Google Cloud): edit the polling frequency. For more about New Relic data management in general, see Manage data.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.77257,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Manage</em> <em>infrastructure</em> <em>data</em> reporting",
        "sections": "<em>Manage</em> <em>infrastructure</em> <em>data</em> reporting",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": " configuration options, see Configuration. For our <em>infrastructure</em> integrations, you can also change the frequency of <em>data</em> reporting: For on-host integrations: use a specific integration&#x27;s interval configuration setting. For cloud integrations (AWS, Azure, Google Cloud): edit the polling frequency. For more about New Relic <em>data</em> management in general, see <em>Manage</em> <em>data</em>."
      },
      "id": "603e775a196a67f470a83de4"
    }
  ],
  "/docs/infrastructure/manage-your-data/data-instrumentation/manage-infrastructure-data-reporting": [
    {
      "sections": [
        "Default infrastructure monitoring data",
        "Important",
        "Infrastructure events",
        "Supported Linux storage systems",
        "Supported Windows storage systems",
        "Query infrastructure data",
        "Manage data",
        "Add custom attributes",
        "Common Amazon EC2 attributes",
        "awsRegion",
        "awsAvailabilityZone",
        "ec2InstanceType",
        "ec2InstanceId",
        "ec2AmiId",
        "ec2SubnetId",
        "ec2VpcId",
        "Other Amazon EC2 attributes"
      ],
      "title": "Default infrastructure monitoring data ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "01647189a48892103f4dc6abe07ce29d5fc13f0d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/default-infrastructure-monitoring-data/",
      "published_at": "2021-05-05T03:38:21Z",
      "updated_at": "2021-03-30T08:36:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important As of April 12, 2021, we are upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. New Relic's infrastructure monitoring agent collects and displays data using six primary events, each with associated attributes that represent assorted metrics and metadata. Understanding infrastructure data can help you: Better understand our infrastructure monitoring UI. Manage your infrastructure data. Create better host filter sets. Run better queries of your data. Set up better monitoring solutions using custom attributes. Infrastructure events The following are events reported by default by the infrastructure agent and some infrastructure integrations. The attributes attached to these events are the metadata and metrics used to create our infrastructure UI visualizations. You can also create custom queries and charts of this data. If you're using integrations, see that integration's doc for more on reported data. For common AWS attributes, see AWS data. Select an event name in the following table to see its attributes. Event Description SystemSample SystemSample contains data describing the current overall state of the entire server, including CPU, memory, disk, and network. We take a snapshot of this data every 5 seconds and package it into a SystemSample event, which is then sent to New Relic. This data appears in the Hosts UI page. ProcessSample ProcessSample gathers detailed resource usage information from programs running on a single system. We take a snapshot of this data every 20 seconds for every active process and package it into a ProcessSample event, which is then sent to New Relic. This data appears on the Processes UI page. Important Process metrics are not sent to New Relic by default for accounts created after July 20, 2020. Enable process metrics to get this data into the Infrastructure monitoring UI. StorageSample StorageSample represents a single storage device associated with a server. Each sample gathers descriptive information about the device, the type of file system it uses, and its current usage and capacity. We take a snapshot of this data every 20 seconds for each mounted file system and package it into a StorageSample event, which is then sent to New Relic. This data appears on the Storage UI page. Important If your server uses disks with file systems other than the supported file systems in the following table, StorageSample events will not be generated for those disks. Supported Linux storage systems Supported Linux storage file systems: xfs vxfs btrfs ext ext2 ext3 ext4 hfs Supported Windows storage systems Supported Windows storage file systems: NTFS ReFS (version 1.0.976 and higher) NetworkSample NetworkSample captures the descriptive and state information for each network device associated with a server. It includes the device's interface and address information, as well as current usage data. We take a snapshot of this data every 10 seconds for each attached network interface and package it into a NetworkSample event, which is then sent to New Relic. This data appears on the Network UI page. ContainerSample ContainerSample collects the descriptive and state information for each Docker container. It includes the container's ID, name, image, image name, as well metrics about CPU, memory and networking. We take a snapshot of this data every 15 seconds for each container and package it into a ContainerSample event, which is then sent to New Relic. This data appears on the Containers UI page. For more information, see Docker monitoring. InfrastructureEvent InfrastructureEvent describes changes (deltas) that occur in a system's live state. When an inventory or system state is added, removed, or changed, New Relic will produce an InfrastructureEvent that logs that activity. This data appears on the Events UI page. To learn about infrastructure integration data, see the documentation for a specific integration. If an AWS integration is enabled, your infrastructure events may also have AWS attributes attached. Query infrastructure data You can query your infrastructure data to troubleshoot a problem or create a chart, or to understand what data is available. For example, to see what data is attached to ProcessSample, you would run this NRQL query: SELECT * FROM ProcessSample Copy You can also query infrastructure using dimensional metrics. Manage data For tips on managing data ingest and reporting, see Manage infrastructure data. Add custom attributes You can create custom attributes in the infrastructure agent's YAML file. Use this metadata to: Create infrastructure filter sets Populate the Group by menu Annotate your infrastructure data Common Amazon EC2 attributes If you connect your Amazon Elastic Compute Cloud (EC2) account to our infrastructure monitoring, we report data from your Amazon EC2 instances. Amazon EC2-related attributes are common attributes that can be used in any event. These attributes are drawn from the EC2 API. No CloudWatch information is collected. These attributes and their values are subject to change if Amazon changes the data they expose. awsRegion The region (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. awsAvailabilityZone The availability zone (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceType The Amazon Web Services instance type, displayed in AWS-specific codes. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceId The Amazon Web Services instance's unique identifying number for the server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2AmiId The Amazon Machine Image (AMI) identification number of the image used by Amazon Web Services to bootstrap the Amazon EC2 instance. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2SubnetId The networking sub-net identifier on which the server is connected. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2VpcId The Virtual Private Cloud identifier (if any) for this server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. Other Amazon EC2 attributes If Amazon Web Services changes the metadata they make available to New Relic, other attributes and values collected also may be available.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 198.75809,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Default <em>infrastructure</em> monitoring <em>data</em> ",
        "sections": "Default <em>infrastructure</em> monitoring <em>data</em>",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": ". <em>Manage</em> <em>your</em> <em>infrastructure</em> <em>data</em>. Create better host filter sets. Run better queries of <em>your</em> <em>data</em>. Set up better monitoring solutions using custom attributes. <em>Infrastructure</em> events The following are events reported by default by the <em>infrastructure</em> agent and some <em>infrastructure</em> integrations"
      },
      "id": "6043edcd28ccbcfa8a2c6086"
    },
    {
      "sections": [
        "On-host integrations metrics",
        "New Relic Integrations Metrics"
      ],
      "title": "On-host integrations metrics",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "fe96c0c4950380504b1a33c3ad861bcb17507cba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics/",
      "published_at": "2021-05-05T03:38:23Z",
      "updated_at": "2021-03-16T15:57:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Integrations Metrics The following table contains the metrics we collect for our infrastructure integrations. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent cpuPercent Agent host.cpuStealPercent cpuStealPercent Agent host.cpuSystemPercent cpuSystemPercent Agent host.cpuUserPercent cpuUserPercent Agent host.disk.avgQueueLen avgQueueLen Agent host.disk.avgReadQueueLen avgReadQueueLen Agent host.disk.avgWriteQueueLen avgWriteQueueLen Agent host.disk.currentQueueLen currentQueueLen Agent host.disk.freeBytes diskFreeBytes Agent host.disk.freePercent diskFreePercent Agent host.disk.inodesFree inodesFree Agent host.disk.inodesTotal inodesTotal Agent host.disk.inodesUsed inodesUsed Agent host.disk.inodesUsedPercent inodesUsedPercent Agent host.disk.readBytesPerSecond readBytesPerSecond Agent host.disk.readIoPerSecond readIoPerSecond Agent host.disk.readUtilizationPercent readUtilizationPercent Agent host.disk.readWriteBytesPerSecond readWriteBytesPerSecond Agent host.disk.totalBytes diskTotalBytes Agent host.disk.totalUtilizationPercent totalUtilizationPercent Agent host.disk.usedBytes diskUsedBytes Agent host.disk.usedPercent diskUsedPercent Agent host.disk.writeBytesPerSecond writeBytesPerSecond Agent host.disk.writeIoPerSecond writeIoPerSecond Agent host.disk.writeUtilizationPercent writeUtilizationPercent Agent host.diskFreeBytes diskFreeBytes Agent host.diskFreePercent diskFreePercent Agent host.diskReadsPerSecond diskReadsPerSecond Agent host.diskReadUtilizationPercent diskReadUtilizationPercent Agent host.diskTotalBytes diskTotalBytes Agent host.diskUsedBytes diskUsedBytes Agent host.diskUsedPercent diskUsedPercent Agent host.diskUtilizationPercent diskUtilizationPercent Agent host.diskWritesPerSecond diskWritesPerSecond Agent host.diskWriteUtilizationPercent diskWriteUtilizationPercent Agent host.loadAverageFifteenMinute loadAverageFifteenMinute Agent host.loadAverageFiveMinute loadAverageFiveMinute Agent host.loadAverageOneMinute loadAverageOneMinute Agent host.memoryFreeBytes memoryFreeBytes Agent host.memoryFreePercent memoryFreePercent Agent host.memoryTotalBytes memoryTotalBytes Agent host.memoryUsedBytes memoryUsedBytes Agent host.memoryUsedPercent memoryUsedPercent Agent host.net.receiveBytesPerSecond receiveBytesPerSecond Agent host.net.receiveDroppedPerSecond receiveDroppedPerSecond Agent host.net.receiveErrorsPerSecond receiveErrorsPerSecond Agent host.net.receivePacketsPerSecond receivePacketsPerSecond Agent host.net.transmitBytesPerSecond transmitBytesPerSecond Agent host.net.transmitDroppedPerSecond transmitDroppedPerSecond Agent host.net.transmitErrorsPerSecond transmitErrorsPerSecond Agent host.net.transmitPacketsPerSecond transmitPacketsPerSecond Agent host.process.cpuPercent cpuPercent Agent host.process.cpuSystemPercent cpuSystemPercent Agent host.process.cpuUserPercent cpuUserPercent Agent host.process.fileDescriptorCount fileDescriptorCount Agent host.process.ioReadBytesPerSecond ioReadBytesPerSecond Agent host.process.ioReadCountPerSecond ioReadCountPerSecond Agent host.process.ioTotalReadBytes ioTotalReadBytes Agent host.process.ioTotalReadCount ioTotalReadCount Agent host.process.ioTotalWriteBytes ioTotalWriteBytes Agent host.process.ioTotalWriteCount ioTotalWriteCount Agent host.process.ioWriteBytesPerSecond ioWriteBytesPerSecond Agent host.process.ioWriteCountPerSecond ioWriteCountPerSecond Agent host.process.memoryResidentSizeBytes memoryResidentSizeBytes Agent host.process.memoryVirtualSizeBytes memoryVirtualSizeBytes Agent host.process.threadCount threadCount Agent host.swapFreeBytes swapFreeBytes Agent host.swapTotalBytes swapTotalBytes Agent host.swapUsedBytes swapUsedBytes Apache apache.server.busyWorkers server.busyWorkers Apache apache.server.idleWorkers server.idleWorkers Apache apache.server.net.bytesPerSecond net.bytesPerSecond Apache apache.server.net.requestsPerSecond net.requestsPerSecond Apache apache.server.scoreboard.closingWorkers server.scoreboard.closingWorkers Apache apache.server.scoreboard.dnsLookupWorkers server.scoreboard.dnsLookupWorkers Apache apache.server.scoreboard.finishingWorkers server.scoreboard.finishingWorkers Apache apache.server.scoreboard.idleCleanupWorkers server.scoreboard.idleCleanupWorkers Apache apache.server.scoreboard.keepAliveWorkers server.scoreboard.keepAliveWorkers Apache apache.server.scoreboard.loggingWorkers server.scoreboard.loggingWorkers Apache apache.server.scoreboard.readingWorkers server.scoreboard.readingWorkers Apache apache.server.scoreboard.startingWorkers server.scoreboard.startingWorkers Apache apache.server.scoreboard.totalWorkers server.scoreboard.totalWorkers Apache apache.server.scoreboard.writingWorkers server.scoreboard.writingWorkers Cassandra cassandra.node.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.node.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.node.client.connectedNativeClients client.connectedNativeClients Cassandra cassandra.node.commitLogCompletedTasksPerSecond db.commitLogCompletedTasksPerSecond Cassandra cassandra.node.commitLogPendingTasks db.commitLogPendindTasks Cassandra cassandra.node.commitLogTotalSizeBytes db.commitLogTotalSizeBytes Cassandra cassandra.node.droppedBatchRemoveMessagesPerSecond db.droppedBatchRemoveMessagesPerSecond Cassandra cassandra.node.droppedBatchStoreMessagesPerSecond db.droppedBatchStoreMessagesPerSecond Cassandra cassandra.node.droppedCounterMutationMessagesPerSecond db.droppedCounterMutationMessagesPerSecond Cassandra cassandra.node.droppedHintMessagesPerSecond db.droppedHintMessagesPerSecond Cassandra cassandra.node.droppedMutationMessagesPerSecond db.droppedMutationMessagesPerSecond Cassandra cassandra.node.droppedPagedRangeMessagesPerSecond db.droppedPagedRangeMessagesPerSecond Cassandra cassandra.node.droppedRangeSliceMessagesPerSecond db.droppedRangeSliceMessagesPerSecond Cassandra cassandra.node.droppedReadMessagesPerSecond db.droppedReadMessagesPerSecond Cassandra cassandra.node.droppedReadRepairMessagesPerSecond db.droppedReadRepairMessagesPerSecond Cassandra cassandra.node.droppedRequestResponseMessagesPerSecond db.droppedRequestResponseMessagesPerSecond Cassandra cassandra.node.droppedTraceMessagesPerSecond db.droppedTraceMessagesPerSecond Cassandra cassandra.node.keyCacheCapacityBytes db.keyCacheCapacityBytes Cassandra cassandra.node.keyCacheHitRate db.keyCacheHitRate Cassandra cassandra.node.keyCacheHitsPerSecond db.keyCacheHitsPerSecond Cassandra cassandra.node.keyCacheRequestsPerSecond db.keyCacheRequestsPerSecond Cassandra cassandra.node.keyCacheSizeBytes db.keyCacheSizeBytes Cassandra cassandra.node.liveSsTableCount db.liveSSTableCount Cassandra cassandra.node.loadBytes db.loadBytes Cassandra cassandra.node.query.casReadRequestsPerSecond query.CASReadRequestsPerSecond Cassandra cassandra.node.query.casWriteRequestsPerSecond query.CASWriteRequestsPerSecond Cassandra cassandra.node.query.rangeSliceRequestsPerSecond query.rangeSliceRequestsPerSecond Cassandra cassandra.node.query.rangeSliceTimeoutsPerSecond query.rangeSliceTimeoutsPerSecond Cassandra cassandra.node.query.rangeSliceUnavailablesPerSecond query.rangeSliceUnavailablesPerSecond Cassandra cassandra.node.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.node.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.node.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.node.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.node.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.node.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.node.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.node.query.readTimeoutsPerSecond query.readTimeoutsPerSecond Cassandra cassandra.node.query.readUnavailablesPerSecond query.readUnavailablesPerSecond Cassandra cassandra.node.query.viewWriteRequestsPerSecond query.viewWriteRequestsPerSecond Cassandra cassandra.node.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.node.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.node.query.writeTimeoutsPerSecond query.writeTimeoutsPerSecond Cassandra cassandra.node.query.writeUnavailablesPerSecond query.writeUnavailablesPerSecond Cassandra cassandra.node.rowCacheCapacityBytes db.rowCacheCapacityBytes Cassandra cassandra.node.rowCacheHitRate db.rowCacheHitRate Cassandra cassandra.node.rowCacheHitsPerSecond db.rowCacheHitsPerSecond Cassandra cassandra.node.rowCacheRequestsPerSecond db.rowCacheRequestsPerSecond Cassandra cassandra.node.rowCacheSizeBytes db.rowCacheSizeBytes Cassandra cassandra.node.storage.exceptionCount storage.exceptionCount Cassandra cassandra.node.threadPool.antiEntropyStage.activeTasks db.threadpool.internalAntiEntropyStageActiveTasks Cassandra cassandra.node.threadPool.antiEntropyStage.completedTasks db.threadpool.internalAntiEntropyStageCompletedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.currentlyBlockedTasks db.threadpool.internalAntiEntropyStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.pendingTasks db.threadpool.internalAntiEntropyStagePendingTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.activeTasks db.threadpool.internalCacheCleanupExecutorActiveTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.completedTasks db.threadpool.internalCacheCleanupExecutorCompletedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.currentlyBlockedTasks db.threadpool.internalCacheCleanupExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.pendingTasks db.threadpool.internalCacheCleanupExecutorPendingTasks Cassandra cassandra.node.threadPool.compactionExecutor.activeTasks db.threadpool.internalCompactionExecutorActiveTasks Cassandra cassandra.node.threadPool.compactionExecutor.completedTasks db.threadpool.internalCompactionExecutorCompletedTasks Cassandra cassandra.node.threadPool.compactionExecutor.currentlyBlockedTasks db.threadpool.internalCompactionExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.compactionExecutor.pendingTasks db.threadpool.internalCompactionExecutorPendingTasks Cassandra cassandra.node.threadPool.counterMutationStage.activeTasks db.threadpool.requestCounterMutationStageActiveTasks Cassandra cassandra.node.threadPool.counterMutationStage.completedTasks db.threadpool.requestCounterMutationStageCompletedTasks Cassandra cassandra.node.threadPool.counterMutationStage.currentlyBlockedTasks db.threadpool.requestCounterMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.counterMutationStage.pendingTasks db.threadpool.requestCounterMutationStagePendingTasks Cassandra cassandra.node.threadPool.gossipStage.activeTasks db.threadpool.internalGossipStageActiveTasks Cassandra cassandra.node.threadPool.gossipStage.completedTasks db.threadpool.internalGossipStageCompletedTasks Cassandra cassandra.node.threadPool.gossipStage.currentlyBlockedTasks db.threadpool.internalGossipStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.gossipStage.pendingTasks db.threadpool.internalGossipStagePendingTasks Cassandra cassandra.node.threadPool.hintsDispatcher.activeTasks db.threadpool.internalHintsDispatcherActiveTasks Cassandra cassandra.node.threadPool.hintsDispatcher.completedTasks db.threadpool.internalHintsDispatcherCompletedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.currentlyBlockedTasks db.threadpool.internalHintsDispatcherCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.pendingTasks db.threadpool.internalHintsDispatcherPendingTasks Cassandra cassandra.node.threadPool.internalResponseStage.activeTasks db.threadpool.internalInternalResponseStageActiveTasks Cassandra cassandra.node.threadPool.internalResponseStage.completedTasks db.threadpool.internalInternalResponseStageCompletedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pCurrentlyBlockedTasks db.threadpool.internalInternalResponseStagePCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pendingTasks db.threadpool.internalInternalResponseStagePendingTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.activeTasks db.threadpool.internalMemtableFlushWriterActiveTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.completedTasks db.threadpool.internalMemtableFlushWriterCompletedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.currentlyBlockedTasks db.threadpool.internalMemtableFlushWriterCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.pendingTasks db.threadpool.internalMemtableFlushWriterPendingTasks Cassandra cassandra.node.threadPool.memtablePostFlush.activeTasks db.threadpool.internalMemtablePostFlushActiveTasks Cassandra cassandra.node.threadPool.memtablePostFlush.completedTasks db.threadpool.internalMemtablePostFlushCompletedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.currentlyBlockedTasks db.threadpool.internalMemtablePostFlushCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.pendingTasks db.threadpool.internalMemtablePostFlushPendingTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.activeTasks db.threadpool.internalMemtableReclaimMemoryActiveTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.completedTasks db.threadpool.internalMemtableReclaimMemoryCompletedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.currentlyBlockedTasks db.threadpool.internalMemtableReclaimMemoryCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.pendingTasks db.threadpool.internalMemtableReclaimMemoryPendingTasks Cassandra cassandra.node.threadPool.migrationStage.activeTasks db.threadpool.internalMigrationStageActiveTasks Cassandra cassandra.node.threadPool.migrationStage.completedTasks db.threadpool.internalMigrationStageCompletedTasks Cassandra cassandra.node.threadPool.migrationStage.currentlyBlockedTasks db.threadpool.internalMigrationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.migrationStage.pendingTasks db.threadpool.internalMigrationStagePendingTasks Cassandra cassandra.node.threadPool.miscStage.activeTasks db.threadpool.internalMiscStageActiveTasks Cassandra cassandra.node.threadPool.miscStage.completedTasks db.threadpool.internalMiscStageCompletedTasks Cassandra cassandra.node.threadPool.miscStage.currentlyBlockedTasks db.threadpool.internalMiscStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.miscStage.pendingTasks db.threadpool.internalMiscStagePendingTasks Cassandra cassandra.node.threadPool.mutationStage.activeTasks db.threadpool.requestMutationStageActiveTasks Cassandra cassandra.node.threadPool.mutationStage.completedTasks db.threadpool.requestMutationStageCompletedTasks Cassandra cassandra.node.threadPool.mutationStage.currentlyBlockedTasks db.threadpool.requestMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.mutationStage.pendingTasks db.threadpool.requestMutationStagePendingTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.activeTasks db.threadpool.internalPendingRangeCalculatorActiveTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.completedTasks db.threadpool.internalPendingRangeCalculatorCompletedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.currentlyBlockedTasks db.threadpool.internalPendingRangeCalculatorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.pendingTasks db.threadpool.internalPendingRangeCalculatorPendingTasks Cassandra cassandra.node.threadPool.readRepairStage.activeTasks db.threadpool.requestReadRepairStageActiveTasks Cassandra cassandra.node.threadPool.readRepairStage.completedTasks db.threadpool.requestReadRepairStageCompletedTasks Cassandra cassandra.node.threadPool.readRepairStage.currentlyBlockedTasks db.threadpool.requestReadRepairStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readRepairStage.pendingTasks db.threadpool.requestReadRepairStagePendingTasks Cassandra cassandra.node.threadPool.readStage.activeTasks db.threadpool.requestReadStageActiveTasks Cassandra cassandra.node.threadPool.readStage.completedTasks db.threadpool.requestReadStageCompletedTasks Cassandra cassandra.node.threadPool.readStage.currentlyBlockedTasks db.threadpool.requestReadStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readStage.pendingTasks db.threadpool.requestReadStagePendingTasks Cassandra cassandra.node.threadPool.requestResponseStage.activeTasks db.threadpool.requestRequestResponseStageActiveTasks Cassandra cassandra.node.threadPool.requestResponseStage.completedTasks db.threadpool.requestRequestResponseStageCompletedTasks Cassandra cassandra.node.threadPool.requestResponseStage.currentlyBlockedTasks db.threadpool.requestRequestResponseStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.requestResponseStage.pendingTasks db.threadpool.requestRequestResponseStagePendingTasks Cassandra cassandra.node.threadPool.sampler.activeTasks db.threadpool.internalSamplerActiveTasks Cassandra cassandra.node.threadPool.sampler.completedTasks db.threadpool.internalSamplerCompletedTasks Cassandra cassandra.node.threadPool.sampler.currentlyBlockedTasks db.threadpool.internalSamplerCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.sampler.pendingTasks db.threadpool.internalSamplerPendingTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.activeTasks db.threadpool.internalSecondaryIndexManagementActiveTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.completedTasks db.threadpool.internalSecondaryIndexManagementCompletedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.currentlyBlockedTasks db.threadpool.internalSecondaryIndexManagementCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.pendingTasks db.threadpool.internalSecondaryIndexManagementPendingTasks Cassandra cassandra.node.threadPool.validationExecutor.activeTasks db.threadpool.internalValidationExecutorActiveTasks Cassandra cassandra.node.threadPool.validationExecutor.completedTasks db.threadpool.internalValidationExecutorCompletedTasks Cassandra cassandra.node.threadPool.validationExecutor.currentlyBlockedTasks db.threadpool.internalValidationExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.validationExecutor.pendingTasks db.threadpool.internalValidationExecutorPendingTasks Cassandra cassandra.node.threadPool.viewMutationStage.activeTasks db.threadpool.requestViewMutationStageActiveTasks Cassandra cassandra.node.threadPool.viewMutationStage.completedTasks db.threadpool.requestViewMutationStageCompletedTasks Cassandra cassandra.node.threadPool.viewMutationStage.currentlyBlockedTasks db.threadpool.requestViewMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.viewMutationStage.pendingTasks db.threadpool.requestViewMutationStagePendingTasks Cassandra cassandra.node.totalHintsInProgress db.totalHintsInProgress Cassandra cassandra.node.totalHintsPerSecond db.totalHintsPerSecond Cassandra cassandra.columnFamily.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.columnFamily.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.columnFamily.bloomFilterFalseRatio db.bloomFilterFalseRatio Cassandra cassandra.columnFamily.liveDiskSpaceUsedBytes db.liveDiskSpaceUsedBytes Cassandra cassandra.columnFamily.liveSsTableCount db.liveSSTableCount Cassandra cassandra.columnFamily.maxRowSize db.maxRowSize Cassandra cassandra.columnFamily.meanRowSize db.meanRowSize Cassandra cassandra.columnFamily.memtableLiveDataSize db.memtableLiveDataSize Cassandra cassandra.columnFamily.minRowSize db.minRowSize Cassandra cassandra.columnFamily.pendingCompactions db.pendingCompactions Cassandra cassandra.columnFamily.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.columnFamily.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.columnFamily.speculativeRetries db.speculativeRetries Cassandra cassandra.columnFamily.ssTablesPerRead50ThPercentileMilliseconds db.SSTablesPerRead50thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead75ThPercentileMilliseconds db.SSTablesPerRead75thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead95ThPercentileMilliseconds db.SSTablesPerRead95thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead98ThPercentileMilliseconds db.SSTablesPerRead98thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead999ThPercentileMilliseconds db.SSTablesPerRead999thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead99ThPercentileMilliseconds db.SSTablesPerRead99thPercentileMilliseconds Cassandra cassandra.columnFamily.tombstoneScannedHistogram50ThPercentile db.tombstoneScannedHistogram50thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram75ThPercentile db.tombstoneScannedHistogram75thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram95ThPercentile db.tombstoneScannedHistogram95thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram98ThPercentile db.tombstoneScannedHistogram98thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram999ThPercentile db.tombstoneScannedHistogram999thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram99ThPercentile db.tombstoneScannedHistogram99thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogramCount db.tombstoneScannedHistogramCount Consul consul.datacenter.catalog.criticalNodes catalog.criticalNodes Consul consul.datacenter.catalog.passingNodes catalog.passingNodes Consul consul.datacenter.catalog.registeredNodes catalog.registeredNodes Consul consul.datacenter.catalog.upNodes catalog.upNodes Consul consul.datacenter.catalog.warningNodes catalog.warningNodes Consul consul.datacenter.cluster.flaps cluster.flaps Consul consul.datacenter.cluster.suspects cluster.suspects Consul consul.datacenter.raft.commitTime raft.commitTimes Consul consul.datacenter.raft.commitTimeAvgInMilliseconds raft.commitTimeAvgInMilliseconds Consul consul.datacenter.raft.commitTimeMaxInMilliseconds raft.commitTimeMaxInMilliseconds Consul consul.datacenter.raft.completedLeaderElections raft.completedLeaderElections Consul consul.datacenter.raft.initiatedLeaderElections raft.initiatedLeaderElections Consul consul.datacenter.raft.lastContactAvgInMilliseconds raft.lastContactAvgInMilliseconds Consul consul.datacenter.raft.lastContactMaxInMilliseconds raft.lastContactMaxInMilliseconds Consul consul.datacenter.raft.lastContacts raft.lastContacts Consul consul.datacenter.raft.logDispatchAvgInMilliseconds raft.logDispatchAvgInMilliseconds Consul consul.datacenter.raft.logDispatches raft.logDispatches Consul consul.datacenter.raft.logDispatchMaxInMilliseconds raft.logDispatchMaxInMilliseconds Consul consul.datacenter.raft.txns raft.txns Consul consul.agent.aclCacheHitPerSecond agent.aclCacheHit Consul consul.agent.aclCacheMissPerSecond agent.aclCacheMiss Consul consul.agent.client.rpcFailed client.rpcFailed Consul consul.agent.client.rpcLoad client.rpcLoad Consul consul.agent.kvStores agent.kvStoress Consul consul.agent.kvStoresAvgInMilliseconds agent.kvStoresAvgInMilliseconds Consul consul.agent.kvStoresMaxInMilliseconds agent.kvStoresMaxInMilliseconds Consul consul.agent.net.agent.maxLatencyInMilliseconds net.agent.maxLatencyInMilliseconds Consul consul.agent.net.medianLatencyInMilliseconds net.agent.medianLatencyInMilliseconds Consul consul.agent.net.minLatencyInMilliseconds net.agent.minLatencyInMilliseconds Consul consul.agent.net.p25LatencyInMilliseconds net.agent.p25LatencyInMilliseconds Consul consul.agent.net.p75LatencyInMilliseconds net.agent.p75LatencyInMilliseconds Consul consul.agent.net.p90LatencyInMilliseconds net.agent.p90LatencyInMilliseconds Consul consul.agent.net.p95LatencyInMilliseconds net.agent.p95LatencyInMilliseconds Consul consul.agent.net.p99LatencyInMilliseconds net.agent.p99LatencyInMilliseconds Consul consul.agent.peers agent.peers Consul consul.agent.runtime.allocations runtime.allocations Consul consul.agent.runtime.allocationsInBytes runtime.allocationsInBytes Consul consul.agent.runtime.frees runtime.frees Consul consul.agent.runtime.gcCycles runtime.gcCycles Consul consul.agent.runtime.gcPauseInMilliseconds runtime.gcPauseInMilliseconds Consul consul.agent.runtime.goroutines runtime.goroutines Consul consul.agent.runtime.heapObjects runtime.heapObjects Consul consul.agent.runtime.virtualAddressSpaceInBytes runtime.virtualAddressSpaceInBytes Consul consul.agent.staleQueries agent.staleQueries Consul consul.agent.txnAvgInMilliseconds agent.txnAvgInMilliseconds Consul consul.agent.txnMaxInMilliseconds agent.txnMaxInMilliseconds Consul consul.agent.txns agent.txns Couchbase couchbase.bucket.activeItemsEnteringDiskQueuePerSecond bucket.activeItemsEnteringDiskQueuePerSecond Couchbase couchbase.bucket.activeItemsInMemory bucket.activeItemsInMemory Couchbase couchbase.bucket.activeResidentItemsRatio bucket.activeResidentItemsRatio Couchbase couchbase.bucket.averageDiskCommitTimeInMilliseconds bucket.averageDiskCommitTimeInMilliseconds Couchbase couchbase.bucket.averageDiskUpdateTimeInMilliseconds bucket.averageDiskUpdateTimeInMilliseconds Couchbase couchbase.bucket.cacheMisses bucket.cacheMisses Couchbase couchbase.bucket.cacheMissRatio bucket.cacheMissRatio Couchbase couchbase.bucket.casHits bucket.casHits Couchbase couchbase.bucket.casMisses bucket.casMisses Couchbase couchbase.bucket.couchDocsFragmentationPercent bucket.couchDocsFragmentationPercent Couchbase couchbase.bucket.currentConnections bucket.currentConnections Couchbase couchbase.bucket.dataUsedInBytes bucket.dataUsedInBytes Couchbase couchbase.bucket.decrementHitsPerSecond bucket.decrementHitsPerSecond Couchbase couchbase.bucket.decrementMissesPerSecond bucket.decrementMissesPerSecond Couchbase couchbase.bucket.deleteHitsPerSecond bucket.deleteHitsPerSecond Couchbase couchbase.bucket.deleteMissesPerSecond bucket.deleteMissesPerSecond Couchbase couchbase.bucket.diskCreateOperationsPerSecond bucket.diskCreateOperationsPerSecond Couchbase couchbase.bucket.diskFetchesPerSecond bucket.diskFetchesPerSecond Couchbase couchbase.bucket.diskReadsPerSecond bucket.diskReadsPerSecond Couchbase couchbase.bucket.diskUpdateOperationsPerSecond bucket.diskUpdateOperationsPerSecond Couchbase couchbase.bucket.diskUsedInBytes bucket.diskUsedInBytes Couchbase couchbase.bucket.diskWriteQueue bucket.diskWriteQueue Couchbase couchbase.bucket.drainedItemsInQueue bucket.drainedItemsInQueue Couchbase couchbase.bucket.drainedItemsOnDiskQueue bucket.drainedItemsOnDiskQueue Couchbase couchbase.bucket.drainedPendingItemsInQueue bucket.drainedPendingItemsInQueue Couchbase couchbase.bucket.ejectionsPerSecond bucket.ejectionsPerSecond Couchbase couchbase.bucket.evictionsPerSecond bucket.evictionsPerSecond Couchbase couchbase.bucket.getHitsPerSecond bucket.getHitsPerSecond Couchbase couchbase.bucket.getMissesPerSecond bucket.getMissesPerSecond Couchbase couchbase.bucket.hitRatio bucket.hitRatio Couchbase couchbase.bucket.incrementHitsPerSecond bucket.incrementHitsPerSecond Couchbase couchbase.bucket.incrementMissesPerSecond bucket.incrementMissesPerSecond Couchbase couchbase.bucket.itemCount bucket.itemCount Couchbase couchbase.bucket.itemsBeingWritten bucket.itemsBeingWritten Couchbase couchbase.bucket.itemsEjectedFromMemoryToDisk bucket.itemsEjectedFromMemoryToDisk Couchbase couchbase.bucket.itemsOnDiskQueue bucket.itemsOnDiskQueue Couchbase couchbase.bucket.itemsQueuedForStorage bucket.itemsQueuedForStorage Couchbase couchbase.bucket.maximumMemoryUsage bucket.maximumMemoryUsage Couchbase couchbase.bucket.memoryHighWaterMarkInBytes bucket.memoryHighWaterMarkInBytes Couchbase couchbase.bucket.memoryLowWaterMarkInBytes bucket.memoryLowWaterMarkInBytes Couchbase couchbase.bucket.memoryUsedInBytes bucket.memoryUsedInBytes Couchbase couchbase.bucket.metadataInRamInBytes bucket.metadataInRAMInBytes Couchbase couchbase.bucket.missesPerSecond bucket.missesPerSecond Couchbase couchbase.bucket.outOfMemoryErrorsPerSecond bucket.outOfMemoryErrorsPerSecond Couchbase couchbase.bucket.overheadInBytes bucket.overheadInBytes Couchbase couchbase.bucket.pendingItemsInDiskQueue bucket.pendingItemsInDiskQueue Couchbase couchbase.bucket.pendingResidentItemsRatio bucket.pendingResidentItemsRatio Couchbase couchbase.bucket.quotaUtilization bucket.quotaUtilization Couchbase couchbase.bucket.readOperationsPerSecond bucket.readOperationsPerSecond Couchbase couchbase.bucket.readRatePerSecond bucket.readRatePerSecond Couchbase couchbase.bucket.recoverableOutOfMemoryCount bucket.recoverableOutOfMemoryCount Couchbase couchbase.bucket.replicaIndex bucket.replicaIndex Couchbase couchbase.bucket.replicaNumber bucket.replicaNumber Couchbase couchbase.bucket.replicaResidentItemsRatio bucket.replicaResidentItemsRatio Couchbase couchbase.bucket.residentItemsRatio bucket.residentItemsRatio Couchbase couchbase.bucket.temporaryOutOfMemoryErrorsPerSecond bucket.temporaryOutOfMemoryErrorsPerSecond Couchbase couchbase.bucket.threadsNumber bucket.threadsNumber Couchbase couchbase.bucket.totalItems bucket.totalItems Couchbase couchbase.bucket.totalOperationsPerSecond bucket.totalOperationsPerSecond Couchbase couchbase.bucket.viewFragmentationPercent bucket.viewFragmentationPercent Couchbase couchbase.bucket.writeOperationsPerSecond bucket.writeOperationsPerSecond Couchbase couchbase.bucket.writeRatePerSecond bucket.writeRatePerSecond Couchbase couchbase.cluster.autoFailoverCount cluster.autoFailoverCount Couchbase couchbase.cluster.autoFailoverEnabled cluster.autoFailoverEnabled Couchbase couchbase.cluster.databaseFragmentationThreshold cluster.databaseFragmentationThreshold Couchbase couchbase.cluster.diskFreeInBytes cluster.diskFreeInBytes Couchbase couchbase.cluster.diskQuotaTotalInBytes cluster.diskQuotaTotalInBytes Couchbase couchbase.cluster.diskTotalInBytes cluster.diskTotalInBytes Couchbase couchbase.cluster.diskUsedByDataInBytes cluster.diskUsedByDataInBytes Couchbase couchbase.cluster.diskUsedInBytes cluster.diskUsedInBytes Couchbase couchbase.cluster.indexFragmentationThreshold cluster.indexFragmentationThreshold Couchbase couchbase.cluster.maximumBucketCount cluster.maximumBucketCount Couchbase couchbase.cluster.memoryQuotaTotalInBytes cluster.memoryQuotaTotalInBytes Couchbase couchbase.cluster.memoryQuotaTotalPerNodeInBytes cluster.memoryQuotaTotalPerNodeInBytes Couchbase couchbase.cluster.memoryQuotaUsedInBytes cluster.memoryQuotaUsedInBytes Couchbase couchbase.cluster.memoryQuotaUsedPerNodeInBytes cluster.memoryQuotaUsedPerNodeInBytes Couchbase couchbase.cluster.memoryTotalInBytes cluster.memoryTotalInBytes Couchbase couchbase.cluster.memoryUsedByDataInBytes cluster.memoryUsedByDataInBytes Couchbase couchbase.cluster.memoryUsedInBytes cluster.memoryUsedInBytes Couchbase couchbase.cluster.viewFragmentationThreshold cluster.viewFragmentationThreshold Couchbase couchbase.node.backgroundFetches node.backgroundFetches Couchbase couchbase.node.cmdGet node.cmdGet Couchbase couchbase.node.couchDocsActualDiskSizeInBytes node.couchDocsActualDiskSizeInBytes Couchbase couchbase.node.couchDocsDataSizeInBytes node.couchDocsDataSizeInBytes Couchbase couchbase.node.couchSpatialDataSizeInBytes node.couchSpatialDataSizeInBytes Couchbase couchbase.node.couchSpatialDiskSizeInBytes node.couchSpatialDiskSizeInBytes Couchbase couchbase.node.couchViewsActualDiskSizeInBytes node.couchViewsActualDiskSizeInBytes Couchbase couchbase.node.couchViewsDataSizeInBytes node.couchViewsDataSizeInBytes Couchbase couchbase.node.cpuUtilization node.cpuUtilization Couchbase couchbase.node.currentItems node.currentItems Couchbase couchbase.node.currentItemsTotal node.currentItemsTotal Couchbase couchbase.node.getHits node.getHits Couchbase couchbase.node.memoryFreeInBytes node.memoryFreeInBytes Couchbase couchbase.node.memoryTotalInBytes node.memoryTotalInBytes Couchbase couchbase.node.memoryUsedInBytes node.memoryUsedInBytes Couchbase couchbase.node.ops node.ops Couchbase couchbase.node.swapTotalInBytes node.swapTotalInBytes Couchbase couchbase.node.swapUsedInBytes node.swapUsedInBytes Couchbase couchbase.node.uptimeInMilliseconds node.uptimeInMilliseconds Couchbase couchbase.node.vbucketActiveNonResidentItems node.vbucketActiveNonResidentItems Couchbase couchbase.node.vbucketInMemoryItems node.vbucketInMemoryItems Couchbase couchbase.queryengine.activeRequests queryengine.activeRequests Couchbase couchbase.queryengine.averageRequestTimeInMilliseconds queryengine.averageRequestTimeInMilliseconds Couchbase couchbase.queryengine.completedLimit queryengine.completedLimit Couchbase couchbase.queryengine.completedRequests queryengine.completedRequests Couchbase couchbase.queryengine.completedThresholdInMilliseconds queryengine.completedThresholdInMilliseconds Couchbase couchbase.queryengine.cores queryengine.cores Couchbase couchbase.queryengine.garbageCollectionNumber queryengine.garbageCollectionNumber Couchbase couchbase.queryengine.garbageCollectionPaused queryengine.garbageCollectionPaused Couchbase couchbase.queryengine.garbageCollectionTimePausedInMilliseconds queryengine.garbageCollectionTimePausedInMilliseconds Couchbase couchbase.queryengine.medianRequestTimeInMilliseconds queryengine.medianRequestTimeInMilliseconds Couchbase couchbase.queryengine.preparedStatementUtilization queryengine.preparedStatementUtilization Couchbase couchbase.queryengine.requestsLast15MinutesPerSecond queryengine.requestsLast15MinutesPerSecond Couchbase couchbase.queryengine.requestsLast1MinutesPerSecond queryengine.requestsLast1MinutesPerSecond Couchbase couchbase.queryengine.requestsLast5MinutesPerSecond queryengine.requestsLast5MinutesPerSecond Couchbase couchbase.queryengine.requestTime80thPercentileInMilliseconds queryengine.requestTime80thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime95thPercentileInMilliseconds queryengine.requestTime95thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime99thPercentileInMilliseconds queryengine.requestTime99thPercentileInMilliseconds Couchbase couchbase.queryengine.systemCpuUtilization queryengine.systemCPUUtilization Couchbase couchbase.queryengine.systemMemoryInBytes queryengine.systemMemoryInBytes Couchbase couchbase.queryengine.totalMemoryInBytes queryengine.totalMemoryInBytes Couchbase couchbase.queryengine.totalThreads queryengine.totalThreads Couchbase couchbase.queryengine.uptimeInMilliseconds queryengine.uptimeInMilliseconds Couchbase couchbase.queryengine.usedMemoryInBytes queryengine.usedMemoryInBytes Couchbase couchbase.queryengine.userCpuUtilization queryengine.userCPUUtilization Docker docker.container.cpuKernelPercent cpuKernelPercent Docker docker.container.cpuLimitCores cpuLimitCores Docker docker.container.cpuPercent cpuPercent Docker docker.container.cpuThrottlePeriods cpuThrottlePeriods Docker docker.container.cpuThrottleTimeMs cpuThrottleTimeMs Docker docker.container.cpuUsedCores cpuUsedCores Docker docker.container.cpuUsedCoresPercent cpuUsedCoresPercent Docker docker.container.cpuUserPercent cpuUserPercent Docker docker.container.ioReadBytesPerSecond ioReadBytesPerSecond Docker docker.container.ioReadCountPerSecond ioReadCountPerSecond Docker docker.container.ioTotalBytes ioTotalBytes Docker docker.container.ioTotalReadBytes ioTotalReadBytes Docker docker.container.ioTotalReadCount ioTotalReadCount Docker docker.container.ioTotalWriteBytes ioTotalWriteBytes Docker docker.container.ioTotalWriteCount ioTotalWriteCount Docker docker.container.ioWriteBytesPerSecond ioWriteBytesPerSecond Docker docker.container.ioWriteCountPerSecond ioWriteCountPerSecond Docker docker.container.memoryCacheBytes memoryCacheBytes Docker docker.container.memoryResidentSizeBytes memoryResidentSizeBytes Docker docker.container.memorySizeLimitBytes memorySizeLimitBytes Docker docker.container.memoryUsageBytes memoryUsageBytes Docker docker.container.memoryUsageLimitPercent memoryUsageLimitPercent Docker docker.container.networkRxBytes networkRxBytes Docker docker.container.networkRxBytesPerSecond networkRxBytesPerSecond Docker docker.container.networkRxDropped networkRxDropped Docker docker.container.networkRxDroppedPerSecond networkRxDroppedPerSecond Docker docker.container.networkRxErrors networkRxErrors Docker docker.container.networkRxErrorsPerSecond networkRxErrorsPerSecond Docker docker.container.networkRxPackets networkRxPackets Docker docker.container.networkRxPacketsPerSecond networkRxPacketsPerSecond Docker docker.container.networkTxBytes networkTxBytes Docker docker.container.networkTxBytesPerSecond networkTxBytesPerSecond Docker docker.container.networkTxDropped networkTxDropped Docker docker.container.networkTxDroppedPerSecond networkTxDroppedPerSecond Docker docker.container.networkTxErrors networkTxErrors Docker docker.container.networkTxErrorsPerSecond networkTxErrorsPerSecond Docker docker.container.networkTxPackets networkTxPackets Docker docker.container.networkTxPacketsPerSecond networkTxPacketsPerSecond Docker docker.container.pids pids Docker docker.container.processCount processCount Docker docker.container.processCountLimit processCountLimit Docker docker.container.restartCount restartCount Docker docker.container.threadCount threadCount Docker docker.container.threadCountLimit threadCountLimit ElasticSearch elasticsearch.cluster.dataNodes cluster.dataNodes ElasticSearch elasticsearch.cluster.nodes cluster.nodes ElasticSearch elasticsearch.cluster.shards.active shards.active ElasticSearch elasticsearch.cluster.shards.initializing shards.initializing ElasticSearch elasticsearch.cluster.shards.primaryActive shards.primaryActive ElasticSearch elasticsearch.cluster.shards.relocating shards.relocating ElasticSearch elasticsearch.cluster.shards.unassigned shards.unassigned ElasticSearch elasticsearch.cluster.tempData temp-data ElasticSearch elasticsearch.index.docs index.docs ElasticSearch elasticsearch.index.docsDeleted index.docsDeleted ElasticSearch elasticsearch.index.primaryShards index.primaryShards ElasticSearch elasticsearch.index.primaryStoreSizeInBytes index.primaryStoreSizeInBytes ElasticSearch elasticsearch.index.replicaShards index.replicaShards ElasticSearch elasticsearch.index.rollup.docsCount primaries.docsnumber ElasticSearch elasticsearch.index.rollup.docsDeleted primaries.docsDeleted ElasticSearch elasticsearch.index.rollup.flushTotal primaries.flushesTotal ElasticSearch elasticsearch.index.rollup.flushTotalTimeInMilliseconds primaries.flushTotalTimeInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsExist primaries.get.documentsExist ElasticSearch elasticsearch.index.rollup.get.documentsExistInMilliseconds primaries.get.documentsExistInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsMissing primaries.get.documentsMissing ElasticSearch elasticsearch.index.rollup.get.documentsMissingInMilliseconds primaries.get.documentsMissingInMilliseconds ElasticSearch elasticsearch.index.rollup.get.requests primaries.get.requests ElasticSearch elasticsearch.index.rollup.get.requestsCurrent primaries.get.requestsCurrent ElasticSearch elasticsearch.index.rollup.get.requestsInMilliseconds primaries.get.requestsInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeleted primaries.index.docsCurrentlyDeleted ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeletedInMilliseconds primaries.index.docsCurrentlyDeletedInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexing primaries.index.docsCurrentlyIndexing ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexingInMilliseconds primaries.index.docsCurrentlyIndexingInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsDeleted primaries.index.docsDeleted ElasticSearch elasticsearch.index.rollup.index.docsTotal primaries.index.docsTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotal primaries.indexRefreshesTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotalInMilliseconds primaries.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.merges.current primaries.merges.current ElasticSearch elasticsearch.index.rollup.merges.docsSegmentsCurrentlyMerged primaries.merges.docsSegmentsCurrentlyMerged ElasticSearch elasticsearch.index.rollup.merges.docsTotal primaries.merges.docsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsCurrentlyMergedInBytes primaries.merges.segmentsCurrentlyMergedInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotal primaries.merges.segmentsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInBytes primaries.merges.segmentsTotalInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInMilliseconds primaries.merges.segmentsTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesInMilliseconds primaries.queriesInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesTotal primaries.queriesTotal ElasticSearch elasticsearch.index.rollup.queryActive primaries.queryActive ElasticSearch elasticsearch.index.rollup.queryFetches primaries.queryFetches ElasticSearch elasticsearch.index.rollup.queryFetchesInMilliseconds primaries.queryFetchesInMilliseconds ElasticSearch elasticsearch.index.rollup.queryFetchesTotal primaries.queryFetchesTotal ElasticSearch elasticsearch.index.rollup.sizeInBytes primaries.sizeInBytes ElasticSearch elasticsearch.index.storeSizeInBytes index.storeSizeInBytes ElasticSearch elasticsearch.node.activeSearches activeSearches ElasticSearch elasticsearch.node.activeSearchesInMilliseconds activeSearchesInMilliseconds ElasticSearch elasticsearch.node.breakers.estimatedSizeFieldDataCircuitBreakerInBytes breakers.estimatedSizeFieldDataCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeParentCircuitBreakerInBytes breakers.estimatedSizeParentCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeRequestCircuitBreakerInBytes breakers.estimatedSizeRequestCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.fieldDataCircuitBreakerTripped breakers.fieldDataCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.parentCircuitBreakerTripped breakers.parentCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.requestCircuitBreakerTripped breakers.requestCircuitBreakerTripped ElasticSearch elasticsearch.node.flush.indexRefreshesTotal flush.indexRefreshesTotal ElasticSearch elasticsearch.node.flush.indexRefreshesTotalInMilliseconds flush.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.node.fs.bytesAvailableJvmInBytes fs.bytesAvailableJVMInBytes ElasticSearch elasticsearch.node.fs.dataRead fs.bytesReadsInBytes ElasticSearch elasticsearch.node.fs.dataWritten fs.writesInBytes ElasticSearch elasticsearch.node.fs.ioOperations fs.iOOperations ElasticSearch elasticsearch.node.fs.readOperations fs.reads ElasticSearch elasticsearch.node.fs.totalSizeInBytes fs.totalSizeInBytes ElasticSearch elasticsearch.node.fs.unallocatedBytes fs.unallocatedBytesInBYtes ElasticSearch elasticsearch.node.fs.writeOperations fs.writeOperations ElasticSearch elasticsearch.node.get.currentRequestsRunning get.currentRequestsRunning ElasticSearch elasticsearch.node.get.requestsDocumentExists get.requestsDocumentExists ElasticSearch elasticsearch.node.get.requestsDocumentExistsInMilliseconds get.requestsDocumentExistsInMilliseconds ElasticSearch elasticsearch.node.get.requestsDocumentMissing get.requestsDocumentMissing ElasticSearch elasticsearch.node.get.requestsDocumentMissingInMilliseconds get.requestsDocumentMissingInMilliseconds ElasticSearch elasticsearch.node.get.timeGetRequestsInMilliseconds get.timeGetRequestsInMilliseconds ElasticSearch elasticsearch.node.get.totalGetRequests get.totalGetRequests ElasticSearch elasticsearch.node.http.currentOpenConnections http.currentOpenConnections ElasticSearch elasticsearch.node.http.openedConnections http.openedConnections ElasticSearch elasticsearch.node.index.indexingOperationsFailed indices.indexingOperationsFailed ElasticSearch elasticsearch.node.index.indexingWaitedThrottlingInMilliseconds indices.indexingWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.memoryQueryCacheInBytes indices.memoryQueryCacheInBytes ElasticSearch elasticsearch.node.index.numberIndices indices.numberIndices ElasticSearch elasticsearch.node.index.queryCacheEvictions indices.queryCacheEvictions ElasticSearch elasticsearch.node.index.queryCacheHits indices.queryCacheHits ElasticSearch elasticsearch.node.index.queryCacheMisses indices.queryCacheMisses ElasticSearch elasticsearch.node.index.recoveryOngoingShardSource indices.recoveryOngoingShardSource ElasticSearch elasticsearch.node.index.recoveryOngoingShardTarget indices.recoveryOngoingShardTarget ElasticSearch elasticsearch.node.index.recoveryWaitedThrottlingInMilliseconds indices.recoveryWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.requestCacheEvictions indices.requestCacheEvictions ElasticSearch elasticsearch.node.index.requestCacheHits indices.requestCacheHits ElasticSearch elasticsearch.node.index.requestCacheMemoryInBytes indices.requestCacheMemoryInBytes ElasticSearch elasticsearch.node.index.requestCacheMisses indices.requestCacheMisses ElasticSearch elasticsearch.node.index.segmentsIndexShard indices.segmentsIndexShard ElasticSearch elasticsearch.node.index.segmentsMemoryUsedDocValuesInBytes indices.segmentsMemoryUsedDocValuesInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedFixedBitSetInBytes indices.segmentsMemoryUsedFixedBitSetInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexSegmentsInBytes indices.segmentsMemoryUsedIndexSegmentsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexWriterInBytes indices.segmentsMemoryUsedIndexWriterInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedNormsInBytes indices.segmentsMemoryUsedNormsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedSegmentVersionMapInBytes indices.segmentsMemoryUsedSegmentVersionMapInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedStoredFieldsInBytes indices.segmentsMemoryUsedStoredFieldsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermsInBytes indices.segmentsMemoryUsedTermsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermVectorsInBytes indices.segmentsMemoryUsedTermVectorsInBytes ElasticSearch elasticsearch.node.index.translogOperations indices.translogOperations ElasticSearch elasticsearch.node.index.translogOperationsInBytes indices.translogOperationsInBytes ElasticSearch elasticsearch.node.indexing.docsCurrentlyDeleted indexing.docsCurrentlyDeleted ElasticSearch elasticsearch.node.indexing.documentsCurrentlyIndexing indexing.documentsCurrentlyIndexing ElasticSearch elasticsearch.node.indexing.documentsIndexed indexing.documentsIndexed ElasticSearch elasticsearch.node.indexing.timeDeletingDocumentsInMilliseconds indexing.timeDeletingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.timeIndexingDocumentsInMilliseconds indexing.timeIndexingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.totalDocumentsDeleted indexing.totalDocumentsDeleted ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjects jvm.gc.majorCollectionsOldGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjects jvm.gc.majorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjects jvm.gc.minorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.mem.heapCommittedInBytes jvm.mem.heapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.heapMaxInBytes jvm.mem.heapMaxInBytes ElasticSearch elasticsearch.node.jvm.mem.heapUsed jvm.mem.heapUsed ElasticSearch elasticsearch.node.jvm.mem.heapUsedInBytes jvm.mem.heapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.maxOldGenerationHeapInBytes jvm.mem.maxOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.maxSurvivorSpaceInBytes jvm.mem.maxSurvivorSpaceInBYtes ElasticSearch elasticsearch.node.jvm.mem.maxYoungGenerationHeapInBytes jvm.mem.maxYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapCommittedInBytes jvm.mem.nonHeapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapUsedInBytes jvm.mem.nonHeapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.usedOldGenerationHeapInBytes jvm.mem.usedOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.usedSurvivorSpaceInBytes jvm.mem.usedSurvivorSpaceInBytes ElasticSearch elasticsearch.node.jvm.mem.usedYoungGenerationHeapInBytes jvm.mem.usedYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.threadsActive jvm.ThreadsActive ElasticSearch elasticsearch.node.jvm.threadsPeak jvm.ThreadsPeak ElasticSearch elasticsearch.node.merges.currentActive merges.currentActive ElasticSearch elasticsearch.node.merges.docsSegmentMerges merges.docsSegmentMerges ElasticSearch elasticsearch.node.merges.docsSegmentsMerging merges.docsSegmentsMerging ElasticSearch elasticsearch.node.merges.mergedSegmentsInBytes merges.mergedSegmentsInBytes ElasticSearch elasticsearch.node.merges.segmentMerges merges.segmentMerges ElasticSearch elasticsearch.node.merges.sizeSegmentsMergingInBytes merges.sizeSegmentsMergingInBytes ElasticSearch elasticsearch.node.merges.totalSegmentMergingInMilliseconds merges.totalSegmentMergingInMilliseconds ElasticSearch elasticsearch.node.openFd openFD ElasticSearch elasticsearch.node.queriesTotal queriesTotal ElasticSearch elasticsearch.node.refresh.total refresh.total ElasticSearch elasticsearch.node.refresh.totalInMilliseconds refresh.totalInMilliseconds ElasticSearch elasticsearch.node.searchFetchCurrentlyRunning searchFetchCurrentlyRunning ElasticSearch elasticsearch.node.searchFetches searchFetches ElasticSearch elasticsearch.node.sizeStoreInBytes sizeStoreInBytes ElasticSearch elasticsearch.node.threadpool.activeFetchShardStarted threadpool.activeFetchShardStarted ElasticSearch elasticsearch.node.threadpool.bulkActive threadpool.bulkActive ElasticSearch elasticsearch.node.threadpool.bulkQueue threadpool.bulkQueue ElasticSearch elasticsearch.node.threadpool.bulkRejected threadpool.bulkRejected ElasticSearch elasticsearch.node.threadpool.bulkThreads threadpool.bulkThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStartedQueue threadpool.fetchShardStartedQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStartedRejected threadpool.fetchShardStartedRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStartedThreads threadpool.fetchShardStartedThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStoreActive threadpool.fetchShardStoreActive ElasticSearch elasticsearch.node.threadpool.fetchShardStoreQueue threadpool.fetchShardStoreQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStoreRejected threadpool.fetchShardStoreRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStoreThreads threadpool.fetchShardStoreThreads ElasticSearch elasticsearch.node.threadpool.flushActive threadpool.flushActive ElasticSearch elasticsearch.node.threadpool.flushQueue threadpool.flushQueue ElasticSearch elasticsearch.node.threadpool.flushRejected threadpool.flushRejected ElasticSearch elasticsearch.node.threadpool.flushThreads threadpool.flushThreads ElasticSearch elasticsearch.node.threadpool.forceMergeActive threadpool.forceMergeActive ElasticSearch elasticsearch.node.threadpool.forceMergeQueue threadpool.forceMergeQueue ElasticSearch elasticsearch.node.threadpool.forceMergeRejected threadpool.forceMergeRejected ElasticSearch elasticsearch.node.threadpool.forceMergeThreads threadpool.forceMergeThreads ElasticSearch elasticsearch.node.threadpool.genericActive threadpool.genericActive ElasticSearch elasticsearch.node.threadpool.genericQueue threadpool.genericQueue ElasticSearch elasticsearch.node.threadpool.genericRejected threadpool.genericRejected ElasticSearch elasticsearch.node.threadpool.genericThreads threadpool.genericThreads ElasticSearch elasticsearch.node.threadpool.getActive threadpool.getActive ElasticSearch elasticsearch.node.threadpool.getQueue threadpool.getQueue ElasticSearch elasticsearch.node.threadpool.getRejected threadpool.getRejected ElasticSearch elasticsearch.node.threadpool.getThreads threadpool.getThreads ElasticSearch elasticsearch.node.threadpool.indexActive threadpool.indexActive ElasticSearch elasticsearch.node.threadpool.indexQueue threadpool.indexQueue ElasticSearch elasticsearch.node.threadpool.indexRejected threadpool.indexRejected ElasticSearch elasticsearch.node.threadpool.indexThreads threadpool.indexThreads ElasticSearch elasticsearch.node.threadpool.listenerActive threadpool.listenerActive ElasticSearch elasticsearch.node.threadpool.listenerQueue threadpool.listenerQueue ElasticSearch elasticsearch.node.threadpool.listenerRejected threadpool.listenerRejected ElasticSearch elasticsearch.node.threadpool.listenerThreads threadpool.listenerThreads ElasticSearch elasticsearch.node.threadpool.managementActive threadpool.managementActive ElasticSearch elasticsearch.node.threadpool.managementQueue threadpool.managementQueue ElasticSearch elasticsearch.node.threadpool.managementRejected threadpool.managementRejected ElasticSearch elasticsearch.node.threadpool.managementThreads threadpool.managementThreads ElasticSearch elasticsearch.node.threadpool.refreshActive threadpool.refreshActive ElasticSearch elasticsearch.node.threadpool.refreshQueue threadpool.refreshQueue ElasticSearch elasticsearch.node.threadpool.refreshRejected threadpool.refreshRejected ElasticSearch elasticsearch.node.threadpool.refreshThreads threadpool.refreshThreads ElasticSearch elasticsearch.node.threadpool.searchActive threadpool.searchActive ElasticSearch elasticsearch.node.threadpool.searchQueue threadpool.searchQueue ElasticSearch elasticsearch.node.threadpool.searchRejected threadpool.searchRejected ElasticSearch elasticsearch.node.threadpool.searchThreads threadpool.searchThreads ElasticSearch elasticsearch.node.threadpool.snapshotActive threadpool.snapshotActive ElasticSearch elasticsearch.node.threadpool.snapshotQueue threadpool.snapshotQueue ElasticSearch elasticsearch.node.threadpool.snapshotRejected threadpool.snapshotRejected ElasticSearch elasticsearch.node.threadpool.snapshotThreads threadpool.snapshotThreads ElasticSearch elasticsearch.node.transport.connectionsOpened transport.connectionsOpened ElasticSearch elasticsearch.node.transport.packetsReceived transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes F5 f5.node.availabilityState node.availabilityState F5 f5.node.connections node.connections F5 f5.node.connectionsPerSecond node.connectionsPerSecond F5 f5.node.enabled node.enabled F5 f5.node.inDataInBytesPerSecond node.inDataInBytesPerSecond F5 f5.node.monitorStatus node.monitorStatus F5 f5.node.outDataInBytesPerSecond node.outDataInBytesPerSecond F5 f5.node.packetsReceivedPerSecond node.packetsReceivedPerSecond F5 f5.node.packetsSentPerSecond node.packetsSentPerSecond F5 f5.node.requestsPerSecond node.requestsPerSecond F5 f5.node.sessions node.sessions F5 f5.node.sessionStatus node.sessionStatus F5 f5.poolMember.availabilityState member.availabilityState F5 f5.poolMember.connections member.connections F5 f5.poolMember.enabled member.enabled F5 f5.poolMember.inDataInBytesPerSecond member.inDataInBytesPerSecond F5 f5.poolMember.monitorStatus member.monitorStatus F5 f5.poolMember.outDataInBytesPerSecond member.outDataInBytesPerSecond F5 f5.poolMember.packetsReceivedPerSecond member.packetsReceivedPerSecond F5 f5.poolMember.packetsSentPerSecond member.packetsSentPerSecond F5 f5.poolMember.requestsPerSecond member.requestsPerSecond F5 f5.poolMember.sessions member.sessions F5 f5.poolMember.sessionStatus member.sessionStatus F5 f5.pool.activeMembers pool.activeMembers F5 f5.pool.availabilityState pool.availabilityState F5 f5.pool.connections pool.connections F5 f5.pool.connqAgeEdm pool.connqAgeEdm F5 f5.pool.connqAgeEma pool.connqAgeEma F5 f5.pool.connqAgeHead pool.connqAgeHead F5 f5.pool.connqAgeMax pool.connqAgeMax F5 f5.pool.connqAllAgeEdm pool.connqAllAgeEdm F5 f5.pool.connqAllAgeEma pool.connqAllAgeEma F5 f5.pool.connqAllAgeHead pool.connqAllAgeHead F5 f5.pool.connqAllAgeMax pool.connqAllAgeMax F5 f5.pool.connqAllDepth pool.connqAllDepth F5 f5.pool.connqDepth pool.connqDepth F5 f5.pool.currentConnections pool.currentConnections F5 f5.pool.enabled pool.enabled F5 f5.pool.inDataInBytesPerSecond pool.inDataInBytesPerSecond F5 f5.pool.minActiveMembers pool.minActiveMembers F5 f5.pool.outDataInBytesPerSecond pool.outDataInBytesPerSecond F5 f5.pool.packetsReceivedPerSecond pool.packetsReceivedPerSecond F5 f5.pool.packetsSentPerSecond pool.packetsSentPerSecond F5 f5.pool.requestsPerSecond pool.requestsPerSecond F5 f5.pool.sessions pool.sessions F5 f5.system.cpuIdleTicksPerSecond system.cpuIdleTicksPerSecond F5 f5.system.cpuIdleUtilization system.cpuIdleUtilization F5 f5.system.cpuInterruptRequestUtilization system.cpuInterruptRequestUtilization F5 f5.system.cpuIoWaitUtilization system.cpuIOWaitUtilization F5 f5.system.cpuNiceLevelUtilization system.cpuNiceLevelUtilization F5 f5.system.cpuSoftInterruptRequestUtilization system.cpuSoftInterruptRequestUtilization F5 f5.system.cpuStolenUtilization system.cpuStolenUtilization F5 f5.system.cpuSystemTicksPerSecond system.cpuSystemTicksPerSecond F5 f5.system.cpuSystemUtilization system.cpuSystemUtilization F5 f5.system.cpuUserTicksPerSecond system.cpuUserTicksPerSecond F5 f5.system.cpuUserUtilization system.cpuUserUtilization F5 f5.system.memoryFreeInBytes system.memoryFreeInBytes F5 f5.system.memoryTotalInBytes system.memoryTotalInBytes F5 f5.system.memoryUsedInBytes system.memoryUsedInBytes F5 f5.system.otherMemoryFreeInBytes system.otherMemoryFreeInBytes F5 f5.system.otherMemoryTotalInBytes system.otherMemoryTotalInBytes F5 f5.system.otherMemoryUsedInBytes system.otherMemoryUsedInBytes F5 f5.system.swapFreeInBytes system.swapFreeInBytes F5 f5.system.swapTotalInBytes system.swapTotalInBytes F5 f5.system.swapUsedInBytes system.swapUsedInBytes F5 f5.system.tmmMemoryFreeInBytes system.tmmMemoryFreeInBytes F5 f5.system.tmmMemoryTotalInBytes system.tmmMemoryTotalInBytes F5 f5.system.tmmMemoryUsedInBytes system.tmmMemoryUsedInBytes F5 f5.virtualserver.availabilityState virtualserver.availabilityState F5 f5.virtualserver.clientsideConnectionsPerSecond virtualserver.clientsideConnectionsPerSecond F5 f5.virtualserver.connections virtualserver.connections F5 f5.virtualserver.csMaxConnDur virtualserver.csMaxConnDur F5 f5.virtualserver.csMeanConnDur virtualserver.csMeanConnDur F5 f5.virtualserver.csMinConnDur virtualserver.csMinConnDur F5 f5.virtualserver.enabled virtualserver.enabled F5 f5.virtualserver.ephemeralBytesInPerSecond virtualserver.ephemeralBytesInPerSecond F5 f5.virtualserver.ephemeralBytesOutPerSecond virtualserver.ephemeralBytesOutPerSecond F5 f5.virtualserver.ephemeralConnectionsPerSecond virtualserver.ephemeralConnectionsPerSecond F5 f5.virtualserver.ephemeralCurrentConnections virtualserver.ephemeralCurrentConnections F5 f5.virtualserver.ephemeralEvictedConnectionsPerSecond virtualserver.ephemeralEvictedConnectionsPerSecond F5 f5.virtualserver.ephemeralMaxConnections virtualserver.ephemeralMaxConnections F5 f5.virtualserver.ephemeralPacketsReceivedPerSecond virtualserver.ephemeralPacketsReceivedPerSecond F5 f5.virtualserver.ephemeralPacketsSentPerSecond virtualserver.ephemeralPacketsSentPerSecond F5 f5.virtualserver.ephemeralSlowKilledPerSecond virtualserver.ephemeralSlowKilledPerSecond F5 f5.virtualserver.evictedConnsPerSecond virtualserver.evictedConnsPerSecond F5 f5.virtualserver.inDataInBytesPerSecond virtualserver.inDataInBytesPerSecond F5 f5.virtualserver.outDataInBytesPerSecond virtualserver.outDataInBytesPerSecond F5 f5.virtualserver.packetsReceivedPerSecond virtualserver.packetsReceivedPerSecond F5 f5.virtualserver.packetsSentPerSecond virtualserver.packetsSentPerSecond F5 f5.virtualserver.requestsPerSecond virtualserver.requestsPerSecond F5 f5.virtualserver.slowKilledPerSecond virtualserver.slowKilledPerSecond F5 f5.virtualserver.usageRatio virtualserver.usageRatio HAProxy haproxy.backend.activeServers backend.activeServers HAProxy haproxy.backend.averageConnectTimeInSeconds backend.averageConnectTimeInSeconds HAProxy haproxy.backend.averageQueueTimeInSeconds backend.averageQueueTimeInSeconds HAProxy haproxy.backend.averageResponseTimeInSeconds backend.averageResponseTimeInSeconds HAProxy haproxy.backend.averageTotalSessionTimeInSeconds backend.averageTotalSessionTimeInSeconds HAProxy haproxy.backend.backupServers backend.backupServers HAProxy haproxy.backend.bytesInPerSecond backend.bytesInPerSecond HAProxy haproxy.backend.bytesOutPerSecond backend.bytesOutPerSecond HAProxy haproxy.backend.bytesThatBypassedCompressorPerSecond backend.bytesThatBypassedCompressorPerSecond HAProxy haproxy.backend.connectingRequestErrorsPerSecond backend.connectingRequestErrorsPerSecond HAProxy haproxy.backend.connectionRetriesPerSecond backend.connectionRetriesPerSecond HAProxy haproxy.backend.currentQueuedRequestsWithoutServer backend.currentQueuedRequestsWithoutServer HAProxy haproxy.backend.currentSessions backend.currentSessions HAProxy haproxy.backend.dataTransfersAbortedByClientPerSecond backend.dataTransfersAbortedByClientPerSecond HAProxy haproxy.backend.dataTransfersAbortedByServerPerSecond backend.dataTransfersAbortedByServerPerSecond HAProxy haproxy.backend.downtimeInSeconds backend.downtimeInSeconds HAProxy haproxy.backend.http100ResponsesPerSecond backend.http100ResponsesPerSecond HAProxy haproxy.backend.http200ResponsesPerSecond backend.http200ResponsesPerSecond HAProxy haproxy.backend.http300ResponsesPerSecond backend.http300ResponsesPerSecond HAProxy haproxy.backend.http400ResponsesPerSecond backend.http400ResponsesPerSecond HAProxy haproxy.backend.http500ResponsesPerSecond backend.http500ResponsesPerSecond HAProxy haproxy.backend.httpOtherResponsesPerSecond backend.httpOtherResponsesPerSecond HAProxy haproxy.backend.httpRequestsPerSecond backend.httpRequestsPerSecond HAProxy haproxy.backend.httpResponseBytesEmittedByCompressorPerSecond backend.httpResponseBytesEmittedByCompressorPerSecond HAProxy haproxy.backend.httpResponseBytesFedToCompressorPerSecond backend.httpResponseBytesFedToCompressorPerSecond HAProxy haproxy.backend.httpResponsesCompressedPerSecond backend.httpResponsesCompressedPerSecond HAProxy haproxy.backend.interceptedRequestsPerSecond backend.interceptedRequestsPerSecond HAProxy haproxy.backend.maxQueuedRequestsWithoutServer backend.maxQueuedRequestsWithoutServer HAProxy haproxy.backend.maxSessions backend.maxSessions HAProxy haproxy.backend.maxSessionsPerSecond backend.maxSessionsPerSecond HAProxy haproxy.backend.requestRedispatchPerSecond backend.requestRedispatchPerSecond HAProxy haproxy.backend.requestsDenied.securityConcernsPerSecond backend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.backend.responseErrorsPerSecond backend.responseErrorsPerSecond HAProxy haproxy.backend.responsesDenied.securityConcernsPerSecond backend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.backend.serverSelectedPerSecond backend.serverSelectedPerSecond HAProxy haproxy.backend.sessionsPerSecond backend.sessionsPerSecond HAProxy haproxy.backend.timeSinceLastSessionAssignedInSeconds backend.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.backend.timeSinceLastUpDownTransitionInSeconds backend.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.backend.totalWeight backend.totalWeight HAProxy haproxy.backend.type backend.type HAProxy haproxy.backend.upToDownTransitionsPerSecond backend.upToDownTransitionsPerSecond HAProxy haproxy.frontend.bytesInPerSecond frontend.bytesInPerSecond HAProxy haproxy.frontend.bytesOutPerSecond frontend.bytesOutPerSecond HAProxy haproxy.frontend.connectionsPerSecond frontend.connectionsPerSecond HAProxy haproxy.frontend.currentSessions frontend.currentSessions HAProxy haproxy.frontend.http100ResponsesPerSecond frontend.http100ResponsesPerSecond HAProxy haproxy.frontend.http200ResponsesPerSecond frontend.http200ResponsesPerSecond HAProxy haproxy.frontend.http300ResponsesPerSecond frontend.http300ResponsesPerSecond HAProxy haproxy.frontend.http400ResponsesPerSecond frontend.http400ResponsesPerSecond HAProxy haproxy.frontend.http500ResponsesPerSecond frontend.http500ResponsesPerSecond HAProxy haproxy.frontend.httpOtherResponsesPerSecond frontend.httpOtherResponsesPerSecond HAProxy haproxy.frontend.httpRequests.maxPerSecond frontend.httpRequests.maxPerSecond HAProxy haproxy.frontend.httpRequestsPerSecond frontend.httpRequestsPerSecond HAProxy haproxy.frontend.interceptedRequestsPerSecond frontend.interceptedRequestsPerSecond HAProxy haproxy.frontend.maxConnectionsPerSecond frontend.maxConnectionsPerSecond HAProxy haproxy.frontend.maxSessions frontend.maxSessions HAProxy haproxy.frontend.maxSessionsPerSecond frontend.maxSessionsPerSecond HAProxy haproxy.frontend.requestErrorsPerSecond frontend.requestErrorsPerSecond HAProxy haproxy.frontend.requestsDenied.securityConcernsPerSecond frontend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestConnectionRulesPerSecond frontend.requestsDenied.tcpRequestConnectionRulesPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestSessionRulesPerSecond frontend.requestsDenied.tcpRequestSessionRulesPerSecond HAProxy haproxy.frontend.responsesDenied.securityConcernsPerSecond frontend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.frontend.sessionsPerSecond frontend.sessionsPerSecond HAProxy haproxy.server.averageConnectTimeInSeconds server.averageConnectTimeInSeconds HAProxy haproxy.server.averageQueueTimeInSeconds server.averageQueueTimeInSeconds HAProxy haproxy.server.averageResponseTimeInSeconds server.averageResponseTimeInSeconds HAProxy haproxy.server.averageTotalSessionTimeInSeconds server.averageTotalSessionTimeInSeconds HAProxy haproxy.server.bytesInPerSecond server.bytesInPerSecond HAProxy haproxy.server.bytesOutPerSecond server.bytesOutPerSecond HAProxy haproxy.server.connectingRequestErrorsPerSecond server.connectingRequestErrorsPerSecond HAProxy haproxy.server.connectionRetriesPerSecond server.connectionRetriesPerSecond HAProxy haproxy.server.currentQueuedRequestsWithoutServer server.currentQueuedRequestsWithoutServer HAProxy haproxy.server.currentSessions server.currentSessions HAProxy haproxy.server.dataTransfersAbortedByClientPerSecond server.dataTransfersAbortedByClientPerSecond HAProxy haproxy.server.dataTransfersAbortedByServerPerSecond server.dataTransfersAbortedByServerPerSecond HAProxy haproxy.server.downtimeInSeconds server.downtimeInSeconds HAProxy haproxy.server.failedChecksPerSecond server.failedChecksPerSecond HAProxy haproxy.server.healthCheckDurationInMilliseconds server.healthCheckDurationInMilliseconds HAProxy haproxy.server.http100ResponsesPerSecond server.http100ResponsesPerSecond HAProxy haproxy.server.http200ResponsesPerSecond server.http200ResponsesPerSecond HAProxy haproxy.server.http300ResponsesPerSecond server.http300ResponsesPerSecond HAProxy haproxy.server.http400ResponsesPerSecond server.http400ResponsesPerSecond HAProxy haproxy.server.http500ResponsesPerSecond server.http500ResponsesPerSecond HAProxy haproxy.server.httpOtherResponsesPerSecond server.httpOtherResponsesPerSecond HAProxy haproxy.server.isActive server.isActive HAProxy haproxy.server.isBackup server.isBackup HAProxy haproxy.server.maxQueuedRequestsWithoutServer server.maxQueuedRequestsWithoutServer HAProxy haproxy.server.maxSessions server.maxSessions HAProxy haproxy.server.maxSessionsPerSecond server.maxSessionsPerSecond HAProxy haproxy.server.requestRedispatchPerSecond server.requestRedispatchPerSecond HAProxy haproxy.server.requestsDenied.securityConcernsPerSecond server.requestsDenied.securityConcernsPerSecond HAProxy haproxy.server.responseErrorsPerSecond server.responseErrorsPerSecond HAProxy haproxy.server.responsesDenied.securityConcernsPerSecond server.responsesDenied.securityConcernsPerSecond HAProxy haproxy.server.serverSelectedPerSecond server.serverSelectedPerSecond HAProxy haproxy.server.serverWeight server.serverWeight HAProxy haproxy.server.sessionsPerSecond server.sessionsPerSecond HAProxy haproxy.server.throttlePercentage server.throttlePercentage HAProxy haproxy.server.timeSinceLastSessionAssignedInSeconds server.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.server.timeSinceLastUpDownTransitionInSeconds server.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.server.type server.type HAProxy haproxy.server.upToDownTransitionsPerSecond server.upToDownTransitionsPerSecond Kafka kafka.broker.bytesWrittenToTopicPerSecond broker.bytesWrittenToTopicPerSecond Kafka kafka.broker.consumer.requestsExpiredPerSecond consumer.requestsExpiredPerSecond Kafka kafka.broker.follower.requestExpirationPerSecond follower.requestExpirationPerSecond Kafka kafka.broker.ioInPerSecond broker.IOInPerSecond Kafka kafka.broker.ioOutPerSecond broker.IOOutPerSecond Kafka kafka.broker.logFlushPerSecond broker.logFlushPerSecond Kafka kafka.broker.messagesInPerSecond broker.messagesInPerSecond Kafka kafka.broker.net.bytesRejectedPerSecond net.bytesRejectedPerSecond Kafka kafka.broker.replication.isrExpandsPerSecond replication.isrExpandsPerSecond Kafka kafka.broker.replication.isrShrinksPerSecond replication.isrShrinksPerSecond Kafka kafka.broker.replication.leaderElectionPerSecond replication.leaderElectionPerSecond Kafka kafka.broker.replication.uncleanLeaderElectionPerSecond replication.uncleanLeaderElectionPerSecond Kafka kafka.broker.replication.unreplicatedPartitions replication.unreplicatedPartitions Kafka kafka.broker.request.avgTimeFetch request.avgTimeFetch Kafka kafka.broker.request.avgTimeMetadata request.avgTimeMetadata Kafka kafka.broker.request.avgTimeMetadata99Percentile request.avgTimeMetadata99Percentile Kafka kafka.broker.request.avgTimeOffset request.avgTimeOffset Kafka kafka.broker.request.avgTimeOffset99Percentile request.avgTimeOffset99Percentile Kafka kafka.broker.request.avgTimeProduceRequest request.avgTimeProduceRequest Kafka kafka.broker.request.avgTimeUpdateMetadata request.avgTimeUpdateMetadata Kafka kafka.broker.request.avgTimeUpdateMetadata99Percentile request.avgTimeUpdateMetadata99Percentile Kafka kafka.broker.request.clientFetchesFailedPerSecond request.clientFetchesFailedPerSecond Kafka kafka.broker.request.fetchConsumerRequestsPerSecond request.fetchConsumerRequestsPerSecond Kafka kafka.broker.request.fetchFollowerRequestsPerSecond request.fetchFollowerRequestsPerSecond Kafka kafka.broker.request.fetchTime99Percentile request.fetchTime99Percentile Kafka kafka.broker.request.handlerIdle request.handlerIdle Kafka kafka.broker.request.listGroupsRequestsPerSecond request.listGroupsRequestsPerSecond Kafka kafka.broker.request.metadataRequestsPerSecond request.metadataRequestsPerSecond Kafka kafka.broker.request.offsetCommitRequestsPerSecond request.offsetCommitRequestsPerSecond Kafka kafka.broker.request.produceRequestsFailedPerSecond request.produceRequestsFailedPerSecond Kafka kafka.broker.request.produceRequestsPerSecond request.produceRequestsPerSecond Kafka kafka.broker.request.produceTime99Percentile request.produceTime99Percentile Kafka kafka.broker.topic.diskSize topic.diskSize Kafka kafka.topic.bytesInPerSec topic.BytesInPerSec Kafka kafka.topic.bytesOutPerSec topic.BytesOutPerSec Kafka kafka.topic.messagesInPerSec topic.MessagesInPerSec Kafka kafka.topic.partitionsWithNonPreferredLeader topic.partitionsWithNonPreferredLeader Kafka kafka.topic.respondsToMetadataRequests topic.respondsToMetadataRequests Kafka kafka.topic.retentionBytesOrTime topic.retentionBytesOrTime Kafka kafka.topic.underReplicatedPartitions topic.underReplicatedPartitions Kafka kafka.producer.ageMetadataUsedInMilliseconds producer.ageMetadataUsedInMilliseconds Kafka kafka.producer.availableBufferInBytes producer.availableBufferInBytes Kafka kafka.producer.avgBytesSentPerRequestInBytes producer.avgBytesSentPerRequestInBytes Kafka kafka.producer.avgCompressionRateRecordBatches producer.avgCompressionRateRecordBatches Kafka kafka.producer.avgRecordAccumulatorsInMilliseconds producer.avgRecordAccumulatorsInMilliseconds Kafka kafka.producer.avgRecordSizeInBytes producer.avgRecordSizeInBytes Kafka kafka.producer.avgRecordsSentPerSecond producer.avgRecordsSentPerSecond Kafka kafka.producer.avgRecordsSentPerTopicPerSecond producer.avgRecordsSentPerTopicPerSecond Kafka kafka.producer.avgRequestLatency producer.avgRequestLatencyPerSecond Kafka kafka.producer.avgThrottleTime producer.avgThrottleTime Kafka kafka.producer.bufferMemoryAvailableInBytes producer.bufferMemoryAvailableInBytes Kafka kafka.producer.bufferpoolWaitTime producer.bufferpoolWaitTime Kafka kafka.producer.bytesOutPerSecond producer.bytesOutPerSecond Kafka kafka.producer.compressionRateRecordBatches producer.compressionRateRecordBatches Kafka kafka.producer.ioWaitTime producer.ioWaitTime Kafka kafka.producer.maxBytesSentPerRequestInBytes producer.maxBytesSentPerRequestInBytes Kafka kafka.producer.maxRecordSizeInBytes producer.maxRecordSizeInBytes Kafka kafka.producer.maxRequestLatencyInMilliseconds producer.maxRequestLatencyInMilliseconds Kafka kafka.producer.maxThrottleTime producer.maxThrottleTime Kafka kafka.producer.requestPerSecond producer.requestPerSecond Kafka kafka.producer.requestsWaitingResponse producer.requestsWaitingResponse Kafka kafka.producer.responsePerSecond producer.responsePerSecond Kafka kafka.producer.threadsWaiting producer.threadsWaiting Kafka kafka.consumer.avgFetchSizeInBytes consumer.avgFetchSizeInBytes Kafka kafka.consumer.avgRecordConsumedPerTopic consumer.avgRecordConsumedPerTopic Kafka kafka.consumer.avgRecordConsumedPerTopicPerSecond consumer.avgRecordConsumedPerTopicPerSecond Kafka kafka.consumer.bytesInPerSecond consumer.bytesInPerSecond Kafka kafka.consumer.fetchPerSecond consumer.fetchPerSecond Kafka kafka.consumer.hwm consumer.hwm Kafka kafka.consumer.lag consumer.lag Kafka kafka.consumer.maxFetchSizeInBytes consumer.maxFetchSizeInBytes Kafka kafka.consumer.maxLag consumer.maxLag Kafka kafka.consumer.messageConsumptionPerSecond consumer.messageConsumptionPerSecond Kafka kafka.consumer.offset consumer.offset Kafka kafka.consumer.totalLag consumer.totalLag Kafka kafka.consumerGroup.maxLag consumerGroup.maxLag Kafka kafka.consumerGroup.totalLag consumerGroup.totalLag Kubernetes k8s.apiserver.goGoroutines goGoroutines Kubernetes k8s.apiserver.goThreads goThreads Kubernetes k8s.apiserver.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.apiserver.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.controllermanager.goGoroutines goGoroutines Kubernetes k8s.controllermanager.goThreads goThreads Kubernetes k8s.controllermanager.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.controllermanager.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.controllermanager.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.goGoroutines goGoroutines Kubernetes k8s.etcd.goThreads goThreads Kubernetes k8s.etcd.mvccDbTotalSizeInBytes etcdMvccDbTotalSizeInBytes Kubernetes k8s.etcd.networkClientGrpcReceivedBytesRate etcdNetworkClientGrpcReceivedBytesRate Kubernetes k8s.etcd.networkClientGrpcSentBytesRate etcdNetworkClientGrpcSentBytesRate Kubernetes k8s.etcd.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.etcd.process.maxFds processMaxFds Kubernetes k8s.etcd.process.openFds processOpenFds Kubernetes k8s.etcd.process.processFdsUtilization processFdsUtilization Kubernetes k8s.etcd.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.serverHasLeader etcdServerHasLeader Kubernetes k8s.etcd.serverLeaderChangesSeenDelta etcdServerLeaderChangesSeenDelta Kubernetes k8s.etcd.serverProposalsAppliedDelta etcdServerProposalsAppliedDelta Kubernetes k8s.etcd.serverProposalsAppliedRate etcdServerProposalsAppliedRate Kubernetes k8s.etcd.serverProposalsCommittedDelta etcdServerProposalsCommittedDelta Kubernetes k8s.etcd.serverProposalsCommittedRate etcdServerProposalsCommittedRate Kubernetes k8s.etcd.serverProposalsFailedDelta etcdServerProposalsFailedDelta Kubernetes k8s.etcd.serverProposalsFailedRate etcdServerProposalsFailedRate Kubernetes k8s.etcd.serverProposalsPending etcdServerProposalsPending Kubernetes k8s.scheduler.goGoroutines goGoroutines Kubernetes k8s.scheduler.goThreads goThreads Kubernetes k8s.scheduler.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.scheduler.podPreemptionVictims schedulerPodPreemptionVictims Kubernetes k8s.scheduler.preemptionAttemptsDelta schedulerPreemptionAttemptsDelta Kubernetes k8s.scheduler.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.scheduler.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.container.cpuCfsPeriodsDelta containerCpuCfsPeriodsDelta Kubernetes k8s.container.cpuCfsPeriodsTotal containerCpuCfsPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledPeriodsDelta containerCpuCfsThrottledPeriodsDelta Kubernetes k8s.container.cpuCfsThrottledPeriodsTotal containerCpuCfsThrottledPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledSecondsDelta containerCpuCfsThrottledSecondsDelta Kubernetes k8s.container.cpuCfsThrottledSecondsTotal containerCpuCfsThrottledSecondsTotal Kubernetes k8s.container.cpuCoresUtilization cpuCoresUtilization Kubernetes k8s.container.cpuLimitCores cpuLimitCores Kubernetes k8s.container.cpuRequestedCores cpuRequestedCores Kubernetes k8s.container.cpuUsedCores cpuUsedCores Kubernetes k8s.container.fsAvailableBytes fsAvailableBytes Kubernetes k8s.container.fsCapacityBytes fsCapacityBytes Kubernetes k8s.container.fsInodes fsInodes Kubernetes k8s.container.fsInodesFree fsInodesFree Kubernetes k8s.container.fsInodesUsed fsInodesUsed Kubernetes k8s.container.fsUsedBytes fsUsedBytes Kubernetes k8s.container.fsUsedPercent fsUsedPercent Kubernetes k8s.container.isReady isReady Kubernetes k8s.container.memoryLimitBytes memoryLimitBytes Kubernetes k8s.container.memoryMappedFileBytes containerMemoryMappedFileBytes Kubernetes k8s.container.memoryRequestedBytes memoryRequestedBytes Kubernetes k8s.container.memoryUsedBytes memoryUsedBytes Kubernetes k8s.container.memoryUtilization memoryUtilization Kubernetes k8s.container.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.container.requestedCpuCoresUtilization requestedCpuCoresUtilization Kubernetes k8s.container.requestedMemoryUtilization requestedMemoryUtilization Kubernetes k8s.container.restartCount restartCount Kubernetes k8s.daemonset.createdAt createdAt Kubernetes k8s.daemonset.metadataGeneration metadataGeneration Kubernetes k8s.daemonset.podsAvailable podsAvailable Kubernetes k8s.daemonset.podsDesired podsDesired Kubernetes k8s.daemonset.podsMisscheduled podsMisscheduled Kubernetes k8s.daemonset.podsReady podsReady Kubernetes k8s.daemonset.podsScheduled podsScheduled Kubernetes k8s.daemonset.podsUnavailable podsUnavailable Kubernetes k8s.daemonset.podsUpdatedScheduled podsUpdatedScheduled Kubernetes k8s.deployment.createdAt createdAt Kubernetes k8s.deployment.podsAvailable podsAvailable Kubernetes k8s.deployment.podsDesired podsDesired Kubernetes k8s.deployment.podsMaxUnavailable podsMaxUnavailable Kubernetes k8s.deployment.podsTotal podsTotal Kubernetes k8s.deployment.podsUnavailable podsUnavailable Kubernetes k8s.deployment.podsUpdated podsUpdated Kubernetes k8s.endpoint.addressAvailable addressAvailable Kubernetes k8s.endpoint.addressNotReady addressNotReady Kubernetes k8s.endpoint.createdAt createdAt Kubernetes k8s.namespace.createdAt createdAt Kubernetes k8s.node.allocatableAttachableVolumes* allocatableAttachableVolumes* Kubernetes k8s.node.allocatableCpuCores allocatableCpuCores Kubernetes k8s.node.allocatableCpuCoresUtilization allocatableCpuCoresUtilization Kubernetes k8s.node.allocatableEphemeralStorageBytes allocatableEphemeralStorageBytes Kubernetes k8s.node.allocatableHugepages* allocatableHugepages* Kubernetes k8s.node.allocatableMemoryBytes allocatableMemoryBytes Kubernetes k8s.node.allocatableMemoryUtilization allocatableMemoryUtilization Kubernetes k8s.node.allocatablePods allocatablePods Kubernetes k8s.node.capacityAttachableVolumes* capacityAttachableVolumes* Kubernetes k8s.node.capacityCpuCores capacityCpuCores Kubernetes k8s.node.capacityEphemeralStorageBytes capacityEphemeralStorageBytes Kubernetes k8s.node.capacityHugepages* capacityHugepages* Kubernetes k8s.node.capacityMemoryBytes capacityMemoryBytes Kubernetes k8s.node.capacityPods capacityPods Kubernetes k8s.node.cpuUsedCoreMilliseconds cpuUsedCoreMilliseconds Kubernetes k8s.node.cpuUsedCores cpuUsedCores Kubernetes k8s.node.fsAvailableBytes fsAvailableBytes Kubernetes k8s.node.fsCapacityBytes fsCapacityBytes Kubernetes k8s.node.fsCapacityUtilization fsCapacityUtilization Kubernetes k8s.node.fsInodes fsInodes Kubernetes k8s.node.fsInodesFree fsInodesFree Kubernetes k8s.node.fsInodesUsed fsInodesUsed Kubernetes k8s.node.fsUsedBytes fsUsedBytes Kubernetes k8s.node.memoryAvailableBytes memoryAvailableBytes Kubernetes k8s.node.memoryMajorPageFaultsPerSecond memoryMajorPageFaultsPerSecond Kubernetes k8s.node.memoryPageFaults memoryPageFaults Kubernetes k8s.node.memoryRssBytes memoryRssBytes Kubernetes k8s.node.memoryUsedBytes memoryUsedBytes Kubernetes k8s.node.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.node.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.node.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.node.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.node.runtimeAvailableBytes runtimeAvailableBytes Kubernetes k8s.node.runtimeCapacityBytes runtimeCapacityBytes Kubernetes k8s.node.runtimeInodes runtimeInodes Kubernetes k8s.node.runtimeInodesFree runtimeInodesFree Kubernetes k8s.node.runtimeInodesUsed runtimeInodesUsed Kubernetes k8s.node.runtimeUsedBytes runtimeUsedBytes Kubernetes k8s.pod.createdAt createdAt Kubernetes k8s.pod.isReady isReady Kubernetes k8s.pod.isScheduled isScheduled Kubernetes k8s.pod.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.pod.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.pod.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.pod.startTime startTime Kubernetes k8s.replicaset.createdAt createdAt Kubernetes k8s.replicaset.observedGeneration observedGeneration Kubernetes k8s.replicaset.podsDesired podsDesired Kubernetes k8s.replicaset.podsFullyLabeled podsFullyLabeled Kubernetes k8s.replicaset.podsMissing podsMissing Kubernetes k8s.replicaset.podsReady podsReady Kubernetes k8s.replicaset.podsTotal podsTotal Kubernetes k8s.service.createdAt createdAt Kubernetes k8s.statefulset.createdAt createdAt Kubernetes k8s.statefulset.currentRevision currentRevision Kubernetes k8s.statefulset.metadataGeneration metadataGeneration Kubernetes k8s.statefulset.observedGeneration observedGeneration Kubernetes k8s.statefulset.podsCurrent podsCurrent Kubernetes k8s.statefulset.podsDesired podsDesired Kubernetes k8s.statefulset.podsReady podsReady Kubernetes k8s.statefulset.podsTotal podsTotal Kubernetes k8s.statefulset.podsUpdated podsUpdated Kubernetes k8s.statefulset.updateRevision updateRevision Kubernetes k8s.volume.fsAvailableBytes fsAvailableBytes Kubernetes k8s.volume.fsCapacityBytes fsCapacityBytes Kubernetes k8s.volume.fsInodes fsInodes Kubernetes k8s.volume.fsInodesFree fsInodesFree Kubernetes k8s.volume.fsInodesUsed fsInodesUsed Kubernetes k8s.volume.fsUsedBytes fsUsedBytes Kubernetes k8s.volume.fsUsedPercent fsUsedPercent Memcached memcached.server.activeSlabs activeSlabs Memcached memcached.server.avgItemSizeInBytes avgItemSizeInBytes Memcached memcached.server.bytesReadServerPerSecond bytesReadServerPerSecond Memcached memcached.server.bytesUsedServerInBytes bytesUsedServerInBytes Memcached memcached.server.bytesWrittenServerPerSecond bytesWrittenServerPerSecond Memcached memcached.server.casHitRatePerSecond casHitRatePerSecond Memcached memcached.server.casMissRatePerSecond casMissRatePerSecond Memcached memcached.server.casWrongRatePerSecond casWrongRatePerSecond Memcached memcached.server.cmdFlushRatePerSecond cmdFlushRatePerSecond Memcached memcached.server.cmdGetRatePerSecond cmdGetRatePerSecond Memcached memcached.server.cmdSetRatePerSecond cmdSetRatePerSecond Memcached memcached.server.connectionRateServerPerSecond connectionRateServerPerSecond Memcached memcached.server.connectionStructuresAllocated connectionStructuresAllocated Memcached memcached.server.currentItemsStoredServer currentItemsStoredServer Memcached memcached.server.deleteCmdNoneRemovedPerSecond deleteCmdNoneRemovedPerSecond Memcached memcached.server.deleteCmdRemovedPerSecond deleteCmdRemovedPerSecond Memcached memcached.server.evictionsPerSecond evictionsPerSecond Memcached memcached.server.getHitPercent getHitPercent Memcached memcached.server.getHitPerSecond getHitPerSecond Memcached memcached.server.getMissPerSecond getMissPerSecond Memcached memcached.server.itemsStoredPerSecond itemsStoredPerSecond Memcached memcached.server.limitBytesStorage limitBytesStorage Memcached memcached.server.limitMaxBytes limitMaxBytes Memcached memcached.server.maxConnectionLimitPerSecond serverMaxConnectionLimitPerSecond Memcached memcached.server.memAllocatedSlabsInBytes memAllocatedSlabsInBytes Memcached memcached.server.openConnectionsServer openConnectionsServer Memcached memcached.server.pointerSize pointerSize Memcached memcached.server.rusageSystem usageRate Memcached memcached.server.rusageUser executionTime Memcached memcached.server.storingItemsPercentMemory storingItemsPercentMemory Memcached memcached.server.threads threads Memcached memcached.server.uptimeInMilliseconds uptimeInMilliseconds Memcached memcached.slab.activeItemsBumpedPerSecond activeItemsBumpedPerSecond Memcached memcached.slab.casBadValPerSecond casBadValPerSecond Memcached memcached.slab.casModifiedSlabPerSecond casModifiedSlabPerSecond Memcached memcached.slab.chunkSizeInBytes chunkSizeInBytes Memcached memcached.slab.chunksPerPage chunksPerPage Memcached memcached.slab.cmdSetRateSlabPerSecond cmdSetRateSlabPerSecond Memcached memcached.slab.decrsModifySlabPerSecond decrsModifySlabPerSecond Memcached memcached.slab.deleteRateSlabPerSecond deleteRateSlabPerSecond Memcached memcached.slab.entriesReclaimedPerSecond entriesReclaimedPerSecond Memcached memcached.slab.evictionsBeforeExpirationPerSecond evictionsBeforeExpirationPerSecond Memcached memcached.slab.evictionsBeforeExplicitExpirationPerSecond evictionsBeforeExplicitExpirationPerSecond Memcached memcached.slab.expiredItemsReclaimedPerSecond expiredItemsReclaimedPerSecond Memcached memcached.slab.freedChunks freedChunks Memcached memcached.slab.freedChunksEnd freedChunksEnd Memcached memcached.slab.getHitRateSlabPerSecond getHitRateSlabPerSecond Memcached memcached.slab.incrsModifySlabPerSecond incrsModifySlabPerSecond Memcached memcached.slab.itemsCold itemsCold Memcached memcached.slab.itemsColdPerSecond itemsColdPerSecond Memcached memcached.slab.itemsDirectReclaimedPerSecond itemsDirectReclaimedPerSecond Memcached memcached.slab.itemsFreedCrawlerPerSecond itemsFreedCrawlerPerSecond Memcached memcached.slab.itemsHot itemsHot Memcached memcached.slab.itemsOldestInMilliseconds itemsOldestInMilliseconds Memcached memcached.slab.itemsRefcountLockedPerSecond itemsRefcountLockedPerSecond Memcached memcached.slab.itemsSlabClass itemsSlabClass Memcached memcached.slab.itemsTimeSinceEvictionInMilliseconds itemsTimeSinceEvictionInMilliseconds Memcached memcached.slab.itemsWarm itemsWarm Memcached memcached.slab.itemsWarmPerSecond itemsWarmPerSecond Memcached memcached.slab.memRequestedSlabInBytesPerSecond memRequestedSlabInBytesPerSecond Memcached memcached.slab.outOfMemoryPerSecond outOfMemoryPerSecond Memcached memcached.slab.selfHealedSlabPerSecond selfHealedSlabPerSecond Memcached memcached.slab.totalChunksSlab totalChunksSlab Memcached memcached.slab.totalPagesSlab totalPagesSlab Memcached memcached.slab.touchHitSlabPerSecond touchHitSlabPerSecond Memcached memcached.slab.usedChunksItems usedChunksItems Memcached memcached.slab.usedChunksPerSecond usedChunksPerSecond Memcached memcached.slab.validItemsEvictedPerSecond validItemsEvictedPerSecond MongoDB mongo.index.accesses collection.indexAccesses MongoDB mongo.index.sizeInBytes collection.indexSizeInBytes MongoDB mongo.collection.avgObjSizeInBytes collection.avgObjSizeInBytes MongoDB mongo.collection.capped collection.capped MongoDB mongo.collection.count collection.count MongoDB mongo.collection.max collection.max MongoDB mongo.collection.maxSizeInBytes collection.maxSizeInBytes MongoDB mongo.collection.nindexes collection.nindexes MongoDB mongo.collection.sizeInBytes collection.sizeInBytes MongoDB mongo.collection.storageSizeInBytes collection.storageSizeInBytes MongoDB mongo.configServer.asserts.messagesPerSecond asserts.messagesPerSecond MongoDB mongo.configServer.asserts.regularPerSecond asserts.regularPerSecond MongoDB mongo.configServer.asserts.rolloversPerSecond asserts.rolloversPerSecond MongoDB mongo.configServer.asserts.userPerSecond asserts.userPerSecond MongoDB mongo.configServer.asserts.warningPerSecond asserts.warningPerSecond MongoDB mongo.configServer.commands.countFailedPerSecond commands.countFailedPerSecond MongoDB mongo.configServer.commands.countPerSecond commands.countPerSecond MongoDB mongo.configServer.commands.createIndexesFailedPerSecond commands.createIndexesFailedPerSecond MongoDB mongo.configServer.commands.createIndexesPerSecond commands.createIndexesPerSecond MongoDB mongo.configServer.commands.deleteFailedPerSecond commands.deleteFailedPerSecond MongoDB mongo.configServer.commands.deletePerSecond commands.deletePerSecond MongoDB mongo.configServer.commands.evalFailedPerSecond commands.evalFailedPerSecond MongoDB mongo.configServer.commands.evalPerSecond commands.evalPerSecond MongoDB mongo.configServer.commands.findAndModifyFailedPerSecond commands.findAndModifyFailedPerSecond MongoDB mongo.configServer.commands.findAndModifyPerSecond commands.findAndModifyPerSecond MongoDB mongo.configServer.commands.insertFailedPerSecond commands.insertFailedPerSecond MongoDB mongo.configServer.commands.insertPerSecond commands.insertPerSecond MongoDB mongo.configServer.commands.updateFailedPerSecond commands.updateFailedPerSecond MongoDB mongo.configServer.commands.updatePerSecond commands.updatePerSecond MongoDB mongo.configServer.connections.available connections.available MongoDB mongo.configServer.connections.current connections.current MongoDB mongo.configServer.connections.totalCreated connections.totalCreated MongoDB mongo.configServer.cursor.openNoTimeout cursor.openNoTimeout MongoDB mongo.configServer.cursor.openPinned cursor.openPinned MongoDB mongo.configServer.cursor.openTotal cursor.openTotal MongoDB mongo.configServer.cursor.timedOutPerSecond cursor.timedOutPerSecond MongoDB mongo.configServer.document.deletedPerSecond document.deletedPerSecond MongoDB mongo.configServer.document.insertedPerSecond document.insertedPerSecond MongoDB mongo.configServer.document.returnedPerSecond document.returnedPerSecond MongoDB mongo.configServer.document.updatedPerSecond document.updatedPerSecond MongoDB mongo.configServer.dur.commits dur.commits MongoDB mongo.configServer.dur.commitsInWriteLock dur.commitsInWriteLock MongoDB mongo.configServer.dur.compression dur.compression MongoDB mongo.configServer.dur.earlyCommits dur.earlyCommits MongoDB mongo.configServer.dur.preparingInMilliseconds dur.preparingInMilliseconds MongoDB mongo.configServer.dur.remappingInMilliseconds dur.remappingInMilliseconds MongoDB mongo.configServer.dur.timeCollectedCommitsInMilliseconds dur.timeCollectedCommitsInMilliseconds MongoDB mongo.configServer.dur.writingDataFilesInMilliseconds dur.writingDataFilesInMilliseconds MongoDB mongo.configServer.dur.writingJournalInMilliseconds dur.writingJournalInMilliseconds MongoDB mongo.configServer.flush.averageInMilliseconds flush.averageInMilliseconds MongoDB mongo.configServer.flush.flushesDisk flush.flushesDisk MongoDB mongo.configServer.flush.lastInMilliseconds flush.lastInMilliseconds MongoDB mongo.configServer.flush.totalInMilliseconds flush.totalInMilliseconds MongoDB mongo.configServer.getlasterror.wtimeMillisPerSecond getlasterror.wtimeMillisPerSecond MongoDB mongo.configServer.getlasterror.wtimeoutsPerSecond getlasterror.wtimeoutsPerSecond MongoDB mongo.configServer.globallock.activeClientsReaders globallock.activeClientsReaders MongoDB mongo.configServer.globallock.activeClientsTotal globallock.activeClientsTotal MongoDB mongo.configServer.globallock.activeClientsWriters globallock.activeClientsWriters MongoDB mongo.configServer.globallock.currentQueueReaders globallock.currentQueueReaders MongoDB mongo.configServer.globallock.currentQueueTotal globallock.currentQueueTotal MongoDB mongo.configServer.globallock.currentQueueWriters globallock.currentQueueWriters MongoDB mongo.configServer.globallock.totalTime globallock.totaltime MongoDB mongo.configServer.locks.collectionAcquireExclusive locks.collectionAcquireExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentExclusive locks.collectionAcquireIntentExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentShared locks.collectionAcquireIntentShared MongoDB mongo.configServer.locks.collectionAcquireWaitCountExclusive locks.collectionAcquireWaitCountExclusive MongoDB mongo.configServer.locks.collectionTimeAcquiringMicrosExclusive locks.collectionTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseAcquireExclusive locks.databaseAcquireExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentExclusive locks.databaseAcquireIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentShared locks.databaseAcquireIntentShared MongoDB mongo.configServer.locks.databaseAcquireShared locks.databaseAcquireShared MongoDB mongo.configServer.locks.databaseAcquireWaitExclusive locks.databaseAcquireWaitExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentExclusive locks.databaseAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentShared locks.databaseAcquireWaitIntentShared MongoDB mongo.configServer.locks.databaseAcquireWaitShared locks.databaseAcquireWaitShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosExclusive locks.databaseTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentExclusive locks.databaseTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentShared locks.databaseTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosShared locks.databaseTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.globalAcquireExclusive locks.globalAcquireExclusive MongoDB mongo.configServer.locks.globalAcquireIntentExclusive locks.globalAcquireIntentExclusive MongoDB mongo.configServer.locks.globalAcquireIntentShared locks.globalAcquireIntentShared MongoDB mongo.configServer.locks.globalAcquireShared locks.globalAcquireShared MongoDB mongo.configServer.locks.globalAcquireWaitExclusive locks.globalAcquireWaitExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentExclusive locks.globalAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentShared locks.globalAcquireWaitIntentShared MongoDB mongo.configServer.locks.globalAcquireWaitShared locks.globalAcquireWaitShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosExclusive locks.globalTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentExclusive locks.globalTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentShared locks.globalTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosShared locks.globalTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.metadataAcquireExclusive locks.metadataAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireExclusive locks.oplogAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentExclusive locks.oplogAcquireIntentExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentShared locks.oplogAcquireIntentShared MongoDB mongo",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.9449,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "New Relic Integrations Metrics The following table contains the metrics we collect for our <em>infrastructure</em> integrations. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent"
      },
      "id": "603e8a8a64441f69a34e8841"
    },
    {
      "sections": [
        "APM data in infrastructure monitoring",
        "How to integrate APM and infrastructure data",
        "View APM charts",
        "Filter by application data",
        "Tip",
        "Switch between infrastructure and APM",
        "APM data in Inventory and Events",
        "View host data in APM",
        "Troubleshoot missing APM data"
      ],
      "title": "APM data in infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "ac221ae748f8f2eb5a0ab7373853c5ea78974e41",
      "image": "https://docs.newrelic.com/static/4ab30e9528ae8a5121a1691143f80d44/ff42b/Infrastructure-APM-application-data-chart.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/apm-data-infrastructure-monitoring/",
      "published_at": "2021-05-05T03:39:35Z",
      "updated_at": "2021-03-16T07:33:36Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The integration of APM and infrastructure data lets you see your APM data and infrastructure data side by side so you can find the root cause of problems more quickly. Let's look at how the APM-Infrastructure integration works and how to make use of the shared data. The main ways to find and use APM data in infrastructure monitoring are: View APM charts on Infrastructure monitoring UI pages Filter hosts by application data Switch between Infrastructure and APM Examine APM data in Inventory and Events pages Infrastructure data appears in APM in the host table on the APM Summary page. How to integrate APM and infrastructure data For APM and infrastructure data to be integrated, all of the following must be true: The APM agent and the infrastructure agent must be installed on the same host. Both agents must use the same New Relic license key. They must use the same hostname. If the integration is not working, see Troubleshooting the APM-Infrastructure integration. View APM charts When your APM and infrastructure data is linked, you have access to APM data charts on these Infrastructure monitoring UI pages: Hosts, Network, Storage, and Processes. To switch to different charts: select the dropdown beside a chart's name and choose a new chart. Application-related charts will be near the top. one.newrelic.com > Infrastructure > Hosts: If your APM and Infrastructure data is linked, the charts in Infrastructure monitoring can be changed to show your application data. Filter by application data When your APM and infrastructure data is linked, you can filter displayed host data using Applications: From the host filter, select Applications. Select the application you want to filter on. Tip On the Hosts page, you can also filter by selecting items in the Applications column. Switch between infrastructure and APM When your APM and infrastructure accounts are linked, you can switch over from infrastructure to APM and vice versa for the same selected time range. You can switch from infrastructure to APM from these locations: From the host filter Applications menu On the Hosts page, when selecting applications in the Applications table column. You can switch from APM to infrastructure from the host table on the APM Summary page. APM data in Inventory and Events When your APM and infrastructure data is linked, you can view and filter on application data on the Infrastructure monitoring UI's Inventory page and the Events page. View host data in APM When your APM and infrastructure data is linked, you have more available host data in APM. The APM Summary page contains a table with data about your app's hosts and instances, including: Apdex Response time Throughput Error rate CPU usage Memory You can toggle between a table view or breakout metric details for the individual hosts by selecting View table or Break out each metric by host. For more information on host data on the APM Summary page, see host details. Troubleshoot missing APM data APM/Infrastructure integration should happen automatically if you have both the APM agent and the infrastructure agent installed on the same host(s) and they use the same New Relic license key and have the same hostname set. If you do not see APM data in infrastructure monitoring, see Troubleshooting.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 189.77257,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM <em>data</em> in <em>infrastructure</em> monitoring",
        "sections": "APM <em>data</em> in <em>infrastructure</em> monitoring",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "The integration of APM and <em>infrastructure</em> <em>data</em> lets you see <em>your</em> APM <em>data</em> and <em>infrastructure</em> <em>data</em> side by side so you can find the root cause of problems more quickly. Let&#x27;s look at how the APM-<em>Infrastructure</em> integration works and how to make use of the shared <em>data</em>. The main ways to find and use"
      },
      "id": "603e88b2e7b9d246932a07f6"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/filter-group/filter-sets-organize-your-infrastructure-hosts": [
    {
      "sections": [
        "Group infrastructure results by specific attributes",
        "Group charts by specific attributes",
        "Combine filter sets and grouping",
        "Increased CPU usage on a single host"
      ],
      "title": "Group infrastructure results by specific attributes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Filter and group"
      ],
      "external_id": "8436b1d0391cd7caa2a79c4080528697ff7a012d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/filter-group/group-infrastructure-results-specific-attributes/",
      "published_at": "2021-05-05T00:33:10Z",
      "updated_at": "2021-03-11T10:49:08Z",
      "document_type": "page",
      "popularity": 1,
      "body": "In our infrastructure monitoring tool, you can use the Group by feature to group chart results by specific attributes. For example, on the Hosts page, you might display the AWS regions with the highest CPU usage grouped by awsRegion. Group by is available near the top of some infrastructure monitoring UI pages. Group charts by specific attributes On some infrastructure pages you can use Group by to group page results and charts by a specific attribute, such as host name, entity ID, or AWS region. The attributes available to group by will depend on your system setup. These may include: Default infrastructure attributes Custom attributes APM-related attributes To group infrastructure results by a specific attribute: On pages that have this feature, select Group by (located beside the time picker). From the dropdown, select an attribute to group by. Combine filter sets and grouping Grouping applies to any filter sets you have selected. By combining filter sets with Group by, you can find detailed system information quickly. Increased CPU usage on a single host On the Filter sets sidebar, you see alert threshold violations as Critical icon or Warning icon on one of your filter sets. To view only the hosts related to the filter set on your Hosts page, click the filter set name. To determine which of the hosts is causing the problem, select Group by, then select the hostname attribute. Review the charts which now show the hosts, by name, with the highest CPU usage.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 209.57806,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Group</em> <em>infrastructure</em> results by specific attributes",
        "sections": "<em>Group</em> <em>infrastructure</em> results by specific attributes",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": " monitoring UI pages. <em>Group</em> charts by specific attributes On some <em>infrastructure</em> pages you can use <em>Group</em> by to <em>group</em> page results and charts by a specific attribute, such as host name, entity ID, or AWS region. The attributes available to <em>group</em> by will depend on <em>your</em> system setup. These may include: Default"
      },
      "id": "6043edcd64441fd920378ecf"
    },
    {
      "sections": [
        "Default infrastructure monitoring data",
        "Important",
        "Infrastructure events",
        "Supported Linux storage systems",
        "Supported Windows storage systems",
        "Query infrastructure data",
        "Manage data",
        "Add custom attributes",
        "Common Amazon EC2 attributes",
        "awsRegion",
        "awsAvailabilityZone",
        "ec2InstanceType",
        "ec2InstanceId",
        "ec2AmiId",
        "ec2SubnetId",
        "ec2VpcId",
        "Other Amazon EC2 attributes"
      ],
      "title": "Default infrastructure monitoring data ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "01647189a48892103f4dc6abe07ce29d5fc13f0d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/default-infrastructure-monitoring-data/",
      "published_at": "2021-05-05T03:38:21Z",
      "updated_at": "2021-03-30T08:36:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important As of April 12, 2021, we are upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. New Relic's infrastructure monitoring agent collects and displays data using six primary events, each with associated attributes that represent assorted metrics and metadata. Understanding infrastructure data can help you: Better understand our infrastructure monitoring UI. Manage your infrastructure data. Create better host filter sets. Run better queries of your data. Set up better monitoring solutions using custom attributes. Infrastructure events The following are events reported by default by the infrastructure agent and some infrastructure integrations. The attributes attached to these events are the metadata and metrics used to create our infrastructure UI visualizations. You can also create custom queries and charts of this data. If you're using integrations, see that integration's doc for more on reported data. For common AWS attributes, see AWS data. Select an event name in the following table to see its attributes. Event Description SystemSample SystemSample contains data describing the current overall state of the entire server, including CPU, memory, disk, and network. We take a snapshot of this data every 5 seconds and package it into a SystemSample event, which is then sent to New Relic. This data appears in the Hosts UI page. ProcessSample ProcessSample gathers detailed resource usage information from programs running on a single system. We take a snapshot of this data every 20 seconds for every active process and package it into a ProcessSample event, which is then sent to New Relic. This data appears on the Processes UI page. Important Process metrics are not sent to New Relic by default for accounts created after July 20, 2020. Enable process metrics to get this data into the Infrastructure monitoring UI. StorageSample StorageSample represents a single storage device associated with a server. Each sample gathers descriptive information about the device, the type of file system it uses, and its current usage and capacity. We take a snapshot of this data every 20 seconds for each mounted file system and package it into a StorageSample event, which is then sent to New Relic. This data appears on the Storage UI page. Important If your server uses disks with file systems other than the supported file systems in the following table, StorageSample events will not be generated for those disks. Supported Linux storage systems Supported Linux storage file systems: xfs vxfs btrfs ext ext2 ext3 ext4 hfs Supported Windows storage systems Supported Windows storage file systems: NTFS ReFS (version 1.0.976 and higher) NetworkSample NetworkSample captures the descriptive and state information for each network device associated with a server. It includes the device's interface and address information, as well as current usage data. We take a snapshot of this data every 10 seconds for each attached network interface and package it into a NetworkSample event, which is then sent to New Relic. This data appears on the Network UI page. ContainerSample ContainerSample collects the descriptive and state information for each Docker container. It includes the container's ID, name, image, image name, as well metrics about CPU, memory and networking. We take a snapshot of this data every 15 seconds for each container and package it into a ContainerSample event, which is then sent to New Relic. This data appears on the Containers UI page. For more information, see Docker monitoring. InfrastructureEvent InfrastructureEvent describes changes (deltas) that occur in a system's live state. When an inventory or system state is added, removed, or changed, New Relic will produce an InfrastructureEvent that logs that activity. This data appears on the Events UI page. To learn about infrastructure integration data, see the documentation for a specific integration. If an AWS integration is enabled, your infrastructure events may also have AWS attributes attached. Query infrastructure data You can query your infrastructure data to troubleshoot a problem or create a chart, or to understand what data is available. For example, to see what data is attached to ProcessSample, you would run this NRQL query: SELECT * FROM ProcessSample Copy You can also query infrastructure using dimensional metrics. Manage data For tips on managing data ingest and reporting, see Manage infrastructure data. Add custom attributes You can create custom attributes in the infrastructure agent's YAML file. Use this metadata to: Create infrastructure filter sets Populate the Group by menu Annotate your infrastructure data Common Amazon EC2 attributes If you connect your Amazon Elastic Compute Cloud (EC2) account to our infrastructure monitoring, we report data from your Amazon EC2 instances. Amazon EC2-related attributes are common attributes that can be used in any event. These attributes are drawn from the EC2 API. No CloudWatch information is collected. These attributes and their values are subject to change if Amazon changes the data they expose. awsRegion The region (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. awsAvailabilityZone The availability zone (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceType The Amazon Web Services instance type, displayed in AWS-specific codes. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceId The Amazon Web Services instance's unique identifying number for the server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2AmiId The Amazon Machine Image (AMI) identification number of the image used by Amazon Web Services to bootstrap the Amazon EC2 instance. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2SubnetId The networking sub-net identifier on which the server is connected. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2VpcId The Virtual Private Cloud identifier (if any) for this server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. Other Amazon EC2 attributes If Amazon Web Services changes the metadata they make available to New Relic, other attributes and values collected also may be available.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 116.51643,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Default <em>infrastructure</em> monitoring <em>data</em> ",
        "sections": "Default <em>infrastructure</em> monitoring <em>data</em>",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": ". <em>Manage</em> <em>your</em> <em>infrastructure</em> <em>data</em>. Create better host <em>filter</em> sets. Run better queries of <em>your</em> <em>data</em>. Set up better monitoring solutions using custom attributes. <em>Infrastructure</em> events The following are events reported by default by the <em>infrastructure</em> agent and some <em>infrastructure</em> integrations"
      },
      "id": "6043edcd28ccbcfa8a2c6086"
    },
    {
      "sections": [
        "On-host integrations metrics",
        "New Relic Integrations Metrics"
      ],
      "title": "On-host integrations metrics",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "fe96c0c4950380504b1a33c3ad861bcb17507cba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics/",
      "published_at": "2021-05-05T03:38:23Z",
      "updated_at": "2021-03-16T15:57:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Integrations Metrics The following table contains the metrics we collect for our infrastructure integrations. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent cpuPercent Agent host.cpuStealPercent cpuStealPercent Agent host.cpuSystemPercent cpuSystemPercent Agent host.cpuUserPercent cpuUserPercent Agent host.disk.avgQueueLen avgQueueLen Agent host.disk.avgReadQueueLen avgReadQueueLen Agent host.disk.avgWriteQueueLen avgWriteQueueLen Agent host.disk.currentQueueLen currentQueueLen Agent host.disk.freeBytes diskFreeBytes Agent host.disk.freePercent diskFreePercent Agent host.disk.inodesFree inodesFree Agent host.disk.inodesTotal inodesTotal Agent host.disk.inodesUsed inodesUsed Agent host.disk.inodesUsedPercent inodesUsedPercent Agent host.disk.readBytesPerSecond readBytesPerSecond Agent host.disk.readIoPerSecond readIoPerSecond Agent host.disk.readUtilizationPercent readUtilizationPercent Agent host.disk.readWriteBytesPerSecond readWriteBytesPerSecond Agent host.disk.totalBytes diskTotalBytes Agent host.disk.totalUtilizationPercent totalUtilizationPercent Agent host.disk.usedBytes diskUsedBytes Agent host.disk.usedPercent diskUsedPercent Agent host.disk.writeBytesPerSecond writeBytesPerSecond Agent host.disk.writeIoPerSecond writeIoPerSecond Agent host.disk.writeUtilizationPercent writeUtilizationPercent Agent host.diskFreeBytes diskFreeBytes Agent host.diskFreePercent diskFreePercent Agent host.diskReadsPerSecond diskReadsPerSecond Agent host.diskReadUtilizationPercent diskReadUtilizationPercent Agent host.diskTotalBytes diskTotalBytes Agent host.diskUsedBytes diskUsedBytes Agent host.diskUsedPercent diskUsedPercent Agent host.diskUtilizationPercent diskUtilizationPercent Agent host.diskWritesPerSecond diskWritesPerSecond Agent host.diskWriteUtilizationPercent diskWriteUtilizationPercent Agent host.loadAverageFifteenMinute loadAverageFifteenMinute Agent host.loadAverageFiveMinute loadAverageFiveMinute Agent host.loadAverageOneMinute loadAverageOneMinute Agent host.memoryFreeBytes memoryFreeBytes Agent host.memoryFreePercent memoryFreePercent Agent host.memoryTotalBytes memoryTotalBytes Agent host.memoryUsedBytes memoryUsedBytes Agent host.memoryUsedPercent memoryUsedPercent Agent host.net.receiveBytesPerSecond receiveBytesPerSecond Agent host.net.receiveDroppedPerSecond receiveDroppedPerSecond Agent host.net.receiveErrorsPerSecond receiveErrorsPerSecond Agent host.net.receivePacketsPerSecond receivePacketsPerSecond Agent host.net.transmitBytesPerSecond transmitBytesPerSecond Agent host.net.transmitDroppedPerSecond transmitDroppedPerSecond Agent host.net.transmitErrorsPerSecond transmitErrorsPerSecond Agent host.net.transmitPacketsPerSecond transmitPacketsPerSecond Agent host.process.cpuPercent cpuPercent Agent host.process.cpuSystemPercent cpuSystemPercent Agent host.process.cpuUserPercent cpuUserPercent Agent host.process.fileDescriptorCount fileDescriptorCount Agent host.process.ioReadBytesPerSecond ioReadBytesPerSecond Agent host.process.ioReadCountPerSecond ioReadCountPerSecond Agent host.process.ioTotalReadBytes ioTotalReadBytes Agent host.process.ioTotalReadCount ioTotalReadCount Agent host.process.ioTotalWriteBytes ioTotalWriteBytes Agent host.process.ioTotalWriteCount ioTotalWriteCount Agent host.process.ioWriteBytesPerSecond ioWriteBytesPerSecond Agent host.process.ioWriteCountPerSecond ioWriteCountPerSecond Agent host.process.memoryResidentSizeBytes memoryResidentSizeBytes Agent host.process.memoryVirtualSizeBytes memoryVirtualSizeBytes Agent host.process.threadCount threadCount Agent host.swapFreeBytes swapFreeBytes Agent host.swapTotalBytes swapTotalBytes Agent host.swapUsedBytes swapUsedBytes Apache apache.server.busyWorkers server.busyWorkers Apache apache.server.idleWorkers server.idleWorkers Apache apache.server.net.bytesPerSecond net.bytesPerSecond Apache apache.server.net.requestsPerSecond net.requestsPerSecond Apache apache.server.scoreboard.closingWorkers server.scoreboard.closingWorkers Apache apache.server.scoreboard.dnsLookupWorkers server.scoreboard.dnsLookupWorkers Apache apache.server.scoreboard.finishingWorkers server.scoreboard.finishingWorkers Apache apache.server.scoreboard.idleCleanupWorkers server.scoreboard.idleCleanupWorkers Apache apache.server.scoreboard.keepAliveWorkers server.scoreboard.keepAliveWorkers Apache apache.server.scoreboard.loggingWorkers server.scoreboard.loggingWorkers Apache apache.server.scoreboard.readingWorkers server.scoreboard.readingWorkers Apache apache.server.scoreboard.startingWorkers server.scoreboard.startingWorkers Apache apache.server.scoreboard.totalWorkers server.scoreboard.totalWorkers Apache apache.server.scoreboard.writingWorkers server.scoreboard.writingWorkers Cassandra cassandra.node.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.node.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.node.client.connectedNativeClients client.connectedNativeClients Cassandra cassandra.node.commitLogCompletedTasksPerSecond db.commitLogCompletedTasksPerSecond Cassandra cassandra.node.commitLogPendingTasks db.commitLogPendindTasks Cassandra cassandra.node.commitLogTotalSizeBytes db.commitLogTotalSizeBytes Cassandra cassandra.node.droppedBatchRemoveMessagesPerSecond db.droppedBatchRemoveMessagesPerSecond Cassandra cassandra.node.droppedBatchStoreMessagesPerSecond db.droppedBatchStoreMessagesPerSecond Cassandra cassandra.node.droppedCounterMutationMessagesPerSecond db.droppedCounterMutationMessagesPerSecond Cassandra cassandra.node.droppedHintMessagesPerSecond db.droppedHintMessagesPerSecond Cassandra cassandra.node.droppedMutationMessagesPerSecond db.droppedMutationMessagesPerSecond Cassandra cassandra.node.droppedPagedRangeMessagesPerSecond db.droppedPagedRangeMessagesPerSecond Cassandra cassandra.node.droppedRangeSliceMessagesPerSecond db.droppedRangeSliceMessagesPerSecond Cassandra cassandra.node.droppedReadMessagesPerSecond db.droppedReadMessagesPerSecond Cassandra cassandra.node.droppedReadRepairMessagesPerSecond db.droppedReadRepairMessagesPerSecond Cassandra cassandra.node.droppedRequestResponseMessagesPerSecond db.droppedRequestResponseMessagesPerSecond Cassandra cassandra.node.droppedTraceMessagesPerSecond db.droppedTraceMessagesPerSecond Cassandra cassandra.node.keyCacheCapacityBytes db.keyCacheCapacityBytes Cassandra cassandra.node.keyCacheHitRate db.keyCacheHitRate Cassandra cassandra.node.keyCacheHitsPerSecond db.keyCacheHitsPerSecond Cassandra cassandra.node.keyCacheRequestsPerSecond db.keyCacheRequestsPerSecond Cassandra cassandra.node.keyCacheSizeBytes db.keyCacheSizeBytes Cassandra cassandra.node.liveSsTableCount db.liveSSTableCount Cassandra cassandra.node.loadBytes db.loadBytes Cassandra cassandra.node.query.casReadRequestsPerSecond query.CASReadRequestsPerSecond Cassandra cassandra.node.query.casWriteRequestsPerSecond query.CASWriteRequestsPerSecond Cassandra cassandra.node.query.rangeSliceRequestsPerSecond query.rangeSliceRequestsPerSecond Cassandra cassandra.node.query.rangeSliceTimeoutsPerSecond query.rangeSliceTimeoutsPerSecond Cassandra cassandra.node.query.rangeSliceUnavailablesPerSecond query.rangeSliceUnavailablesPerSecond Cassandra cassandra.node.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.node.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.node.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.node.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.node.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.node.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.node.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.node.query.readTimeoutsPerSecond query.readTimeoutsPerSecond Cassandra cassandra.node.query.readUnavailablesPerSecond query.readUnavailablesPerSecond Cassandra cassandra.node.query.viewWriteRequestsPerSecond query.viewWriteRequestsPerSecond Cassandra cassandra.node.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.node.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.node.query.writeTimeoutsPerSecond query.writeTimeoutsPerSecond Cassandra cassandra.node.query.writeUnavailablesPerSecond query.writeUnavailablesPerSecond Cassandra cassandra.node.rowCacheCapacityBytes db.rowCacheCapacityBytes Cassandra cassandra.node.rowCacheHitRate db.rowCacheHitRate Cassandra cassandra.node.rowCacheHitsPerSecond db.rowCacheHitsPerSecond Cassandra cassandra.node.rowCacheRequestsPerSecond db.rowCacheRequestsPerSecond Cassandra cassandra.node.rowCacheSizeBytes db.rowCacheSizeBytes Cassandra cassandra.node.storage.exceptionCount storage.exceptionCount Cassandra cassandra.node.threadPool.antiEntropyStage.activeTasks db.threadpool.internalAntiEntropyStageActiveTasks Cassandra cassandra.node.threadPool.antiEntropyStage.completedTasks db.threadpool.internalAntiEntropyStageCompletedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.currentlyBlockedTasks db.threadpool.internalAntiEntropyStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.pendingTasks db.threadpool.internalAntiEntropyStagePendingTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.activeTasks db.threadpool.internalCacheCleanupExecutorActiveTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.completedTasks db.threadpool.internalCacheCleanupExecutorCompletedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.currentlyBlockedTasks db.threadpool.internalCacheCleanupExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.pendingTasks db.threadpool.internalCacheCleanupExecutorPendingTasks Cassandra cassandra.node.threadPool.compactionExecutor.activeTasks db.threadpool.internalCompactionExecutorActiveTasks Cassandra cassandra.node.threadPool.compactionExecutor.completedTasks db.threadpool.internalCompactionExecutorCompletedTasks Cassandra cassandra.node.threadPool.compactionExecutor.currentlyBlockedTasks db.threadpool.internalCompactionExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.compactionExecutor.pendingTasks db.threadpool.internalCompactionExecutorPendingTasks Cassandra cassandra.node.threadPool.counterMutationStage.activeTasks db.threadpool.requestCounterMutationStageActiveTasks Cassandra cassandra.node.threadPool.counterMutationStage.completedTasks db.threadpool.requestCounterMutationStageCompletedTasks Cassandra cassandra.node.threadPool.counterMutationStage.currentlyBlockedTasks db.threadpool.requestCounterMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.counterMutationStage.pendingTasks db.threadpool.requestCounterMutationStagePendingTasks Cassandra cassandra.node.threadPool.gossipStage.activeTasks db.threadpool.internalGossipStageActiveTasks Cassandra cassandra.node.threadPool.gossipStage.completedTasks db.threadpool.internalGossipStageCompletedTasks Cassandra cassandra.node.threadPool.gossipStage.currentlyBlockedTasks db.threadpool.internalGossipStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.gossipStage.pendingTasks db.threadpool.internalGossipStagePendingTasks Cassandra cassandra.node.threadPool.hintsDispatcher.activeTasks db.threadpool.internalHintsDispatcherActiveTasks Cassandra cassandra.node.threadPool.hintsDispatcher.completedTasks db.threadpool.internalHintsDispatcherCompletedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.currentlyBlockedTasks db.threadpool.internalHintsDispatcherCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.pendingTasks db.threadpool.internalHintsDispatcherPendingTasks Cassandra cassandra.node.threadPool.internalResponseStage.activeTasks db.threadpool.internalInternalResponseStageActiveTasks Cassandra cassandra.node.threadPool.internalResponseStage.completedTasks db.threadpool.internalInternalResponseStageCompletedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pCurrentlyBlockedTasks db.threadpool.internalInternalResponseStagePCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pendingTasks db.threadpool.internalInternalResponseStagePendingTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.activeTasks db.threadpool.internalMemtableFlushWriterActiveTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.completedTasks db.threadpool.internalMemtableFlushWriterCompletedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.currentlyBlockedTasks db.threadpool.internalMemtableFlushWriterCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.pendingTasks db.threadpool.internalMemtableFlushWriterPendingTasks Cassandra cassandra.node.threadPool.memtablePostFlush.activeTasks db.threadpool.internalMemtablePostFlushActiveTasks Cassandra cassandra.node.threadPool.memtablePostFlush.completedTasks db.threadpool.internalMemtablePostFlushCompletedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.currentlyBlockedTasks db.threadpool.internalMemtablePostFlushCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.pendingTasks db.threadpool.internalMemtablePostFlushPendingTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.activeTasks db.threadpool.internalMemtableReclaimMemoryActiveTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.completedTasks db.threadpool.internalMemtableReclaimMemoryCompletedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.currentlyBlockedTasks db.threadpool.internalMemtableReclaimMemoryCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.pendingTasks db.threadpool.internalMemtableReclaimMemoryPendingTasks Cassandra cassandra.node.threadPool.migrationStage.activeTasks db.threadpool.internalMigrationStageActiveTasks Cassandra cassandra.node.threadPool.migrationStage.completedTasks db.threadpool.internalMigrationStageCompletedTasks Cassandra cassandra.node.threadPool.migrationStage.currentlyBlockedTasks db.threadpool.internalMigrationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.migrationStage.pendingTasks db.threadpool.internalMigrationStagePendingTasks Cassandra cassandra.node.threadPool.miscStage.activeTasks db.threadpool.internalMiscStageActiveTasks Cassandra cassandra.node.threadPool.miscStage.completedTasks db.threadpool.internalMiscStageCompletedTasks Cassandra cassandra.node.threadPool.miscStage.currentlyBlockedTasks db.threadpool.internalMiscStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.miscStage.pendingTasks db.threadpool.internalMiscStagePendingTasks Cassandra cassandra.node.threadPool.mutationStage.activeTasks db.threadpool.requestMutationStageActiveTasks Cassandra cassandra.node.threadPool.mutationStage.completedTasks db.threadpool.requestMutationStageCompletedTasks Cassandra cassandra.node.threadPool.mutationStage.currentlyBlockedTasks db.threadpool.requestMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.mutationStage.pendingTasks db.threadpool.requestMutationStagePendingTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.activeTasks db.threadpool.internalPendingRangeCalculatorActiveTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.completedTasks db.threadpool.internalPendingRangeCalculatorCompletedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.currentlyBlockedTasks db.threadpool.internalPendingRangeCalculatorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.pendingTasks db.threadpool.internalPendingRangeCalculatorPendingTasks Cassandra cassandra.node.threadPool.readRepairStage.activeTasks db.threadpool.requestReadRepairStageActiveTasks Cassandra cassandra.node.threadPool.readRepairStage.completedTasks db.threadpool.requestReadRepairStageCompletedTasks Cassandra cassandra.node.threadPool.readRepairStage.currentlyBlockedTasks db.threadpool.requestReadRepairStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readRepairStage.pendingTasks db.threadpool.requestReadRepairStagePendingTasks Cassandra cassandra.node.threadPool.readStage.activeTasks db.threadpool.requestReadStageActiveTasks Cassandra cassandra.node.threadPool.readStage.completedTasks db.threadpool.requestReadStageCompletedTasks Cassandra cassandra.node.threadPool.readStage.currentlyBlockedTasks db.threadpool.requestReadStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readStage.pendingTasks db.threadpool.requestReadStagePendingTasks Cassandra cassandra.node.threadPool.requestResponseStage.activeTasks db.threadpool.requestRequestResponseStageActiveTasks Cassandra cassandra.node.threadPool.requestResponseStage.completedTasks db.threadpool.requestRequestResponseStageCompletedTasks Cassandra cassandra.node.threadPool.requestResponseStage.currentlyBlockedTasks db.threadpool.requestRequestResponseStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.requestResponseStage.pendingTasks db.threadpool.requestRequestResponseStagePendingTasks Cassandra cassandra.node.threadPool.sampler.activeTasks db.threadpool.internalSamplerActiveTasks Cassandra cassandra.node.threadPool.sampler.completedTasks db.threadpool.internalSamplerCompletedTasks Cassandra cassandra.node.threadPool.sampler.currentlyBlockedTasks db.threadpool.internalSamplerCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.sampler.pendingTasks db.threadpool.internalSamplerPendingTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.activeTasks db.threadpool.internalSecondaryIndexManagementActiveTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.completedTasks db.threadpool.internalSecondaryIndexManagementCompletedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.currentlyBlockedTasks db.threadpool.internalSecondaryIndexManagementCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.pendingTasks db.threadpool.internalSecondaryIndexManagementPendingTasks Cassandra cassandra.node.threadPool.validationExecutor.activeTasks db.threadpool.internalValidationExecutorActiveTasks Cassandra cassandra.node.threadPool.validationExecutor.completedTasks db.threadpool.internalValidationExecutorCompletedTasks Cassandra cassandra.node.threadPool.validationExecutor.currentlyBlockedTasks db.threadpool.internalValidationExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.validationExecutor.pendingTasks db.threadpool.internalValidationExecutorPendingTasks Cassandra cassandra.node.threadPool.viewMutationStage.activeTasks db.threadpool.requestViewMutationStageActiveTasks Cassandra cassandra.node.threadPool.viewMutationStage.completedTasks db.threadpool.requestViewMutationStageCompletedTasks Cassandra cassandra.node.threadPool.viewMutationStage.currentlyBlockedTasks db.threadpool.requestViewMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.viewMutationStage.pendingTasks db.threadpool.requestViewMutationStagePendingTasks Cassandra cassandra.node.totalHintsInProgress db.totalHintsInProgress Cassandra cassandra.node.totalHintsPerSecond db.totalHintsPerSecond Cassandra cassandra.columnFamily.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.columnFamily.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.columnFamily.bloomFilterFalseRatio db.bloomFilterFalseRatio Cassandra cassandra.columnFamily.liveDiskSpaceUsedBytes db.liveDiskSpaceUsedBytes Cassandra cassandra.columnFamily.liveSsTableCount db.liveSSTableCount Cassandra cassandra.columnFamily.maxRowSize db.maxRowSize Cassandra cassandra.columnFamily.meanRowSize db.meanRowSize Cassandra cassandra.columnFamily.memtableLiveDataSize db.memtableLiveDataSize Cassandra cassandra.columnFamily.minRowSize db.minRowSize Cassandra cassandra.columnFamily.pendingCompactions db.pendingCompactions Cassandra cassandra.columnFamily.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.columnFamily.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.columnFamily.speculativeRetries db.speculativeRetries Cassandra cassandra.columnFamily.ssTablesPerRead50ThPercentileMilliseconds db.SSTablesPerRead50thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead75ThPercentileMilliseconds db.SSTablesPerRead75thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead95ThPercentileMilliseconds db.SSTablesPerRead95thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead98ThPercentileMilliseconds db.SSTablesPerRead98thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead999ThPercentileMilliseconds db.SSTablesPerRead999thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead99ThPercentileMilliseconds db.SSTablesPerRead99thPercentileMilliseconds Cassandra cassandra.columnFamily.tombstoneScannedHistogram50ThPercentile db.tombstoneScannedHistogram50thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram75ThPercentile db.tombstoneScannedHistogram75thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram95ThPercentile db.tombstoneScannedHistogram95thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram98ThPercentile db.tombstoneScannedHistogram98thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram999ThPercentile db.tombstoneScannedHistogram999thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram99ThPercentile db.tombstoneScannedHistogram99thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogramCount db.tombstoneScannedHistogramCount Consul consul.datacenter.catalog.criticalNodes catalog.criticalNodes Consul consul.datacenter.catalog.passingNodes catalog.passingNodes Consul consul.datacenter.catalog.registeredNodes catalog.registeredNodes Consul consul.datacenter.catalog.upNodes catalog.upNodes Consul consul.datacenter.catalog.warningNodes catalog.warningNodes Consul consul.datacenter.cluster.flaps cluster.flaps Consul consul.datacenter.cluster.suspects cluster.suspects Consul consul.datacenter.raft.commitTime raft.commitTimes Consul consul.datacenter.raft.commitTimeAvgInMilliseconds raft.commitTimeAvgInMilliseconds Consul consul.datacenter.raft.commitTimeMaxInMilliseconds raft.commitTimeMaxInMilliseconds Consul consul.datacenter.raft.completedLeaderElections raft.completedLeaderElections Consul consul.datacenter.raft.initiatedLeaderElections raft.initiatedLeaderElections Consul consul.datacenter.raft.lastContactAvgInMilliseconds raft.lastContactAvgInMilliseconds Consul consul.datacenter.raft.lastContactMaxInMilliseconds raft.lastContactMaxInMilliseconds Consul consul.datacenter.raft.lastContacts raft.lastContacts Consul consul.datacenter.raft.logDispatchAvgInMilliseconds raft.logDispatchAvgInMilliseconds Consul consul.datacenter.raft.logDispatches raft.logDispatches Consul consul.datacenter.raft.logDispatchMaxInMilliseconds raft.logDispatchMaxInMilliseconds Consul consul.datacenter.raft.txns raft.txns Consul consul.agent.aclCacheHitPerSecond agent.aclCacheHit Consul consul.agent.aclCacheMissPerSecond agent.aclCacheMiss Consul consul.agent.client.rpcFailed client.rpcFailed Consul consul.agent.client.rpcLoad client.rpcLoad Consul consul.agent.kvStores agent.kvStoress Consul consul.agent.kvStoresAvgInMilliseconds agent.kvStoresAvgInMilliseconds Consul consul.agent.kvStoresMaxInMilliseconds agent.kvStoresMaxInMilliseconds Consul consul.agent.net.agent.maxLatencyInMilliseconds net.agent.maxLatencyInMilliseconds Consul consul.agent.net.medianLatencyInMilliseconds net.agent.medianLatencyInMilliseconds Consul consul.agent.net.minLatencyInMilliseconds net.agent.minLatencyInMilliseconds Consul consul.agent.net.p25LatencyInMilliseconds net.agent.p25LatencyInMilliseconds Consul consul.agent.net.p75LatencyInMilliseconds net.agent.p75LatencyInMilliseconds Consul consul.agent.net.p90LatencyInMilliseconds net.agent.p90LatencyInMilliseconds Consul consul.agent.net.p95LatencyInMilliseconds net.agent.p95LatencyInMilliseconds Consul consul.agent.net.p99LatencyInMilliseconds net.agent.p99LatencyInMilliseconds Consul consul.agent.peers agent.peers Consul consul.agent.runtime.allocations runtime.allocations Consul consul.agent.runtime.allocationsInBytes runtime.allocationsInBytes Consul consul.agent.runtime.frees runtime.frees Consul consul.agent.runtime.gcCycles runtime.gcCycles Consul consul.agent.runtime.gcPauseInMilliseconds runtime.gcPauseInMilliseconds Consul consul.agent.runtime.goroutines runtime.goroutines Consul consul.agent.runtime.heapObjects runtime.heapObjects Consul consul.agent.runtime.virtualAddressSpaceInBytes runtime.virtualAddressSpaceInBytes Consul consul.agent.staleQueries agent.staleQueries Consul consul.agent.txnAvgInMilliseconds agent.txnAvgInMilliseconds Consul consul.agent.txnMaxInMilliseconds agent.txnMaxInMilliseconds Consul consul.agent.txns agent.txns Couchbase couchbase.bucket.activeItemsEnteringDiskQueuePerSecond bucket.activeItemsEnteringDiskQueuePerSecond Couchbase couchbase.bucket.activeItemsInMemory bucket.activeItemsInMemory Couchbase couchbase.bucket.activeResidentItemsRatio bucket.activeResidentItemsRatio Couchbase couchbase.bucket.averageDiskCommitTimeInMilliseconds bucket.averageDiskCommitTimeInMilliseconds Couchbase couchbase.bucket.averageDiskUpdateTimeInMilliseconds bucket.averageDiskUpdateTimeInMilliseconds Couchbase couchbase.bucket.cacheMisses bucket.cacheMisses Couchbase couchbase.bucket.cacheMissRatio bucket.cacheMissRatio Couchbase couchbase.bucket.casHits bucket.casHits Couchbase couchbase.bucket.casMisses bucket.casMisses Couchbase couchbase.bucket.couchDocsFragmentationPercent bucket.couchDocsFragmentationPercent Couchbase couchbase.bucket.currentConnections bucket.currentConnections Couchbase couchbase.bucket.dataUsedInBytes bucket.dataUsedInBytes Couchbase couchbase.bucket.decrementHitsPerSecond bucket.decrementHitsPerSecond Couchbase couchbase.bucket.decrementMissesPerSecond bucket.decrementMissesPerSecond Couchbase couchbase.bucket.deleteHitsPerSecond bucket.deleteHitsPerSecond Couchbase couchbase.bucket.deleteMissesPerSecond bucket.deleteMissesPerSecond Couchbase couchbase.bucket.diskCreateOperationsPerSecond bucket.diskCreateOperationsPerSecond Couchbase couchbase.bucket.diskFetchesPerSecond bucket.diskFetchesPerSecond Couchbase couchbase.bucket.diskReadsPerSecond bucket.diskReadsPerSecond Couchbase couchbase.bucket.diskUpdateOperationsPerSecond bucket.diskUpdateOperationsPerSecond Couchbase couchbase.bucket.diskUsedInBytes bucket.diskUsedInBytes Couchbase couchbase.bucket.diskWriteQueue bucket.diskWriteQueue Couchbase couchbase.bucket.drainedItemsInQueue bucket.drainedItemsInQueue Couchbase couchbase.bucket.drainedItemsOnDiskQueue bucket.drainedItemsOnDiskQueue Couchbase couchbase.bucket.drainedPendingItemsInQueue bucket.drainedPendingItemsInQueue Couchbase couchbase.bucket.ejectionsPerSecond bucket.ejectionsPerSecond Couchbase couchbase.bucket.evictionsPerSecond bucket.evictionsPerSecond Couchbase couchbase.bucket.getHitsPerSecond bucket.getHitsPerSecond Couchbase couchbase.bucket.getMissesPerSecond bucket.getMissesPerSecond Couchbase couchbase.bucket.hitRatio bucket.hitRatio Couchbase couchbase.bucket.incrementHitsPerSecond bucket.incrementHitsPerSecond Couchbase couchbase.bucket.incrementMissesPerSecond bucket.incrementMissesPerSecond Couchbase couchbase.bucket.itemCount bucket.itemCount Couchbase couchbase.bucket.itemsBeingWritten bucket.itemsBeingWritten Couchbase couchbase.bucket.itemsEjectedFromMemoryToDisk bucket.itemsEjectedFromMemoryToDisk Couchbase couchbase.bucket.itemsOnDiskQueue bucket.itemsOnDiskQueue Couchbase couchbase.bucket.itemsQueuedForStorage bucket.itemsQueuedForStorage Couchbase couchbase.bucket.maximumMemoryUsage bucket.maximumMemoryUsage Couchbase couchbase.bucket.memoryHighWaterMarkInBytes bucket.memoryHighWaterMarkInBytes Couchbase couchbase.bucket.memoryLowWaterMarkInBytes bucket.memoryLowWaterMarkInBytes Couchbase couchbase.bucket.memoryUsedInBytes bucket.memoryUsedInBytes Couchbase couchbase.bucket.metadataInRamInBytes bucket.metadataInRAMInBytes Couchbase couchbase.bucket.missesPerSecond bucket.missesPerSecond Couchbase couchbase.bucket.outOfMemoryErrorsPerSecond bucket.outOfMemoryErrorsPerSecond Couchbase couchbase.bucket.overheadInBytes bucket.overheadInBytes Couchbase couchbase.bucket.pendingItemsInDiskQueue bucket.pendingItemsInDiskQueue Couchbase couchbase.bucket.pendingResidentItemsRatio bucket.pendingResidentItemsRatio Couchbase couchbase.bucket.quotaUtilization bucket.quotaUtilization Couchbase couchbase.bucket.readOperationsPerSecond bucket.readOperationsPerSecond Couchbase couchbase.bucket.readRatePerSecond bucket.readRatePerSecond Couchbase couchbase.bucket.recoverableOutOfMemoryCount bucket.recoverableOutOfMemoryCount Couchbase couchbase.bucket.replicaIndex bucket.replicaIndex Couchbase couchbase.bucket.replicaNumber bucket.replicaNumber Couchbase couchbase.bucket.replicaResidentItemsRatio bucket.replicaResidentItemsRatio Couchbase couchbase.bucket.residentItemsRatio bucket.residentItemsRatio Couchbase couchbase.bucket.temporaryOutOfMemoryErrorsPerSecond bucket.temporaryOutOfMemoryErrorsPerSecond Couchbase couchbase.bucket.threadsNumber bucket.threadsNumber Couchbase couchbase.bucket.totalItems bucket.totalItems Couchbase couchbase.bucket.totalOperationsPerSecond bucket.totalOperationsPerSecond Couchbase couchbase.bucket.viewFragmentationPercent bucket.viewFragmentationPercent Couchbase couchbase.bucket.writeOperationsPerSecond bucket.writeOperationsPerSecond Couchbase couchbase.bucket.writeRatePerSecond bucket.writeRatePerSecond Couchbase couchbase.cluster.autoFailoverCount cluster.autoFailoverCount Couchbase couchbase.cluster.autoFailoverEnabled cluster.autoFailoverEnabled Couchbase couchbase.cluster.databaseFragmentationThreshold cluster.databaseFragmentationThreshold Couchbase couchbase.cluster.diskFreeInBytes cluster.diskFreeInBytes Couchbase couchbase.cluster.diskQuotaTotalInBytes cluster.diskQuotaTotalInBytes Couchbase couchbase.cluster.diskTotalInBytes cluster.diskTotalInBytes Couchbase couchbase.cluster.diskUsedByDataInBytes cluster.diskUsedByDataInBytes Couchbase couchbase.cluster.diskUsedInBytes cluster.diskUsedInBytes Couchbase couchbase.cluster.indexFragmentationThreshold cluster.indexFragmentationThreshold Couchbase couchbase.cluster.maximumBucketCount cluster.maximumBucketCount Couchbase couchbase.cluster.memoryQuotaTotalInBytes cluster.memoryQuotaTotalInBytes Couchbase couchbase.cluster.memoryQuotaTotalPerNodeInBytes cluster.memoryQuotaTotalPerNodeInBytes Couchbase couchbase.cluster.memoryQuotaUsedInBytes cluster.memoryQuotaUsedInBytes Couchbase couchbase.cluster.memoryQuotaUsedPerNodeInBytes cluster.memoryQuotaUsedPerNodeInBytes Couchbase couchbase.cluster.memoryTotalInBytes cluster.memoryTotalInBytes Couchbase couchbase.cluster.memoryUsedByDataInBytes cluster.memoryUsedByDataInBytes Couchbase couchbase.cluster.memoryUsedInBytes cluster.memoryUsedInBytes Couchbase couchbase.cluster.viewFragmentationThreshold cluster.viewFragmentationThreshold Couchbase couchbase.node.backgroundFetches node.backgroundFetches Couchbase couchbase.node.cmdGet node.cmdGet Couchbase couchbase.node.couchDocsActualDiskSizeInBytes node.couchDocsActualDiskSizeInBytes Couchbase couchbase.node.couchDocsDataSizeInBytes node.couchDocsDataSizeInBytes Couchbase couchbase.node.couchSpatialDataSizeInBytes node.couchSpatialDataSizeInBytes Couchbase couchbase.node.couchSpatialDiskSizeInBytes node.couchSpatialDiskSizeInBytes Couchbase couchbase.node.couchViewsActualDiskSizeInBytes node.couchViewsActualDiskSizeInBytes Couchbase couchbase.node.couchViewsDataSizeInBytes node.couchViewsDataSizeInBytes Couchbase couchbase.node.cpuUtilization node.cpuUtilization Couchbase couchbase.node.currentItems node.currentItems Couchbase couchbase.node.currentItemsTotal node.currentItemsTotal Couchbase couchbase.node.getHits node.getHits Couchbase couchbase.node.memoryFreeInBytes node.memoryFreeInBytes Couchbase couchbase.node.memoryTotalInBytes node.memoryTotalInBytes Couchbase couchbase.node.memoryUsedInBytes node.memoryUsedInBytes Couchbase couchbase.node.ops node.ops Couchbase couchbase.node.swapTotalInBytes node.swapTotalInBytes Couchbase couchbase.node.swapUsedInBytes node.swapUsedInBytes Couchbase couchbase.node.uptimeInMilliseconds node.uptimeInMilliseconds Couchbase couchbase.node.vbucketActiveNonResidentItems node.vbucketActiveNonResidentItems Couchbase couchbase.node.vbucketInMemoryItems node.vbucketInMemoryItems Couchbase couchbase.queryengine.activeRequests queryengine.activeRequests Couchbase couchbase.queryengine.averageRequestTimeInMilliseconds queryengine.averageRequestTimeInMilliseconds Couchbase couchbase.queryengine.completedLimit queryengine.completedLimit Couchbase couchbase.queryengine.completedRequests queryengine.completedRequests Couchbase couchbase.queryengine.completedThresholdInMilliseconds queryengine.completedThresholdInMilliseconds Couchbase couchbase.queryengine.cores queryengine.cores Couchbase couchbase.queryengine.garbageCollectionNumber queryengine.garbageCollectionNumber Couchbase couchbase.queryengine.garbageCollectionPaused queryengine.garbageCollectionPaused Couchbase couchbase.queryengine.garbageCollectionTimePausedInMilliseconds queryengine.garbageCollectionTimePausedInMilliseconds Couchbase couchbase.queryengine.medianRequestTimeInMilliseconds queryengine.medianRequestTimeInMilliseconds Couchbase couchbase.queryengine.preparedStatementUtilization queryengine.preparedStatementUtilization Couchbase couchbase.queryengine.requestsLast15MinutesPerSecond queryengine.requestsLast15MinutesPerSecond Couchbase couchbase.queryengine.requestsLast1MinutesPerSecond queryengine.requestsLast1MinutesPerSecond Couchbase couchbase.queryengine.requestsLast5MinutesPerSecond queryengine.requestsLast5MinutesPerSecond Couchbase couchbase.queryengine.requestTime80thPercentileInMilliseconds queryengine.requestTime80thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime95thPercentileInMilliseconds queryengine.requestTime95thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime99thPercentileInMilliseconds queryengine.requestTime99thPercentileInMilliseconds Couchbase couchbase.queryengine.systemCpuUtilization queryengine.systemCPUUtilization Couchbase couchbase.queryengine.systemMemoryInBytes queryengine.systemMemoryInBytes Couchbase couchbase.queryengine.totalMemoryInBytes queryengine.totalMemoryInBytes Couchbase couchbase.queryengine.totalThreads queryengine.totalThreads Couchbase couchbase.queryengine.uptimeInMilliseconds queryengine.uptimeInMilliseconds Couchbase couchbase.queryengine.usedMemoryInBytes queryengine.usedMemoryInBytes Couchbase couchbase.queryengine.userCpuUtilization queryengine.userCPUUtilization Docker docker.container.cpuKernelPercent cpuKernelPercent Docker docker.container.cpuLimitCores cpuLimitCores Docker docker.container.cpuPercent cpuPercent Docker docker.container.cpuThrottlePeriods cpuThrottlePeriods Docker docker.container.cpuThrottleTimeMs cpuThrottleTimeMs Docker docker.container.cpuUsedCores cpuUsedCores Docker docker.container.cpuUsedCoresPercent cpuUsedCoresPercent Docker docker.container.cpuUserPercent cpuUserPercent Docker docker.container.ioReadBytesPerSecond ioReadBytesPerSecond Docker docker.container.ioReadCountPerSecond ioReadCountPerSecond Docker docker.container.ioTotalBytes ioTotalBytes Docker docker.container.ioTotalReadBytes ioTotalReadBytes Docker docker.container.ioTotalReadCount ioTotalReadCount Docker docker.container.ioTotalWriteBytes ioTotalWriteBytes Docker docker.container.ioTotalWriteCount ioTotalWriteCount Docker docker.container.ioWriteBytesPerSecond ioWriteBytesPerSecond Docker docker.container.ioWriteCountPerSecond ioWriteCountPerSecond Docker docker.container.memoryCacheBytes memoryCacheBytes Docker docker.container.memoryResidentSizeBytes memoryResidentSizeBytes Docker docker.container.memorySizeLimitBytes memorySizeLimitBytes Docker docker.container.memoryUsageBytes memoryUsageBytes Docker docker.container.memoryUsageLimitPercent memoryUsageLimitPercent Docker docker.container.networkRxBytes networkRxBytes Docker docker.container.networkRxBytesPerSecond networkRxBytesPerSecond Docker docker.container.networkRxDropped networkRxDropped Docker docker.container.networkRxDroppedPerSecond networkRxDroppedPerSecond Docker docker.container.networkRxErrors networkRxErrors Docker docker.container.networkRxErrorsPerSecond networkRxErrorsPerSecond Docker docker.container.networkRxPackets networkRxPackets Docker docker.container.networkRxPacketsPerSecond networkRxPacketsPerSecond Docker docker.container.networkTxBytes networkTxBytes Docker docker.container.networkTxBytesPerSecond networkTxBytesPerSecond Docker docker.container.networkTxDropped networkTxDropped Docker docker.container.networkTxDroppedPerSecond networkTxDroppedPerSecond Docker docker.container.networkTxErrors networkTxErrors Docker docker.container.networkTxErrorsPerSecond networkTxErrorsPerSecond Docker docker.container.networkTxPackets networkTxPackets Docker docker.container.networkTxPacketsPerSecond networkTxPacketsPerSecond Docker docker.container.pids pids Docker docker.container.processCount processCount Docker docker.container.processCountLimit processCountLimit Docker docker.container.restartCount restartCount Docker docker.container.threadCount threadCount Docker docker.container.threadCountLimit threadCountLimit ElasticSearch elasticsearch.cluster.dataNodes cluster.dataNodes ElasticSearch elasticsearch.cluster.nodes cluster.nodes ElasticSearch elasticsearch.cluster.shards.active shards.active ElasticSearch elasticsearch.cluster.shards.initializing shards.initializing ElasticSearch elasticsearch.cluster.shards.primaryActive shards.primaryActive ElasticSearch elasticsearch.cluster.shards.relocating shards.relocating ElasticSearch elasticsearch.cluster.shards.unassigned shards.unassigned ElasticSearch elasticsearch.cluster.tempData temp-data ElasticSearch elasticsearch.index.docs index.docs ElasticSearch elasticsearch.index.docsDeleted index.docsDeleted ElasticSearch elasticsearch.index.primaryShards index.primaryShards ElasticSearch elasticsearch.index.primaryStoreSizeInBytes index.primaryStoreSizeInBytes ElasticSearch elasticsearch.index.replicaShards index.replicaShards ElasticSearch elasticsearch.index.rollup.docsCount primaries.docsnumber ElasticSearch elasticsearch.index.rollup.docsDeleted primaries.docsDeleted ElasticSearch elasticsearch.index.rollup.flushTotal primaries.flushesTotal ElasticSearch elasticsearch.index.rollup.flushTotalTimeInMilliseconds primaries.flushTotalTimeInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsExist primaries.get.documentsExist ElasticSearch elasticsearch.index.rollup.get.documentsExistInMilliseconds primaries.get.documentsExistInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsMissing primaries.get.documentsMissing ElasticSearch elasticsearch.index.rollup.get.documentsMissingInMilliseconds primaries.get.documentsMissingInMilliseconds ElasticSearch elasticsearch.index.rollup.get.requests primaries.get.requests ElasticSearch elasticsearch.index.rollup.get.requestsCurrent primaries.get.requestsCurrent ElasticSearch elasticsearch.index.rollup.get.requestsInMilliseconds primaries.get.requestsInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeleted primaries.index.docsCurrentlyDeleted ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeletedInMilliseconds primaries.index.docsCurrentlyDeletedInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexing primaries.index.docsCurrentlyIndexing ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexingInMilliseconds primaries.index.docsCurrentlyIndexingInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsDeleted primaries.index.docsDeleted ElasticSearch elasticsearch.index.rollup.index.docsTotal primaries.index.docsTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotal primaries.indexRefreshesTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotalInMilliseconds primaries.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.merges.current primaries.merges.current ElasticSearch elasticsearch.index.rollup.merges.docsSegmentsCurrentlyMerged primaries.merges.docsSegmentsCurrentlyMerged ElasticSearch elasticsearch.index.rollup.merges.docsTotal primaries.merges.docsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsCurrentlyMergedInBytes primaries.merges.segmentsCurrentlyMergedInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotal primaries.merges.segmentsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInBytes primaries.merges.segmentsTotalInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInMilliseconds primaries.merges.segmentsTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesInMilliseconds primaries.queriesInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesTotal primaries.queriesTotal ElasticSearch elasticsearch.index.rollup.queryActive primaries.queryActive ElasticSearch elasticsearch.index.rollup.queryFetches primaries.queryFetches ElasticSearch elasticsearch.index.rollup.queryFetchesInMilliseconds primaries.queryFetchesInMilliseconds ElasticSearch elasticsearch.index.rollup.queryFetchesTotal primaries.queryFetchesTotal ElasticSearch elasticsearch.index.rollup.sizeInBytes primaries.sizeInBytes ElasticSearch elasticsearch.index.storeSizeInBytes index.storeSizeInBytes ElasticSearch elasticsearch.node.activeSearches activeSearches ElasticSearch elasticsearch.node.activeSearchesInMilliseconds activeSearchesInMilliseconds ElasticSearch elasticsearch.node.breakers.estimatedSizeFieldDataCircuitBreakerInBytes breakers.estimatedSizeFieldDataCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeParentCircuitBreakerInBytes breakers.estimatedSizeParentCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeRequestCircuitBreakerInBytes breakers.estimatedSizeRequestCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.fieldDataCircuitBreakerTripped breakers.fieldDataCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.parentCircuitBreakerTripped breakers.parentCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.requestCircuitBreakerTripped breakers.requestCircuitBreakerTripped ElasticSearch elasticsearch.node.flush.indexRefreshesTotal flush.indexRefreshesTotal ElasticSearch elasticsearch.node.flush.indexRefreshesTotalInMilliseconds flush.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.node.fs.bytesAvailableJvmInBytes fs.bytesAvailableJVMInBytes ElasticSearch elasticsearch.node.fs.dataRead fs.bytesReadsInBytes ElasticSearch elasticsearch.node.fs.dataWritten fs.writesInBytes ElasticSearch elasticsearch.node.fs.ioOperations fs.iOOperations ElasticSearch elasticsearch.node.fs.readOperations fs.reads ElasticSearch elasticsearch.node.fs.totalSizeInBytes fs.totalSizeInBytes ElasticSearch elasticsearch.node.fs.unallocatedBytes fs.unallocatedBytesInBYtes ElasticSearch elasticsearch.node.fs.writeOperations fs.writeOperations ElasticSearch elasticsearch.node.get.currentRequestsRunning get.currentRequestsRunning ElasticSearch elasticsearch.node.get.requestsDocumentExists get.requestsDocumentExists ElasticSearch elasticsearch.node.get.requestsDocumentExistsInMilliseconds get.requestsDocumentExistsInMilliseconds ElasticSearch elasticsearch.node.get.requestsDocumentMissing get.requestsDocumentMissing ElasticSearch elasticsearch.node.get.requestsDocumentMissingInMilliseconds get.requestsDocumentMissingInMilliseconds ElasticSearch elasticsearch.node.get.timeGetRequestsInMilliseconds get.timeGetRequestsInMilliseconds ElasticSearch elasticsearch.node.get.totalGetRequests get.totalGetRequests ElasticSearch elasticsearch.node.http.currentOpenConnections http.currentOpenConnections ElasticSearch elasticsearch.node.http.openedConnections http.openedConnections ElasticSearch elasticsearch.node.index.indexingOperationsFailed indices.indexingOperationsFailed ElasticSearch elasticsearch.node.index.indexingWaitedThrottlingInMilliseconds indices.indexingWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.memoryQueryCacheInBytes indices.memoryQueryCacheInBytes ElasticSearch elasticsearch.node.index.numberIndices indices.numberIndices ElasticSearch elasticsearch.node.index.queryCacheEvictions indices.queryCacheEvictions ElasticSearch elasticsearch.node.index.queryCacheHits indices.queryCacheHits ElasticSearch elasticsearch.node.index.queryCacheMisses indices.queryCacheMisses ElasticSearch elasticsearch.node.index.recoveryOngoingShardSource indices.recoveryOngoingShardSource ElasticSearch elasticsearch.node.index.recoveryOngoingShardTarget indices.recoveryOngoingShardTarget ElasticSearch elasticsearch.node.index.recoveryWaitedThrottlingInMilliseconds indices.recoveryWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.requestCacheEvictions indices.requestCacheEvictions ElasticSearch elasticsearch.node.index.requestCacheHits indices.requestCacheHits ElasticSearch elasticsearch.node.index.requestCacheMemoryInBytes indices.requestCacheMemoryInBytes ElasticSearch elasticsearch.node.index.requestCacheMisses indices.requestCacheMisses ElasticSearch elasticsearch.node.index.segmentsIndexShard indices.segmentsIndexShard ElasticSearch elasticsearch.node.index.segmentsMemoryUsedDocValuesInBytes indices.segmentsMemoryUsedDocValuesInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedFixedBitSetInBytes indices.segmentsMemoryUsedFixedBitSetInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexSegmentsInBytes indices.segmentsMemoryUsedIndexSegmentsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexWriterInBytes indices.segmentsMemoryUsedIndexWriterInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedNormsInBytes indices.segmentsMemoryUsedNormsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedSegmentVersionMapInBytes indices.segmentsMemoryUsedSegmentVersionMapInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedStoredFieldsInBytes indices.segmentsMemoryUsedStoredFieldsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermsInBytes indices.segmentsMemoryUsedTermsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermVectorsInBytes indices.segmentsMemoryUsedTermVectorsInBytes ElasticSearch elasticsearch.node.index.translogOperations indices.translogOperations ElasticSearch elasticsearch.node.index.translogOperationsInBytes indices.translogOperationsInBytes ElasticSearch elasticsearch.node.indexing.docsCurrentlyDeleted indexing.docsCurrentlyDeleted ElasticSearch elasticsearch.node.indexing.documentsCurrentlyIndexing indexing.documentsCurrentlyIndexing ElasticSearch elasticsearch.node.indexing.documentsIndexed indexing.documentsIndexed ElasticSearch elasticsearch.node.indexing.timeDeletingDocumentsInMilliseconds indexing.timeDeletingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.timeIndexingDocumentsInMilliseconds indexing.timeIndexingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.totalDocumentsDeleted indexing.totalDocumentsDeleted ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjects jvm.gc.majorCollectionsOldGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjects jvm.gc.majorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjects jvm.gc.minorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.mem.heapCommittedInBytes jvm.mem.heapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.heapMaxInBytes jvm.mem.heapMaxInBytes ElasticSearch elasticsearch.node.jvm.mem.heapUsed jvm.mem.heapUsed ElasticSearch elasticsearch.node.jvm.mem.heapUsedInBytes jvm.mem.heapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.maxOldGenerationHeapInBytes jvm.mem.maxOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.maxSurvivorSpaceInBytes jvm.mem.maxSurvivorSpaceInBYtes ElasticSearch elasticsearch.node.jvm.mem.maxYoungGenerationHeapInBytes jvm.mem.maxYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapCommittedInBytes jvm.mem.nonHeapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapUsedInBytes jvm.mem.nonHeapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.usedOldGenerationHeapInBytes jvm.mem.usedOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.usedSurvivorSpaceInBytes jvm.mem.usedSurvivorSpaceInBytes ElasticSearch elasticsearch.node.jvm.mem.usedYoungGenerationHeapInBytes jvm.mem.usedYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.threadsActive jvm.ThreadsActive ElasticSearch elasticsearch.node.jvm.threadsPeak jvm.ThreadsPeak ElasticSearch elasticsearch.node.merges.currentActive merges.currentActive ElasticSearch elasticsearch.node.merges.docsSegmentMerges merges.docsSegmentMerges ElasticSearch elasticsearch.node.merges.docsSegmentsMerging merges.docsSegmentsMerging ElasticSearch elasticsearch.node.merges.mergedSegmentsInBytes merges.mergedSegmentsInBytes ElasticSearch elasticsearch.node.merges.segmentMerges merges.segmentMerges ElasticSearch elasticsearch.node.merges.sizeSegmentsMergingInBytes merges.sizeSegmentsMergingInBytes ElasticSearch elasticsearch.node.merges.totalSegmentMergingInMilliseconds merges.totalSegmentMergingInMilliseconds ElasticSearch elasticsearch.node.openFd openFD ElasticSearch elasticsearch.node.queriesTotal queriesTotal ElasticSearch elasticsearch.node.refresh.total refresh.total ElasticSearch elasticsearch.node.refresh.totalInMilliseconds refresh.totalInMilliseconds ElasticSearch elasticsearch.node.searchFetchCurrentlyRunning searchFetchCurrentlyRunning ElasticSearch elasticsearch.node.searchFetches searchFetches ElasticSearch elasticsearch.node.sizeStoreInBytes sizeStoreInBytes ElasticSearch elasticsearch.node.threadpool.activeFetchShardStarted threadpool.activeFetchShardStarted ElasticSearch elasticsearch.node.threadpool.bulkActive threadpool.bulkActive ElasticSearch elasticsearch.node.threadpool.bulkQueue threadpool.bulkQueue ElasticSearch elasticsearch.node.threadpool.bulkRejected threadpool.bulkRejected ElasticSearch elasticsearch.node.threadpool.bulkThreads threadpool.bulkThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStartedQueue threadpool.fetchShardStartedQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStartedRejected threadpool.fetchShardStartedRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStartedThreads threadpool.fetchShardStartedThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStoreActive threadpool.fetchShardStoreActive ElasticSearch elasticsearch.node.threadpool.fetchShardStoreQueue threadpool.fetchShardStoreQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStoreRejected threadpool.fetchShardStoreRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStoreThreads threadpool.fetchShardStoreThreads ElasticSearch elasticsearch.node.threadpool.flushActive threadpool.flushActive ElasticSearch elasticsearch.node.threadpool.flushQueue threadpool.flushQueue ElasticSearch elasticsearch.node.threadpool.flushRejected threadpool.flushRejected ElasticSearch elasticsearch.node.threadpool.flushThreads threadpool.flushThreads ElasticSearch elasticsearch.node.threadpool.forceMergeActive threadpool.forceMergeActive ElasticSearch elasticsearch.node.threadpool.forceMergeQueue threadpool.forceMergeQueue ElasticSearch elasticsearch.node.threadpool.forceMergeRejected threadpool.forceMergeRejected ElasticSearch elasticsearch.node.threadpool.forceMergeThreads threadpool.forceMergeThreads ElasticSearch elasticsearch.node.threadpool.genericActive threadpool.genericActive ElasticSearch elasticsearch.node.threadpool.genericQueue threadpool.genericQueue ElasticSearch elasticsearch.node.threadpool.genericRejected threadpool.genericRejected ElasticSearch elasticsearch.node.threadpool.genericThreads threadpool.genericThreads ElasticSearch elasticsearch.node.threadpool.getActive threadpool.getActive ElasticSearch elasticsearch.node.threadpool.getQueue threadpool.getQueue ElasticSearch elasticsearch.node.threadpool.getRejected threadpool.getRejected ElasticSearch elasticsearch.node.threadpool.getThreads threadpool.getThreads ElasticSearch elasticsearch.node.threadpool.indexActive threadpool.indexActive ElasticSearch elasticsearch.node.threadpool.indexQueue threadpool.indexQueue ElasticSearch elasticsearch.node.threadpool.indexRejected threadpool.indexRejected ElasticSearch elasticsearch.node.threadpool.indexThreads threadpool.indexThreads ElasticSearch elasticsearch.node.threadpool.listenerActive threadpool.listenerActive ElasticSearch elasticsearch.node.threadpool.listenerQueue threadpool.listenerQueue ElasticSearch elasticsearch.node.threadpool.listenerRejected threadpool.listenerRejected ElasticSearch elasticsearch.node.threadpool.listenerThreads threadpool.listenerThreads ElasticSearch elasticsearch.node.threadpool.managementActive threadpool.managementActive ElasticSearch elasticsearch.node.threadpool.managementQueue threadpool.managementQueue ElasticSearch elasticsearch.node.threadpool.managementRejected threadpool.managementRejected ElasticSearch elasticsearch.node.threadpool.managementThreads threadpool.managementThreads ElasticSearch elasticsearch.node.threadpool.refreshActive threadpool.refreshActive ElasticSearch elasticsearch.node.threadpool.refreshQueue threadpool.refreshQueue ElasticSearch elasticsearch.node.threadpool.refreshRejected threadpool.refreshRejected ElasticSearch elasticsearch.node.threadpool.refreshThreads threadpool.refreshThreads ElasticSearch elasticsearch.node.threadpool.searchActive threadpool.searchActive ElasticSearch elasticsearch.node.threadpool.searchQueue threadpool.searchQueue ElasticSearch elasticsearch.node.threadpool.searchRejected threadpool.searchRejected ElasticSearch elasticsearch.node.threadpool.searchThreads threadpool.searchThreads ElasticSearch elasticsearch.node.threadpool.snapshotActive threadpool.snapshotActive ElasticSearch elasticsearch.node.threadpool.snapshotQueue threadpool.snapshotQueue ElasticSearch elasticsearch.node.threadpool.snapshotRejected threadpool.snapshotRejected ElasticSearch elasticsearch.node.threadpool.snapshotThreads threadpool.snapshotThreads ElasticSearch elasticsearch.node.transport.connectionsOpened transport.connectionsOpened ElasticSearch elasticsearch.node.transport.packetsReceived transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes F5 f5.node.availabilityState node.availabilityState F5 f5.node.connections node.connections F5 f5.node.connectionsPerSecond node.connectionsPerSecond F5 f5.node.enabled node.enabled F5 f5.node.inDataInBytesPerSecond node.inDataInBytesPerSecond F5 f5.node.monitorStatus node.monitorStatus F5 f5.node.outDataInBytesPerSecond node.outDataInBytesPerSecond F5 f5.node.packetsReceivedPerSecond node.packetsReceivedPerSecond F5 f5.node.packetsSentPerSecond node.packetsSentPerSecond F5 f5.node.requestsPerSecond node.requestsPerSecond F5 f5.node.sessions node.sessions F5 f5.node.sessionStatus node.sessionStatus F5 f5.poolMember.availabilityState member.availabilityState F5 f5.poolMember.connections member.connections F5 f5.poolMember.enabled member.enabled F5 f5.poolMember.inDataInBytesPerSecond member.inDataInBytesPerSecond F5 f5.poolMember.monitorStatus member.monitorStatus F5 f5.poolMember.outDataInBytesPerSecond member.outDataInBytesPerSecond F5 f5.poolMember.packetsReceivedPerSecond member.packetsReceivedPerSecond F5 f5.poolMember.packetsSentPerSecond member.packetsSentPerSecond F5 f5.poolMember.requestsPerSecond member.requestsPerSecond F5 f5.poolMember.sessions member.sessions F5 f5.poolMember.sessionStatus member.sessionStatus F5 f5.pool.activeMembers pool.activeMembers F5 f5.pool.availabilityState pool.availabilityState F5 f5.pool.connections pool.connections F5 f5.pool.connqAgeEdm pool.connqAgeEdm F5 f5.pool.connqAgeEma pool.connqAgeEma F5 f5.pool.connqAgeHead pool.connqAgeHead F5 f5.pool.connqAgeMax pool.connqAgeMax F5 f5.pool.connqAllAgeEdm pool.connqAllAgeEdm F5 f5.pool.connqAllAgeEma pool.connqAllAgeEma F5 f5.pool.connqAllAgeHead pool.connqAllAgeHead F5 f5.pool.connqAllAgeMax pool.connqAllAgeMax F5 f5.pool.connqAllDepth pool.connqAllDepth F5 f5.pool.connqDepth pool.connqDepth F5 f5.pool.currentConnections pool.currentConnections F5 f5.pool.enabled pool.enabled F5 f5.pool.inDataInBytesPerSecond pool.inDataInBytesPerSecond F5 f5.pool.minActiveMembers pool.minActiveMembers F5 f5.pool.outDataInBytesPerSecond pool.outDataInBytesPerSecond F5 f5.pool.packetsReceivedPerSecond pool.packetsReceivedPerSecond F5 f5.pool.packetsSentPerSecond pool.packetsSentPerSecond F5 f5.pool.requestsPerSecond pool.requestsPerSecond F5 f5.pool.sessions pool.sessions F5 f5.system.cpuIdleTicksPerSecond system.cpuIdleTicksPerSecond F5 f5.system.cpuIdleUtilization system.cpuIdleUtilization F5 f5.system.cpuInterruptRequestUtilization system.cpuInterruptRequestUtilization F5 f5.system.cpuIoWaitUtilization system.cpuIOWaitUtilization F5 f5.system.cpuNiceLevelUtilization system.cpuNiceLevelUtilization F5 f5.system.cpuSoftInterruptRequestUtilization system.cpuSoftInterruptRequestUtilization F5 f5.system.cpuStolenUtilization system.cpuStolenUtilization F5 f5.system.cpuSystemTicksPerSecond system.cpuSystemTicksPerSecond F5 f5.system.cpuSystemUtilization system.cpuSystemUtilization F5 f5.system.cpuUserTicksPerSecond system.cpuUserTicksPerSecond F5 f5.system.cpuUserUtilization system.cpuUserUtilization F5 f5.system.memoryFreeInBytes system.memoryFreeInBytes F5 f5.system.memoryTotalInBytes system.memoryTotalInBytes F5 f5.system.memoryUsedInBytes system.memoryUsedInBytes F5 f5.system.otherMemoryFreeInBytes system.otherMemoryFreeInBytes F5 f5.system.otherMemoryTotalInBytes system.otherMemoryTotalInBytes F5 f5.system.otherMemoryUsedInBytes system.otherMemoryUsedInBytes F5 f5.system.swapFreeInBytes system.swapFreeInBytes F5 f5.system.swapTotalInBytes system.swapTotalInBytes F5 f5.system.swapUsedInBytes system.swapUsedInBytes F5 f5.system.tmmMemoryFreeInBytes system.tmmMemoryFreeInBytes F5 f5.system.tmmMemoryTotalInBytes system.tmmMemoryTotalInBytes F5 f5.system.tmmMemoryUsedInBytes system.tmmMemoryUsedInBytes F5 f5.virtualserver.availabilityState virtualserver.availabilityState F5 f5.virtualserver.clientsideConnectionsPerSecond virtualserver.clientsideConnectionsPerSecond F5 f5.virtualserver.connections virtualserver.connections F5 f5.virtualserver.csMaxConnDur virtualserver.csMaxConnDur F5 f5.virtualserver.csMeanConnDur virtualserver.csMeanConnDur F5 f5.virtualserver.csMinConnDur virtualserver.csMinConnDur F5 f5.virtualserver.enabled virtualserver.enabled F5 f5.virtualserver.ephemeralBytesInPerSecond virtualserver.ephemeralBytesInPerSecond F5 f5.virtualserver.ephemeralBytesOutPerSecond virtualserver.ephemeralBytesOutPerSecond F5 f5.virtualserver.ephemeralConnectionsPerSecond virtualserver.ephemeralConnectionsPerSecond F5 f5.virtualserver.ephemeralCurrentConnections virtualserver.ephemeralCurrentConnections F5 f5.virtualserver.ephemeralEvictedConnectionsPerSecond virtualserver.ephemeralEvictedConnectionsPerSecond F5 f5.virtualserver.ephemeralMaxConnections virtualserver.ephemeralMaxConnections F5 f5.virtualserver.ephemeralPacketsReceivedPerSecond virtualserver.ephemeralPacketsReceivedPerSecond F5 f5.virtualserver.ephemeralPacketsSentPerSecond virtualserver.ephemeralPacketsSentPerSecond F5 f5.virtualserver.ephemeralSlowKilledPerSecond virtualserver.ephemeralSlowKilledPerSecond F5 f5.virtualserver.evictedConnsPerSecond virtualserver.evictedConnsPerSecond F5 f5.virtualserver.inDataInBytesPerSecond virtualserver.inDataInBytesPerSecond F5 f5.virtualserver.outDataInBytesPerSecond virtualserver.outDataInBytesPerSecond F5 f5.virtualserver.packetsReceivedPerSecond virtualserver.packetsReceivedPerSecond F5 f5.virtualserver.packetsSentPerSecond virtualserver.packetsSentPerSecond F5 f5.virtualserver.requestsPerSecond virtualserver.requestsPerSecond F5 f5.virtualserver.slowKilledPerSecond virtualserver.slowKilledPerSecond F5 f5.virtualserver.usageRatio virtualserver.usageRatio HAProxy haproxy.backend.activeServers backend.activeServers HAProxy haproxy.backend.averageConnectTimeInSeconds backend.averageConnectTimeInSeconds HAProxy haproxy.backend.averageQueueTimeInSeconds backend.averageQueueTimeInSeconds HAProxy haproxy.backend.averageResponseTimeInSeconds backend.averageResponseTimeInSeconds HAProxy haproxy.backend.averageTotalSessionTimeInSeconds backend.averageTotalSessionTimeInSeconds HAProxy haproxy.backend.backupServers backend.backupServers HAProxy haproxy.backend.bytesInPerSecond backend.bytesInPerSecond HAProxy haproxy.backend.bytesOutPerSecond backend.bytesOutPerSecond HAProxy haproxy.backend.bytesThatBypassedCompressorPerSecond backend.bytesThatBypassedCompressorPerSecond HAProxy haproxy.backend.connectingRequestErrorsPerSecond backend.connectingRequestErrorsPerSecond HAProxy haproxy.backend.connectionRetriesPerSecond backend.connectionRetriesPerSecond HAProxy haproxy.backend.currentQueuedRequestsWithoutServer backend.currentQueuedRequestsWithoutServer HAProxy haproxy.backend.currentSessions backend.currentSessions HAProxy haproxy.backend.dataTransfersAbortedByClientPerSecond backend.dataTransfersAbortedByClientPerSecond HAProxy haproxy.backend.dataTransfersAbortedByServerPerSecond backend.dataTransfersAbortedByServerPerSecond HAProxy haproxy.backend.downtimeInSeconds backend.downtimeInSeconds HAProxy haproxy.backend.http100ResponsesPerSecond backend.http100ResponsesPerSecond HAProxy haproxy.backend.http200ResponsesPerSecond backend.http200ResponsesPerSecond HAProxy haproxy.backend.http300ResponsesPerSecond backend.http300ResponsesPerSecond HAProxy haproxy.backend.http400ResponsesPerSecond backend.http400ResponsesPerSecond HAProxy haproxy.backend.http500ResponsesPerSecond backend.http500ResponsesPerSecond HAProxy haproxy.backend.httpOtherResponsesPerSecond backend.httpOtherResponsesPerSecond HAProxy haproxy.backend.httpRequestsPerSecond backend.httpRequestsPerSecond HAProxy haproxy.backend.httpResponseBytesEmittedByCompressorPerSecond backend.httpResponseBytesEmittedByCompressorPerSecond HAProxy haproxy.backend.httpResponseBytesFedToCompressorPerSecond backend.httpResponseBytesFedToCompressorPerSecond HAProxy haproxy.backend.httpResponsesCompressedPerSecond backend.httpResponsesCompressedPerSecond HAProxy haproxy.backend.interceptedRequestsPerSecond backend.interceptedRequestsPerSecond HAProxy haproxy.backend.maxQueuedRequestsWithoutServer backend.maxQueuedRequestsWithoutServer HAProxy haproxy.backend.maxSessions backend.maxSessions HAProxy haproxy.backend.maxSessionsPerSecond backend.maxSessionsPerSecond HAProxy haproxy.backend.requestRedispatchPerSecond backend.requestRedispatchPerSecond HAProxy haproxy.backend.requestsDenied.securityConcernsPerSecond backend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.backend.responseErrorsPerSecond backend.responseErrorsPerSecond HAProxy haproxy.backend.responsesDenied.securityConcernsPerSecond backend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.backend.serverSelectedPerSecond backend.serverSelectedPerSecond HAProxy haproxy.backend.sessionsPerSecond backend.sessionsPerSecond HAProxy haproxy.backend.timeSinceLastSessionAssignedInSeconds backend.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.backend.timeSinceLastUpDownTransitionInSeconds backend.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.backend.totalWeight backend.totalWeight HAProxy haproxy.backend.type backend.type HAProxy haproxy.backend.upToDownTransitionsPerSecond backend.upToDownTransitionsPerSecond HAProxy haproxy.frontend.bytesInPerSecond frontend.bytesInPerSecond HAProxy haproxy.frontend.bytesOutPerSecond frontend.bytesOutPerSecond HAProxy haproxy.frontend.connectionsPerSecond frontend.connectionsPerSecond HAProxy haproxy.frontend.currentSessions frontend.currentSessions HAProxy haproxy.frontend.http100ResponsesPerSecond frontend.http100ResponsesPerSecond HAProxy haproxy.frontend.http200ResponsesPerSecond frontend.http200ResponsesPerSecond HAProxy haproxy.frontend.http300ResponsesPerSecond frontend.http300ResponsesPerSecond HAProxy haproxy.frontend.http400ResponsesPerSecond frontend.http400ResponsesPerSecond HAProxy haproxy.frontend.http500ResponsesPerSecond frontend.http500ResponsesPerSecond HAProxy haproxy.frontend.httpOtherResponsesPerSecond frontend.httpOtherResponsesPerSecond HAProxy haproxy.frontend.httpRequests.maxPerSecond frontend.httpRequests.maxPerSecond HAProxy haproxy.frontend.httpRequestsPerSecond frontend.httpRequestsPerSecond HAProxy haproxy.frontend.interceptedRequestsPerSecond frontend.interceptedRequestsPerSecond HAProxy haproxy.frontend.maxConnectionsPerSecond frontend.maxConnectionsPerSecond HAProxy haproxy.frontend.maxSessions frontend.maxSessions HAProxy haproxy.frontend.maxSessionsPerSecond frontend.maxSessionsPerSecond HAProxy haproxy.frontend.requestErrorsPerSecond frontend.requestErrorsPerSecond HAProxy haproxy.frontend.requestsDenied.securityConcernsPerSecond frontend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestConnectionRulesPerSecond frontend.requestsDenied.tcpRequestConnectionRulesPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestSessionRulesPerSecond frontend.requestsDenied.tcpRequestSessionRulesPerSecond HAProxy haproxy.frontend.responsesDenied.securityConcernsPerSecond frontend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.frontend.sessionsPerSecond frontend.sessionsPerSecond HAProxy haproxy.server.averageConnectTimeInSeconds server.averageConnectTimeInSeconds HAProxy haproxy.server.averageQueueTimeInSeconds server.averageQueueTimeInSeconds HAProxy haproxy.server.averageResponseTimeInSeconds server.averageResponseTimeInSeconds HAProxy haproxy.server.averageTotalSessionTimeInSeconds server.averageTotalSessionTimeInSeconds HAProxy haproxy.server.bytesInPerSecond server.bytesInPerSecond HAProxy haproxy.server.bytesOutPerSecond server.bytesOutPerSecond HAProxy haproxy.server.connectingRequestErrorsPerSecond server.connectingRequestErrorsPerSecond HAProxy haproxy.server.connectionRetriesPerSecond server.connectionRetriesPerSecond HAProxy haproxy.server.currentQueuedRequestsWithoutServer server.currentQueuedRequestsWithoutServer HAProxy haproxy.server.currentSessions server.currentSessions HAProxy haproxy.server.dataTransfersAbortedByClientPerSecond server.dataTransfersAbortedByClientPerSecond HAProxy haproxy.server.dataTransfersAbortedByServerPerSecond server.dataTransfersAbortedByServerPerSecond HAProxy haproxy.server.downtimeInSeconds server.downtimeInSeconds HAProxy haproxy.server.failedChecksPerSecond server.failedChecksPerSecond HAProxy haproxy.server.healthCheckDurationInMilliseconds server.healthCheckDurationInMilliseconds HAProxy haproxy.server.http100ResponsesPerSecond server.http100ResponsesPerSecond HAProxy haproxy.server.http200ResponsesPerSecond server.http200ResponsesPerSecond HAProxy haproxy.server.http300ResponsesPerSecond server.http300ResponsesPerSecond HAProxy haproxy.server.http400ResponsesPerSecond server.http400ResponsesPerSecond HAProxy haproxy.server.http500ResponsesPerSecond server.http500ResponsesPerSecond HAProxy haproxy.server.httpOtherResponsesPerSecond server.httpOtherResponsesPerSecond HAProxy haproxy.server.isActive server.isActive HAProxy haproxy.server.isBackup server.isBackup HAProxy haproxy.server.maxQueuedRequestsWithoutServer server.maxQueuedRequestsWithoutServer HAProxy haproxy.server.maxSessions server.maxSessions HAProxy haproxy.server.maxSessionsPerSecond server.maxSessionsPerSecond HAProxy haproxy.server.requestRedispatchPerSecond server.requestRedispatchPerSecond HAProxy haproxy.server.requestsDenied.securityConcernsPerSecond server.requestsDenied.securityConcernsPerSecond HAProxy haproxy.server.responseErrorsPerSecond server.responseErrorsPerSecond HAProxy haproxy.server.responsesDenied.securityConcernsPerSecond server.responsesDenied.securityConcernsPerSecond HAProxy haproxy.server.serverSelectedPerSecond server.serverSelectedPerSecond HAProxy haproxy.server.serverWeight server.serverWeight HAProxy haproxy.server.sessionsPerSecond server.sessionsPerSecond HAProxy haproxy.server.throttlePercentage server.throttlePercentage HAProxy haproxy.server.timeSinceLastSessionAssignedInSeconds server.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.server.timeSinceLastUpDownTransitionInSeconds server.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.server.type server.type HAProxy haproxy.server.upToDownTransitionsPerSecond server.upToDownTransitionsPerSecond Kafka kafka.broker.bytesWrittenToTopicPerSecond broker.bytesWrittenToTopicPerSecond Kafka kafka.broker.consumer.requestsExpiredPerSecond consumer.requestsExpiredPerSecond Kafka kafka.broker.follower.requestExpirationPerSecond follower.requestExpirationPerSecond Kafka kafka.broker.ioInPerSecond broker.IOInPerSecond Kafka kafka.broker.ioOutPerSecond broker.IOOutPerSecond Kafka kafka.broker.logFlushPerSecond broker.logFlushPerSecond Kafka kafka.broker.messagesInPerSecond broker.messagesInPerSecond Kafka kafka.broker.net.bytesRejectedPerSecond net.bytesRejectedPerSecond Kafka kafka.broker.replication.isrExpandsPerSecond replication.isrExpandsPerSecond Kafka kafka.broker.replication.isrShrinksPerSecond replication.isrShrinksPerSecond Kafka kafka.broker.replication.leaderElectionPerSecond replication.leaderElectionPerSecond Kafka kafka.broker.replication.uncleanLeaderElectionPerSecond replication.uncleanLeaderElectionPerSecond Kafka kafka.broker.replication.unreplicatedPartitions replication.unreplicatedPartitions Kafka kafka.broker.request.avgTimeFetch request.avgTimeFetch Kafka kafka.broker.request.avgTimeMetadata request.avgTimeMetadata Kafka kafka.broker.request.avgTimeMetadata99Percentile request.avgTimeMetadata99Percentile Kafka kafka.broker.request.avgTimeOffset request.avgTimeOffset Kafka kafka.broker.request.avgTimeOffset99Percentile request.avgTimeOffset99Percentile Kafka kafka.broker.request.avgTimeProduceRequest request.avgTimeProduceRequest Kafka kafka.broker.request.avgTimeUpdateMetadata request.avgTimeUpdateMetadata Kafka kafka.broker.request.avgTimeUpdateMetadata99Percentile request.avgTimeUpdateMetadata99Percentile Kafka kafka.broker.request.clientFetchesFailedPerSecond request.clientFetchesFailedPerSecond Kafka kafka.broker.request.fetchConsumerRequestsPerSecond request.fetchConsumerRequestsPerSecond Kafka kafka.broker.request.fetchFollowerRequestsPerSecond request.fetchFollowerRequestsPerSecond Kafka kafka.broker.request.fetchTime99Percentile request.fetchTime99Percentile Kafka kafka.broker.request.handlerIdle request.handlerIdle Kafka kafka.broker.request.listGroupsRequestsPerSecond request.listGroupsRequestsPerSecond Kafka kafka.broker.request.metadataRequestsPerSecond request.metadataRequestsPerSecond Kafka kafka.broker.request.offsetCommitRequestsPerSecond request.offsetCommitRequestsPerSecond Kafka kafka.broker.request.produceRequestsFailedPerSecond request.produceRequestsFailedPerSecond Kafka kafka.broker.request.produceRequestsPerSecond request.produceRequestsPerSecond Kafka kafka.broker.request.produceTime99Percentile request.produceTime99Percentile Kafka kafka.broker.topic.diskSize topic.diskSize Kafka kafka.topic.bytesInPerSec topic.BytesInPerSec Kafka kafka.topic.bytesOutPerSec topic.BytesOutPerSec Kafka kafka.topic.messagesInPerSec topic.MessagesInPerSec Kafka kafka.topic.partitionsWithNonPreferredLeader topic.partitionsWithNonPreferredLeader Kafka kafka.topic.respondsToMetadataRequests topic.respondsToMetadataRequests Kafka kafka.topic.retentionBytesOrTime topic.retentionBytesOrTime Kafka kafka.topic.underReplicatedPartitions topic.underReplicatedPartitions Kafka kafka.producer.ageMetadataUsedInMilliseconds producer.ageMetadataUsedInMilliseconds Kafka kafka.producer.availableBufferInBytes producer.availableBufferInBytes Kafka kafka.producer.avgBytesSentPerRequestInBytes producer.avgBytesSentPerRequestInBytes Kafka kafka.producer.avgCompressionRateRecordBatches producer.avgCompressionRateRecordBatches Kafka kafka.producer.avgRecordAccumulatorsInMilliseconds producer.avgRecordAccumulatorsInMilliseconds Kafka kafka.producer.avgRecordSizeInBytes producer.avgRecordSizeInBytes Kafka kafka.producer.avgRecordsSentPerSecond producer.avgRecordsSentPerSecond Kafka kafka.producer.avgRecordsSentPerTopicPerSecond producer.avgRecordsSentPerTopicPerSecond Kafka kafka.producer.avgRequestLatency producer.avgRequestLatencyPerSecond Kafka kafka.producer.avgThrottleTime producer.avgThrottleTime Kafka kafka.producer.bufferMemoryAvailableInBytes producer.bufferMemoryAvailableInBytes Kafka kafka.producer.bufferpoolWaitTime producer.bufferpoolWaitTime Kafka kafka.producer.bytesOutPerSecond producer.bytesOutPerSecond Kafka kafka.producer.compressionRateRecordBatches producer.compressionRateRecordBatches Kafka kafka.producer.ioWaitTime producer.ioWaitTime Kafka kafka.producer.maxBytesSentPerRequestInBytes producer.maxBytesSentPerRequestInBytes Kafka kafka.producer.maxRecordSizeInBytes producer.maxRecordSizeInBytes Kafka kafka.producer.maxRequestLatencyInMilliseconds producer.maxRequestLatencyInMilliseconds Kafka kafka.producer.maxThrottleTime producer.maxThrottleTime Kafka kafka.producer.requestPerSecond producer.requestPerSecond Kafka kafka.producer.requestsWaitingResponse producer.requestsWaitingResponse Kafka kafka.producer.responsePerSecond producer.responsePerSecond Kafka kafka.producer.threadsWaiting producer.threadsWaiting Kafka kafka.consumer.avgFetchSizeInBytes consumer.avgFetchSizeInBytes Kafka kafka.consumer.avgRecordConsumedPerTopic consumer.avgRecordConsumedPerTopic Kafka kafka.consumer.avgRecordConsumedPerTopicPerSecond consumer.avgRecordConsumedPerTopicPerSecond Kafka kafka.consumer.bytesInPerSecond consumer.bytesInPerSecond Kafka kafka.consumer.fetchPerSecond consumer.fetchPerSecond Kafka kafka.consumer.hwm consumer.hwm Kafka kafka.consumer.lag consumer.lag Kafka kafka.consumer.maxFetchSizeInBytes consumer.maxFetchSizeInBytes Kafka kafka.consumer.maxLag consumer.maxLag Kafka kafka.consumer.messageConsumptionPerSecond consumer.messageConsumptionPerSecond Kafka kafka.consumer.offset consumer.offset Kafka kafka.consumer.totalLag consumer.totalLag Kafka kafka.consumerGroup.maxLag consumerGroup.maxLag Kafka kafka.consumerGroup.totalLag consumerGroup.totalLag Kubernetes k8s.apiserver.goGoroutines goGoroutines Kubernetes k8s.apiserver.goThreads goThreads Kubernetes k8s.apiserver.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.apiserver.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.controllermanager.goGoroutines goGoroutines Kubernetes k8s.controllermanager.goThreads goThreads Kubernetes k8s.controllermanager.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.controllermanager.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.controllermanager.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.goGoroutines goGoroutines Kubernetes k8s.etcd.goThreads goThreads Kubernetes k8s.etcd.mvccDbTotalSizeInBytes etcdMvccDbTotalSizeInBytes Kubernetes k8s.etcd.networkClientGrpcReceivedBytesRate etcdNetworkClientGrpcReceivedBytesRate Kubernetes k8s.etcd.networkClientGrpcSentBytesRate etcdNetworkClientGrpcSentBytesRate Kubernetes k8s.etcd.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.etcd.process.maxFds processMaxFds Kubernetes k8s.etcd.process.openFds processOpenFds Kubernetes k8s.etcd.process.processFdsUtilization processFdsUtilization Kubernetes k8s.etcd.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.serverHasLeader etcdServerHasLeader Kubernetes k8s.etcd.serverLeaderChangesSeenDelta etcdServerLeaderChangesSeenDelta Kubernetes k8s.etcd.serverProposalsAppliedDelta etcdServerProposalsAppliedDelta Kubernetes k8s.etcd.serverProposalsAppliedRate etcdServerProposalsAppliedRate Kubernetes k8s.etcd.serverProposalsCommittedDelta etcdServerProposalsCommittedDelta Kubernetes k8s.etcd.serverProposalsCommittedRate etcdServerProposalsCommittedRate Kubernetes k8s.etcd.serverProposalsFailedDelta etcdServerProposalsFailedDelta Kubernetes k8s.etcd.serverProposalsFailedRate etcdServerProposalsFailedRate Kubernetes k8s.etcd.serverProposalsPending etcdServerProposalsPending Kubernetes k8s.scheduler.goGoroutines goGoroutines Kubernetes k8s.scheduler.goThreads goThreads Kubernetes k8s.scheduler.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.scheduler.podPreemptionVictims schedulerPodPreemptionVictims Kubernetes k8s.scheduler.preemptionAttemptsDelta schedulerPreemptionAttemptsDelta Kubernetes k8s.scheduler.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.scheduler.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.container.cpuCfsPeriodsDelta containerCpuCfsPeriodsDelta Kubernetes k8s.container.cpuCfsPeriodsTotal containerCpuCfsPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledPeriodsDelta containerCpuCfsThrottledPeriodsDelta Kubernetes k8s.container.cpuCfsThrottledPeriodsTotal containerCpuCfsThrottledPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledSecondsDelta containerCpuCfsThrottledSecondsDelta Kubernetes k8s.container.cpuCfsThrottledSecondsTotal containerCpuCfsThrottledSecondsTotal Kubernetes k8s.container.cpuCoresUtilization cpuCoresUtilization Kubernetes k8s.container.cpuLimitCores cpuLimitCores Kubernetes k8s.container.cpuRequestedCores cpuRequestedCores Kubernetes k8s.container.cpuUsedCores cpuUsedCores Kubernetes k8s.container.fsAvailableBytes fsAvailableBytes Kubernetes k8s.container.fsCapacityBytes fsCapacityBytes Kubernetes k8s.container.fsInodes fsInodes Kubernetes k8s.container.fsInodesFree fsInodesFree Kubernetes k8s.container.fsInodesUsed fsInodesUsed Kubernetes k8s.container.fsUsedBytes fsUsedBytes Kubernetes k8s.container.fsUsedPercent fsUsedPercent Kubernetes k8s.container.isReady isReady Kubernetes k8s.container.memoryLimitBytes memoryLimitBytes Kubernetes k8s.container.memoryMappedFileBytes containerMemoryMappedFileBytes Kubernetes k8s.container.memoryRequestedBytes memoryRequestedBytes Kubernetes k8s.container.memoryUsedBytes memoryUsedBytes Kubernetes k8s.container.memoryUtilization memoryUtilization Kubernetes k8s.container.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.container.requestedCpuCoresUtilization requestedCpuCoresUtilization Kubernetes k8s.container.requestedMemoryUtilization requestedMemoryUtilization Kubernetes k8s.container.restartCount restartCount Kubernetes k8s.daemonset.createdAt createdAt Kubernetes k8s.daemonset.metadataGeneration metadataGeneration Kubernetes k8s.daemonset.podsAvailable podsAvailable Kubernetes k8s.daemonset.podsDesired podsDesired Kubernetes k8s.daemonset.podsMisscheduled podsMisscheduled Kubernetes k8s.daemonset.podsReady podsReady Kubernetes k8s.daemonset.podsScheduled podsScheduled Kubernetes k8s.daemonset.podsUnavailable podsUnavailable Kubernetes k8s.daemonset.podsUpdatedScheduled podsUpdatedScheduled Kubernetes k8s.deployment.createdAt createdAt Kubernetes k8s.deployment.podsAvailable podsAvailable Kubernetes k8s.deployment.podsDesired podsDesired Kubernetes k8s.deployment.podsMaxUnavailable podsMaxUnavailable Kubernetes k8s.deployment.podsTotal podsTotal Kubernetes k8s.deployment.podsUnavailable podsUnavailable Kubernetes k8s.deployment.podsUpdated podsUpdated Kubernetes k8s.endpoint.addressAvailable addressAvailable Kubernetes k8s.endpoint.addressNotReady addressNotReady Kubernetes k8s.endpoint.createdAt createdAt Kubernetes k8s.namespace.createdAt createdAt Kubernetes k8s.node.allocatableAttachableVolumes* allocatableAttachableVolumes* Kubernetes k8s.node.allocatableCpuCores allocatableCpuCores Kubernetes k8s.node.allocatableCpuCoresUtilization allocatableCpuCoresUtilization Kubernetes k8s.node.allocatableEphemeralStorageBytes allocatableEphemeralStorageBytes Kubernetes k8s.node.allocatableHugepages* allocatableHugepages* Kubernetes k8s.node.allocatableMemoryBytes allocatableMemoryBytes Kubernetes k8s.node.allocatableMemoryUtilization allocatableMemoryUtilization Kubernetes k8s.node.allocatablePods allocatablePods Kubernetes k8s.node.capacityAttachableVolumes* capacityAttachableVolumes* Kubernetes k8s.node.capacityCpuCores capacityCpuCores Kubernetes k8s.node.capacityEphemeralStorageBytes capacityEphemeralStorageBytes Kubernetes k8s.node.capacityHugepages* capacityHugepages* Kubernetes k8s.node.capacityMemoryBytes capacityMemoryBytes Kubernetes k8s.node.capacityPods capacityPods Kubernetes k8s.node.cpuUsedCoreMilliseconds cpuUsedCoreMilliseconds Kubernetes k8s.node.cpuUsedCores cpuUsedCores Kubernetes k8s.node.fsAvailableBytes fsAvailableBytes Kubernetes k8s.node.fsCapacityBytes fsCapacityBytes Kubernetes k8s.node.fsCapacityUtilization fsCapacityUtilization Kubernetes k8s.node.fsInodes fsInodes Kubernetes k8s.node.fsInodesFree fsInodesFree Kubernetes k8s.node.fsInodesUsed fsInodesUsed Kubernetes k8s.node.fsUsedBytes fsUsedBytes Kubernetes k8s.node.memoryAvailableBytes memoryAvailableBytes Kubernetes k8s.node.memoryMajorPageFaultsPerSecond memoryMajorPageFaultsPerSecond Kubernetes k8s.node.memoryPageFaults memoryPageFaults Kubernetes k8s.node.memoryRssBytes memoryRssBytes Kubernetes k8s.node.memoryUsedBytes memoryUsedBytes Kubernetes k8s.node.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.node.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.node.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.node.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.node.runtimeAvailableBytes runtimeAvailableBytes Kubernetes k8s.node.runtimeCapacityBytes runtimeCapacityBytes Kubernetes k8s.node.runtimeInodes runtimeInodes Kubernetes k8s.node.runtimeInodesFree runtimeInodesFree Kubernetes k8s.node.runtimeInodesUsed runtimeInodesUsed Kubernetes k8s.node.runtimeUsedBytes runtimeUsedBytes Kubernetes k8s.pod.createdAt createdAt Kubernetes k8s.pod.isReady isReady Kubernetes k8s.pod.isScheduled isScheduled Kubernetes k8s.pod.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.pod.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.pod.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.pod.startTime startTime Kubernetes k8s.replicaset.createdAt createdAt Kubernetes k8s.replicaset.observedGeneration observedGeneration Kubernetes k8s.replicaset.podsDesired podsDesired Kubernetes k8s.replicaset.podsFullyLabeled podsFullyLabeled Kubernetes k8s.replicaset.podsMissing podsMissing Kubernetes k8s.replicaset.podsReady podsReady Kubernetes k8s.replicaset.podsTotal podsTotal Kubernetes k8s.service.createdAt createdAt Kubernetes k8s.statefulset.createdAt createdAt Kubernetes k8s.statefulset.currentRevision currentRevision Kubernetes k8s.statefulset.metadataGeneration metadataGeneration Kubernetes k8s.statefulset.observedGeneration observedGeneration Kubernetes k8s.statefulset.podsCurrent podsCurrent Kubernetes k8s.statefulset.podsDesired podsDesired Kubernetes k8s.statefulset.podsReady podsReady Kubernetes k8s.statefulset.podsTotal podsTotal Kubernetes k8s.statefulset.podsUpdated podsUpdated Kubernetes k8s.statefulset.updateRevision updateRevision Kubernetes k8s.volume.fsAvailableBytes fsAvailableBytes Kubernetes k8s.volume.fsCapacityBytes fsCapacityBytes Kubernetes k8s.volume.fsInodes fsInodes Kubernetes k8s.volume.fsInodesFree fsInodesFree Kubernetes k8s.volume.fsInodesUsed fsInodesUsed Kubernetes k8s.volume.fsUsedBytes fsUsedBytes Kubernetes k8s.volume.fsUsedPercent fsUsedPercent Memcached memcached.server.activeSlabs activeSlabs Memcached memcached.server.avgItemSizeInBytes avgItemSizeInBytes Memcached memcached.server.bytesReadServerPerSecond bytesReadServerPerSecond Memcached memcached.server.bytesUsedServerInBytes bytesUsedServerInBytes Memcached memcached.server.bytesWrittenServerPerSecond bytesWrittenServerPerSecond Memcached memcached.server.casHitRatePerSecond casHitRatePerSecond Memcached memcached.server.casMissRatePerSecond casMissRatePerSecond Memcached memcached.server.casWrongRatePerSecond casWrongRatePerSecond Memcached memcached.server.cmdFlushRatePerSecond cmdFlushRatePerSecond Memcached memcached.server.cmdGetRatePerSecond cmdGetRatePerSecond Memcached memcached.server.cmdSetRatePerSecond cmdSetRatePerSecond Memcached memcached.server.connectionRateServerPerSecond connectionRateServerPerSecond Memcached memcached.server.connectionStructuresAllocated connectionStructuresAllocated Memcached memcached.server.currentItemsStoredServer currentItemsStoredServer Memcached memcached.server.deleteCmdNoneRemovedPerSecond deleteCmdNoneRemovedPerSecond Memcached memcached.server.deleteCmdRemovedPerSecond deleteCmdRemovedPerSecond Memcached memcached.server.evictionsPerSecond evictionsPerSecond Memcached memcached.server.getHitPercent getHitPercent Memcached memcached.server.getHitPerSecond getHitPerSecond Memcached memcached.server.getMissPerSecond getMissPerSecond Memcached memcached.server.itemsStoredPerSecond itemsStoredPerSecond Memcached memcached.server.limitBytesStorage limitBytesStorage Memcached memcached.server.limitMaxBytes limitMaxBytes Memcached memcached.server.maxConnectionLimitPerSecond serverMaxConnectionLimitPerSecond Memcached memcached.server.memAllocatedSlabsInBytes memAllocatedSlabsInBytes Memcached memcached.server.openConnectionsServer openConnectionsServer Memcached memcached.server.pointerSize pointerSize Memcached memcached.server.rusageSystem usageRate Memcached memcached.server.rusageUser executionTime Memcached memcached.server.storingItemsPercentMemory storingItemsPercentMemory Memcached memcached.server.threads threads Memcached memcached.server.uptimeInMilliseconds uptimeInMilliseconds Memcached memcached.slab.activeItemsBumpedPerSecond activeItemsBumpedPerSecond Memcached memcached.slab.casBadValPerSecond casBadValPerSecond Memcached memcached.slab.casModifiedSlabPerSecond casModifiedSlabPerSecond Memcached memcached.slab.chunkSizeInBytes chunkSizeInBytes Memcached memcached.slab.chunksPerPage chunksPerPage Memcached memcached.slab.cmdSetRateSlabPerSecond cmdSetRateSlabPerSecond Memcached memcached.slab.decrsModifySlabPerSecond decrsModifySlabPerSecond Memcached memcached.slab.deleteRateSlabPerSecond deleteRateSlabPerSecond Memcached memcached.slab.entriesReclaimedPerSecond entriesReclaimedPerSecond Memcached memcached.slab.evictionsBeforeExpirationPerSecond evictionsBeforeExpirationPerSecond Memcached memcached.slab.evictionsBeforeExplicitExpirationPerSecond evictionsBeforeExplicitExpirationPerSecond Memcached memcached.slab.expiredItemsReclaimedPerSecond expiredItemsReclaimedPerSecond Memcached memcached.slab.freedChunks freedChunks Memcached memcached.slab.freedChunksEnd freedChunksEnd Memcached memcached.slab.getHitRateSlabPerSecond getHitRateSlabPerSecond Memcached memcached.slab.incrsModifySlabPerSecond incrsModifySlabPerSecond Memcached memcached.slab.itemsCold itemsCold Memcached memcached.slab.itemsColdPerSecond itemsColdPerSecond Memcached memcached.slab.itemsDirectReclaimedPerSecond itemsDirectReclaimedPerSecond Memcached memcached.slab.itemsFreedCrawlerPerSecond itemsFreedCrawlerPerSecond Memcached memcached.slab.itemsHot itemsHot Memcached memcached.slab.itemsOldestInMilliseconds itemsOldestInMilliseconds Memcached memcached.slab.itemsRefcountLockedPerSecond itemsRefcountLockedPerSecond Memcached memcached.slab.itemsSlabClass itemsSlabClass Memcached memcached.slab.itemsTimeSinceEvictionInMilliseconds itemsTimeSinceEvictionInMilliseconds Memcached memcached.slab.itemsWarm itemsWarm Memcached memcached.slab.itemsWarmPerSecond itemsWarmPerSecond Memcached memcached.slab.memRequestedSlabInBytesPerSecond memRequestedSlabInBytesPerSecond Memcached memcached.slab.outOfMemoryPerSecond outOfMemoryPerSecond Memcached memcached.slab.selfHealedSlabPerSecond selfHealedSlabPerSecond Memcached memcached.slab.totalChunksSlab totalChunksSlab Memcached memcached.slab.totalPagesSlab totalPagesSlab Memcached memcached.slab.touchHitSlabPerSecond touchHitSlabPerSecond Memcached memcached.slab.usedChunksItems usedChunksItems Memcached memcached.slab.usedChunksPerSecond usedChunksPerSecond Memcached memcached.slab.validItemsEvictedPerSecond validItemsEvictedPerSecond MongoDB mongo.index.accesses collection.indexAccesses MongoDB mongo.index.sizeInBytes collection.indexSizeInBytes MongoDB mongo.collection.avgObjSizeInBytes collection.avgObjSizeInBytes MongoDB mongo.collection.capped collection.capped MongoDB mongo.collection.count collection.count MongoDB mongo.collection.max collection.max MongoDB mongo.collection.maxSizeInBytes collection.maxSizeInBytes MongoDB mongo.collection.nindexes collection.nindexes MongoDB mongo.collection.sizeInBytes collection.sizeInBytes MongoDB mongo.collection.storageSizeInBytes collection.storageSizeInBytes MongoDB mongo.configServer.asserts.messagesPerSecond asserts.messagesPerSecond MongoDB mongo.configServer.asserts.regularPerSecond asserts.regularPerSecond MongoDB mongo.configServer.asserts.rolloversPerSecond asserts.rolloversPerSecond MongoDB mongo.configServer.asserts.userPerSecond asserts.userPerSecond MongoDB mongo.configServer.asserts.warningPerSecond asserts.warningPerSecond MongoDB mongo.configServer.commands.countFailedPerSecond commands.countFailedPerSecond MongoDB mongo.configServer.commands.countPerSecond commands.countPerSecond MongoDB mongo.configServer.commands.createIndexesFailedPerSecond commands.createIndexesFailedPerSecond MongoDB mongo.configServer.commands.createIndexesPerSecond commands.createIndexesPerSecond MongoDB mongo.configServer.commands.deleteFailedPerSecond commands.deleteFailedPerSecond MongoDB mongo.configServer.commands.deletePerSecond commands.deletePerSecond MongoDB mongo.configServer.commands.evalFailedPerSecond commands.evalFailedPerSecond MongoDB mongo.configServer.commands.evalPerSecond commands.evalPerSecond MongoDB mongo.configServer.commands.findAndModifyFailedPerSecond commands.findAndModifyFailedPerSecond MongoDB mongo.configServer.commands.findAndModifyPerSecond commands.findAndModifyPerSecond MongoDB mongo.configServer.commands.insertFailedPerSecond commands.insertFailedPerSecond MongoDB mongo.configServer.commands.insertPerSecond commands.insertPerSecond MongoDB mongo.configServer.commands.updateFailedPerSecond commands.updateFailedPerSecond MongoDB mongo.configServer.commands.updatePerSecond commands.updatePerSecond MongoDB mongo.configServer.connections.available connections.available MongoDB mongo.configServer.connections.current connections.current MongoDB mongo.configServer.connections.totalCreated connections.totalCreated MongoDB mongo.configServer.cursor.openNoTimeout cursor.openNoTimeout MongoDB mongo.configServer.cursor.openPinned cursor.openPinned MongoDB mongo.configServer.cursor.openTotal cursor.openTotal MongoDB mongo.configServer.cursor.timedOutPerSecond cursor.timedOutPerSecond MongoDB mongo.configServer.document.deletedPerSecond document.deletedPerSecond MongoDB mongo.configServer.document.insertedPerSecond document.insertedPerSecond MongoDB mongo.configServer.document.returnedPerSecond document.returnedPerSecond MongoDB mongo.configServer.document.updatedPerSecond document.updatedPerSecond MongoDB mongo.configServer.dur.commits dur.commits MongoDB mongo.configServer.dur.commitsInWriteLock dur.commitsInWriteLock MongoDB mongo.configServer.dur.compression dur.compression MongoDB mongo.configServer.dur.earlyCommits dur.earlyCommits MongoDB mongo.configServer.dur.preparingInMilliseconds dur.preparingInMilliseconds MongoDB mongo.configServer.dur.remappingInMilliseconds dur.remappingInMilliseconds MongoDB mongo.configServer.dur.timeCollectedCommitsInMilliseconds dur.timeCollectedCommitsInMilliseconds MongoDB mongo.configServer.dur.writingDataFilesInMilliseconds dur.writingDataFilesInMilliseconds MongoDB mongo.configServer.dur.writingJournalInMilliseconds dur.writingJournalInMilliseconds MongoDB mongo.configServer.flush.averageInMilliseconds flush.averageInMilliseconds MongoDB mongo.configServer.flush.flushesDisk flush.flushesDisk MongoDB mongo.configServer.flush.lastInMilliseconds flush.lastInMilliseconds MongoDB mongo.configServer.flush.totalInMilliseconds flush.totalInMilliseconds MongoDB mongo.configServer.getlasterror.wtimeMillisPerSecond getlasterror.wtimeMillisPerSecond MongoDB mongo.configServer.getlasterror.wtimeoutsPerSecond getlasterror.wtimeoutsPerSecond MongoDB mongo.configServer.globallock.activeClientsReaders globallock.activeClientsReaders MongoDB mongo.configServer.globallock.activeClientsTotal globallock.activeClientsTotal MongoDB mongo.configServer.globallock.activeClientsWriters globallock.activeClientsWriters MongoDB mongo.configServer.globallock.currentQueueReaders globallock.currentQueueReaders MongoDB mongo.configServer.globallock.currentQueueTotal globallock.currentQueueTotal MongoDB mongo.configServer.globallock.currentQueueWriters globallock.currentQueueWriters MongoDB mongo.configServer.globallock.totalTime globallock.totaltime MongoDB mongo.configServer.locks.collectionAcquireExclusive locks.collectionAcquireExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentExclusive locks.collectionAcquireIntentExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentShared locks.collectionAcquireIntentShared MongoDB mongo.configServer.locks.collectionAcquireWaitCountExclusive locks.collectionAcquireWaitCountExclusive MongoDB mongo.configServer.locks.collectionTimeAcquiringMicrosExclusive locks.collectionTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseAcquireExclusive locks.databaseAcquireExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentExclusive locks.databaseAcquireIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentShared locks.databaseAcquireIntentShared MongoDB mongo.configServer.locks.databaseAcquireShared locks.databaseAcquireShared MongoDB mongo.configServer.locks.databaseAcquireWaitExclusive locks.databaseAcquireWaitExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentExclusive locks.databaseAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentShared locks.databaseAcquireWaitIntentShared MongoDB mongo.configServer.locks.databaseAcquireWaitShared locks.databaseAcquireWaitShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosExclusive locks.databaseTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentExclusive locks.databaseTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentShared locks.databaseTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosShared locks.databaseTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.globalAcquireExclusive locks.globalAcquireExclusive MongoDB mongo.configServer.locks.globalAcquireIntentExclusive locks.globalAcquireIntentExclusive MongoDB mongo.configServer.locks.globalAcquireIntentShared locks.globalAcquireIntentShared MongoDB mongo.configServer.locks.globalAcquireShared locks.globalAcquireShared MongoDB mongo.configServer.locks.globalAcquireWaitExclusive locks.globalAcquireWaitExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentExclusive locks.globalAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentShared locks.globalAcquireWaitIntentShared MongoDB mongo.configServer.locks.globalAcquireWaitShared locks.globalAcquireWaitShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosExclusive locks.globalTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentExclusive locks.globalTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentShared locks.globalTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosShared locks.globalTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.metadataAcquireExclusive locks.metadataAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireExclusive locks.oplogAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentExclusive locks.oplogAcquireIntentExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentShared locks.oplogAcquireIntentShared MongoDB mongo",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.26195,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "New Relic Integrations Metrics The following table contains the metrics we collect for our <em>infrastructure</em> integrations. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent"
      },
      "id": "603e8a8a64441f69a34e8841"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/filter-group/group-infrastructure-results-specific-attributes": [
    {
      "sections": [
        "Filter sets: Organize your hosts",
        "Benefits of filter sets",
        "Create filter sets",
        "Edit filter sets",
        "Delete filter sets",
        "Combine filter sets with grouping",
        "Copy filters from filter set to alerts",
        "Important",
        "Filter set logic",
        "Inclusion and exclusion",
        "Recommended: Select values by matching a string",
        "Tip",
        "Select values individually",
        "And/Or"
      ],
      "title": "Filter sets: Organize your hosts",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Filter and group"
      ],
      "external_id": "ae70ce239865f3cb006e2ed47fc9bf3fc0598d81",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/filter-group/filter-sets-organize-your-infrastructure-hosts/",
      "published_at": "2021-05-05T00:33:10Z",
      "updated_at": "2021-03-11T08:50:03Z",
      "document_type": "page",
      "popularity": 1,
      "body": "With New Relic infrastructure monitoring, you can combine filters into a filter set to organize hosts based on criteria that matter the most to you. Read on to learn about the benefits, use, and logic of filter sets. Benefits of filter sets You can create filter sets using available attributes or tags. For example, you can organize your infrastructure into categories such as: Regions Operating system versions Hosts associated with Docker containers Test environments You can share filter sets with other people in your account, and you can quickly identify infrastructure problems by checking the color-coded health status of each host in the filter set. Create filter sets The default infrastructure filter set is All hosts, and it serves as a template for you to create filter sets. To create a filter set: Go to one.newrelic.com > Infrastructure and click Hosts, Inventory, or Events. If All hosts is not displayed in the left sidebar, open that filter set by selecting Saved filter sets > All hosts. In All hosts, click Filter hosts. In the list, click an item to see a list of values. Click Include or Exclude (see Filter set logic). Click values individually or enter text to match multiple values. Continue adding filters until you have the filter set you want. To name your filter, click the icon, type a name, and click Save. Edit filter sets To change an existing filter set: Go to one.newrelic.com > Infrastructure and click Hosts, Inventory, or Events. In the sidebar, click Saved filter sets to open a list. Locate the filter set by scrolling or by entering a search term. Click the filter set to open it. In the sidebar, click an option to update your filter set, and then save. Delete filter sets You can delete any saved filter set except the default All hosts. Go to one.newrelic.com > Infrastructure > Settings > Filter sets. Click to delete the filter set. Combine filter sets with grouping On some pages you can use Group by to group chart results by specific attributes. For example, on the Hosts page, you can group by awsRegion to display the AWS regions with the highest CPU usage. Grouping applies to the selected filter sets. By combining filter sets with grouping, you can find detailed system information quickly. For an example of using these tools to troubleshoot a problem, see Combining filter sets and grouping. Copy filters from filter set to alerts When you create an alert condition, you can build filters individually, or you can copy all the filters from a filter set into a new alert condition. This is a quick shortcut to populate a new alert condition with some filters. Important When you copy filter set filters to a new alert condition, these filters are no longer tied to the filter set. If you make changes to the filter set, the alert filters are not affected. To copy filter set filters to a new alert condition: Go to one.newrelic.com > Infrastructure and click Hosts, Inventory, or Events. In the sidebar, click Saved filter sets to open a list. Locate the filter set by scrolling or by entering a search term. Click the filter set to open it. Mouse over any chart and click > Create alert. Enter an alert condition name. Make adjustments to filters as necessary. Complete the remaining alert fields (see Create alert conditions). Filter set logic When you create a filter set, you generate a list of attributes and/or tags that narrow the results. This section explains how filter sets apply various rules to the list. Inclusion and exclusion As part of building a filter set, you designate whether a filter should include or exclude entities that match certain values. The way the inclusion or exclusion works depends on how you select values: Recommended: Select values by matching a string You can generate a list of values by entering a string that you want values to match. This is useful for matching multiple values. Tip String matching efficiently generates a list of values, and this approach scales as you add new entities. Here is the logic filter sets use with string matching: Comparator Logic Include If you click Include and then enter a string that you want values to match, the filter uses the comparator LIKE, which means the results include any entities that are like the string. For example, you could filter by the term East-, and all entities that contain that term are returned. Exclude If you click Exclude and then enter a string that you want values to match, the filter uses the comparator NOT LIKE, which means the results exclude any entities that are like the string. For example, you could filter by the term West-, and all entities that do not contain that term are returned. Select values individually You can click through the list of attributes/tags to identify individual values. Tip This approach does not scale well if you add new entities. Here is the logic filter sets use with individual value selection: Comparator Logic Include If you click Include and then click specific values, the filter uses the comparator IN, which means the filter looks for entities that exactly match one or more values in your list of selections. Exclude If you click Exclude and then click specific values, the filter set uses the comparator NOT IN, which means the filter returns all entities that do not exactly match one or more values in your list of selections. And/Or Filter sets use the logical operators AND and OR behind the scenes to join the data. Here are the rules for AND and OR: When you click values from multiple attributes or tags, they are joined by AND. When you click values from within an attribute or tags, they are joined by OR. The filter results display hosts for which both of the following are true: Hosts containing any one of the selected infrastructure agent versions Hosts in any one of the selected AWS availability zones",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 209.53937,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Filter</em> sets: Organize <em>your</em> hosts",
        "sections": "<em>Filter</em> sets: Organize <em>your</em> hosts",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": " or tags. For example, you can organize <em>your</em> <em>infrastructure</em> into categories such as: Regions Operating system versions Hosts associated with Docker containers Test environments You can share <em>filter</em> sets with other people in <em>your</em> account, and you can quickly identify <em>infrastructure</em> problems by checking"
      },
      "id": "6043ed8ee7b9d289955799cb"
    },
    {
      "sections": [
        "Default infrastructure monitoring data",
        "Important",
        "Infrastructure events",
        "Supported Linux storage systems",
        "Supported Windows storage systems",
        "Query infrastructure data",
        "Manage data",
        "Add custom attributes",
        "Common Amazon EC2 attributes",
        "awsRegion",
        "awsAvailabilityZone",
        "ec2InstanceType",
        "ec2InstanceId",
        "ec2AmiId",
        "ec2SubnetId",
        "ec2VpcId",
        "Other Amazon EC2 attributes"
      ],
      "title": "Default infrastructure monitoring data ",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "01647189a48892103f4dc6abe07ce29d5fc13f0d",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/default-infrastructure-monitoring-data/",
      "published_at": "2021-05-05T03:38:21Z",
      "updated_at": "2021-03-30T08:36:48Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Important As of April 12, 2021, we are upgrading Insights to an improved web and mobile experience! All of your Insights URLs will be redirected automatically to the corresponding dashboards in New Relic One. For more details about this migration and how you can easily plan for this transition, see our Explorers Hub post. New Relic's infrastructure monitoring agent collects and displays data using six primary events, each with associated attributes that represent assorted metrics and metadata. Understanding infrastructure data can help you: Better understand our infrastructure monitoring UI. Manage your infrastructure data. Create better host filter sets. Run better queries of your data. Set up better monitoring solutions using custom attributes. Infrastructure events The following are events reported by default by the infrastructure agent and some infrastructure integrations. The attributes attached to these events are the metadata and metrics used to create our infrastructure UI visualizations. You can also create custom queries and charts of this data. If you're using integrations, see that integration's doc for more on reported data. For common AWS attributes, see AWS data. Select an event name in the following table to see its attributes. Event Description SystemSample SystemSample contains data describing the current overall state of the entire server, including CPU, memory, disk, and network. We take a snapshot of this data every 5 seconds and package it into a SystemSample event, which is then sent to New Relic. This data appears in the Hosts UI page. ProcessSample ProcessSample gathers detailed resource usage information from programs running on a single system. We take a snapshot of this data every 20 seconds for every active process and package it into a ProcessSample event, which is then sent to New Relic. This data appears on the Processes UI page. Important Process metrics are not sent to New Relic by default for accounts created after July 20, 2020. Enable process metrics to get this data into the Infrastructure monitoring UI. StorageSample StorageSample represents a single storage device associated with a server. Each sample gathers descriptive information about the device, the type of file system it uses, and its current usage and capacity. We take a snapshot of this data every 20 seconds for each mounted file system and package it into a StorageSample event, which is then sent to New Relic. This data appears on the Storage UI page. Important If your server uses disks with file systems other than the supported file systems in the following table, StorageSample events will not be generated for those disks. Supported Linux storage systems Supported Linux storage file systems: xfs vxfs btrfs ext ext2 ext3 ext4 hfs Supported Windows storage systems Supported Windows storage file systems: NTFS ReFS (version 1.0.976 and higher) NetworkSample NetworkSample captures the descriptive and state information for each network device associated with a server. It includes the device's interface and address information, as well as current usage data. We take a snapshot of this data every 10 seconds for each attached network interface and package it into a NetworkSample event, which is then sent to New Relic. This data appears on the Network UI page. ContainerSample ContainerSample collects the descriptive and state information for each Docker container. It includes the container's ID, name, image, image name, as well metrics about CPU, memory and networking. We take a snapshot of this data every 15 seconds for each container and package it into a ContainerSample event, which is then sent to New Relic. This data appears on the Containers UI page. For more information, see Docker monitoring. InfrastructureEvent InfrastructureEvent describes changes (deltas) that occur in a system's live state. When an inventory or system state is added, removed, or changed, New Relic will produce an InfrastructureEvent that logs that activity. This data appears on the Events UI page. To learn about infrastructure integration data, see the documentation for a specific integration. If an AWS integration is enabled, your infrastructure events may also have AWS attributes attached. Query infrastructure data You can query your infrastructure data to troubleshoot a problem or create a chart, or to understand what data is available. For example, to see what data is attached to ProcessSample, you would run this NRQL query: SELECT * FROM ProcessSample Copy You can also query infrastructure using dimensional metrics. Manage data For tips on managing data ingest and reporting, see Manage infrastructure data. Add custom attributes You can create custom attributes in the infrastructure agent's YAML file. Use this metadata to: Create infrastructure filter sets Populate the Group by menu Annotate your infrastructure data Common Amazon EC2 attributes If you connect your Amazon Elastic Compute Cloud (EC2) account to our infrastructure monitoring, we report data from your Amazon EC2 instances. Amazon EC2-related attributes are common attributes that can be used in any event. These attributes are drawn from the EC2 API. No CloudWatch information is collected. These attributes and their values are subject to change if Amazon changes the data they expose. awsRegion The region (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. awsAvailabilityZone The availability zone (determined by Amazon Web Services) where the AWS server exists. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceType The Amazon Web Services instance type, displayed in AWS-specific codes. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2InstanceId The Amazon Web Services instance's unique identifying number for the server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2AmiId The Amazon Machine Image (AMI) identification number of the image used by Amazon Web Services to bootstrap the Amazon EC2 instance. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2SubnetId The networking sub-net identifier on which the server is connected. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. ec2VpcId The Virtual Private Cloud identifier (if any) for this server. This attribute exists only for customers using New Relic to monitor Amazon EC2 servers. Other Amazon EC2 attributes If Amazon Web Services changes the metadata they make available to New Relic, other attributes and values collected also may be available.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 116.51643,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Default <em>infrastructure</em> monitoring <em>data</em> ",
        "sections": "Default <em>infrastructure</em> monitoring <em>data</em>",
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": ". <em>Manage</em> <em>your</em> <em>infrastructure</em> <em>data</em>. Create better host <em>filter</em> sets. Run better queries of <em>your</em> <em>data</em>. Set up better monitoring solutions using custom attributes. <em>Infrastructure</em> events The following are events reported by default by the <em>infrastructure</em> agent and some <em>infrastructure</em> integrations"
      },
      "id": "6043edcd28ccbcfa8a2c6086"
    },
    {
      "sections": [
        "On-host integrations metrics",
        "New Relic Integrations Metrics"
      ],
      "title": "On-host integrations metrics",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Manage your data",
        "Data and instrumentation"
      ],
      "external_id": "fe96c0c4950380504b1a33c3ad861bcb17507cba",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/manage-your-data/data-instrumentation/host-integrations-metrics/",
      "published_at": "2021-05-05T03:38:23Z",
      "updated_at": "2021-03-16T15:57:07Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic Integrations Metrics The following table contains the metrics we collect for our infrastructure integrations. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent cpuPercent Agent host.cpuStealPercent cpuStealPercent Agent host.cpuSystemPercent cpuSystemPercent Agent host.cpuUserPercent cpuUserPercent Agent host.disk.avgQueueLen avgQueueLen Agent host.disk.avgReadQueueLen avgReadQueueLen Agent host.disk.avgWriteQueueLen avgWriteQueueLen Agent host.disk.currentQueueLen currentQueueLen Agent host.disk.freeBytes diskFreeBytes Agent host.disk.freePercent diskFreePercent Agent host.disk.inodesFree inodesFree Agent host.disk.inodesTotal inodesTotal Agent host.disk.inodesUsed inodesUsed Agent host.disk.inodesUsedPercent inodesUsedPercent Agent host.disk.readBytesPerSecond readBytesPerSecond Agent host.disk.readIoPerSecond readIoPerSecond Agent host.disk.readUtilizationPercent readUtilizationPercent Agent host.disk.readWriteBytesPerSecond readWriteBytesPerSecond Agent host.disk.totalBytes diskTotalBytes Agent host.disk.totalUtilizationPercent totalUtilizationPercent Agent host.disk.usedBytes diskUsedBytes Agent host.disk.usedPercent diskUsedPercent Agent host.disk.writeBytesPerSecond writeBytesPerSecond Agent host.disk.writeIoPerSecond writeIoPerSecond Agent host.disk.writeUtilizationPercent writeUtilizationPercent Agent host.diskFreeBytes diskFreeBytes Agent host.diskFreePercent diskFreePercent Agent host.diskReadsPerSecond diskReadsPerSecond Agent host.diskReadUtilizationPercent diskReadUtilizationPercent Agent host.diskTotalBytes diskTotalBytes Agent host.diskUsedBytes diskUsedBytes Agent host.diskUsedPercent diskUsedPercent Agent host.diskUtilizationPercent diskUtilizationPercent Agent host.diskWritesPerSecond diskWritesPerSecond Agent host.diskWriteUtilizationPercent diskWriteUtilizationPercent Agent host.loadAverageFifteenMinute loadAverageFifteenMinute Agent host.loadAverageFiveMinute loadAverageFiveMinute Agent host.loadAverageOneMinute loadAverageOneMinute Agent host.memoryFreeBytes memoryFreeBytes Agent host.memoryFreePercent memoryFreePercent Agent host.memoryTotalBytes memoryTotalBytes Agent host.memoryUsedBytes memoryUsedBytes Agent host.memoryUsedPercent memoryUsedPercent Agent host.net.receiveBytesPerSecond receiveBytesPerSecond Agent host.net.receiveDroppedPerSecond receiveDroppedPerSecond Agent host.net.receiveErrorsPerSecond receiveErrorsPerSecond Agent host.net.receivePacketsPerSecond receivePacketsPerSecond Agent host.net.transmitBytesPerSecond transmitBytesPerSecond Agent host.net.transmitDroppedPerSecond transmitDroppedPerSecond Agent host.net.transmitErrorsPerSecond transmitErrorsPerSecond Agent host.net.transmitPacketsPerSecond transmitPacketsPerSecond Agent host.process.cpuPercent cpuPercent Agent host.process.cpuSystemPercent cpuSystemPercent Agent host.process.cpuUserPercent cpuUserPercent Agent host.process.fileDescriptorCount fileDescriptorCount Agent host.process.ioReadBytesPerSecond ioReadBytesPerSecond Agent host.process.ioReadCountPerSecond ioReadCountPerSecond Agent host.process.ioTotalReadBytes ioTotalReadBytes Agent host.process.ioTotalReadCount ioTotalReadCount Agent host.process.ioTotalWriteBytes ioTotalWriteBytes Agent host.process.ioTotalWriteCount ioTotalWriteCount Agent host.process.ioWriteBytesPerSecond ioWriteBytesPerSecond Agent host.process.ioWriteCountPerSecond ioWriteCountPerSecond Agent host.process.memoryResidentSizeBytes memoryResidentSizeBytes Agent host.process.memoryVirtualSizeBytes memoryVirtualSizeBytes Agent host.process.threadCount threadCount Agent host.swapFreeBytes swapFreeBytes Agent host.swapTotalBytes swapTotalBytes Agent host.swapUsedBytes swapUsedBytes Apache apache.server.busyWorkers server.busyWorkers Apache apache.server.idleWorkers server.idleWorkers Apache apache.server.net.bytesPerSecond net.bytesPerSecond Apache apache.server.net.requestsPerSecond net.requestsPerSecond Apache apache.server.scoreboard.closingWorkers server.scoreboard.closingWorkers Apache apache.server.scoreboard.dnsLookupWorkers server.scoreboard.dnsLookupWorkers Apache apache.server.scoreboard.finishingWorkers server.scoreboard.finishingWorkers Apache apache.server.scoreboard.idleCleanupWorkers server.scoreboard.idleCleanupWorkers Apache apache.server.scoreboard.keepAliveWorkers server.scoreboard.keepAliveWorkers Apache apache.server.scoreboard.loggingWorkers server.scoreboard.loggingWorkers Apache apache.server.scoreboard.readingWorkers server.scoreboard.readingWorkers Apache apache.server.scoreboard.startingWorkers server.scoreboard.startingWorkers Apache apache.server.scoreboard.totalWorkers server.scoreboard.totalWorkers Apache apache.server.scoreboard.writingWorkers server.scoreboard.writingWorkers Cassandra cassandra.node.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.node.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.node.client.connectedNativeClients client.connectedNativeClients Cassandra cassandra.node.commitLogCompletedTasksPerSecond db.commitLogCompletedTasksPerSecond Cassandra cassandra.node.commitLogPendingTasks db.commitLogPendindTasks Cassandra cassandra.node.commitLogTotalSizeBytes db.commitLogTotalSizeBytes Cassandra cassandra.node.droppedBatchRemoveMessagesPerSecond db.droppedBatchRemoveMessagesPerSecond Cassandra cassandra.node.droppedBatchStoreMessagesPerSecond db.droppedBatchStoreMessagesPerSecond Cassandra cassandra.node.droppedCounterMutationMessagesPerSecond db.droppedCounterMutationMessagesPerSecond Cassandra cassandra.node.droppedHintMessagesPerSecond db.droppedHintMessagesPerSecond Cassandra cassandra.node.droppedMutationMessagesPerSecond db.droppedMutationMessagesPerSecond Cassandra cassandra.node.droppedPagedRangeMessagesPerSecond db.droppedPagedRangeMessagesPerSecond Cassandra cassandra.node.droppedRangeSliceMessagesPerSecond db.droppedRangeSliceMessagesPerSecond Cassandra cassandra.node.droppedReadMessagesPerSecond db.droppedReadMessagesPerSecond Cassandra cassandra.node.droppedReadRepairMessagesPerSecond db.droppedReadRepairMessagesPerSecond Cassandra cassandra.node.droppedRequestResponseMessagesPerSecond db.droppedRequestResponseMessagesPerSecond Cassandra cassandra.node.droppedTraceMessagesPerSecond db.droppedTraceMessagesPerSecond Cassandra cassandra.node.keyCacheCapacityBytes db.keyCacheCapacityBytes Cassandra cassandra.node.keyCacheHitRate db.keyCacheHitRate Cassandra cassandra.node.keyCacheHitsPerSecond db.keyCacheHitsPerSecond Cassandra cassandra.node.keyCacheRequestsPerSecond db.keyCacheRequestsPerSecond Cassandra cassandra.node.keyCacheSizeBytes db.keyCacheSizeBytes Cassandra cassandra.node.liveSsTableCount db.liveSSTableCount Cassandra cassandra.node.loadBytes db.loadBytes Cassandra cassandra.node.query.casReadRequestsPerSecond query.CASReadRequestsPerSecond Cassandra cassandra.node.query.casWriteRequestsPerSecond query.CASWriteRequestsPerSecond Cassandra cassandra.node.query.rangeSliceRequestsPerSecond query.rangeSliceRequestsPerSecond Cassandra cassandra.node.query.rangeSliceTimeoutsPerSecond query.rangeSliceTimeoutsPerSecond Cassandra cassandra.node.query.rangeSliceUnavailablesPerSecond query.rangeSliceUnavailablesPerSecond Cassandra cassandra.node.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.node.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.node.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.node.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.node.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.node.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.node.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.node.query.readTimeoutsPerSecond query.readTimeoutsPerSecond Cassandra cassandra.node.query.readUnavailablesPerSecond query.readUnavailablesPerSecond Cassandra cassandra.node.query.viewWriteRequestsPerSecond query.viewWriteRequestsPerSecond Cassandra cassandra.node.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.node.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.node.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.node.query.writeTimeoutsPerSecond query.writeTimeoutsPerSecond Cassandra cassandra.node.query.writeUnavailablesPerSecond query.writeUnavailablesPerSecond Cassandra cassandra.node.rowCacheCapacityBytes db.rowCacheCapacityBytes Cassandra cassandra.node.rowCacheHitRate db.rowCacheHitRate Cassandra cassandra.node.rowCacheHitsPerSecond db.rowCacheHitsPerSecond Cassandra cassandra.node.rowCacheRequestsPerSecond db.rowCacheRequestsPerSecond Cassandra cassandra.node.rowCacheSizeBytes db.rowCacheSizeBytes Cassandra cassandra.node.storage.exceptionCount storage.exceptionCount Cassandra cassandra.node.threadPool.antiEntropyStage.activeTasks db.threadpool.internalAntiEntropyStageActiveTasks Cassandra cassandra.node.threadPool.antiEntropyStage.completedTasks db.threadpool.internalAntiEntropyStageCompletedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.currentlyBlockedTasks db.threadpool.internalAntiEntropyStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.antiEntropyStage.pendingTasks db.threadpool.internalAntiEntropyStagePendingTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.activeTasks db.threadpool.internalCacheCleanupExecutorActiveTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.completedTasks db.threadpool.internalCacheCleanupExecutorCompletedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.currentlyBlockedTasks db.threadpool.internalCacheCleanupExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.cacheCleanupExecutor.pendingTasks db.threadpool.internalCacheCleanupExecutorPendingTasks Cassandra cassandra.node.threadPool.compactionExecutor.activeTasks db.threadpool.internalCompactionExecutorActiveTasks Cassandra cassandra.node.threadPool.compactionExecutor.completedTasks db.threadpool.internalCompactionExecutorCompletedTasks Cassandra cassandra.node.threadPool.compactionExecutor.currentlyBlockedTasks db.threadpool.internalCompactionExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.compactionExecutor.pendingTasks db.threadpool.internalCompactionExecutorPendingTasks Cassandra cassandra.node.threadPool.counterMutationStage.activeTasks db.threadpool.requestCounterMutationStageActiveTasks Cassandra cassandra.node.threadPool.counterMutationStage.completedTasks db.threadpool.requestCounterMutationStageCompletedTasks Cassandra cassandra.node.threadPool.counterMutationStage.currentlyBlockedTasks db.threadpool.requestCounterMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.counterMutationStage.pendingTasks db.threadpool.requestCounterMutationStagePendingTasks Cassandra cassandra.node.threadPool.gossipStage.activeTasks db.threadpool.internalGossipStageActiveTasks Cassandra cassandra.node.threadPool.gossipStage.completedTasks db.threadpool.internalGossipStageCompletedTasks Cassandra cassandra.node.threadPool.gossipStage.currentlyBlockedTasks db.threadpool.internalGossipStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.gossipStage.pendingTasks db.threadpool.internalGossipStagePendingTasks Cassandra cassandra.node.threadPool.hintsDispatcher.activeTasks db.threadpool.internalHintsDispatcherActiveTasks Cassandra cassandra.node.threadPool.hintsDispatcher.completedTasks db.threadpool.internalHintsDispatcherCompletedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.currentlyBlockedTasks db.threadpool.internalHintsDispatcherCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.hintsDispatcher.pendingTasks db.threadpool.internalHintsDispatcherPendingTasks Cassandra cassandra.node.threadPool.internalResponseStage.activeTasks db.threadpool.internalInternalResponseStageActiveTasks Cassandra cassandra.node.threadPool.internalResponseStage.completedTasks db.threadpool.internalInternalResponseStageCompletedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pCurrentlyBlockedTasks db.threadpool.internalInternalResponseStagePCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.internalResponseStage.pendingTasks db.threadpool.internalInternalResponseStagePendingTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.activeTasks db.threadpool.internalMemtableFlushWriterActiveTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.completedTasks db.threadpool.internalMemtableFlushWriterCompletedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.currentlyBlockedTasks db.threadpool.internalMemtableFlushWriterCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableFlushWriter.pendingTasks db.threadpool.internalMemtableFlushWriterPendingTasks Cassandra cassandra.node.threadPool.memtablePostFlush.activeTasks db.threadpool.internalMemtablePostFlushActiveTasks Cassandra cassandra.node.threadPool.memtablePostFlush.completedTasks db.threadpool.internalMemtablePostFlushCompletedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.currentlyBlockedTasks db.threadpool.internalMemtablePostFlushCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtablePostFlush.pendingTasks db.threadpool.internalMemtablePostFlushPendingTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.activeTasks db.threadpool.internalMemtableReclaimMemoryActiveTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.completedTasks db.threadpool.internalMemtableReclaimMemoryCompletedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.currentlyBlockedTasks db.threadpool.internalMemtableReclaimMemoryCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.memtableReclaimMemory.pendingTasks db.threadpool.internalMemtableReclaimMemoryPendingTasks Cassandra cassandra.node.threadPool.migrationStage.activeTasks db.threadpool.internalMigrationStageActiveTasks Cassandra cassandra.node.threadPool.migrationStage.completedTasks db.threadpool.internalMigrationStageCompletedTasks Cassandra cassandra.node.threadPool.migrationStage.currentlyBlockedTasks db.threadpool.internalMigrationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.migrationStage.pendingTasks db.threadpool.internalMigrationStagePendingTasks Cassandra cassandra.node.threadPool.miscStage.activeTasks db.threadpool.internalMiscStageActiveTasks Cassandra cassandra.node.threadPool.miscStage.completedTasks db.threadpool.internalMiscStageCompletedTasks Cassandra cassandra.node.threadPool.miscStage.currentlyBlockedTasks db.threadpool.internalMiscStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.miscStage.pendingTasks db.threadpool.internalMiscStagePendingTasks Cassandra cassandra.node.threadPool.mutationStage.activeTasks db.threadpool.requestMutationStageActiveTasks Cassandra cassandra.node.threadPool.mutationStage.completedTasks db.threadpool.requestMutationStageCompletedTasks Cassandra cassandra.node.threadPool.mutationStage.currentlyBlockedTasks db.threadpool.requestMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.mutationStage.pendingTasks db.threadpool.requestMutationStagePendingTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.activeTasks db.threadpool.internalPendingRangeCalculatorActiveTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.completedTasks db.threadpool.internalPendingRangeCalculatorCompletedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.currentlyBlockedTasks db.threadpool.internalPendingRangeCalculatorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.pendingRangeCalculator.pendingTasks db.threadpool.internalPendingRangeCalculatorPendingTasks Cassandra cassandra.node.threadPool.readRepairStage.activeTasks db.threadpool.requestReadRepairStageActiveTasks Cassandra cassandra.node.threadPool.readRepairStage.completedTasks db.threadpool.requestReadRepairStageCompletedTasks Cassandra cassandra.node.threadPool.readRepairStage.currentlyBlockedTasks db.threadpool.requestReadRepairStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readRepairStage.pendingTasks db.threadpool.requestReadRepairStagePendingTasks Cassandra cassandra.node.threadPool.readStage.activeTasks db.threadpool.requestReadStageActiveTasks Cassandra cassandra.node.threadPool.readStage.completedTasks db.threadpool.requestReadStageCompletedTasks Cassandra cassandra.node.threadPool.readStage.currentlyBlockedTasks db.threadpool.requestReadStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.readStage.pendingTasks db.threadpool.requestReadStagePendingTasks Cassandra cassandra.node.threadPool.requestResponseStage.activeTasks db.threadpool.requestRequestResponseStageActiveTasks Cassandra cassandra.node.threadPool.requestResponseStage.completedTasks db.threadpool.requestRequestResponseStageCompletedTasks Cassandra cassandra.node.threadPool.requestResponseStage.currentlyBlockedTasks db.threadpool.requestRequestResponseStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.requestResponseStage.pendingTasks db.threadpool.requestRequestResponseStagePendingTasks Cassandra cassandra.node.threadPool.sampler.activeTasks db.threadpool.internalSamplerActiveTasks Cassandra cassandra.node.threadPool.sampler.completedTasks db.threadpool.internalSamplerCompletedTasks Cassandra cassandra.node.threadPool.sampler.currentlyBlockedTasks db.threadpool.internalSamplerCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.sampler.pendingTasks db.threadpool.internalSamplerPendingTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.activeTasks db.threadpool.internalSecondaryIndexManagementActiveTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.completedTasks db.threadpool.internalSecondaryIndexManagementCompletedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.currentlyBlockedTasks db.threadpool.internalSecondaryIndexManagementCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.secondaryIndexManagement.pendingTasks db.threadpool.internalSecondaryIndexManagementPendingTasks Cassandra cassandra.node.threadPool.validationExecutor.activeTasks db.threadpool.internalValidationExecutorActiveTasks Cassandra cassandra.node.threadPool.validationExecutor.completedTasks db.threadpool.internalValidationExecutorCompletedTasks Cassandra cassandra.node.threadPool.validationExecutor.currentlyBlockedTasks db.threadpool.internalValidationExecutorCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.validationExecutor.pendingTasks db.threadpool.internalValidationExecutorPendingTasks Cassandra cassandra.node.threadPool.viewMutationStage.activeTasks db.threadpool.requestViewMutationStageActiveTasks Cassandra cassandra.node.threadPool.viewMutationStage.completedTasks db.threadpool.requestViewMutationStageCompletedTasks Cassandra cassandra.node.threadPool.viewMutationStage.currentlyBlockedTasks db.threadpool.requestViewMutationStageCurrentlyBlockedTasks Cassandra cassandra.node.threadPool.viewMutationStage.pendingTasks db.threadpool.requestViewMutationStagePendingTasks Cassandra cassandra.node.totalHintsInProgress db.totalHintsInProgress Cassandra cassandra.node.totalHintsPerSecond db.totalHintsPerSecond Cassandra cassandra.columnFamily.allMemtablesOffHeapSizeBytes db.allMemtablesOffHeapSizeBytes Cassandra cassandra.columnFamily.allMemtablesOnHeapSizeBytes db.allMemtablesOnHeapSizeBytes Cassandra cassandra.columnFamily.bloomFilterFalseRatio db.bloomFilterFalseRatio Cassandra cassandra.columnFamily.liveDiskSpaceUsedBytes db.liveDiskSpaceUsedBytes Cassandra cassandra.columnFamily.liveSsTableCount db.liveSSTableCount Cassandra cassandra.columnFamily.maxRowSize db.maxRowSize Cassandra cassandra.columnFamily.meanRowSize db.meanRowSize Cassandra cassandra.columnFamily.memtableLiveDataSize db.memtableLiveDataSize Cassandra cassandra.columnFamily.minRowSize db.minRowSize Cassandra cassandra.columnFamily.pendingCompactions db.pendingCompactions Cassandra cassandra.columnFamily.query.readLatency50ThPercentileMilliseconds query.readLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency75ThPercentileMilliseconds query.readLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency95ThPercentileMilliseconds query.readLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency98ThPercentileMilliseconds query.readLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency999ThPercentileMilliseconds query.readLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readLatency99ThPercentileMilliseconds query.readLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.readRequestsPerSecond query.readRequestsPerSecond Cassandra cassandra.columnFamily.query.writeLatency50ThPercentileMilliseconds query.writeLatency50thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency75ThPercentileMilliseconds query.writeLatency75thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency95ThPercentileMilliseconds query.writeLatency95thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency98ThPercentileMilliseconds query.writeLatency98thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency999ThPercentileMilliseconds query.writeLatency999thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeLatency99ThPercentileMilliseconds query.writeLatency99thPercentileMilliseconds Cassandra cassandra.columnFamily.query.writeRequestsPerSecond query.writeRequestsPerSecond Cassandra cassandra.columnFamily.speculativeRetries db.speculativeRetries Cassandra cassandra.columnFamily.ssTablesPerRead50ThPercentileMilliseconds db.SSTablesPerRead50thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead75ThPercentileMilliseconds db.SSTablesPerRead75thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead95ThPercentileMilliseconds db.SSTablesPerRead95thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead98ThPercentileMilliseconds db.SSTablesPerRead98thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead999ThPercentileMilliseconds db.SSTablesPerRead999thPercentileMilliseconds Cassandra cassandra.columnFamily.ssTablesPerRead99ThPercentileMilliseconds db.SSTablesPerRead99thPercentileMilliseconds Cassandra cassandra.columnFamily.tombstoneScannedHistogram50ThPercentile db.tombstoneScannedHistogram50thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram75ThPercentile db.tombstoneScannedHistogram75thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram95ThPercentile db.tombstoneScannedHistogram95thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram98ThPercentile db.tombstoneScannedHistogram98thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram999ThPercentile db.tombstoneScannedHistogram999thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogram99ThPercentile db.tombstoneScannedHistogram99thPercentile Cassandra cassandra.columnFamily.tombstoneScannedHistogramCount db.tombstoneScannedHistogramCount Consul consul.datacenter.catalog.criticalNodes catalog.criticalNodes Consul consul.datacenter.catalog.passingNodes catalog.passingNodes Consul consul.datacenter.catalog.registeredNodes catalog.registeredNodes Consul consul.datacenter.catalog.upNodes catalog.upNodes Consul consul.datacenter.catalog.warningNodes catalog.warningNodes Consul consul.datacenter.cluster.flaps cluster.flaps Consul consul.datacenter.cluster.suspects cluster.suspects Consul consul.datacenter.raft.commitTime raft.commitTimes Consul consul.datacenter.raft.commitTimeAvgInMilliseconds raft.commitTimeAvgInMilliseconds Consul consul.datacenter.raft.commitTimeMaxInMilliseconds raft.commitTimeMaxInMilliseconds Consul consul.datacenter.raft.completedLeaderElections raft.completedLeaderElections Consul consul.datacenter.raft.initiatedLeaderElections raft.initiatedLeaderElections Consul consul.datacenter.raft.lastContactAvgInMilliseconds raft.lastContactAvgInMilliseconds Consul consul.datacenter.raft.lastContactMaxInMilliseconds raft.lastContactMaxInMilliseconds Consul consul.datacenter.raft.lastContacts raft.lastContacts Consul consul.datacenter.raft.logDispatchAvgInMilliseconds raft.logDispatchAvgInMilliseconds Consul consul.datacenter.raft.logDispatches raft.logDispatches Consul consul.datacenter.raft.logDispatchMaxInMilliseconds raft.logDispatchMaxInMilliseconds Consul consul.datacenter.raft.txns raft.txns Consul consul.agent.aclCacheHitPerSecond agent.aclCacheHit Consul consul.agent.aclCacheMissPerSecond agent.aclCacheMiss Consul consul.agent.client.rpcFailed client.rpcFailed Consul consul.agent.client.rpcLoad client.rpcLoad Consul consul.agent.kvStores agent.kvStoress Consul consul.agent.kvStoresAvgInMilliseconds agent.kvStoresAvgInMilliseconds Consul consul.agent.kvStoresMaxInMilliseconds agent.kvStoresMaxInMilliseconds Consul consul.agent.net.agent.maxLatencyInMilliseconds net.agent.maxLatencyInMilliseconds Consul consul.agent.net.medianLatencyInMilliseconds net.agent.medianLatencyInMilliseconds Consul consul.agent.net.minLatencyInMilliseconds net.agent.minLatencyInMilliseconds Consul consul.agent.net.p25LatencyInMilliseconds net.agent.p25LatencyInMilliseconds Consul consul.agent.net.p75LatencyInMilliseconds net.agent.p75LatencyInMilliseconds Consul consul.agent.net.p90LatencyInMilliseconds net.agent.p90LatencyInMilliseconds Consul consul.agent.net.p95LatencyInMilliseconds net.agent.p95LatencyInMilliseconds Consul consul.agent.net.p99LatencyInMilliseconds net.agent.p99LatencyInMilliseconds Consul consul.agent.peers agent.peers Consul consul.agent.runtime.allocations runtime.allocations Consul consul.agent.runtime.allocationsInBytes runtime.allocationsInBytes Consul consul.agent.runtime.frees runtime.frees Consul consul.agent.runtime.gcCycles runtime.gcCycles Consul consul.agent.runtime.gcPauseInMilliseconds runtime.gcPauseInMilliseconds Consul consul.agent.runtime.goroutines runtime.goroutines Consul consul.agent.runtime.heapObjects runtime.heapObjects Consul consul.agent.runtime.virtualAddressSpaceInBytes runtime.virtualAddressSpaceInBytes Consul consul.agent.staleQueries agent.staleQueries Consul consul.agent.txnAvgInMilliseconds agent.txnAvgInMilliseconds Consul consul.agent.txnMaxInMilliseconds agent.txnMaxInMilliseconds Consul consul.agent.txns agent.txns Couchbase couchbase.bucket.activeItemsEnteringDiskQueuePerSecond bucket.activeItemsEnteringDiskQueuePerSecond Couchbase couchbase.bucket.activeItemsInMemory bucket.activeItemsInMemory Couchbase couchbase.bucket.activeResidentItemsRatio bucket.activeResidentItemsRatio Couchbase couchbase.bucket.averageDiskCommitTimeInMilliseconds bucket.averageDiskCommitTimeInMilliseconds Couchbase couchbase.bucket.averageDiskUpdateTimeInMilliseconds bucket.averageDiskUpdateTimeInMilliseconds Couchbase couchbase.bucket.cacheMisses bucket.cacheMisses Couchbase couchbase.bucket.cacheMissRatio bucket.cacheMissRatio Couchbase couchbase.bucket.casHits bucket.casHits Couchbase couchbase.bucket.casMisses bucket.casMisses Couchbase couchbase.bucket.couchDocsFragmentationPercent bucket.couchDocsFragmentationPercent Couchbase couchbase.bucket.currentConnections bucket.currentConnections Couchbase couchbase.bucket.dataUsedInBytes bucket.dataUsedInBytes Couchbase couchbase.bucket.decrementHitsPerSecond bucket.decrementHitsPerSecond Couchbase couchbase.bucket.decrementMissesPerSecond bucket.decrementMissesPerSecond Couchbase couchbase.bucket.deleteHitsPerSecond bucket.deleteHitsPerSecond Couchbase couchbase.bucket.deleteMissesPerSecond bucket.deleteMissesPerSecond Couchbase couchbase.bucket.diskCreateOperationsPerSecond bucket.diskCreateOperationsPerSecond Couchbase couchbase.bucket.diskFetchesPerSecond bucket.diskFetchesPerSecond Couchbase couchbase.bucket.diskReadsPerSecond bucket.diskReadsPerSecond Couchbase couchbase.bucket.diskUpdateOperationsPerSecond bucket.diskUpdateOperationsPerSecond Couchbase couchbase.bucket.diskUsedInBytes bucket.diskUsedInBytes Couchbase couchbase.bucket.diskWriteQueue bucket.diskWriteQueue Couchbase couchbase.bucket.drainedItemsInQueue bucket.drainedItemsInQueue Couchbase couchbase.bucket.drainedItemsOnDiskQueue bucket.drainedItemsOnDiskQueue Couchbase couchbase.bucket.drainedPendingItemsInQueue bucket.drainedPendingItemsInQueue Couchbase couchbase.bucket.ejectionsPerSecond bucket.ejectionsPerSecond Couchbase couchbase.bucket.evictionsPerSecond bucket.evictionsPerSecond Couchbase couchbase.bucket.getHitsPerSecond bucket.getHitsPerSecond Couchbase couchbase.bucket.getMissesPerSecond bucket.getMissesPerSecond Couchbase couchbase.bucket.hitRatio bucket.hitRatio Couchbase couchbase.bucket.incrementHitsPerSecond bucket.incrementHitsPerSecond Couchbase couchbase.bucket.incrementMissesPerSecond bucket.incrementMissesPerSecond Couchbase couchbase.bucket.itemCount bucket.itemCount Couchbase couchbase.bucket.itemsBeingWritten bucket.itemsBeingWritten Couchbase couchbase.bucket.itemsEjectedFromMemoryToDisk bucket.itemsEjectedFromMemoryToDisk Couchbase couchbase.bucket.itemsOnDiskQueue bucket.itemsOnDiskQueue Couchbase couchbase.bucket.itemsQueuedForStorage bucket.itemsQueuedForStorage Couchbase couchbase.bucket.maximumMemoryUsage bucket.maximumMemoryUsage Couchbase couchbase.bucket.memoryHighWaterMarkInBytes bucket.memoryHighWaterMarkInBytes Couchbase couchbase.bucket.memoryLowWaterMarkInBytes bucket.memoryLowWaterMarkInBytes Couchbase couchbase.bucket.memoryUsedInBytes bucket.memoryUsedInBytes Couchbase couchbase.bucket.metadataInRamInBytes bucket.metadataInRAMInBytes Couchbase couchbase.bucket.missesPerSecond bucket.missesPerSecond Couchbase couchbase.bucket.outOfMemoryErrorsPerSecond bucket.outOfMemoryErrorsPerSecond Couchbase couchbase.bucket.overheadInBytes bucket.overheadInBytes Couchbase couchbase.bucket.pendingItemsInDiskQueue bucket.pendingItemsInDiskQueue Couchbase couchbase.bucket.pendingResidentItemsRatio bucket.pendingResidentItemsRatio Couchbase couchbase.bucket.quotaUtilization bucket.quotaUtilization Couchbase couchbase.bucket.readOperationsPerSecond bucket.readOperationsPerSecond Couchbase couchbase.bucket.readRatePerSecond bucket.readRatePerSecond Couchbase couchbase.bucket.recoverableOutOfMemoryCount bucket.recoverableOutOfMemoryCount Couchbase couchbase.bucket.replicaIndex bucket.replicaIndex Couchbase couchbase.bucket.replicaNumber bucket.replicaNumber Couchbase couchbase.bucket.replicaResidentItemsRatio bucket.replicaResidentItemsRatio Couchbase couchbase.bucket.residentItemsRatio bucket.residentItemsRatio Couchbase couchbase.bucket.temporaryOutOfMemoryErrorsPerSecond bucket.temporaryOutOfMemoryErrorsPerSecond Couchbase couchbase.bucket.threadsNumber bucket.threadsNumber Couchbase couchbase.bucket.totalItems bucket.totalItems Couchbase couchbase.bucket.totalOperationsPerSecond bucket.totalOperationsPerSecond Couchbase couchbase.bucket.viewFragmentationPercent bucket.viewFragmentationPercent Couchbase couchbase.bucket.writeOperationsPerSecond bucket.writeOperationsPerSecond Couchbase couchbase.bucket.writeRatePerSecond bucket.writeRatePerSecond Couchbase couchbase.cluster.autoFailoverCount cluster.autoFailoverCount Couchbase couchbase.cluster.autoFailoverEnabled cluster.autoFailoverEnabled Couchbase couchbase.cluster.databaseFragmentationThreshold cluster.databaseFragmentationThreshold Couchbase couchbase.cluster.diskFreeInBytes cluster.diskFreeInBytes Couchbase couchbase.cluster.diskQuotaTotalInBytes cluster.diskQuotaTotalInBytes Couchbase couchbase.cluster.diskTotalInBytes cluster.diskTotalInBytes Couchbase couchbase.cluster.diskUsedByDataInBytes cluster.diskUsedByDataInBytes Couchbase couchbase.cluster.diskUsedInBytes cluster.diskUsedInBytes Couchbase couchbase.cluster.indexFragmentationThreshold cluster.indexFragmentationThreshold Couchbase couchbase.cluster.maximumBucketCount cluster.maximumBucketCount Couchbase couchbase.cluster.memoryQuotaTotalInBytes cluster.memoryQuotaTotalInBytes Couchbase couchbase.cluster.memoryQuotaTotalPerNodeInBytes cluster.memoryQuotaTotalPerNodeInBytes Couchbase couchbase.cluster.memoryQuotaUsedInBytes cluster.memoryQuotaUsedInBytes Couchbase couchbase.cluster.memoryQuotaUsedPerNodeInBytes cluster.memoryQuotaUsedPerNodeInBytes Couchbase couchbase.cluster.memoryTotalInBytes cluster.memoryTotalInBytes Couchbase couchbase.cluster.memoryUsedByDataInBytes cluster.memoryUsedByDataInBytes Couchbase couchbase.cluster.memoryUsedInBytes cluster.memoryUsedInBytes Couchbase couchbase.cluster.viewFragmentationThreshold cluster.viewFragmentationThreshold Couchbase couchbase.node.backgroundFetches node.backgroundFetches Couchbase couchbase.node.cmdGet node.cmdGet Couchbase couchbase.node.couchDocsActualDiskSizeInBytes node.couchDocsActualDiskSizeInBytes Couchbase couchbase.node.couchDocsDataSizeInBytes node.couchDocsDataSizeInBytes Couchbase couchbase.node.couchSpatialDataSizeInBytes node.couchSpatialDataSizeInBytes Couchbase couchbase.node.couchSpatialDiskSizeInBytes node.couchSpatialDiskSizeInBytes Couchbase couchbase.node.couchViewsActualDiskSizeInBytes node.couchViewsActualDiskSizeInBytes Couchbase couchbase.node.couchViewsDataSizeInBytes node.couchViewsDataSizeInBytes Couchbase couchbase.node.cpuUtilization node.cpuUtilization Couchbase couchbase.node.currentItems node.currentItems Couchbase couchbase.node.currentItemsTotal node.currentItemsTotal Couchbase couchbase.node.getHits node.getHits Couchbase couchbase.node.memoryFreeInBytes node.memoryFreeInBytes Couchbase couchbase.node.memoryTotalInBytes node.memoryTotalInBytes Couchbase couchbase.node.memoryUsedInBytes node.memoryUsedInBytes Couchbase couchbase.node.ops node.ops Couchbase couchbase.node.swapTotalInBytes node.swapTotalInBytes Couchbase couchbase.node.swapUsedInBytes node.swapUsedInBytes Couchbase couchbase.node.uptimeInMilliseconds node.uptimeInMilliseconds Couchbase couchbase.node.vbucketActiveNonResidentItems node.vbucketActiveNonResidentItems Couchbase couchbase.node.vbucketInMemoryItems node.vbucketInMemoryItems Couchbase couchbase.queryengine.activeRequests queryengine.activeRequests Couchbase couchbase.queryengine.averageRequestTimeInMilliseconds queryengine.averageRequestTimeInMilliseconds Couchbase couchbase.queryengine.completedLimit queryengine.completedLimit Couchbase couchbase.queryengine.completedRequests queryengine.completedRequests Couchbase couchbase.queryengine.completedThresholdInMilliseconds queryengine.completedThresholdInMilliseconds Couchbase couchbase.queryengine.cores queryengine.cores Couchbase couchbase.queryengine.garbageCollectionNumber queryengine.garbageCollectionNumber Couchbase couchbase.queryengine.garbageCollectionPaused queryengine.garbageCollectionPaused Couchbase couchbase.queryengine.garbageCollectionTimePausedInMilliseconds queryengine.garbageCollectionTimePausedInMilliseconds Couchbase couchbase.queryengine.medianRequestTimeInMilliseconds queryengine.medianRequestTimeInMilliseconds Couchbase couchbase.queryengine.preparedStatementUtilization queryengine.preparedStatementUtilization Couchbase couchbase.queryengine.requestsLast15MinutesPerSecond queryengine.requestsLast15MinutesPerSecond Couchbase couchbase.queryengine.requestsLast1MinutesPerSecond queryengine.requestsLast1MinutesPerSecond Couchbase couchbase.queryengine.requestsLast5MinutesPerSecond queryengine.requestsLast5MinutesPerSecond Couchbase couchbase.queryengine.requestTime80thPercentileInMilliseconds queryengine.requestTime80thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime95thPercentileInMilliseconds queryengine.requestTime95thPercentileInMilliseconds Couchbase couchbase.queryengine.requestTime99thPercentileInMilliseconds queryengine.requestTime99thPercentileInMilliseconds Couchbase couchbase.queryengine.systemCpuUtilization queryengine.systemCPUUtilization Couchbase couchbase.queryengine.systemMemoryInBytes queryengine.systemMemoryInBytes Couchbase couchbase.queryengine.totalMemoryInBytes queryengine.totalMemoryInBytes Couchbase couchbase.queryengine.totalThreads queryengine.totalThreads Couchbase couchbase.queryengine.uptimeInMilliseconds queryengine.uptimeInMilliseconds Couchbase couchbase.queryengine.usedMemoryInBytes queryengine.usedMemoryInBytes Couchbase couchbase.queryengine.userCpuUtilization queryengine.userCPUUtilization Docker docker.container.cpuKernelPercent cpuKernelPercent Docker docker.container.cpuLimitCores cpuLimitCores Docker docker.container.cpuPercent cpuPercent Docker docker.container.cpuThrottlePeriods cpuThrottlePeriods Docker docker.container.cpuThrottleTimeMs cpuThrottleTimeMs Docker docker.container.cpuUsedCores cpuUsedCores Docker docker.container.cpuUsedCoresPercent cpuUsedCoresPercent Docker docker.container.cpuUserPercent cpuUserPercent Docker docker.container.ioReadBytesPerSecond ioReadBytesPerSecond Docker docker.container.ioReadCountPerSecond ioReadCountPerSecond Docker docker.container.ioTotalBytes ioTotalBytes Docker docker.container.ioTotalReadBytes ioTotalReadBytes Docker docker.container.ioTotalReadCount ioTotalReadCount Docker docker.container.ioTotalWriteBytes ioTotalWriteBytes Docker docker.container.ioTotalWriteCount ioTotalWriteCount Docker docker.container.ioWriteBytesPerSecond ioWriteBytesPerSecond Docker docker.container.ioWriteCountPerSecond ioWriteCountPerSecond Docker docker.container.memoryCacheBytes memoryCacheBytes Docker docker.container.memoryResidentSizeBytes memoryResidentSizeBytes Docker docker.container.memorySizeLimitBytes memorySizeLimitBytes Docker docker.container.memoryUsageBytes memoryUsageBytes Docker docker.container.memoryUsageLimitPercent memoryUsageLimitPercent Docker docker.container.networkRxBytes networkRxBytes Docker docker.container.networkRxBytesPerSecond networkRxBytesPerSecond Docker docker.container.networkRxDropped networkRxDropped Docker docker.container.networkRxDroppedPerSecond networkRxDroppedPerSecond Docker docker.container.networkRxErrors networkRxErrors Docker docker.container.networkRxErrorsPerSecond networkRxErrorsPerSecond Docker docker.container.networkRxPackets networkRxPackets Docker docker.container.networkRxPacketsPerSecond networkRxPacketsPerSecond Docker docker.container.networkTxBytes networkTxBytes Docker docker.container.networkTxBytesPerSecond networkTxBytesPerSecond Docker docker.container.networkTxDropped networkTxDropped Docker docker.container.networkTxDroppedPerSecond networkTxDroppedPerSecond Docker docker.container.networkTxErrors networkTxErrors Docker docker.container.networkTxErrorsPerSecond networkTxErrorsPerSecond Docker docker.container.networkTxPackets networkTxPackets Docker docker.container.networkTxPacketsPerSecond networkTxPacketsPerSecond Docker docker.container.pids pids Docker docker.container.processCount processCount Docker docker.container.processCountLimit processCountLimit Docker docker.container.restartCount restartCount Docker docker.container.threadCount threadCount Docker docker.container.threadCountLimit threadCountLimit ElasticSearch elasticsearch.cluster.dataNodes cluster.dataNodes ElasticSearch elasticsearch.cluster.nodes cluster.nodes ElasticSearch elasticsearch.cluster.shards.active shards.active ElasticSearch elasticsearch.cluster.shards.initializing shards.initializing ElasticSearch elasticsearch.cluster.shards.primaryActive shards.primaryActive ElasticSearch elasticsearch.cluster.shards.relocating shards.relocating ElasticSearch elasticsearch.cluster.shards.unassigned shards.unassigned ElasticSearch elasticsearch.cluster.tempData temp-data ElasticSearch elasticsearch.index.docs index.docs ElasticSearch elasticsearch.index.docsDeleted index.docsDeleted ElasticSearch elasticsearch.index.primaryShards index.primaryShards ElasticSearch elasticsearch.index.primaryStoreSizeInBytes index.primaryStoreSizeInBytes ElasticSearch elasticsearch.index.replicaShards index.replicaShards ElasticSearch elasticsearch.index.rollup.docsCount primaries.docsnumber ElasticSearch elasticsearch.index.rollup.docsDeleted primaries.docsDeleted ElasticSearch elasticsearch.index.rollup.flushTotal primaries.flushesTotal ElasticSearch elasticsearch.index.rollup.flushTotalTimeInMilliseconds primaries.flushTotalTimeInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsExist primaries.get.documentsExist ElasticSearch elasticsearch.index.rollup.get.documentsExistInMilliseconds primaries.get.documentsExistInMilliseconds ElasticSearch elasticsearch.index.rollup.get.documentsMissing primaries.get.documentsMissing ElasticSearch elasticsearch.index.rollup.get.documentsMissingInMilliseconds primaries.get.documentsMissingInMilliseconds ElasticSearch elasticsearch.index.rollup.get.requests primaries.get.requests ElasticSearch elasticsearch.index.rollup.get.requestsCurrent primaries.get.requestsCurrent ElasticSearch elasticsearch.index.rollup.get.requestsInMilliseconds primaries.get.requestsInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeleted primaries.index.docsCurrentlyDeleted ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyDeletedInMilliseconds primaries.index.docsCurrentlyDeletedInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexing primaries.index.docsCurrentlyIndexing ElasticSearch elasticsearch.index.rollup.index.docsCurrentlyIndexingInMilliseconds primaries.index.docsCurrentlyIndexingInMilliseconds ElasticSearch elasticsearch.index.rollup.index.docsDeleted primaries.index.docsDeleted ElasticSearch elasticsearch.index.rollup.index.docsTotal primaries.index.docsTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotal primaries.indexRefreshesTotal ElasticSearch elasticsearch.index.rollup.indexRefreshesTotalInMilliseconds primaries.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.merges.current primaries.merges.current ElasticSearch elasticsearch.index.rollup.merges.docsSegmentsCurrentlyMerged primaries.merges.docsSegmentsCurrentlyMerged ElasticSearch elasticsearch.index.rollup.merges.docsTotal primaries.merges.docsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsCurrentlyMergedInBytes primaries.merges.segmentsCurrentlyMergedInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotal primaries.merges.segmentsTotal ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInBytes primaries.merges.segmentsTotalInBytes ElasticSearch elasticsearch.index.rollup.merges.segmentsTotalInMilliseconds primaries.merges.segmentsTotalInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesInMilliseconds primaries.queriesInMilliseconds ElasticSearch elasticsearch.index.rollup.queriesTotal primaries.queriesTotal ElasticSearch elasticsearch.index.rollup.queryActive primaries.queryActive ElasticSearch elasticsearch.index.rollup.queryFetches primaries.queryFetches ElasticSearch elasticsearch.index.rollup.queryFetchesInMilliseconds primaries.queryFetchesInMilliseconds ElasticSearch elasticsearch.index.rollup.queryFetchesTotal primaries.queryFetchesTotal ElasticSearch elasticsearch.index.rollup.sizeInBytes primaries.sizeInBytes ElasticSearch elasticsearch.index.storeSizeInBytes index.storeSizeInBytes ElasticSearch elasticsearch.node.activeSearches activeSearches ElasticSearch elasticsearch.node.activeSearchesInMilliseconds activeSearchesInMilliseconds ElasticSearch elasticsearch.node.breakers.estimatedSizeFieldDataCircuitBreakerInBytes breakers.estimatedSizeFieldDataCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeParentCircuitBreakerInBytes breakers.estimatedSizeParentCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.estimatedSizeRequestCircuitBreakerInBytes breakers.estimatedSizeRequestCircuitBreakerInBytes ElasticSearch elasticsearch.node.breakers.fieldDataCircuitBreakerTripped breakers.fieldDataCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.parentCircuitBreakerTripped breakers.parentCircuitBreakerTripped ElasticSearch elasticsearch.node.breakers.requestCircuitBreakerTripped breakers.requestCircuitBreakerTripped ElasticSearch elasticsearch.node.flush.indexRefreshesTotal flush.indexRefreshesTotal ElasticSearch elasticsearch.node.flush.indexRefreshesTotalInMilliseconds flush.indexRefreshesTotalInMilliseconds ElasticSearch elasticsearch.node.fs.bytesAvailableJvmInBytes fs.bytesAvailableJVMInBytes ElasticSearch elasticsearch.node.fs.dataRead fs.bytesReadsInBytes ElasticSearch elasticsearch.node.fs.dataWritten fs.writesInBytes ElasticSearch elasticsearch.node.fs.ioOperations fs.iOOperations ElasticSearch elasticsearch.node.fs.readOperations fs.reads ElasticSearch elasticsearch.node.fs.totalSizeInBytes fs.totalSizeInBytes ElasticSearch elasticsearch.node.fs.unallocatedBytes fs.unallocatedBytesInBYtes ElasticSearch elasticsearch.node.fs.writeOperations fs.writeOperations ElasticSearch elasticsearch.node.get.currentRequestsRunning get.currentRequestsRunning ElasticSearch elasticsearch.node.get.requestsDocumentExists get.requestsDocumentExists ElasticSearch elasticsearch.node.get.requestsDocumentExistsInMilliseconds get.requestsDocumentExistsInMilliseconds ElasticSearch elasticsearch.node.get.requestsDocumentMissing get.requestsDocumentMissing ElasticSearch elasticsearch.node.get.requestsDocumentMissingInMilliseconds get.requestsDocumentMissingInMilliseconds ElasticSearch elasticsearch.node.get.timeGetRequestsInMilliseconds get.timeGetRequestsInMilliseconds ElasticSearch elasticsearch.node.get.totalGetRequests get.totalGetRequests ElasticSearch elasticsearch.node.http.currentOpenConnections http.currentOpenConnections ElasticSearch elasticsearch.node.http.openedConnections http.openedConnections ElasticSearch elasticsearch.node.index.indexingOperationsFailed indices.indexingOperationsFailed ElasticSearch elasticsearch.node.index.indexingWaitedThrottlingInMilliseconds indices.indexingWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.memoryQueryCacheInBytes indices.memoryQueryCacheInBytes ElasticSearch elasticsearch.node.index.numberIndices indices.numberIndices ElasticSearch elasticsearch.node.index.queryCacheEvictions indices.queryCacheEvictions ElasticSearch elasticsearch.node.index.queryCacheHits indices.queryCacheHits ElasticSearch elasticsearch.node.index.queryCacheMisses indices.queryCacheMisses ElasticSearch elasticsearch.node.index.recoveryOngoingShardSource indices.recoveryOngoingShardSource ElasticSearch elasticsearch.node.index.recoveryOngoingShardTarget indices.recoveryOngoingShardTarget ElasticSearch elasticsearch.node.index.recoveryWaitedThrottlingInMilliseconds indices.recoveryWaitedThrottlingInMilliseconds ElasticSearch elasticsearch.node.index.requestCacheEvictions indices.requestCacheEvictions ElasticSearch elasticsearch.node.index.requestCacheHits indices.requestCacheHits ElasticSearch elasticsearch.node.index.requestCacheMemoryInBytes indices.requestCacheMemoryInBytes ElasticSearch elasticsearch.node.index.requestCacheMisses indices.requestCacheMisses ElasticSearch elasticsearch.node.index.segmentsIndexShard indices.segmentsIndexShard ElasticSearch elasticsearch.node.index.segmentsMemoryUsedDocValuesInBytes indices.segmentsMemoryUsedDocValuesInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedFixedBitSetInBytes indices.segmentsMemoryUsedFixedBitSetInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexSegmentsInBytes indices.segmentsMemoryUsedIndexSegmentsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedIndexWriterInBytes indices.segmentsMemoryUsedIndexWriterInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedNormsInBytes indices.segmentsMemoryUsedNormsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedSegmentVersionMapInBytes indices.segmentsMemoryUsedSegmentVersionMapInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedStoredFieldsInBytes indices.segmentsMemoryUsedStoredFieldsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermsInBytes indices.segmentsMemoryUsedTermsInBytes ElasticSearch elasticsearch.node.index.segmentsMemoryUsedTermVectorsInBytes indices.segmentsMemoryUsedTermVectorsInBytes ElasticSearch elasticsearch.node.index.translogOperations indices.translogOperations ElasticSearch elasticsearch.node.index.translogOperationsInBytes indices.translogOperationsInBytes ElasticSearch elasticsearch.node.indexing.docsCurrentlyDeleted indexing.docsCurrentlyDeleted ElasticSearch elasticsearch.node.indexing.documentsCurrentlyIndexing indexing.documentsCurrentlyIndexing ElasticSearch elasticsearch.node.indexing.documentsIndexed indexing.documentsIndexed ElasticSearch elasticsearch.node.indexing.timeDeletingDocumentsInMilliseconds indexing.timeDeletingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.timeIndexingDocumentsInMilliseconds indexing.timeIndexingDocumentsInMilliseconds ElasticSearch elasticsearch.node.indexing.totalDocumentsDeleted indexing.totalDocumentsDeleted ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjects jvm.gc.majorCollectionsOldGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds jvm.gc.majorCollectionsOldGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjects jvm.gc.majorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.majorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjects jvm.gc.minorCollectionsYoungGenerationObjects ElasticSearch elasticsearch.node.jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds jvm.gc.minorCollectionsYoungGenerationObjectsInMilliseconds ElasticSearch elasticsearch.node.jvm.mem.heapCommittedInBytes jvm.mem.heapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.heapMaxInBytes jvm.mem.heapMaxInBytes ElasticSearch elasticsearch.node.jvm.mem.heapUsed jvm.mem.heapUsed ElasticSearch elasticsearch.node.jvm.mem.heapUsedInBytes jvm.mem.heapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.maxOldGenerationHeapInBytes jvm.mem.maxOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.maxSurvivorSpaceInBytes jvm.mem.maxSurvivorSpaceInBYtes ElasticSearch elasticsearch.node.jvm.mem.maxYoungGenerationHeapInBytes jvm.mem.maxYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapCommittedInBytes jvm.mem.nonHeapCommittedInBytes ElasticSearch elasticsearch.node.jvm.mem.nonHeapUsedInBytes jvm.mem.nonHeapUsedInBytes ElasticSearch elasticsearch.node.jvm.mem.usedOldGenerationHeapInBytes jvm.mem.usedOldGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.mem.usedSurvivorSpaceInBytes jvm.mem.usedSurvivorSpaceInBytes ElasticSearch elasticsearch.node.jvm.mem.usedYoungGenerationHeapInBytes jvm.mem.usedYoungGenerationHeapInBytes ElasticSearch elasticsearch.node.jvm.threadsActive jvm.ThreadsActive ElasticSearch elasticsearch.node.jvm.threadsPeak jvm.ThreadsPeak ElasticSearch elasticsearch.node.merges.currentActive merges.currentActive ElasticSearch elasticsearch.node.merges.docsSegmentMerges merges.docsSegmentMerges ElasticSearch elasticsearch.node.merges.docsSegmentsMerging merges.docsSegmentsMerging ElasticSearch elasticsearch.node.merges.mergedSegmentsInBytes merges.mergedSegmentsInBytes ElasticSearch elasticsearch.node.merges.segmentMerges merges.segmentMerges ElasticSearch elasticsearch.node.merges.sizeSegmentsMergingInBytes merges.sizeSegmentsMergingInBytes ElasticSearch elasticsearch.node.merges.totalSegmentMergingInMilliseconds merges.totalSegmentMergingInMilliseconds ElasticSearch elasticsearch.node.openFd openFD ElasticSearch elasticsearch.node.queriesTotal queriesTotal ElasticSearch elasticsearch.node.refresh.total refresh.total ElasticSearch elasticsearch.node.refresh.totalInMilliseconds refresh.totalInMilliseconds ElasticSearch elasticsearch.node.searchFetchCurrentlyRunning searchFetchCurrentlyRunning ElasticSearch elasticsearch.node.searchFetches searchFetches ElasticSearch elasticsearch.node.sizeStoreInBytes sizeStoreInBytes ElasticSearch elasticsearch.node.threadpool.activeFetchShardStarted threadpool.activeFetchShardStarted ElasticSearch elasticsearch.node.threadpool.bulkActive threadpool.bulkActive ElasticSearch elasticsearch.node.threadpool.bulkQueue threadpool.bulkQueue ElasticSearch elasticsearch.node.threadpool.bulkRejected threadpool.bulkRejected ElasticSearch elasticsearch.node.threadpool.bulkThreads threadpool.bulkThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStartedQueue threadpool.fetchShardStartedQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStartedRejected threadpool.fetchShardStartedRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStartedThreads threadpool.fetchShardStartedThreads ElasticSearch elasticsearch.node.threadpool.fetchShardStoreActive threadpool.fetchShardStoreActive ElasticSearch elasticsearch.node.threadpool.fetchShardStoreQueue threadpool.fetchShardStoreQueue ElasticSearch elasticsearch.node.threadpool.fetchShardStoreRejected threadpool.fetchShardStoreRejected ElasticSearch elasticsearch.node.threadpool.fetchShardStoreThreads threadpool.fetchShardStoreThreads ElasticSearch elasticsearch.node.threadpool.flushActive threadpool.flushActive ElasticSearch elasticsearch.node.threadpool.flushQueue threadpool.flushQueue ElasticSearch elasticsearch.node.threadpool.flushRejected threadpool.flushRejected ElasticSearch elasticsearch.node.threadpool.flushThreads threadpool.flushThreads ElasticSearch elasticsearch.node.threadpool.forceMergeActive threadpool.forceMergeActive ElasticSearch elasticsearch.node.threadpool.forceMergeQueue threadpool.forceMergeQueue ElasticSearch elasticsearch.node.threadpool.forceMergeRejected threadpool.forceMergeRejected ElasticSearch elasticsearch.node.threadpool.forceMergeThreads threadpool.forceMergeThreads ElasticSearch elasticsearch.node.threadpool.genericActive threadpool.genericActive ElasticSearch elasticsearch.node.threadpool.genericQueue threadpool.genericQueue ElasticSearch elasticsearch.node.threadpool.genericRejected threadpool.genericRejected ElasticSearch elasticsearch.node.threadpool.genericThreads threadpool.genericThreads ElasticSearch elasticsearch.node.threadpool.getActive threadpool.getActive ElasticSearch elasticsearch.node.threadpool.getQueue threadpool.getQueue ElasticSearch elasticsearch.node.threadpool.getRejected threadpool.getRejected ElasticSearch elasticsearch.node.threadpool.getThreads threadpool.getThreads ElasticSearch elasticsearch.node.threadpool.indexActive threadpool.indexActive ElasticSearch elasticsearch.node.threadpool.indexQueue threadpool.indexQueue ElasticSearch elasticsearch.node.threadpool.indexRejected threadpool.indexRejected ElasticSearch elasticsearch.node.threadpool.indexThreads threadpool.indexThreads ElasticSearch elasticsearch.node.threadpool.listenerActive threadpool.listenerActive ElasticSearch elasticsearch.node.threadpool.listenerQueue threadpool.listenerQueue ElasticSearch elasticsearch.node.threadpool.listenerRejected threadpool.listenerRejected ElasticSearch elasticsearch.node.threadpool.listenerThreads threadpool.listenerThreads ElasticSearch elasticsearch.node.threadpool.managementActive threadpool.managementActive ElasticSearch elasticsearch.node.threadpool.managementQueue threadpool.managementQueue ElasticSearch elasticsearch.node.threadpool.managementRejected threadpool.managementRejected ElasticSearch elasticsearch.node.threadpool.managementThreads threadpool.managementThreads ElasticSearch elasticsearch.node.threadpool.refreshActive threadpool.refreshActive ElasticSearch elasticsearch.node.threadpool.refreshQueue threadpool.refreshQueue ElasticSearch elasticsearch.node.threadpool.refreshRejected threadpool.refreshRejected ElasticSearch elasticsearch.node.threadpool.refreshThreads threadpool.refreshThreads ElasticSearch elasticsearch.node.threadpool.searchActive threadpool.searchActive ElasticSearch elasticsearch.node.threadpool.searchQueue threadpool.searchQueue ElasticSearch elasticsearch.node.threadpool.searchRejected threadpool.searchRejected ElasticSearch elasticsearch.node.threadpool.searchThreads threadpool.searchThreads ElasticSearch elasticsearch.node.threadpool.snapshotActive threadpool.snapshotActive ElasticSearch elasticsearch.node.threadpool.snapshotQueue threadpool.snapshotQueue ElasticSearch elasticsearch.node.threadpool.snapshotRejected threadpool.snapshotRejected ElasticSearch elasticsearch.node.threadpool.snapshotThreads threadpool.snapshotThreads ElasticSearch elasticsearch.node.transport.connectionsOpened transport.connectionsOpened ElasticSearch elasticsearch.node.transport.packetsReceived transport.packetsReceived ElasticSearch elasticsearch.node.transport.packetsReceivedInBytes transport.packetsReceivedInBytes ElasticSearch elasticsearch.node.transport.packetsSent transport.packetsSent ElasticSearch elasticsearch.node.transport.packetsSentInBytes transport.packetsSentInBytes F5 f5.node.availabilityState node.availabilityState F5 f5.node.connections node.connections F5 f5.node.connectionsPerSecond node.connectionsPerSecond F5 f5.node.enabled node.enabled F5 f5.node.inDataInBytesPerSecond node.inDataInBytesPerSecond F5 f5.node.monitorStatus node.monitorStatus F5 f5.node.outDataInBytesPerSecond node.outDataInBytesPerSecond F5 f5.node.packetsReceivedPerSecond node.packetsReceivedPerSecond F5 f5.node.packetsSentPerSecond node.packetsSentPerSecond F5 f5.node.requestsPerSecond node.requestsPerSecond F5 f5.node.sessions node.sessions F5 f5.node.sessionStatus node.sessionStatus F5 f5.poolMember.availabilityState member.availabilityState F5 f5.poolMember.connections member.connections F5 f5.poolMember.enabled member.enabled F5 f5.poolMember.inDataInBytesPerSecond member.inDataInBytesPerSecond F5 f5.poolMember.monitorStatus member.monitorStatus F5 f5.poolMember.outDataInBytesPerSecond member.outDataInBytesPerSecond F5 f5.poolMember.packetsReceivedPerSecond member.packetsReceivedPerSecond F5 f5.poolMember.packetsSentPerSecond member.packetsSentPerSecond F5 f5.poolMember.requestsPerSecond member.requestsPerSecond F5 f5.poolMember.sessions member.sessions F5 f5.poolMember.sessionStatus member.sessionStatus F5 f5.pool.activeMembers pool.activeMembers F5 f5.pool.availabilityState pool.availabilityState F5 f5.pool.connections pool.connections F5 f5.pool.connqAgeEdm pool.connqAgeEdm F5 f5.pool.connqAgeEma pool.connqAgeEma F5 f5.pool.connqAgeHead pool.connqAgeHead F5 f5.pool.connqAgeMax pool.connqAgeMax F5 f5.pool.connqAllAgeEdm pool.connqAllAgeEdm F5 f5.pool.connqAllAgeEma pool.connqAllAgeEma F5 f5.pool.connqAllAgeHead pool.connqAllAgeHead F5 f5.pool.connqAllAgeMax pool.connqAllAgeMax F5 f5.pool.connqAllDepth pool.connqAllDepth F5 f5.pool.connqDepth pool.connqDepth F5 f5.pool.currentConnections pool.currentConnections F5 f5.pool.enabled pool.enabled F5 f5.pool.inDataInBytesPerSecond pool.inDataInBytesPerSecond F5 f5.pool.minActiveMembers pool.minActiveMembers F5 f5.pool.outDataInBytesPerSecond pool.outDataInBytesPerSecond F5 f5.pool.packetsReceivedPerSecond pool.packetsReceivedPerSecond F5 f5.pool.packetsSentPerSecond pool.packetsSentPerSecond F5 f5.pool.requestsPerSecond pool.requestsPerSecond F5 f5.pool.sessions pool.sessions F5 f5.system.cpuIdleTicksPerSecond system.cpuIdleTicksPerSecond F5 f5.system.cpuIdleUtilization system.cpuIdleUtilization F5 f5.system.cpuInterruptRequestUtilization system.cpuInterruptRequestUtilization F5 f5.system.cpuIoWaitUtilization system.cpuIOWaitUtilization F5 f5.system.cpuNiceLevelUtilization system.cpuNiceLevelUtilization F5 f5.system.cpuSoftInterruptRequestUtilization system.cpuSoftInterruptRequestUtilization F5 f5.system.cpuStolenUtilization system.cpuStolenUtilization F5 f5.system.cpuSystemTicksPerSecond system.cpuSystemTicksPerSecond F5 f5.system.cpuSystemUtilization system.cpuSystemUtilization F5 f5.system.cpuUserTicksPerSecond system.cpuUserTicksPerSecond F5 f5.system.cpuUserUtilization system.cpuUserUtilization F5 f5.system.memoryFreeInBytes system.memoryFreeInBytes F5 f5.system.memoryTotalInBytes system.memoryTotalInBytes F5 f5.system.memoryUsedInBytes system.memoryUsedInBytes F5 f5.system.otherMemoryFreeInBytes system.otherMemoryFreeInBytes F5 f5.system.otherMemoryTotalInBytes system.otherMemoryTotalInBytes F5 f5.system.otherMemoryUsedInBytes system.otherMemoryUsedInBytes F5 f5.system.swapFreeInBytes system.swapFreeInBytes F5 f5.system.swapTotalInBytes system.swapTotalInBytes F5 f5.system.swapUsedInBytes system.swapUsedInBytes F5 f5.system.tmmMemoryFreeInBytes system.tmmMemoryFreeInBytes F5 f5.system.tmmMemoryTotalInBytes system.tmmMemoryTotalInBytes F5 f5.system.tmmMemoryUsedInBytes system.tmmMemoryUsedInBytes F5 f5.virtualserver.availabilityState virtualserver.availabilityState F5 f5.virtualserver.clientsideConnectionsPerSecond virtualserver.clientsideConnectionsPerSecond F5 f5.virtualserver.connections virtualserver.connections F5 f5.virtualserver.csMaxConnDur virtualserver.csMaxConnDur F5 f5.virtualserver.csMeanConnDur virtualserver.csMeanConnDur F5 f5.virtualserver.csMinConnDur virtualserver.csMinConnDur F5 f5.virtualserver.enabled virtualserver.enabled F5 f5.virtualserver.ephemeralBytesInPerSecond virtualserver.ephemeralBytesInPerSecond F5 f5.virtualserver.ephemeralBytesOutPerSecond virtualserver.ephemeralBytesOutPerSecond F5 f5.virtualserver.ephemeralConnectionsPerSecond virtualserver.ephemeralConnectionsPerSecond F5 f5.virtualserver.ephemeralCurrentConnections virtualserver.ephemeralCurrentConnections F5 f5.virtualserver.ephemeralEvictedConnectionsPerSecond virtualserver.ephemeralEvictedConnectionsPerSecond F5 f5.virtualserver.ephemeralMaxConnections virtualserver.ephemeralMaxConnections F5 f5.virtualserver.ephemeralPacketsReceivedPerSecond virtualserver.ephemeralPacketsReceivedPerSecond F5 f5.virtualserver.ephemeralPacketsSentPerSecond virtualserver.ephemeralPacketsSentPerSecond F5 f5.virtualserver.ephemeralSlowKilledPerSecond virtualserver.ephemeralSlowKilledPerSecond F5 f5.virtualserver.evictedConnsPerSecond virtualserver.evictedConnsPerSecond F5 f5.virtualserver.inDataInBytesPerSecond virtualserver.inDataInBytesPerSecond F5 f5.virtualserver.outDataInBytesPerSecond virtualserver.outDataInBytesPerSecond F5 f5.virtualserver.packetsReceivedPerSecond virtualserver.packetsReceivedPerSecond F5 f5.virtualserver.packetsSentPerSecond virtualserver.packetsSentPerSecond F5 f5.virtualserver.requestsPerSecond virtualserver.requestsPerSecond F5 f5.virtualserver.slowKilledPerSecond virtualserver.slowKilledPerSecond F5 f5.virtualserver.usageRatio virtualserver.usageRatio HAProxy haproxy.backend.activeServers backend.activeServers HAProxy haproxy.backend.averageConnectTimeInSeconds backend.averageConnectTimeInSeconds HAProxy haproxy.backend.averageQueueTimeInSeconds backend.averageQueueTimeInSeconds HAProxy haproxy.backend.averageResponseTimeInSeconds backend.averageResponseTimeInSeconds HAProxy haproxy.backend.averageTotalSessionTimeInSeconds backend.averageTotalSessionTimeInSeconds HAProxy haproxy.backend.backupServers backend.backupServers HAProxy haproxy.backend.bytesInPerSecond backend.bytesInPerSecond HAProxy haproxy.backend.bytesOutPerSecond backend.bytesOutPerSecond HAProxy haproxy.backend.bytesThatBypassedCompressorPerSecond backend.bytesThatBypassedCompressorPerSecond HAProxy haproxy.backend.connectingRequestErrorsPerSecond backend.connectingRequestErrorsPerSecond HAProxy haproxy.backend.connectionRetriesPerSecond backend.connectionRetriesPerSecond HAProxy haproxy.backend.currentQueuedRequestsWithoutServer backend.currentQueuedRequestsWithoutServer HAProxy haproxy.backend.currentSessions backend.currentSessions HAProxy haproxy.backend.dataTransfersAbortedByClientPerSecond backend.dataTransfersAbortedByClientPerSecond HAProxy haproxy.backend.dataTransfersAbortedByServerPerSecond backend.dataTransfersAbortedByServerPerSecond HAProxy haproxy.backend.downtimeInSeconds backend.downtimeInSeconds HAProxy haproxy.backend.http100ResponsesPerSecond backend.http100ResponsesPerSecond HAProxy haproxy.backend.http200ResponsesPerSecond backend.http200ResponsesPerSecond HAProxy haproxy.backend.http300ResponsesPerSecond backend.http300ResponsesPerSecond HAProxy haproxy.backend.http400ResponsesPerSecond backend.http400ResponsesPerSecond HAProxy haproxy.backend.http500ResponsesPerSecond backend.http500ResponsesPerSecond HAProxy haproxy.backend.httpOtherResponsesPerSecond backend.httpOtherResponsesPerSecond HAProxy haproxy.backend.httpRequestsPerSecond backend.httpRequestsPerSecond HAProxy haproxy.backend.httpResponseBytesEmittedByCompressorPerSecond backend.httpResponseBytesEmittedByCompressorPerSecond HAProxy haproxy.backend.httpResponseBytesFedToCompressorPerSecond backend.httpResponseBytesFedToCompressorPerSecond HAProxy haproxy.backend.httpResponsesCompressedPerSecond backend.httpResponsesCompressedPerSecond HAProxy haproxy.backend.interceptedRequestsPerSecond backend.interceptedRequestsPerSecond HAProxy haproxy.backend.maxQueuedRequestsWithoutServer backend.maxQueuedRequestsWithoutServer HAProxy haproxy.backend.maxSessions backend.maxSessions HAProxy haproxy.backend.maxSessionsPerSecond backend.maxSessionsPerSecond HAProxy haproxy.backend.requestRedispatchPerSecond backend.requestRedispatchPerSecond HAProxy haproxy.backend.requestsDenied.securityConcernsPerSecond backend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.backend.responseErrorsPerSecond backend.responseErrorsPerSecond HAProxy haproxy.backend.responsesDenied.securityConcernsPerSecond backend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.backend.serverSelectedPerSecond backend.serverSelectedPerSecond HAProxy haproxy.backend.sessionsPerSecond backend.sessionsPerSecond HAProxy haproxy.backend.timeSinceLastSessionAssignedInSeconds backend.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.backend.timeSinceLastUpDownTransitionInSeconds backend.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.backend.totalWeight backend.totalWeight HAProxy haproxy.backend.type backend.type HAProxy haproxy.backend.upToDownTransitionsPerSecond backend.upToDownTransitionsPerSecond HAProxy haproxy.frontend.bytesInPerSecond frontend.bytesInPerSecond HAProxy haproxy.frontend.bytesOutPerSecond frontend.bytesOutPerSecond HAProxy haproxy.frontend.connectionsPerSecond frontend.connectionsPerSecond HAProxy haproxy.frontend.currentSessions frontend.currentSessions HAProxy haproxy.frontend.http100ResponsesPerSecond frontend.http100ResponsesPerSecond HAProxy haproxy.frontend.http200ResponsesPerSecond frontend.http200ResponsesPerSecond HAProxy haproxy.frontend.http300ResponsesPerSecond frontend.http300ResponsesPerSecond HAProxy haproxy.frontend.http400ResponsesPerSecond frontend.http400ResponsesPerSecond HAProxy haproxy.frontend.http500ResponsesPerSecond frontend.http500ResponsesPerSecond HAProxy haproxy.frontend.httpOtherResponsesPerSecond frontend.httpOtherResponsesPerSecond HAProxy haproxy.frontend.httpRequests.maxPerSecond frontend.httpRequests.maxPerSecond HAProxy haproxy.frontend.httpRequestsPerSecond frontend.httpRequestsPerSecond HAProxy haproxy.frontend.interceptedRequestsPerSecond frontend.interceptedRequestsPerSecond HAProxy haproxy.frontend.maxConnectionsPerSecond frontend.maxConnectionsPerSecond HAProxy haproxy.frontend.maxSessions frontend.maxSessions HAProxy haproxy.frontend.maxSessionsPerSecond frontend.maxSessionsPerSecond HAProxy haproxy.frontend.requestErrorsPerSecond frontend.requestErrorsPerSecond HAProxy haproxy.frontend.requestsDenied.securityConcernsPerSecond frontend.requestsDenied.securityConcernsPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestConnectionRulesPerSecond frontend.requestsDenied.tcpRequestConnectionRulesPerSecond HAProxy haproxy.frontend.requestsDenied.tcpRequestSessionRulesPerSecond frontend.requestsDenied.tcpRequestSessionRulesPerSecond HAProxy haproxy.frontend.responsesDenied.securityConcernsPerSecond frontend.responsesDenied.securityConcernsPerSecond HAProxy haproxy.frontend.sessionsPerSecond frontend.sessionsPerSecond HAProxy haproxy.server.averageConnectTimeInSeconds server.averageConnectTimeInSeconds HAProxy haproxy.server.averageQueueTimeInSeconds server.averageQueueTimeInSeconds HAProxy haproxy.server.averageResponseTimeInSeconds server.averageResponseTimeInSeconds HAProxy haproxy.server.averageTotalSessionTimeInSeconds server.averageTotalSessionTimeInSeconds HAProxy haproxy.server.bytesInPerSecond server.bytesInPerSecond HAProxy haproxy.server.bytesOutPerSecond server.bytesOutPerSecond HAProxy haproxy.server.connectingRequestErrorsPerSecond server.connectingRequestErrorsPerSecond HAProxy haproxy.server.connectionRetriesPerSecond server.connectionRetriesPerSecond HAProxy haproxy.server.currentQueuedRequestsWithoutServer server.currentQueuedRequestsWithoutServer HAProxy haproxy.server.currentSessions server.currentSessions HAProxy haproxy.server.dataTransfersAbortedByClientPerSecond server.dataTransfersAbortedByClientPerSecond HAProxy haproxy.server.dataTransfersAbortedByServerPerSecond server.dataTransfersAbortedByServerPerSecond HAProxy haproxy.server.downtimeInSeconds server.downtimeInSeconds HAProxy haproxy.server.failedChecksPerSecond server.failedChecksPerSecond HAProxy haproxy.server.healthCheckDurationInMilliseconds server.healthCheckDurationInMilliseconds HAProxy haproxy.server.http100ResponsesPerSecond server.http100ResponsesPerSecond HAProxy haproxy.server.http200ResponsesPerSecond server.http200ResponsesPerSecond HAProxy haproxy.server.http300ResponsesPerSecond server.http300ResponsesPerSecond HAProxy haproxy.server.http400ResponsesPerSecond server.http400ResponsesPerSecond HAProxy haproxy.server.http500ResponsesPerSecond server.http500ResponsesPerSecond HAProxy haproxy.server.httpOtherResponsesPerSecond server.httpOtherResponsesPerSecond HAProxy haproxy.server.isActive server.isActive HAProxy haproxy.server.isBackup server.isBackup HAProxy haproxy.server.maxQueuedRequestsWithoutServer server.maxQueuedRequestsWithoutServer HAProxy haproxy.server.maxSessions server.maxSessions HAProxy haproxy.server.maxSessionsPerSecond server.maxSessionsPerSecond HAProxy haproxy.server.requestRedispatchPerSecond server.requestRedispatchPerSecond HAProxy haproxy.server.requestsDenied.securityConcernsPerSecond server.requestsDenied.securityConcernsPerSecond HAProxy haproxy.server.responseErrorsPerSecond server.responseErrorsPerSecond HAProxy haproxy.server.responsesDenied.securityConcernsPerSecond server.responsesDenied.securityConcernsPerSecond HAProxy haproxy.server.serverSelectedPerSecond server.serverSelectedPerSecond HAProxy haproxy.server.serverWeight server.serverWeight HAProxy haproxy.server.sessionsPerSecond server.sessionsPerSecond HAProxy haproxy.server.throttlePercentage server.throttlePercentage HAProxy haproxy.server.timeSinceLastSessionAssignedInSeconds server.timeSinceLastSessionAssignedInSeconds HAProxy haproxy.server.timeSinceLastUpDownTransitionInSeconds server.timeSinceLastUpDownTransitionInSeconds HAProxy haproxy.server.type server.type HAProxy haproxy.server.upToDownTransitionsPerSecond server.upToDownTransitionsPerSecond Kafka kafka.broker.bytesWrittenToTopicPerSecond broker.bytesWrittenToTopicPerSecond Kafka kafka.broker.consumer.requestsExpiredPerSecond consumer.requestsExpiredPerSecond Kafka kafka.broker.follower.requestExpirationPerSecond follower.requestExpirationPerSecond Kafka kafka.broker.ioInPerSecond broker.IOInPerSecond Kafka kafka.broker.ioOutPerSecond broker.IOOutPerSecond Kafka kafka.broker.logFlushPerSecond broker.logFlushPerSecond Kafka kafka.broker.messagesInPerSecond broker.messagesInPerSecond Kafka kafka.broker.net.bytesRejectedPerSecond net.bytesRejectedPerSecond Kafka kafka.broker.replication.isrExpandsPerSecond replication.isrExpandsPerSecond Kafka kafka.broker.replication.isrShrinksPerSecond replication.isrShrinksPerSecond Kafka kafka.broker.replication.leaderElectionPerSecond replication.leaderElectionPerSecond Kafka kafka.broker.replication.uncleanLeaderElectionPerSecond replication.uncleanLeaderElectionPerSecond Kafka kafka.broker.replication.unreplicatedPartitions replication.unreplicatedPartitions Kafka kafka.broker.request.avgTimeFetch request.avgTimeFetch Kafka kafka.broker.request.avgTimeMetadata request.avgTimeMetadata Kafka kafka.broker.request.avgTimeMetadata99Percentile request.avgTimeMetadata99Percentile Kafka kafka.broker.request.avgTimeOffset request.avgTimeOffset Kafka kafka.broker.request.avgTimeOffset99Percentile request.avgTimeOffset99Percentile Kafka kafka.broker.request.avgTimeProduceRequest request.avgTimeProduceRequest Kafka kafka.broker.request.avgTimeUpdateMetadata request.avgTimeUpdateMetadata Kafka kafka.broker.request.avgTimeUpdateMetadata99Percentile request.avgTimeUpdateMetadata99Percentile Kafka kafka.broker.request.clientFetchesFailedPerSecond request.clientFetchesFailedPerSecond Kafka kafka.broker.request.fetchConsumerRequestsPerSecond request.fetchConsumerRequestsPerSecond Kafka kafka.broker.request.fetchFollowerRequestsPerSecond request.fetchFollowerRequestsPerSecond Kafka kafka.broker.request.fetchTime99Percentile request.fetchTime99Percentile Kafka kafka.broker.request.handlerIdle request.handlerIdle Kafka kafka.broker.request.listGroupsRequestsPerSecond request.listGroupsRequestsPerSecond Kafka kafka.broker.request.metadataRequestsPerSecond request.metadataRequestsPerSecond Kafka kafka.broker.request.offsetCommitRequestsPerSecond request.offsetCommitRequestsPerSecond Kafka kafka.broker.request.produceRequestsFailedPerSecond request.produceRequestsFailedPerSecond Kafka kafka.broker.request.produceRequestsPerSecond request.produceRequestsPerSecond Kafka kafka.broker.request.produceTime99Percentile request.produceTime99Percentile Kafka kafka.broker.topic.diskSize topic.diskSize Kafka kafka.topic.bytesInPerSec topic.BytesInPerSec Kafka kafka.topic.bytesOutPerSec topic.BytesOutPerSec Kafka kafka.topic.messagesInPerSec topic.MessagesInPerSec Kafka kafka.topic.partitionsWithNonPreferredLeader topic.partitionsWithNonPreferredLeader Kafka kafka.topic.respondsToMetadataRequests topic.respondsToMetadataRequests Kafka kafka.topic.retentionBytesOrTime topic.retentionBytesOrTime Kafka kafka.topic.underReplicatedPartitions topic.underReplicatedPartitions Kafka kafka.producer.ageMetadataUsedInMilliseconds producer.ageMetadataUsedInMilliseconds Kafka kafka.producer.availableBufferInBytes producer.availableBufferInBytes Kafka kafka.producer.avgBytesSentPerRequestInBytes producer.avgBytesSentPerRequestInBytes Kafka kafka.producer.avgCompressionRateRecordBatches producer.avgCompressionRateRecordBatches Kafka kafka.producer.avgRecordAccumulatorsInMilliseconds producer.avgRecordAccumulatorsInMilliseconds Kafka kafka.producer.avgRecordSizeInBytes producer.avgRecordSizeInBytes Kafka kafka.producer.avgRecordsSentPerSecond producer.avgRecordsSentPerSecond Kafka kafka.producer.avgRecordsSentPerTopicPerSecond producer.avgRecordsSentPerTopicPerSecond Kafka kafka.producer.avgRequestLatency producer.avgRequestLatencyPerSecond Kafka kafka.producer.avgThrottleTime producer.avgThrottleTime Kafka kafka.producer.bufferMemoryAvailableInBytes producer.bufferMemoryAvailableInBytes Kafka kafka.producer.bufferpoolWaitTime producer.bufferpoolWaitTime Kafka kafka.producer.bytesOutPerSecond producer.bytesOutPerSecond Kafka kafka.producer.compressionRateRecordBatches producer.compressionRateRecordBatches Kafka kafka.producer.ioWaitTime producer.ioWaitTime Kafka kafka.producer.maxBytesSentPerRequestInBytes producer.maxBytesSentPerRequestInBytes Kafka kafka.producer.maxRecordSizeInBytes producer.maxRecordSizeInBytes Kafka kafka.producer.maxRequestLatencyInMilliseconds producer.maxRequestLatencyInMilliseconds Kafka kafka.producer.maxThrottleTime producer.maxThrottleTime Kafka kafka.producer.requestPerSecond producer.requestPerSecond Kafka kafka.producer.requestsWaitingResponse producer.requestsWaitingResponse Kafka kafka.producer.responsePerSecond producer.responsePerSecond Kafka kafka.producer.threadsWaiting producer.threadsWaiting Kafka kafka.consumer.avgFetchSizeInBytes consumer.avgFetchSizeInBytes Kafka kafka.consumer.avgRecordConsumedPerTopic consumer.avgRecordConsumedPerTopic Kafka kafka.consumer.avgRecordConsumedPerTopicPerSecond consumer.avgRecordConsumedPerTopicPerSecond Kafka kafka.consumer.bytesInPerSecond consumer.bytesInPerSecond Kafka kafka.consumer.fetchPerSecond consumer.fetchPerSecond Kafka kafka.consumer.hwm consumer.hwm Kafka kafka.consumer.lag consumer.lag Kafka kafka.consumer.maxFetchSizeInBytes consumer.maxFetchSizeInBytes Kafka kafka.consumer.maxLag consumer.maxLag Kafka kafka.consumer.messageConsumptionPerSecond consumer.messageConsumptionPerSecond Kafka kafka.consumer.offset consumer.offset Kafka kafka.consumer.totalLag consumer.totalLag Kafka kafka.consumerGroup.maxLag consumerGroup.maxLag Kafka kafka.consumerGroup.totalLag consumerGroup.totalLag Kubernetes k8s.apiserver.goGoroutines goGoroutines Kubernetes k8s.apiserver.goThreads goThreads Kubernetes k8s.apiserver.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.apiserver.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.controllermanager.goGoroutines goGoroutines Kubernetes k8s.controllermanager.goThreads goThreads Kubernetes k8s.controllermanager.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.controllermanager.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.controllermanager.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.goGoroutines goGoroutines Kubernetes k8s.etcd.goThreads goThreads Kubernetes k8s.etcd.mvccDbTotalSizeInBytes etcdMvccDbTotalSizeInBytes Kubernetes k8s.etcd.networkClientGrpcReceivedBytesRate etcdNetworkClientGrpcReceivedBytesRate Kubernetes k8s.etcd.networkClientGrpcSentBytesRate etcdNetworkClientGrpcSentBytesRate Kubernetes k8s.etcd.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.etcd.process.maxFds processMaxFds Kubernetes k8s.etcd.process.openFds processOpenFds Kubernetes k8s.etcd.process.processFdsUtilization processFdsUtilization Kubernetes k8s.etcd.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.etcd.serverHasLeader etcdServerHasLeader Kubernetes k8s.etcd.serverLeaderChangesSeenDelta etcdServerLeaderChangesSeenDelta Kubernetes k8s.etcd.serverProposalsAppliedDelta etcdServerProposalsAppliedDelta Kubernetes k8s.etcd.serverProposalsAppliedRate etcdServerProposalsAppliedRate Kubernetes k8s.etcd.serverProposalsCommittedDelta etcdServerProposalsCommittedDelta Kubernetes k8s.etcd.serverProposalsCommittedRate etcdServerProposalsCommittedRate Kubernetes k8s.etcd.serverProposalsFailedDelta etcdServerProposalsFailedDelta Kubernetes k8s.etcd.serverProposalsFailedRate etcdServerProposalsFailedRate Kubernetes k8s.etcd.serverProposalsPending etcdServerProposalsPending Kubernetes k8s.scheduler.goGoroutines goGoroutines Kubernetes k8s.scheduler.goThreads goThreads Kubernetes k8s.scheduler.leaderElectionMasterStatus leaderElectionMasterStatus Kubernetes k8s.scheduler.podPreemptionVictims schedulerPodPreemptionVictims Kubernetes k8s.scheduler.preemptionAttemptsDelta schedulerPreemptionAttemptsDelta Kubernetes k8s.scheduler.process.cpuSecondsDelta processCpuSecondsDelta Kubernetes k8s.scheduler.process.residentMemoryBytes processResidentMemoryBytes Kubernetes k8s.container.cpuCfsPeriodsDelta containerCpuCfsPeriodsDelta Kubernetes k8s.container.cpuCfsPeriodsTotal containerCpuCfsPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledPeriodsDelta containerCpuCfsThrottledPeriodsDelta Kubernetes k8s.container.cpuCfsThrottledPeriodsTotal containerCpuCfsThrottledPeriodsTotal Kubernetes k8s.container.cpuCfsThrottledSecondsDelta containerCpuCfsThrottledSecondsDelta Kubernetes k8s.container.cpuCfsThrottledSecondsTotal containerCpuCfsThrottledSecondsTotal Kubernetes k8s.container.cpuCoresUtilization cpuCoresUtilization Kubernetes k8s.container.cpuLimitCores cpuLimitCores Kubernetes k8s.container.cpuRequestedCores cpuRequestedCores Kubernetes k8s.container.cpuUsedCores cpuUsedCores Kubernetes k8s.container.fsAvailableBytes fsAvailableBytes Kubernetes k8s.container.fsCapacityBytes fsCapacityBytes Kubernetes k8s.container.fsInodes fsInodes Kubernetes k8s.container.fsInodesFree fsInodesFree Kubernetes k8s.container.fsInodesUsed fsInodesUsed Kubernetes k8s.container.fsUsedBytes fsUsedBytes Kubernetes k8s.container.fsUsedPercent fsUsedPercent Kubernetes k8s.container.isReady isReady Kubernetes k8s.container.memoryLimitBytes memoryLimitBytes Kubernetes k8s.container.memoryMappedFileBytes containerMemoryMappedFileBytes Kubernetes k8s.container.memoryRequestedBytes memoryRequestedBytes Kubernetes k8s.container.memoryUsedBytes memoryUsedBytes Kubernetes k8s.container.memoryUtilization memoryUtilization Kubernetes k8s.container.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.container.requestedCpuCoresUtilization requestedCpuCoresUtilization Kubernetes k8s.container.requestedMemoryUtilization requestedMemoryUtilization Kubernetes k8s.container.restartCount restartCount Kubernetes k8s.daemonset.createdAt createdAt Kubernetes k8s.daemonset.metadataGeneration metadataGeneration Kubernetes k8s.daemonset.podsAvailable podsAvailable Kubernetes k8s.daemonset.podsDesired podsDesired Kubernetes k8s.daemonset.podsMisscheduled podsMisscheduled Kubernetes k8s.daemonset.podsReady podsReady Kubernetes k8s.daemonset.podsScheduled podsScheduled Kubernetes k8s.daemonset.podsUnavailable podsUnavailable Kubernetes k8s.daemonset.podsUpdatedScheduled podsUpdatedScheduled Kubernetes k8s.deployment.createdAt createdAt Kubernetes k8s.deployment.podsAvailable podsAvailable Kubernetes k8s.deployment.podsDesired podsDesired Kubernetes k8s.deployment.podsMaxUnavailable podsMaxUnavailable Kubernetes k8s.deployment.podsTotal podsTotal Kubernetes k8s.deployment.podsUnavailable podsUnavailable Kubernetes k8s.deployment.podsUpdated podsUpdated Kubernetes k8s.endpoint.addressAvailable addressAvailable Kubernetes k8s.endpoint.addressNotReady addressNotReady Kubernetes k8s.endpoint.createdAt createdAt Kubernetes k8s.namespace.createdAt createdAt Kubernetes k8s.node.allocatableAttachableVolumes* allocatableAttachableVolumes* Kubernetes k8s.node.allocatableCpuCores allocatableCpuCores Kubernetes k8s.node.allocatableCpuCoresUtilization allocatableCpuCoresUtilization Kubernetes k8s.node.allocatableEphemeralStorageBytes allocatableEphemeralStorageBytes Kubernetes k8s.node.allocatableHugepages* allocatableHugepages* Kubernetes k8s.node.allocatableMemoryBytes allocatableMemoryBytes Kubernetes k8s.node.allocatableMemoryUtilization allocatableMemoryUtilization Kubernetes k8s.node.allocatablePods allocatablePods Kubernetes k8s.node.capacityAttachableVolumes* capacityAttachableVolumes* Kubernetes k8s.node.capacityCpuCores capacityCpuCores Kubernetes k8s.node.capacityEphemeralStorageBytes capacityEphemeralStorageBytes Kubernetes k8s.node.capacityHugepages* capacityHugepages* Kubernetes k8s.node.capacityMemoryBytes capacityMemoryBytes Kubernetes k8s.node.capacityPods capacityPods Kubernetes k8s.node.cpuUsedCoreMilliseconds cpuUsedCoreMilliseconds Kubernetes k8s.node.cpuUsedCores cpuUsedCores Kubernetes k8s.node.fsAvailableBytes fsAvailableBytes Kubernetes k8s.node.fsCapacityBytes fsCapacityBytes Kubernetes k8s.node.fsCapacityUtilization fsCapacityUtilization Kubernetes k8s.node.fsInodes fsInodes Kubernetes k8s.node.fsInodesFree fsInodesFree Kubernetes k8s.node.fsInodesUsed fsInodesUsed Kubernetes k8s.node.fsUsedBytes fsUsedBytes Kubernetes k8s.node.memoryAvailableBytes memoryAvailableBytes Kubernetes k8s.node.memoryMajorPageFaultsPerSecond memoryMajorPageFaultsPerSecond Kubernetes k8s.node.memoryPageFaults memoryPageFaults Kubernetes k8s.node.memoryRssBytes memoryRssBytes Kubernetes k8s.node.memoryUsedBytes memoryUsedBytes Kubernetes k8s.node.memoryWorkingSetBytes memoryWorkingSetBytes Kubernetes k8s.node.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.node.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.node.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.node.runtimeAvailableBytes runtimeAvailableBytes Kubernetes k8s.node.runtimeCapacityBytes runtimeCapacityBytes Kubernetes k8s.node.runtimeInodes runtimeInodes Kubernetes k8s.node.runtimeInodesFree runtimeInodesFree Kubernetes k8s.node.runtimeInodesUsed runtimeInodesUsed Kubernetes k8s.node.runtimeUsedBytes runtimeUsedBytes Kubernetes k8s.pod.createdAt createdAt Kubernetes k8s.pod.isReady isReady Kubernetes k8s.pod.isScheduled isScheduled Kubernetes k8s.pod.netErrorsPerSecond net.errorsPerSecond Kubernetes k8s.pod.netRxBytesPerSecond net.rxBytesPerSecond Kubernetes k8s.pod.netTxBytesPerSecond net.txBytesPerSecond Kubernetes k8s.pod.startTime startTime Kubernetes k8s.replicaset.createdAt createdAt Kubernetes k8s.replicaset.observedGeneration observedGeneration Kubernetes k8s.replicaset.podsDesired podsDesired Kubernetes k8s.replicaset.podsFullyLabeled podsFullyLabeled Kubernetes k8s.replicaset.podsMissing podsMissing Kubernetes k8s.replicaset.podsReady podsReady Kubernetes k8s.replicaset.podsTotal podsTotal Kubernetes k8s.service.createdAt createdAt Kubernetes k8s.statefulset.createdAt createdAt Kubernetes k8s.statefulset.currentRevision currentRevision Kubernetes k8s.statefulset.metadataGeneration metadataGeneration Kubernetes k8s.statefulset.observedGeneration observedGeneration Kubernetes k8s.statefulset.podsCurrent podsCurrent Kubernetes k8s.statefulset.podsDesired podsDesired Kubernetes k8s.statefulset.podsReady podsReady Kubernetes k8s.statefulset.podsTotal podsTotal Kubernetes k8s.statefulset.podsUpdated podsUpdated Kubernetes k8s.statefulset.updateRevision updateRevision Kubernetes k8s.volume.fsAvailableBytes fsAvailableBytes Kubernetes k8s.volume.fsCapacityBytes fsCapacityBytes Kubernetes k8s.volume.fsInodes fsInodes Kubernetes k8s.volume.fsInodesFree fsInodesFree Kubernetes k8s.volume.fsInodesUsed fsInodesUsed Kubernetes k8s.volume.fsUsedBytes fsUsedBytes Kubernetes k8s.volume.fsUsedPercent fsUsedPercent Memcached memcached.server.activeSlabs activeSlabs Memcached memcached.server.avgItemSizeInBytes avgItemSizeInBytes Memcached memcached.server.bytesReadServerPerSecond bytesReadServerPerSecond Memcached memcached.server.bytesUsedServerInBytes bytesUsedServerInBytes Memcached memcached.server.bytesWrittenServerPerSecond bytesWrittenServerPerSecond Memcached memcached.server.casHitRatePerSecond casHitRatePerSecond Memcached memcached.server.casMissRatePerSecond casMissRatePerSecond Memcached memcached.server.casWrongRatePerSecond casWrongRatePerSecond Memcached memcached.server.cmdFlushRatePerSecond cmdFlushRatePerSecond Memcached memcached.server.cmdGetRatePerSecond cmdGetRatePerSecond Memcached memcached.server.cmdSetRatePerSecond cmdSetRatePerSecond Memcached memcached.server.connectionRateServerPerSecond connectionRateServerPerSecond Memcached memcached.server.connectionStructuresAllocated connectionStructuresAllocated Memcached memcached.server.currentItemsStoredServer currentItemsStoredServer Memcached memcached.server.deleteCmdNoneRemovedPerSecond deleteCmdNoneRemovedPerSecond Memcached memcached.server.deleteCmdRemovedPerSecond deleteCmdRemovedPerSecond Memcached memcached.server.evictionsPerSecond evictionsPerSecond Memcached memcached.server.getHitPercent getHitPercent Memcached memcached.server.getHitPerSecond getHitPerSecond Memcached memcached.server.getMissPerSecond getMissPerSecond Memcached memcached.server.itemsStoredPerSecond itemsStoredPerSecond Memcached memcached.server.limitBytesStorage limitBytesStorage Memcached memcached.server.limitMaxBytes limitMaxBytes Memcached memcached.server.maxConnectionLimitPerSecond serverMaxConnectionLimitPerSecond Memcached memcached.server.memAllocatedSlabsInBytes memAllocatedSlabsInBytes Memcached memcached.server.openConnectionsServer openConnectionsServer Memcached memcached.server.pointerSize pointerSize Memcached memcached.server.rusageSystem usageRate Memcached memcached.server.rusageUser executionTime Memcached memcached.server.storingItemsPercentMemory storingItemsPercentMemory Memcached memcached.server.threads threads Memcached memcached.server.uptimeInMilliseconds uptimeInMilliseconds Memcached memcached.slab.activeItemsBumpedPerSecond activeItemsBumpedPerSecond Memcached memcached.slab.casBadValPerSecond casBadValPerSecond Memcached memcached.slab.casModifiedSlabPerSecond casModifiedSlabPerSecond Memcached memcached.slab.chunkSizeInBytes chunkSizeInBytes Memcached memcached.slab.chunksPerPage chunksPerPage Memcached memcached.slab.cmdSetRateSlabPerSecond cmdSetRateSlabPerSecond Memcached memcached.slab.decrsModifySlabPerSecond decrsModifySlabPerSecond Memcached memcached.slab.deleteRateSlabPerSecond deleteRateSlabPerSecond Memcached memcached.slab.entriesReclaimedPerSecond entriesReclaimedPerSecond Memcached memcached.slab.evictionsBeforeExpirationPerSecond evictionsBeforeExpirationPerSecond Memcached memcached.slab.evictionsBeforeExplicitExpirationPerSecond evictionsBeforeExplicitExpirationPerSecond Memcached memcached.slab.expiredItemsReclaimedPerSecond expiredItemsReclaimedPerSecond Memcached memcached.slab.freedChunks freedChunks Memcached memcached.slab.freedChunksEnd freedChunksEnd Memcached memcached.slab.getHitRateSlabPerSecond getHitRateSlabPerSecond Memcached memcached.slab.incrsModifySlabPerSecond incrsModifySlabPerSecond Memcached memcached.slab.itemsCold itemsCold Memcached memcached.slab.itemsColdPerSecond itemsColdPerSecond Memcached memcached.slab.itemsDirectReclaimedPerSecond itemsDirectReclaimedPerSecond Memcached memcached.slab.itemsFreedCrawlerPerSecond itemsFreedCrawlerPerSecond Memcached memcached.slab.itemsHot itemsHot Memcached memcached.slab.itemsOldestInMilliseconds itemsOldestInMilliseconds Memcached memcached.slab.itemsRefcountLockedPerSecond itemsRefcountLockedPerSecond Memcached memcached.slab.itemsSlabClass itemsSlabClass Memcached memcached.slab.itemsTimeSinceEvictionInMilliseconds itemsTimeSinceEvictionInMilliseconds Memcached memcached.slab.itemsWarm itemsWarm Memcached memcached.slab.itemsWarmPerSecond itemsWarmPerSecond Memcached memcached.slab.memRequestedSlabInBytesPerSecond memRequestedSlabInBytesPerSecond Memcached memcached.slab.outOfMemoryPerSecond outOfMemoryPerSecond Memcached memcached.slab.selfHealedSlabPerSecond selfHealedSlabPerSecond Memcached memcached.slab.totalChunksSlab totalChunksSlab Memcached memcached.slab.totalPagesSlab totalPagesSlab Memcached memcached.slab.touchHitSlabPerSecond touchHitSlabPerSecond Memcached memcached.slab.usedChunksItems usedChunksItems Memcached memcached.slab.usedChunksPerSecond usedChunksPerSecond Memcached memcached.slab.validItemsEvictedPerSecond validItemsEvictedPerSecond MongoDB mongo.index.accesses collection.indexAccesses MongoDB mongo.index.sizeInBytes collection.indexSizeInBytes MongoDB mongo.collection.avgObjSizeInBytes collection.avgObjSizeInBytes MongoDB mongo.collection.capped collection.capped MongoDB mongo.collection.count collection.count MongoDB mongo.collection.max collection.max MongoDB mongo.collection.maxSizeInBytes collection.maxSizeInBytes MongoDB mongo.collection.nindexes collection.nindexes MongoDB mongo.collection.sizeInBytes collection.sizeInBytes MongoDB mongo.collection.storageSizeInBytes collection.storageSizeInBytes MongoDB mongo.configServer.asserts.messagesPerSecond asserts.messagesPerSecond MongoDB mongo.configServer.asserts.regularPerSecond asserts.regularPerSecond MongoDB mongo.configServer.asserts.rolloversPerSecond asserts.rolloversPerSecond MongoDB mongo.configServer.asserts.userPerSecond asserts.userPerSecond MongoDB mongo.configServer.asserts.warningPerSecond asserts.warningPerSecond MongoDB mongo.configServer.commands.countFailedPerSecond commands.countFailedPerSecond MongoDB mongo.configServer.commands.countPerSecond commands.countPerSecond MongoDB mongo.configServer.commands.createIndexesFailedPerSecond commands.createIndexesFailedPerSecond MongoDB mongo.configServer.commands.createIndexesPerSecond commands.createIndexesPerSecond MongoDB mongo.configServer.commands.deleteFailedPerSecond commands.deleteFailedPerSecond MongoDB mongo.configServer.commands.deletePerSecond commands.deletePerSecond MongoDB mongo.configServer.commands.evalFailedPerSecond commands.evalFailedPerSecond MongoDB mongo.configServer.commands.evalPerSecond commands.evalPerSecond MongoDB mongo.configServer.commands.findAndModifyFailedPerSecond commands.findAndModifyFailedPerSecond MongoDB mongo.configServer.commands.findAndModifyPerSecond commands.findAndModifyPerSecond MongoDB mongo.configServer.commands.insertFailedPerSecond commands.insertFailedPerSecond MongoDB mongo.configServer.commands.insertPerSecond commands.insertPerSecond MongoDB mongo.configServer.commands.updateFailedPerSecond commands.updateFailedPerSecond MongoDB mongo.configServer.commands.updatePerSecond commands.updatePerSecond MongoDB mongo.configServer.connections.available connections.available MongoDB mongo.configServer.connections.current connections.current MongoDB mongo.configServer.connections.totalCreated connections.totalCreated MongoDB mongo.configServer.cursor.openNoTimeout cursor.openNoTimeout MongoDB mongo.configServer.cursor.openPinned cursor.openPinned MongoDB mongo.configServer.cursor.openTotal cursor.openTotal MongoDB mongo.configServer.cursor.timedOutPerSecond cursor.timedOutPerSecond MongoDB mongo.configServer.document.deletedPerSecond document.deletedPerSecond MongoDB mongo.configServer.document.insertedPerSecond document.insertedPerSecond MongoDB mongo.configServer.document.returnedPerSecond document.returnedPerSecond MongoDB mongo.configServer.document.updatedPerSecond document.updatedPerSecond MongoDB mongo.configServer.dur.commits dur.commits MongoDB mongo.configServer.dur.commitsInWriteLock dur.commitsInWriteLock MongoDB mongo.configServer.dur.compression dur.compression MongoDB mongo.configServer.dur.earlyCommits dur.earlyCommits MongoDB mongo.configServer.dur.preparingInMilliseconds dur.preparingInMilliseconds MongoDB mongo.configServer.dur.remappingInMilliseconds dur.remappingInMilliseconds MongoDB mongo.configServer.dur.timeCollectedCommitsInMilliseconds dur.timeCollectedCommitsInMilliseconds MongoDB mongo.configServer.dur.writingDataFilesInMilliseconds dur.writingDataFilesInMilliseconds MongoDB mongo.configServer.dur.writingJournalInMilliseconds dur.writingJournalInMilliseconds MongoDB mongo.configServer.flush.averageInMilliseconds flush.averageInMilliseconds MongoDB mongo.configServer.flush.flushesDisk flush.flushesDisk MongoDB mongo.configServer.flush.lastInMilliseconds flush.lastInMilliseconds MongoDB mongo.configServer.flush.totalInMilliseconds flush.totalInMilliseconds MongoDB mongo.configServer.getlasterror.wtimeMillisPerSecond getlasterror.wtimeMillisPerSecond MongoDB mongo.configServer.getlasterror.wtimeoutsPerSecond getlasterror.wtimeoutsPerSecond MongoDB mongo.configServer.globallock.activeClientsReaders globallock.activeClientsReaders MongoDB mongo.configServer.globallock.activeClientsTotal globallock.activeClientsTotal MongoDB mongo.configServer.globallock.activeClientsWriters globallock.activeClientsWriters MongoDB mongo.configServer.globallock.currentQueueReaders globallock.currentQueueReaders MongoDB mongo.configServer.globallock.currentQueueTotal globallock.currentQueueTotal MongoDB mongo.configServer.globallock.currentQueueWriters globallock.currentQueueWriters MongoDB mongo.configServer.globallock.totalTime globallock.totaltime MongoDB mongo.configServer.locks.collectionAcquireExclusive locks.collectionAcquireExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentExclusive locks.collectionAcquireIntentExclusive MongoDB mongo.configServer.locks.collectionAcquireIntentShared locks.collectionAcquireIntentShared MongoDB mongo.configServer.locks.collectionAcquireWaitCountExclusive locks.collectionAcquireWaitCountExclusive MongoDB mongo.configServer.locks.collectionTimeAcquiringMicrosExclusive locks.collectionTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseAcquireExclusive locks.databaseAcquireExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentExclusive locks.databaseAcquireIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireIntentShared locks.databaseAcquireIntentShared MongoDB mongo.configServer.locks.databaseAcquireShared locks.databaseAcquireShared MongoDB mongo.configServer.locks.databaseAcquireWaitExclusive locks.databaseAcquireWaitExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentExclusive locks.databaseAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.databaseAcquireWaitIntentShared locks.databaseAcquireWaitIntentShared MongoDB mongo.configServer.locks.databaseAcquireWaitShared locks.databaseAcquireWaitShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosExclusive locks.databaseTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentExclusive locks.databaseTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosIntentShared locks.databaseTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.databaseTimeAcquiringMicrosShared locks.databaseTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.globalAcquireExclusive locks.globalAcquireExclusive MongoDB mongo.configServer.locks.globalAcquireIntentExclusive locks.globalAcquireIntentExclusive MongoDB mongo.configServer.locks.globalAcquireIntentShared locks.globalAcquireIntentShared MongoDB mongo.configServer.locks.globalAcquireShared locks.globalAcquireShared MongoDB mongo.configServer.locks.globalAcquireWaitExclusive locks.globalAcquireWaitExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentExclusive locks.globalAcquireWaitIntentExclusive MongoDB mongo.configServer.locks.globalAcquireWaitIntentShared locks.globalAcquireWaitIntentShared MongoDB mongo.configServer.locks.globalAcquireWaitShared locks.globalAcquireWaitShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosExclusive locks.globalTimeAcquiringMicrosExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentExclusive locks.globalTimeAcquiringMicrosIntentExclusive MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosIntentShared locks.globalTimeAcquiringMicrosIntentShared MongoDB mongo.configServer.locks.globalTimeAcquiringMicrosShared locks.globalTimeAcquiringMicrosShared MongoDB mongo.configServer.locks.metadataAcquireExclusive locks.metadataAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireExclusive locks.oplogAcquireExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentExclusive locks.oplogAcquireIntentExclusive MongoDB mongo.configServer.locks.oplogAcquireIntentShared locks.oplogAcquireIntentShared MongoDB mongo",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 100.26195,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Manage</em> <em>your</em> <em>data</em>",
        "body": "New Relic Integrations Metrics The following table contains the metrics we collect for our <em>infrastructure</em> integrations. Integration Dimensional Metric Name (new) Sample Metric Name (previous) Agent host.cpuIdlePercent cpuIdlePercent Agent host.cpuIoWaitPercent cpuIOWaitPercent Agent host.cpuPercent"
      },
      "id": "603e8a8a64441f69a34e8841"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/alert-infrastructure-processes": [
    {
      "sections": [
        "REST API calls for infrastructure alerts",
        "Requirements",
        "Tip",
        "Using infrastructure API calls",
        "GET infrastructure conditions",
        "GET a list of infrastructure conditions",
        "Example GET a list of conditions",
        "GET a specific infrastructure condition",
        "Example GET a specific condition",
        "Create (POST) an infrastructure condition",
        "Important",
        "Update (PUT) an infrastructure condition",
        "Example update (PUT) a condition",
        "Remove (DELETE) an infrastructure condition",
        "Types of conditions",
        "Process running conditions API data",
        "Example condition types",
        "Metric conditions API data",
        "Example",
        "Host not reporting condition",
        "Definitions",
        "value",
        "duration_minutes",
        "time_function"
      ],
      "title": "REST API calls for infrastructure alerts",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "c35aa43cdb6645473d02886a49d6f9aeb37e577f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/rest-api-calls-new-relic-infrastructure-alerts/",
      "published_at": "2021-05-04T17:32:15Z",
      "updated_at": "2021-04-06T06:04:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the infrastructure REST API to add, update, delete, and list alerting conditions. You can also manage individual alerting conditions using the infrastructure monitoring UI. REST API calls for infrastructure alerts are not available in the API Explorer. Why use the API Examples Consistency Define the same set of conditions for every cluster without having to set up identical conditions in the Infrastructure monitoring UI each time. Manage multiple conditions quickly, without having to update them one by one using the UI. Flexibility Create conditions for an arbitrary group of hosts. Disable or delete conditions for hosts taken offline anytime. Create a condition with exclusionary filtering (for instance, environment NOT LIKE x). For more on this, see this post on exclusion filtering. For AWS Cloud integrations, select attributes that haven't been sent up by AWS yet. Create compound alert conditions by using the where_clause, which allows you to specify the limits on a secondary or tertiary metric. Exceed the 500-facet limitation on NRQL alert conditions. Reliability Audit when a condition was last updated. Requirements In order to use the Infrastructure REST API, you need: An API key The alerting condition's related policy_id from New Relic, available via GET list of conditions or via the Alerts REST API The condition id, available via GET list of conditions, or via the condition's URL in the Infrastructure monitoring UI Tip If your account hosts data in the EU data center, make sure you are using the proper API endpoints for EU region accounts. Using infrastructure API calls Here are some basic cURL commands and their responses for Infrastructure alert conditions. Depending on the type of condition, the DATA information you provide in the call will vary for POST (add) and PUT (update) calls. Definitions of each attribute used in the data blocks can be found in the Definitions section. GET infrastructure conditions You can either GET a list of infrastructure conditions or GET a specific infrastructure condition. Here are a few tips for listing infrastructure conditions. For pagination, use limit (records per page) and offset (how many records to skip) parameters. Default is 50 records per page, and offset starts at 0 (skip no records). To scope the results to a specific policy, use policy_id. Tip If you want to use the GET response as a template for your PUT or POST input, be sure to remove the created_at_epoch_millis, updated_at_epoch_millis and id information. GET a list of infrastructure conditions curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111\" Copy Example GET a list of conditions Response showing 2 of the 3 conditions for the example policy (formatted for readability and truncated): HTTP/1.1 200 OK Content-Length: 622 Content-Type: application/json { \"data\":[ { \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(`hostname` LIKE '%cassandra%')\", \"id\":13890, \"created_at_epoch_millis\":1490996713872, \"updated_at_epoch_millis\":1490996713872, \"policy_id\":111111, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(`commandName` = 'java')\" }, { \"created_at_epoch_millis\": 1501704525462, \"critical_threshold\": { \"duration_minutes\": 5 }, \"enabled\": true, \"filter\": { \"and\": [ { \"like\": { \"fullHostname\": \"Production_1\" } } ] }, \"id\": 448036, \"name\": \"PROD - Host Machine's Agent Not Responding ....\", \"policy_id\": 98485, \"type\": \"infra_host_not_reporting\", \"updated_at_epoch_millis\": 1504879191220 } . . . ], \"meta\":{ \"limit\":50, \"offset\":0, \"total\":3 }, \"links\":{} } Copy To get a list of the 10 Infrastructure conditions beyond the 50 limit: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111&offset=50&list=10\" Copy GET a specific infrastructure condition To get information about a single Infrastructure condition: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition-id\" Copy Example GET a specific condition Response (formatted for readability): HTTP/1.1 200 OK Content-Length: 246 Content-Type: application/json { \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"demo condition\", \"enabled\":false, \"id\":13887, \"created_at_epoch_millis\":1490981583580, \"updated_at_epoch_millis\":1490981583580, \"policy_id\":23635, \"critical_threshold\":{ \"duration_minutes\":100 } } } Copy Create (POST) an infrastructure condition Important Do not include an \"id\": when adding a new condition (POST). It will be generated when the condition is created. To add an infrastructure condition, use this basic cURL command: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are adding: Process running conditions API data Metric conditions API data Host not reporting conditions API data Update (PUT) an infrastructure condition You only need to include the fields that need to be changed when updating an infrastructure condition. The API keeps the existing values for any missing fields. Important If you want to change the condition type, do not use PUT. Instead, delete the existing condition, then add (POST) a new condition with the new condition type and all fields. To update an infrastructure condition, use this basic cURL command. To indicate which condition is to be updated, be sure to include the \"id\": . Example update (PUT) a condition curl -X PUT 'https://infra-api.newrelic.com/v2/alerts/conditions/condition-id' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are updating: Process running conditions API data Metric conditions API data Host not reporting conditions API data Remove (DELETE) an infrastructure condition To delete an infrastructure condition, use this basic cURL command: curl -v -X DELETE --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition_id\" Copy Types of conditions Process running conditions API data A process running condition alerts you when the number of processes is above, below, or equal to the threshold you define. To add (POST) or update (PUT) a process running condition, use your API key, and refer to the definitions to customize your values in the API call. Example condition types For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(commandName = '\\''java'\\'')\" } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause and process_where_clause Metric conditions API data A metric condition alerts you when the metric of your choice is above, below, or equal to the threshold you define. This includes: System metrics Process metrics Network metrics Storage metrics Cloud integration metrics To add (POST) or update (PUT) a metric condition, use your API key, and refer to the definitions to customize your values in the API call. If you are adding or updating a cloud integration alert condition: For the event_type field, enter the event type generated by your selected cloud integration service (for example, ComputeSample for the AWS EC2 integration). If you are setting up an alert condition on a cloud integration service that requires a provider value (for example, AWS RDS uses DatastoreSample with a provider value of RdsDbInstance or RdsDbCluster), you will need to add the \"integration_provider\" field and use the value that is appropriate for the service your alert condition is targeting (for example, \"integration_provider\":\"RdsDbInstance\"). For the select_value field, build the metric name by using the following syntax, where provider is a standard prefix string: provider.metric.aggregation_type Copy metric: Use the metric name as described in the New Relic documentation for your integration. aggregation_type: Use Sum, Average, Minimum, or Maximum. Refer to the original documentation by the integration's cloud provider to see which statistic aggregations are available for each metric. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_metric\", \"name\":\"Disk Space Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"event_type\":\"StorageSample\", \"select_value\":\"diskFreePercent\", \"comparison\":\"below\", \"critical_threshold\":{ \"value\":10, \"duration_minutes\":1, \"time_function\":\"any\" }, \"warning_threshold\":{ \"value\":30, \"duration_minutes\":2, \"time_function\":\"any\" } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Host not reporting condition A host not reporting condition alerts you when a host stops reporting. To add (POST) or update (PUT) a host not reporting condition, use your API key, and refer to the definitions to customize your values in the API call. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"Cassandra Host Reporting Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"critical_threshold\":{ \"duration_minutes\":12 } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Definitions When formatting your cURL commands, use these values as needed. These are listed in alphabetical order, not the order they appear in your API calls. Field Definition comparison (enum) Condition type: infra_metric, infra_process_running The value used to define the threshold; for example, \"[\"above\", \"below\", \"equal\"]. critical_threshold and warning_threshold Condition type: all This object identifies the threshold value before opening a violation. The critical_threshold is required. The warning_threshold is optional and may only be used with infra_metric conditions. The keys of this object depend on the condition type. Condition type: infra_metric format: \"critical_threshold\":{ \"value\":<number>, \"duration_minutes\":<integer>, \"time_function\":\"any\" or \"all\" }, Copy Condition type: infra_process_running format: \"critical_threshold\":{ \"value\":<integer>, \"duration_minutes\":<integer>, }, Copy Condition type: infra_host_not_reporting format: \"critical_threshold\":{ \"duration_minutes\":<integer>, }, Copy value The numeric value that must be breached for the condition to open a violation duration_minutes The number of minutes the value must be passed or met for the condition to open a violation time_function Indicates if the condition needs to be sustained for a certain period of time to create a violation, or if it only needs to break the threshold once within a certain period of time. If you're setting up a for at least x minutes threshold, use all; for an at least once in x minutes threshold, use any. enabled (boolean) Condition type: all Whether the condition is turned on or off; true or false. event_type (string) Condition type: infra_metric The metric event; for example, system metrics, process metrics, storage metrics, or network metrics. This automatically populates for infrastructure integrations; for example, StorageSample or SystemSample. filter (string) Condition type: all If the condition was made in the UI, filter appears instead of where_clause; for example: {and: [{is: {ec2InstanceType: \"m3.medium\"}}]} Copy Recommendation: Use where_clause when creating a new condition. id (integer) Condition type: all The condition ID located in the URL. GET: This value appears in the GET response. PUT: Include this value in the DATA section. POST: Do not include this in the DATA section. DELETE: Include this value in the -X DELETE call. integration_provider (string) Condition type: infra_metric For alerts on integrations, use integration_provider instead of event_type. To see valid values: From the New Relic documentation for your cloud service, check the Find and use data section. Example: In the AWS RDS monitoring integration documentation, you can see that the DatastoreSample event type can be used with an integration_provider value of either RdsDbInstance for DB instances, or RdsDbCluster for Aurora DB clusters. name (string) Condition type: all The infrastructure alerting condition's name; for example: \"[test] process running\" Copy policy_id (integer) Condition type: all The unique ID for the alert policy's account ID associated with the condition; for example, 1234567890. This is not the policy's global ID. process_where_clause (string) Condition type: infra_process_running Any filters applied to processes, specifically in process running alert conditions. This parameter is mandatory for those types of alert conditions. For example: \"commandName = '\\''java'\\''\" Copy runbook_url (string) Condition type: all The runbook URL to display in notifications. select_value (string) Condition type: infra_metric The attribute name to identify the metric being targeted; for example, \"cpuPercent\", \"diskFreePercent\", \"memoryResidentSizeBytes\", or \"memoryFreeBytes/memoryTotalBytes*100\". This automatically populates for Infrastructure Integrations; for example, diskFreePercent. type (enum) Condition type: all The type of infrastructure alert condition: \"infra_process_running\", \"infra_metric\", or \"infra_host_not_reporting\". violation_close_timer (integer) Condition type: all The Violation time limit setting, expressed as hours. Possible values are 0, 1, 2, 4, 8,12, 24, 48, 72. This determines how much time will pass before a violation is automatically closed. Setting the time limit to 0 prevents a violation from being force-closed. For new conditions, if a value is not provided, the following default values are used: Host Not Responding (HNR) conditions: 0 (disabled) All other conditions: 24 When updating existing conditions, if a value is provided, it overrides the existing value, but does not affect already opened violations. where_clause (string) Condition type: all If applicable, this identifies any infrastructure host filters used; for example: \"(`hostname` LIKE '\\''%cassandra%'\\'')\", Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 223.77554,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "sections": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use the <em>infrastructure</em> REST API to add, update, delete, and list alerting <em>conditions</em>. You can also manage individual alerting <em>conditions</em> using the <em>infrastructure</em> monitoring UI. REST API calls for <em>infrastructure</em> <em>alerts</em> are not available in the API Explorer. Why use the API Examples Consistency"
      },
      "id": "6043fa6c196a678ae2960f31"
    },
    {
      "sections": [
        "Create infrastructure \"host not reporting\" condition",
        "Features",
        "Create \"host not reporting\" condition",
        "Investigate the problem",
        "Intentional outages"
      ],
      "title": "Create infrastructure \"host not reporting\" condition",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "0a74e7e65e3eeb5268eac310c11802ca2e78a614",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-alerts/infrastructure-alert-conditions/create-infrastructure-host-not-reporting-condition/",
      "published_at": "2021-05-05T03:39:35Z",
      "updated_at": "2021-03-16T08:30:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use Infrastructure monitoring's Host not reporting condition to notify you when we've stopped receiving data from an infrastructure agent. This feature allows you to dynamically alert on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of alerts notifications. Features You can define conditions based on the sets of hosts most important to you, and configure thresholds appropriate for each filter set. The Host not reporting event triggers when data from the infrastructure agent doesn't reach our collector within the time frame you specify. This feature's flexibility allows you to easily customize what to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Host not reporting condition Features What to monitor You can use filter sets to select which hosts you want to be monitored with the alert condition. The condition will also automatically apply to any hosts you add in the future that match these filters. How to notify Conditions are contained in policies. You can select an existing policy or create a new policy with email notifications from the Infrastructure monitoring UI. If you want to create a new policy with other types of notification channels, use the UI. When to notify Email addresses (identified in the policy) will be notified automatically about threshold violations for any host matching the filters you have applied, depending on the policy's incident preferences. Where to troubleshoot The link at the top of the email notification will take you to the infrastructure Events page centered on the time when the host disconnected. Additional links in the email will take you to additional detail. Create \"host not reporting\" condition To define the Host not reporting condition criteria: Follow standard procedures to create an infrastructure condition. Select Host not reporting as the Alert type. Define the Critical threshold for triggering the notification: minimum 5 minutes, maximum 60 minutes. Enable 'Don't trigger alerts for hosts that perform a clean shutdown' option, if you want to prevent false alerts when you have hosts set to shut down via command line. Currently this feature is supported on all Windows systems and Linux systems using systemd. Depending on the policy's incident preferences, it will define which notification channels to use when the defined Critical threshold for the condition passes. To avoid \"false positives,\" the host must stop reporting for the entire time period before a violation is opened. Example: You create a condition to open a violation when any of the filtered set of hosts stop reporting data for seven minutes. If any host stops reporting for five minutes, then resumes reporting, the condition does not open a violation. If any host stops reporting for seven minutes, even if the others are fine, the condition does open a violation. Investigate the problem To further investigate why a host is not reporting data: Review the details in the email notification. Use the link from the email notification to monitor ongoing changes in your environment from Infrastructure monitoring's Events page. For example, use the Events page to help determine if a host disconnected right after a root user made a configuration change to the host. Optional: Use the email notification's Acknowledge link to verify you are aware of and taking ownership of the alerting incident. Use the email links to examine additional details in the Incident details page. Intentional outages We can distinguish between unexpected situations and planned situations with the option Don't trigger alerts for hosts that perform a clean shutdown. Use this option for situations such as: Host has been taken offline intentionally. Host has planned downtime for maintenance. Host has been shut down or decommissioned. Autoscaling hosts or shutting down instances in a cloud console. We rely on Linux and Windows shutdown signals to flag a clean shutdown. We've confirmed that these scenarios are detected by the agent: AWS Auto-scaling event with EC2 instances that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) User-initiated shutdown of Windows systems User-initiated shutdown of Linux systems that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) We know that these scenarios are not detected by the agent: User-initiated shutdown of Linux systems that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other modern Linux systems that still use Upstart or SysV init systems. AWS Auto-scaling event with EC2 instances that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other more modern Linux systems that still use Upstart or SysV init systems.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.63498,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "sections": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use <em>Infrastructure</em> monitoring&#x27;s Host not reporting condition to notify you when we&#x27;ve stopped receiving data from an <em>infrastructure</em> agent. This feature allows you to dynamically <em>alert</em> on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of <em>alerts</em>"
      },
      "id": "603ea06c196a67cd47a83dc1"
    },
    {
      "sections": [
        "Infrastructure alerting examples",
        "Examples: Infrastructure pages",
        "Examples: Threshold options",
        "Integrations providers",
        "CPU, disk, load average, memory, swap",
        "Byte size"
      ],
      "title": "Infrastructure alerting examples",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "1ec5f86b745413b2a8d6a5b676ecbe622c674ab1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/infrastructure-alerting-examples/",
      "published_at": "2021-05-05T03:39:35Z",
      "updated_at": "2021-03-16T07:35:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Alert type field in infrastructure monitoring's Settings > Alerts page shows what options you can select to create infrastructure alert conditions. You can also create alert conditions from any infrastructure chart by selecting the ellipses icon and then Create alert. Examples: Infrastructure pages Here are some examples of how to create alert conditions within the context of the Infrastructure monitoring UI page you are currently viewing. To create an alerts condition from any chart, select the ellipses icon and then Create alert. New Relic will automatically select the appropriate Alert type. Example Problem and solution High CPU usage Problem: Your Ops team monitors a filtered set of host clusters in your eastern region and notices that the CPU usage is constantly high. Solution: Use the CPU chart on Infrastructure monitoring's Hosts page to create an alert condition for system metrics. Virtual memory capacity Problem: Your night shift needs to be alerted when virtual memory for a set of background workers reaches an average of 10G for at least two minutes. Solution: Use the Top memory consumers chart on Infrastructure monitoring's Processes page to create an alert condition for process metrics. Limited bandwidth Problem: You want to monitor performance based on the average number of errors received or transmitted. Solution: Use the Top bandwidth chart on Infrastructure monitoring's Network page to create an alert condition for network metrics. I/O read and write operations Problem: You are testing a new set of hosts in your staging environment, and you want to be notified when their read or write capacity rises above your test threshold level. Solution: Use the Top I/O operations chart on Infrastructure monitoring's Storage page to create an alert condition for storage metrics. Host not reporting Problem: You want to be notified when we have stopped receiving data from an infrastructure agent. Solution: From the Hosts, Processes, Network, or Storage pages, create a host not reporting alert condition. Processes not running as expected Problem: You want to be notified if any of the processes on your hosts stop reporting. OR A process you expected to start on a host (such as a new program) is not actually running. Solution: From the Processes page (or from the Hosts, Network, or Storage pages), create a process running alert condition. Examples: Threshold options Use the thresholds dropdown for the selected Alert type to further define how you want to be alerted. Here are some examples of the options available. Integrations providers With infrastructure integrations, you can create an alert condition from your Integrations page. Depending on the type of provider selected (CloudFront, DynamoDB, EBS, etc.), options will vary from the Define thresholds dropdown; for example, bytes, errors, requests, CPU, connections, memory, records, latency, etc. CPU, disk, load average, memory, swap The System metrics thresholds dropdown allows you to select various criteria for CPU, disk, load average, memory, and swap metrics. Byte size The Network metrics thresholds provide flexibility with your business needs. Depending on the size of your network, you can easily set the threshold in bytes, KB, MB, GB, or TB.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.61472,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>alerting</em> examples",
        "sections": "<em>Infrastructure</em> <em>alerting</em> examples",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "The <em>Alert</em> type field in <em>infrastructure</em> monitoring&#x27;s Settings &gt; <em>Alerts</em> page shows what options you can select to create <em>infrastructure</em> <em>alert</em> <em>conditions</em>. You can also create <em>alert</em> <em>conditions</em> from any <em>infrastructure</em> chart by selecting the ellipses icon and then Create <em>alert</em>. Examples: <em>Infrastructure</em>"
      },
      "id": "603eb52a28ccbc9027eba7bb"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/infrastructure-alerting-examples": [
    {
      "sections": [
        "REST API calls for infrastructure alerts",
        "Requirements",
        "Tip",
        "Using infrastructure API calls",
        "GET infrastructure conditions",
        "GET a list of infrastructure conditions",
        "Example GET a list of conditions",
        "GET a specific infrastructure condition",
        "Example GET a specific condition",
        "Create (POST) an infrastructure condition",
        "Important",
        "Update (PUT) an infrastructure condition",
        "Example update (PUT) a condition",
        "Remove (DELETE) an infrastructure condition",
        "Types of conditions",
        "Process running conditions API data",
        "Example condition types",
        "Metric conditions API data",
        "Example",
        "Host not reporting condition",
        "Definitions",
        "value",
        "duration_minutes",
        "time_function"
      ],
      "title": "REST API calls for infrastructure alerts",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "c35aa43cdb6645473d02886a49d6f9aeb37e577f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/rest-api-calls-new-relic-infrastructure-alerts/",
      "published_at": "2021-05-04T17:32:15Z",
      "updated_at": "2021-04-06T06:04:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the infrastructure REST API to add, update, delete, and list alerting conditions. You can also manage individual alerting conditions using the infrastructure monitoring UI. REST API calls for infrastructure alerts are not available in the API Explorer. Why use the API Examples Consistency Define the same set of conditions for every cluster without having to set up identical conditions in the Infrastructure monitoring UI each time. Manage multiple conditions quickly, without having to update them one by one using the UI. Flexibility Create conditions for an arbitrary group of hosts. Disable or delete conditions for hosts taken offline anytime. Create a condition with exclusionary filtering (for instance, environment NOT LIKE x). For more on this, see this post on exclusion filtering. For AWS Cloud integrations, select attributes that haven't been sent up by AWS yet. Create compound alert conditions by using the where_clause, which allows you to specify the limits on a secondary or tertiary metric. Exceed the 500-facet limitation on NRQL alert conditions. Reliability Audit when a condition was last updated. Requirements In order to use the Infrastructure REST API, you need: An API key The alerting condition's related policy_id from New Relic, available via GET list of conditions or via the Alerts REST API The condition id, available via GET list of conditions, or via the condition's URL in the Infrastructure monitoring UI Tip If your account hosts data in the EU data center, make sure you are using the proper API endpoints for EU region accounts. Using infrastructure API calls Here are some basic cURL commands and their responses for Infrastructure alert conditions. Depending on the type of condition, the DATA information you provide in the call will vary for POST (add) and PUT (update) calls. Definitions of each attribute used in the data blocks can be found in the Definitions section. GET infrastructure conditions You can either GET a list of infrastructure conditions or GET a specific infrastructure condition. Here are a few tips for listing infrastructure conditions. For pagination, use limit (records per page) and offset (how many records to skip) parameters. Default is 50 records per page, and offset starts at 0 (skip no records). To scope the results to a specific policy, use policy_id. Tip If you want to use the GET response as a template for your PUT or POST input, be sure to remove the created_at_epoch_millis, updated_at_epoch_millis and id information. GET a list of infrastructure conditions curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111\" Copy Example GET a list of conditions Response showing 2 of the 3 conditions for the example policy (formatted for readability and truncated): HTTP/1.1 200 OK Content-Length: 622 Content-Type: application/json { \"data\":[ { \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(`hostname` LIKE '%cassandra%')\", \"id\":13890, \"created_at_epoch_millis\":1490996713872, \"updated_at_epoch_millis\":1490996713872, \"policy_id\":111111, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(`commandName` = 'java')\" }, { \"created_at_epoch_millis\": 1501704525462, \"critical_threshold\": { \"duration_minutes\": 5 }, \"enabled\": true, \"filter\": { \"and\": [ { \"like\": { \"fullHostname\": \"Production_1\" } } ] }, \"id\": 448036, \"name\": \"PROD - Host Machine's Agent Not Responding ....\", \"policy_id\": 98485, \"type\": \"infra_host_not_reporting\", \"updated_at_epoch_millis\": 1504879191220 } . . . ], \"meta\":{ \"limit\":50, \"offset\":0, \"total\":3 }, \"links\":{} } Copy To get a list of the 10 Infrastructure conditions beyond the 50 limit: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111&offset=50&list=10\" Copy GET a specific infrastructure condition To get information about a single Infrastructure condition: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition-id\" Copy Example GET a specific condition Response (formatted for readability): HTTP/1.1 200 OK Content-Length: 246 Content-Type: application/json { \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"demo condition\", \"enabled\":false, \"id\":13887, \"created_at_epoch_millis\":1490981583580, \"updated_at_epoch_millis\":1490981583580, \"policy_id\":23635, \"critical_threshold\":{ \"duration_minutes\":100 } } } Copy Create (POST) an infrastructure condition Important Do not include an \"id\": when adding a new condition (POST). It will be generated when the condition is created. To add an infrastructure condition, use this basic cURL command: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are adding: Process running conditions API data Metric conditions API data Host not reporting conditions API data Update (PUT) an infrastructure condition You only need to include the fields that need to be changed when updating an infrastructure condition. The API keeps the existing values for any missing fields. Important If you want to change the condition type, do not use PUT. Instead, delete the existing condition, then add (POST) a new condition with the new condition type and all fields. To update an infrastructure condition, use this basic cURL command. To indicate which condition is to be updated, be sure to include the \"id\": . Example update (PUT) a condition curl -X PUT 'https://infra-api.newrelic.com/v2/alerts/conditions/condition-id' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are updating: Process running conditions API data Metric conditions API data Host not reporting conditions API data Remove (DELETE) an infrastructure condition To delete an infrastructure condition, use this basic cURL command: curl -v -X DELETE --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition_id\" Copy Types of conditions Process running conditions API data A process running condition alerts you when the number of processes is above, below, or equal to the threshold you define. To add (POST) or update (PUT) a process running condition, use your API key, and refer to the definitions to customize your values in the API call. Example condition types For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(commandName = '\\''java'\\'')\" } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause and process_where_clause Metric conditions API data A metric condition alerts you when the metric of your choice is above, below, or equal to the threshold you define. This includes: System metrics Process metrics Network metrics Storage metrics Cloud integration metrics To add (POST) or update (PUT) a metric condition, use your API key, and refer to the definitions to customize your values in the API call. If you are adding or updating a cloud integration alert condition: For the event_type field, enter the event type generated by your selected cloud integration service (for example, ComputeSample for the AWS EC2 integration). If you are setting up an alert condition on a cloud integration service that requires a provider value (for example, AWS RDS uses DatastoreSample with a provider value of RdsDbInstance or RdsDbCluster), you will need to add the \"integration_provider\" field and use the value that is appropriate for the service your alert condition is targeting (for example, \"integration_provider\":\"RdsDbInstance\"). For the select_value field, build the metric name by using the following syntax, where provider is a standard prefix string: provider.metric.aggregation_type Copy metric: Use the metric name as described in the New Relic documentation for your integration. aggregation_type: Use Sum, Average, Minimum, or Maximum. Refer to the original documentation by the integration's cloud provider to see which statistic aggregations are available for each metric. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_metric\", \"name\":\"Disk Space Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"event_type\":\"StorageSample\", \"select_value\":\"diskFreePercent\", \"comparison\":\"below\", \"critical_threshold\":{ \"value\":10, \"duration_minutes\":1, \"time_function\":\"any\" }, \"warning_threshold\":{ \"value\":30, \"duration_minutes\":2, \"time_function\":\"any\" } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Host not reporting condition A host not reporting condition alerts you when a host stops reporting. To add (POST) or update (PUT) a host not reporting condition, use your API key, and refer to the definitions to customize your values in the API call. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"Cassandra Host Reporting Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"critical_threshold\":{ \"duration_minutes\":12 } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Definitions When formatting your cURL commands, use these values as needed. These are listed in alphabetical order, not the order they appear in your API calls. Field Definition comparison (enum) Condition type: infra_metric, infra_process_running The value used to define the threshold; for example, \"[\"above\", \"below\", \"equal\"]. critical_threshold and warning_threshold Condition type: all This object identifies the threshold value before opening a violation. The critical_threshold is required. The warning_threshold is optional and may only be used with infra_metric conditions. The keys of this object depend on the condition type. Condition type: infra_metric format: \"critical_threshold\":{ \"value\":<number>, \"duration_minutes\":<integer>, \"time_function\":\"any\" or \"all\" }, Copy Condition type: infra_process_running format: \"critical_threshold\":{ \"value\":<integer>, \"duration_minutes\":<integer>, }, Copy Condition type: infra_host_not_reporting format: \"critical_threshold\":{ \"duration_minutes\":<integer>, }, Copy value The numeric value that must be breached for the condition to open a violation duration_minutes The number of minutes the value must be passed or met for the condition to open a violation time_function Indicates if the condition needs to be sustained for a certain period of time to create a violation, or if it only needs to break the threshold once within a certain period of time. If you're setting up a for at least x minutes threshold, use all; for an at least once in x minutes threshold, use any. enabled (boolean) Condition type: all Whether the condition is turned on or off; true or false. event_type (string) Condition type: infra_metric The metric event; for example, system metrics, process metrics, storage metrics, or network metrics. This automatically populates for infrastructure integrations; for example, StorageSample or SystemSample. filter (string) Condition type: all If the condition was made in the UI, filter appears instead of where_clause; for example: {and: [{is: {ec2InstanceType: \"m3.medium\"}}]} Copy Recommendation: Use where_clause when creating a new condition. id (integer) Condition type: all The condition ID located in the URL. GET: This value appears in the GET response. PUT: Include this value in the DATA section. POST: Do not include this in the DATA section. DELETE: Include this value in the -X DELETE call. integration_provider (string) Condition type: infra_metric For alerts on integrations, use integration_provider instead of event_type. To see valid values: From the New Relic documentation for your cloud service, check the Find and use data section. Example: In the AWS RDS monitoring integration documentation, you can see that the DatastoreSample event type can be used with an integration_provider value of either RdsDbInstance for DB instances, or RdsDbCluster for Aurora DB clusters. name (string) Condition type: all The infrastructure alerting condition's name; for example: \"[test] process running\" Copy policy_id (integer) Condition type: all The unique ID for the alert policy's account ID associated with the condition; for example, 1234567890. This is not the policy's global ID. process_where_clause (string) Condition type: infra_process_running Any filters applied to processes, specifically in process running alert conditions. This parameter is mandatory for those types of alert conditions. For example: \"commandName = '\\''java'\\''\" Copy runbook_url (string) Condition type: all The runbook URL to display in notifications. select_value (string) Condition type: infra_metric The attribute name to identify the metric being targeted; for example, \"cpuPercent\", \"diskFreePercent\", \"memoryResidentSizeBytes\", or \"memoryFreeBytes/memoryTotalBytes*100\". This automatically populates for Infrastructure Integrations; for example, diskFreePercent. type (enum) Condition type: all The type of infrastructure alert condition: \"infra_process_running\", \"infra_metric\", or \"infra_host_not_reporting\". violation_close_timer (integer) Condition type: all The Violation time limit setting, expressed as hours. Possible values are 0, 1, 2, 4, 8,12, 24, 48, 72. This determines how much time will pass before a violation is automatically closed. Setting the time limit to 0 prevents a violation from being force-closed. For new conditions, if a value is not provided, the following default values are used: Host Not Responding (HNR) conditions: 0 (disabled) All other conditions: 24 When updating existing conditions, if a value is provided, it overrides the existing value, but does not affect already opened violations. where_clause (string) Condition type: all If applicable, this identifies any infrastructure host filters used; for example: \"(`hostname` LIKE '\\''%cassandra%'\\'')\", Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 223.77554,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "sections": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use the <em>infrastructure</em> REST API to add, update, delete, and list alerting <em>conditions</em>. You can also manage individual alerting <em>conditions</em> using the <em>infrastructure</em> monitoring UI. REST API calls for <em>infrastructure</em> <em>alerts</em> are not available in the API Explorer. Why use the API Examples Consistency"
      },
      "id": "6043fa6c196a678ae2960f31"
    },
    {
      "sections": [
        "Alert on infrastructure processes",
        "Important",
        "Examples",
        "Ensure enough processes are running to satisfy load",
        "Ensure that critical services run constantly",
        "Monitor startup for critical processes that require special attention",
        "Make sure a job doesn't take too long",
        "Validate that services started successfully",
        "Watch for runaway processes or configuration problems",
        "Create an infrastructure process running condition"
      ],
      "title": "Alert on infrastructure processes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "5fcbe11b9beb16723ff2521fca981f19a4c716ce",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/alert-infrastructure-processes/",
      "published_at": "2021-05-05T03:39:35Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use New Relic infrastructure's Process running alert condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process you expected to start on a host (such as a new program) is not actually running This feature's flexibility allows you to easily filter what hosts and processes to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Examples By applying filters to the hosts and processes that are important to your business, you can define alerting thresholds to decide when violations open and New Relic sends an email notification to you depending on the policy's incident preferences. These examples illustrate how to use infrastructure monitoring's Process running condition to monitor your processes. Ensure enough processes are running to satisfy load Problem: Some load balancers and application servers work by running many worker processes in parallel. Here, for example, you may want an alert violation when fewer than eight processes are running for a service like gunicorn. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Ensure that critical services run constantly Problem: A service, such as a database or application server, is expected to be running constantly on certain hosts, and you need to know when it has stopped. Solution: Use the No processes are running (default) threshold. Monitor startup for critical processes that require special attention Problem: You have processes requiring special attention due to security or potential performance impact. Solution: Use the At least one process is running threshold with condition filters set to a username and specific executable so that New Relic can open a violation when the process is running. Make sure a job doesn't take too long Problem: You have a job that runs periodically, and you want to open a violation when it has been running longer than an expected number of minutes. Solution: Use the At least one process is running threshold. Validate that services started successfully Problem: When provisioning new hosts, you want to open a violation if a required service fails to successfully start up. Solution: Use the No processes are running (default) threshold. Watch for runaway processes or configuration problems Problem: Sometimes problems with processes can be solved with changes to your configuration. For example, you have more than one Chef process running, and you may need to address an issue with how that service is configured. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Create an infrastructure process running condition To define the Process running alert criteria: Follow standard procedures to create an infrastructure alert condition. Select Process running as the Alert type. Filter what hosts and processes you want the alert condition to apply to. Define the Critical threshold for triggering the alert notification: minimum 1 minute, default 5 minutes, maximum 60 minutes. If you create the alert condition directly with infrastructure monitoring, New Relic will send an email notification when the defined threshold for the alert condition passes depending on the policy's incident preferences. Your alert policy defines which personnel or teams and which notification channels we use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.63676,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alert</em> on <em>infrastructure</em> processes",
        "sections": "<em>Alert</em> on <em>infrastructure</em> processes",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use New Relic <em>infrastructure</em>&#x27;s Process running <em>alert</em> condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process you expected to start"
      },
      "id": "603eb49128ccbca939eba74a"
    },
    {
      "sections": [
        "Create infrastructure \"host not reporting\" condition",
        "Features",
        "Create \"host not reporting\" condition",
        "Investigate the problem",
        "Intentional outages"
      ],
      "title": "Create infrastructure \"host not reporting\" condition",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "0a74e7e65e3eeb5268eac310c11802ca2e78a614",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-alerts/infrastructure-alert-conditions/create-infrastructure-host-not-reporting-condition/",
      "published_at": "2021-05-05T03:39:35Z",
      "updated_at": "2021-03-16T08:30:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use Infrastructure monitoring's Host not reporting condition to notify you when we've stopped receiving data from an infrastructure agent. This feature allows you to dynamically alert on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of alerts notifications. Features You can define conditions based on the sets of hosts most important to you, and configure thresholds appropriate for each filter set. The Host not reporting event triggers when data from the infrastructure agent doesn't reach our collector within the time frame you specify. This feature's flexibility allows you to easily customize what to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Host not reporting condition Features What to monitor You can use filter sets to select which hosts you want to be monitored with the alert condition. The condition will also automatically apply to any hosts you add in the future that match these filters. How to notify Conditions are contained in policies. You can select an existing policy or create a new policy with email notifications from the Infrastructure monitoring UI. If you want to create a new policy with other types of notification channels, use the UI. When to notify Email addresses (identified in the policy) will be notified automatically about threshold violations for any host matching the filters you have applied, depending on the policy's incident preferences. Where to troubleshoot The link at the top of the email notification will take you to the infrastructure Events page centered on the time when the host disconnected. Additional links in the email will take you to additional detail. Create \"host not reporting\" condition To define the Host not reporting condition criteria: Follow standard procedures to create an infrastructure condition. Select Host not reporting as the Alert type. Define the Critical threshold for triggering the notification: minimum 5 minutes, maximum 60 minutes. Enable 'Don't trigger alerts for hosts that perform a clean shutdown' option, if you want to prevent false alerts when you have hosts set to shut down via command line. Currently this feature is supported on all Windows systems and Linux systems using systemd. Depending on the policy's incident preferences, it will define which notification channels to use when the defined Critical threshold for the condition passes. To avoid \"false positives,\" the host must stop reporting for the entire time period before a violation is opened. Example: You create a condition to open a violation when any of the filtered set of hosts stop reporting data for seven minutes. If any host stops reporting for five minutes, then resumes reporting, the condition does not open a violation. If any host stops reporting for seven minutes, even if the others are fine, the condition does open a violation. Investigate the problem To further investigate why a host is not reporting data: Review the details in the email notification. Use the link from the email notification to monitor ongoing changes in your environment from Infrastructure monitoring's Events page. For example, use the Events page to help determine if a host disconnected right after a root user made a configuration change to the host. Optional: Use the email notification's Acknowledge link to verify you are aware of and taking ownership of the alerting incident. Use the email links to examine additional details in the Incident details page. Intentional outages We can distinguish between unexpected situations and planned situations with the option Don't trigger alerts for hosts that perform a clean shutdown. Use this option for situations such as: Host has been taken offline intentionally. Host has planned downtime for maintenance. Host has been shut down or decommissioned. Autoscaling hosts or shutting down instances in a cloud console. We rely on Linux and Windows shutdown signals to flag a clean shutdown. We've confirmed that these scenarios are detected by the agent: AWS Auto-scaling event with EC2 instances that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) User-initiated shutdown of Windows systems User-initiated shutdown of Linux systems that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) We know that these scenarios are not detected by the agent: User-initiated shutdown of Linux systems that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other modern Linux systems that still use Upstart or SysV init systems. AWS Auto-scaling event with EC2 instances that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other more modern Linux systems that still use Upstart or SysV init systems.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.63498,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "sections": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use <em>Infrastructure</em> monitoring&#x27;s Host not reporting condition to notify you when we&#x27;ve stopped receiving data from an <em>infrastructure</em> agent. This feature allows you to dynamically <em>alert</em> on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of <em>alerts</em>"
      },
      "id": "603ea06c196a67cd47a83dc1"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/infrastructure-alerts-add-edit-or-view-host-alert-information": [
    {
      "sections": [
        "REST API calls for infrastructure alerts",
        "Requirements",
        "Tip",
        "Using infrastructure API calls",
        "GET infrastructure conditions",
        "GET a list of infrastructure conditions",
        "Example GET a list of conditions",
        "GET a specific infrastructure condition",
        "Example GET a specific condition",
        "Create (POST) an infrastructure condition",
        "Important",
        "Update (PUT) an infrastructure condition",
        "Example update (PUT) a condition",
        "Remove (DELETE) an infrastructure condition",
        "Types of conditions",
        "Process running conditions API data",
        "Example condition types",
        "Metric conditions API data",
        "Example",
        "Host not reporting condition",
        "Definitions",
        "value",
        "duration_minutes",
        "time_function"
      ],
      "title": "REST API calls for infrastructure alerts",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "c35aa43cdb6645473d02886a49d6f9aeb37e577f",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/rest-api-calls-new-relic-infrastructure-alerts/",
      "published_at": "2021-05-04T17:32:15Z",
      "updated_at": "2021-04-06T06:04:31Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the infrastructure REST API to add, update, delete, and list alerting conditions. You can also manage individual alerting conditions using the infrastructure monitoring UI. REST API calls for infrastructure alerts are not available in the API Explorer. Why use the API Examples Consistency Define the same set of conditions for every cluster without having to set up identical conditions in the Infrastructure monitoring UI each time. Manage multiple conditions quickly, without having to update them one by one using the UI. Flexibility Create conditions for an arbitrary group of hosts. Disable or delete conditions for hosts taken offline anytime. Create a condition with exclusionary filtering (for instance, environment NOT LIKE x). For more on this, see this post on exclusion filtering. For AWS Cloud integrations, select attributes that haven't been sent up by AWS yet. Create compound alert conditions by using the where_clause, which allows you to specify the limits on a secondary or tertiary metric. Exceed the 500-facet limitation on NRQL alert conditions. Reliability Audit when a condition was last updated. Requirements In order to use the Infrastructure REST API, you need: An API key The alerting condition's related policy_id from New Relic, available via GET list of conditions or via the Alerts REST API The condition id, available via GET list of conditions, or via the condition's URL in the Infrastructure monitoring UI Tip If your account hosts data in the EU data center, make sure you are using the proper API endpoints for EU region accounts. Using infrastructure API calls Here are some basic cURL commands and their responses for Infrastructure alert conditions. Depending on the type of condition, the DATA information you provide in the call will vary for POST (add) and PUT (update) calls. Definitions of each attribute used in the data blocks can be found in the Definitions section. GET infrastructure conditions You can either GET a list of infrastructure conditions or GET a specific infrastructure condition. Here are a few tips for listing infrastructure conditions. For pagination, use limit (records per page) and offset (how many records to skip) parameters. Default is 50 records per page, and offset starts at 0 (skip no records). To scope the results to a specific policy, use policy_id. Tip If you want to use the GET response as a template for your PUT or POST input, be sure to remove the created_at_epoch_millis, updated_at_epoch_millis and id information. GET a list of infrastructure conditions curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111\" Copy Example GET a list of conditions Response showing 2 of the 3 conditions for the example policy (formatted for readability and truncated): HTTP/1.1 200 OK Content-Length: 622 Content-Type: application/json { \"data\":[ { \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(`hostname` LIKE '%cassandra%')\", \"id\":13890, \"created_at_epoch_millis\":1490996713872, \"updated_at_epoch_millis\":1490996713872, \"policy_id\":111111, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(`commandName` = 'java')\" }, { \"created_at_epoch_millis\": 1501704525462, \"critical_threshold\": { \"duration_minutes\": 5 }, \"enabled\": true, \"filter\": { \"and\": [ { \"like\": { \"fullHostname\": \"Production_1\" } } ] }, \"id\": 448036, \"name\": \"PROD - Host Machine's Agent Not Responding ....\", \"policy_id\": 98485, \"type\": \"infra_host_not_reporting\", \"updated_at_epoch_millis\": 1504879191220 } . . . ], \"meta\":{ \"limit\":50, \"offset\":0, \"total\":3 }, \"links\":{} } Copy To get a list of the 10 Infrastructure conditions beyond the 50 limit: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions?policy_id=111111&offset=50&list=10\" Copy GET a specific infrastructure condition To get information about a single Infrastructure condition: curl -v -X GET --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition-id\" Copy Example GET a specific condition Response (formatted for readability): HTTP/1.1 200 OK Content-Length: 246 Content-Type: application/json { \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"demo condition\", \"enabled\":false, \"id\":13887, \"created_at_epoch_millis\":1490981583580, \"updated_at_epoch_millis\":1490981583580, \"policy_id\":23635, \"critical_threshold\":{ \"duration_minutes\":100 } } } Copy Create (POST) an infrastructure condition Important Do not include an \"id\": when adding a new condition (POST). It will be generated when the condition is created. To add an infrastructure condition, use this basic cURL command: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are adding: Process running conditions API data Metric conditions API data Host not reporting conditions API data Update (PUT) an infrastructure condition You only need to include the fields that need to be changed when updating an infrastructure condition. The API keeps the existing values for any missing fields. Important If you want to change the condition type, do not use PUT. Instead, delete the existing condition, then add (POST) a new condition with the new condition type and all fields. To update an infrastructure condition, use this basic cURL command. To indicate which condition is to be updated, be sure to include the \"id\": . Example update (PUT) a condition curl -X PUT 'https://infra-api.newrelic.com/v2/alerts/conditions/condition-id' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{\"data\":{DATA object details}}' Copy Include details in the DATA object (-d \\ section) for the type of infrastructure condition you are updating: Process running conditions API data Metric conditions API data Host not reporting conditions API data Remove (DELETE) an infrastructure condition To delete an infrastructure condition, use this basic cURL command: curl -v -X DELETE --header \"Api-Key: $API_KEY\" \"https://infra-api.newrelic.com/v2/alerts/conditions/condition_id\" Copy Types of conditions Process running conditions API data A process running condition alerts you when the number of processes is above, below, or equal to the threshold you define. To add (POST) or update (PUT) a process running condition, use your API key, and refer to the definitions to customize your values in the API call. Example condition types For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_process_running\", \"name\":\"Java is running\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"comparison\":\"equal\", \"critical_threshold\":{ \"value\":0, \"duration_minutes\":6 }, \"process_where_clause\":\"(commandName = '\\''java'\\'')\" } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause and process_where_clause Metric conditions API data A metric condition alerts you when the metric of your choice is above, below, or equal to the threshold you define. This includes: System metrics Process metrics Network metrics Storage metrics Cloud integration metrics To add (POST) or update (PUT) a metric condition, use your API key, and refer to the definitions to customize your values in the API call. If you are adding or updating a cloud integration alert condition: For the event_type field, enter the event type generated by your selected cloud integration service (for example, ComputeSample for the AWS EC2 integration). If you are setting up an alert condition on a cloud integration service that requires a provider value (for example, AWS RDS uses DatastoreSample with a provider value of RdsDbInstance or RdsDbCluster), you will need to add the \"integration_provider\" field and use the value that is appropriate for the service your alert condition is targeting (for example, \"integration_provider\":\"RdsDbInstance\"). For the select_value field, build the metric name by using the following syntax, where provider is a standard prefix string: provider.metric.aggregation_type Copy metric: Use the metric name as described in the New Relic documentation for your integration. aggregation_type: Use Sum, Average, Minimum, or Maximum. Refer to the original documentation by the integration's cloud provider to see which statistic aggregations are available for each metric. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_metric\", \"name\":\"Disk Space Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"event_type\":\"StorageSample\", \"select_value\":\"diskFreePercent\", \"comparison\":\"below\", \"critical_threshold\":{ \"value\":10, \"duration_minutes\":1, \"time_function\":\"any\" }, \"warning_threshold\":{ \"value\":30, \"duration_minutes\":2, \"time_function\":\"any\" } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Host not reporting condition A host not reporting condition alerts you when a host stops reporting. To add (POST) or update (PUT) a host not reporting condition, use your API key, and refer to the definitions to customize your values in the API call. Example For example: curl -X POST 'https://infra-api.newrelic.com/v2/alerts/conditions' -H 'Api-Key:$API_KEY' -i -H 'Content-Type: application/json' -d '{ \"data\":{ \"type\":\"infra_host_not_reporting\", \"name\":\"Cassandra Host Reporting Condition\", \"enabled\":true, \"where_clause\":\"(hostname LIKE '\\''%cassandra%'\\'')\", \"policy_id\":policy_id, \"critical_threshold\":{ \"duration_minutes\":12 } } }' Copy Important Note the extra single quotes escaping the single quote around the where_clause Definitions When formatting your cURL commands, use these values as needed. These are listed in alphabetical order, not the order they appear in your API calls. Field Definition comparison (enum) Condition type: infra_metric, infra_process_running The value used to define the threshold; for example, \"[\"above\", \"below\", \"equal\"]. critical_threshold and warning_threshold Condition type: all This object identifies the threshold value before opening a violation. The critical_threshold is required. The warning_threshold is optional and may only be used with infra_metric conditions. The keys of this object depend on the condition type. Condition type: infra_metric format: \"critical_threshold\":{ \"value\":<number>, \"duration_minutes\":<integer>, \"time_function\":\"any\" or \"all\" }, Copy Condition type: infra_process_running format: \"critical_threshold\":{ \"value\":<integer>, \"duration_minutes\":<integer>, }, Copy Condition type: infra_host_not_reporting format: \"critical_threshold\":{ \"duration_minutes\":<integer>, }, Copy value The numeric value that must be breached for the condition to open a violation duration_minutes The number of minutes the value must be passed or met for the condition to open a violation time_function Indicates if the condition needs to be sustained for a certain period of time to create a violation, or if it only needs to break the threshold once within a certain period of time. If you're setting up a for at least x minutes threshold, use all; for an at least once in x minutes threshold, use any. enabled (boolean) Condition type: all Whether the condition is turned on or off; true or false. event_type (string) Condition type: infra_metric The metric event; for example, system metrics, process metrics, storage metrics, or network metrics. This automatically populates for infrastructure integrations; for example, StorageSample or SystemSample. filter (string) Condition type: all If the condition was made in the UI, filter appears instead of where_clause; for example: {and: [{is: {ec2InstanceType: \"m3.medium\"}}]} Copy Recommendation: Use where_clause when creating a new condition. id (integer) Condition type: all The condition ID located in the URL. GET: This value appears in the GET response. PUT: Include this value in the DATA section. POST: Do not include this in the DATA section. DELETE: Include this value in the -X DELETE call. integration_provider (string) Condition type: infra_metric For alerts on integrations, use integration_provider instead of event_type. To see valid values: From the New Relic documentation for your cloud service, check the Find and use data section. Example: In the AWS RDS monitoring integration documentation, you can see that the DatastoreSample event type can be used with an integration_provider value of either RdsDbInstance for DB instances, or RdsDbCluster for Aurora DB clusters. name (string) Condition type: all The infrastructure alerting condition's name; for example: \"[test] process running\" Copy policy_id (integer) Condition type: all The unique ID for the alert policy's account ID associated with the condition; for example, 1234567890. This is not the policy's global ID. process_where_clause (string) Condition type: infra_process_running Any filters applied to processes, specifically in process running alert conditions. This parameter is mandatory for those types of alert conditions. For example: \"commandName = '\\''java'\\''\" Copy runbook_url (string) Condition type: all The runbook URL to display in notifications. select_value (string) Condition type: infra_metric The attribute name to identify the metric being targeted; for example, \"cpuPercent\", \"diskFreePercent\", \"memoryResidentSizeBytes\", or \"memoryFreeBytes/memoryTotalBytes*100\". This automatically populates for Infrastructure Integrations; for example, diskFreePercent. type (enum) Condition type: all The type of infrastructure alert condition: \"infra_process_running\", \"infra_metric\", or \"infra_host_not_reporting\". violation_close_timer (integer) Condition type: all The Violation time limit setting, expressed as hours. Possible values are 0, 1, 2, 4, 8,12, 24, 48, 72. This determines how much time will pass before a violation is automatically closed. Setting the time limit to 0 prevents a violation from being force-closed. For new conditions, if a value is not provided, the following default values are used: Host Not Responding (HNR) conditions: 0 (disabled) All other conditions: 24 When updating existing conditions, if a value is provided, it overrides the existing value, but does not affect already opened violations. where_clause (string) Condition type: all If applicable, this identifies any infrastructure host filters used; for example: \"(`hostname` LIKE '\\''%cassandra%'\\'')\", Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 223.77551,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "sections": "REST API calls for <em>infrastructure</em> <em>alerts</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use the <em>infrastructure</em> REST API to add, update, delete, and list alerting <em>conditions</em>. You can also manage individual alerting <em>conditions</em> using the <em>infrastructure</em> monitoring UI. REST API calls for <em>infrastructure</em> <em>alerts</em> are not available in the API Explorer. Why use the API Examples Consistency"
      },
      "id": "6043fa6c196a678ae2960f31"
    },
    {
      "sections": [
        "Alert on infrastructure processes",
        "Important",
        "Examples",
        "Ensure enough processes are running to satisfy load",
        "Ensure that critical services run constantly",
        "Monitor startup for critical processes that require special attention",
        "Make sure a job doesn't take too long",
        "Validate that services started successfully",
        "Watch for runaway processes or configuration problems",
        "Create an infrastructure process running condition"
      ],
      "title": "Alert on infrastructure processes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "5fcbe11b9beb16723ff2521fca981f19a4c716ce",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/alert-infrastructure-processes/",
      "published_at": "2021-05-05T03:39:35Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use New Relic infrastructure's Process running alert condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process you expected to start on a host (such as a new program) is not actually running This feature's flexibility allows you to easily filter what hosts and processes to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Examples By applying filters to the hosts and processes that are important to your business, you can define alerting thresholds to decide when violations open and New Relic sends an email notification to you depending on the policy's incident preferences. These examples illustrate how to use infrastructure monitoring's Process running condition to monitor your processes. Ensure enough processes are running to satisfy load Problem: Some load balancers and application servers work by running many worker processes in parallel. Here, for example, you may want an alert violation when fewer than eight processes are running for a service like gunicorn. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Ensure that critical services run constantly Problem: A service, such as a database or application server, is expected to be running constantly on certain hosts, and you need to know when it has stopped. Solution: Use the No processes are running (default) threshold. Monitor startup for critical processes that require special attention Problem: You have processes requiring special attention due to security or potential performance impact. Solution: Use the At least one process is running threshold with condition filters set to a username and specific executable so that New Relic can open a violation when the process is running. Make sure a job doesn't take too long Problem: You have a job that runs periodically, and you want to open a violation when it has been running longer than an expected number of minutes. Solution: Use the At least one process is running threshold. Validate that services started successfully Problem: When provisioning new hosts, you want to open a violation if a required service fails to successfully start up. Solution: Use the No processes are running (default) threshold. Watch for runaway processes or configuration problems Problem: Sometimes problems with processes can be solved with changes to your configuration. For example, you have more than one Chef process running, and you may need to address an issue with how that service is configured. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Create an infrastructure process running condition To define the Process running alert criteria: Follow standard procedures to create an infrastructure alert condition. Select Process running as the Alert type. Filter what hosts and processes you want the alert condition to apply to. Define the Critical threshold for triggering the alert notification: minimum 1 minute, default 5 minutes, maximum 60 minutes. If you create the alert condition directly with infrastructure monitoring, New Relic will send an email notification when the defined threshold for the alert condition passes depending on the policy's incident preferences. Your alert policy defines which personnel or teams and which notification channels we use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.63675,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alert</em> on <em>infrastructure</em> processes",
        "sections": "<em>Alert</em> on <em>infrastructure</em> processes",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use New Relic <em>infrastructure</em>&#x27;s Process running <em>alert</em> condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process you expected to start"
      },
      "id": "603eb49128ccbca939eba74a"
    },
    {
      "sections": [
        "Create infrastructure \"host not reporting\" condition",
        "Features",
        "Create \"host not reporting\" condition",
        "Investigate the problem",
        "Intentional outages"
      ],
      "title": "Create infrastructure \"host not reporting\" condition",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "0a74e7e65e3eeb5268eac310c11802ca2e78a614",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-alerts/infrastructure-alert-conditions/create-infrastructure-host-not-reporting-condition/",
      "published_at": "2021-05-05T03:39:35Z",
      "updated_at": "2021-03-16T08:30:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use Infrastructure monitoring's Host not reporting condition to notify you when we've stopped receiving data from an infrastructure agent. This feature allows you to dynamically alert on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of alerts notifications. Features You can define conditions based on the sets of hosts most important to you, and configure thresholds appropriate for each filter set. The Host not reporting event triggers when data from the infrastructure agent doesn't reach our collector within the time frame you specify. This feature's flexibility allows you to easily customize what to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Host not reporting condition Features What to monitor You can use filter sets to select which hosts you want to be monitored with the alert condition. The condition will also automatically apply to any hosts you add in the future that match these filters. How to notify Conditions are contained in policies. You can select an existing policy or create a new policy with email notifications from the Infrastructure monitoring UI. If you want to create a new policy with other types of notification channels, use the UI. When to notify Email addresses (identified in the policy) will be notified automatically about threshold violations for any host matching the filters you have applied, depending on the policy's incident preferences. Where to troubleshoot The link at the top of the email notification will take you to the infrastructure Events page centered on the time when the host disconnected. Additional links in the email will take you to additional detail. Create \"host not reporting\" condition To define the Host not reporting condition criteria: Follow standard procedures to create an infrastructure condition. Select Host not reporting as the Alert type. Define the Critical threshold for triggering the notification: minimum 5 minutes, maximum 60 minutes. Enable 'Don't trigger alerts for hosts that perform a clean shutdown' option, if you want to prevent false alerts when you have hosts set to shut down via command line. Currently this feature is supported on all Windows systems and Linux systems using systemd. Depending on the policy's incident preferences, it will define which notification channels to use when the defined Critical threshold for the condition passes. To avoid \"false positives,\" the host must stop reporting for the entire time period before a violation is opened. Example: You create a condition to open a violation when any of the filtered set of hosts stop reporting data for seven minutes. If any host stops reporting for five minutes, then resumes reporting, the condition does not open a violation. If any host stops reporting for seven minutes, even if the others are fine, the condition does open a violation. Investigate the problem To further investigate why a host is not reporting data: Review the details in the email notification. Use the link from the email notification to monitor ongoing changes in your environment from Infrastructure monitoring's Events page. For example, use the Events page to help determine if a host disconnected right after a root user made a configuration change to the host. Optional: Use the email notification's Acknowledge link to verify you are aware of and taking ownership of the alerting incident. Use the email links to examine additional details in the Incident details page. Intentional outages We can distinguish between unexpected situations and planned situations with the option Don't trigger alerts for hosts that perform a clean shutdown. Use this option for situations such as: Host has been taken offline intentionally. Host has planned downtime for maintenance. Host has been shut down or decommissioned. Autoscaling hosts or shutting down instances in a cloud console. We rely on Linux and Windows shutdown signals to flag a clean shutdown. We've confirmed that these scenarios are detected by the agent: AWS Auto-scaling event with EC2 instances that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) User-initiated shutdown of Windows systems User-initiated shutdown of Linux systems that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) We know that these scenarios are not detected by the agent: User-initiated shutdown of Linux systems that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other modern Linux systems that still use Upstart or SysV init systems. AWS Auto-scaling event with EC2 instances that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other more modern Linux systems that still use Upstart or SysV init systems.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.63498,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "sections": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use <em>Infrastructure</em> monitoring&#x27;s Host not reporting condition to notify you when we&#x27;ve stopped receiving data from an <em>infrastructure</em> agent. This feature allows you to dynamically <em>alert</em> on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of <em>alerts</em>"
      },
      "id": "603ea06c196a67cd47a83dc1"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/rest-api-calls-new-relic-infrastructure-alerts": [
    {
      "sections": [
        "Alert on infrastructure processes",
        "Important",
        "Examples",
        "Ensure enough processes are running to satisfy load",
        "Ensure that critical services run constantly",
        "Monitor startup for critical processes that require special attention",
        "Make sure a job doesn't take too long",
        "Validate that services started successfully",
        "Watch for runaway processes or configuration problems",
        "Create an infrastructure process running condition"
      ],
      "title": "Alert on infrastructure processes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "5fcbe11b9beb16723ff2521fca981f19a4c716ce",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/alert-infrastructure-processes/",
      "published_at": "2021-05-05T03:39:35Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use New Relic infrastructure's Process running alert condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process you expected to start on a host (such as a new program) is not actually running This feature's flexibility allows you to easily filter what hosts and processes to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Examples By applying filters to the hosts and processes that are important to your business, you can define alerting thresholds to decide when violations open and New Relic sends an email notification to you depending on the policy's incident preferences. These examples illustrate how to use infrastructure monitoring's Process running condition to monitor your processes. Ensure enough processes are running to satisfy load Problem: Some load balancers and application servers work by running many worker processes in parallel. Here, for example, you may want an alert violation when fewer than eight processes are running for a service like gunicorn. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Ensure that critical services run constantly Problem: A service, such as a database or application server, is expected to be running constantly on certain hosts, and you need to know when it has stopped. Solution: Use the No processes are running (default) threshold. Monitor startup for critical processes that require special attention Problem: You have processes requiring special attention due to security or potential performance impact. Solution: Use the At least one process is running threshold with condition filters set to a username and specific executable so that New Relic can open a violation when the process is running. Make sure a job doesn't take too long Problem: You have a job that runs periodically, and you want to open a violation when it has been running longer than an expected number of minutes. Solution: Use the At least one process is running threshold. Validate that services started successfully Problem: When provisioning new hosts, you want to open a violation if a required service fails to successfully start up. Solution: Use the No processes are running (default) threshold. Watch for runaway processes or configuration problems Problem: Sometimes problems with processes can be solved with changes to your configuration. For example, you have more than one Chef process running, and you may need to address an issue with how that service is configured. Solution: Depending on the situation, use any of these Process running thresholds options as needed: More than the defined number of processes are running Exactly the defined number of processes are running Fewer than the defined number of processes are running Create an infrastructure process running condition To define the Process running alert criteria: Follow standard procedures to create an infrastructure alert condition. Select Process running as the Alert type. Filter what hosts and processes you want the alert condition to apply to. Define the Critical threshold for triggering the alert notification: minimum 1 minute, default 5 minutes, maximum 60 minutes. If you create the alert condition directly with infrastructure monitoring, New Relic will send an email notification when the defined threshold for the alert condition passes depending on the policy's incident preferences. Your alert policy defines which personnel or teams and which notification channels we use.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.63675,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Alert</em> on <em>infrastructure</em> processes",
        "sections": "<em>Alert</em> on <em>infrastructure</em> processes",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use New Relic <em>infrastructure</em>&#x27;s Process running <em>alert</em> condition to be notified when a set of processes on your filtered hosts stop running for a configurable number of minutes. This is useful, for example, when: Any of the processes on the hosts stop reporting A process you expected to start"
      },
      "id": "603eb49128ccbca939eba74a"
    },
    {
      "sections": [
        "Create infrastructure \"host not reporting\" condition",
        "Features",
        "Create \"host not reporting\" condition",
        "Investigate the problem",
        "Intentional outages"
      ],
      "title": "Create infrastructure \"host not reporting\" condition",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "0a74e7e65e3eeb5268eac310c11802ca2e78a614",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-alerts/infrastructure-alert-conditions/create-infrastructure-host-not-reporting-condition/",
      "published_at": "2021-05-05T03:39:35Z",
      "updated_at": "2021-03-16T08:30:13Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use Infrastructure monitoring's Host not reporting condition to notify you when we've stopped receiving data from an infrastructure agent. This feature allows you to dynamically alert on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of alerts notifications. Features You can define conditions based on the sets of hosts most important to you, and configure thresholds appropriate for each filter set. The Host not reporting event triggers when data from the infrastructure agent doesn't reach our collector within the time frame you specify. This feature's flexibility allows you to easily customize what to monitor and when to notify selected individuals or teams. In addition, the email notification includes links to help you quickly troubleshoot the situation. Host not reporting condition Features What to monitor You can use filter sets to select which hosts you want to be monitored with the alert condition. The condition will also automatically apply to any hosts you add in the future that match these filters. How to notify Conditions are contained in policies. You can select an existing policy or create a new policy with email notifications from the Infrastructure monitoring UI. If you want to create a new policy with other types of notification channels, use the UI. When to notify Email addresses (identified in the policy) will be notified automatically about threshold violations for any host matching the filters you have applied, depending on the policy's incident preferences. Where to troubleshoot The link at the top of the email notification will take you to the infrastructure Events page centered on the time when the host disconnected. Additional links in the email will take you to additional detail. Create \"host not reporting\" condition To define the Host not reporting condition criteria: Follow standard procedures to create an infrastructure condition. Select Host not reporting as the Alert type. Define the Critical threshold for triggering the notification: minimum 5 minutes, maximum 60 minutes. Enable 'Don't trigger alerts for hosts that perform a clean shutdown' option, if you want to prevent false alerts when you have hosts set to shut down via command line. Currently this feature is supported on all Windows systems and Linux systems using systemd. Depending on the policy's incident preferences, it will define which notification channels to use when the defined Critical threshold for the condition passes. To avoid \"false positives,\" the host must stop reporting for the entire time period before a violation is opened. Example: You create a condition to open a violation when any of the filtered set of hosts stop reporting data for seven minutes. If any host stops reporting for five minutes, then resumes reporting, the condition does not open a violation. If any host stops reporting for seven minutes, even if the others are fine, the condition does open a violation. Investigate the problem To further investigate why a host is not reporting data: Review the details in the email notification. Use the link from the email notification to monitor ongoing changes in your environment from Infrastructure monitoring's Events page. For example, use the Events page to help determine if a host disconnected right after a root user made a configuration change to the host. Optional: Use the email notification's Acknowledge link to verify you are aware of and taking ownership of the alerting incident. Use the email links to examine additional details in the Incident details page. Intentional outages We can distinguish between unexpected situations and planned situations with the option Don't trigger alerts for hosts that perform a clean shutdown. Use this option for situations such as: Host has been taken offline intentionally. Host has planned downtime for maintenance. Host has been shut down or decommissioned. Autoscaling hosts or shutting down instances in a cloud console. We rely on Linux and Windows shutdown signals to flag a clean shutdown. We've confirmed that these scenarios are detected by the agent: AWS Auto-scaling event with EC2 instances that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) User-initiated shutdown of Windows systems User-initiated shutdown of Linux systems that use systemd (Amazon Linux, CentOs/RedHat 7 and newer, Ubuntu 16 and newer, Suse 12 and newer, Debian 9 and newer) We know that these scenarios are not detected by the agent: User-initiated shutdown of Linux systems that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other modern Linux systems that still use Upstart or SysV init systems. AWS Auto-scaling event with EC2 instances that don't use systemd (CentOs/RedHat 6 and earlier, Ubuntu 14, Debian 8). This includes other more modern Linux systems that still use Upstart or SysV init systems.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.63498,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "sections": "Create <em>infrastructure</em> &quot;host not reporting&quot; <em>condition</em>",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "Use <em>Infrastructure</em> monitoring&#x27;s Host not reporting condition to notify you when we&#x27;ve stopped receiving data from an <em>infrastructure</em> agent. This feature allows you to dynamically <em>alert</em> on groups of hosts, configure the time window from five to 60 minutes, and take full advantage of <em>alerts</em>"
      },
      "id": "603ea06c196a67cd47a83dc1"
    },
    {
      "sections": [
        "Infrastructure alerting examples",
        "Examples: Infrastructure pages",
        "Examples: Threshold options",
        "Integrations providers",
        "CPU, disk, load average, memory, swap",
        "Byte size"
      ],
      "title": "Infrastructure alerting examples",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure alerts",
        "Infrastructure alert conditions"
      ],
      "external_id": "1ec5f86b745413b2a8d6a5b676ecbe622c674ab1",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-alert-conditions/infrastructure-alerting-examples/",
      "published_at": "2021-05-05T03:39:35Z",
      "updated_at": "2021-03-16T07:35:33Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Alert type field in infrastructure monitoring's Settings > Alerts page shows what options you can select to create infrastructure alert conditions. You can also create alert conditions from any infrastructure chart by selecting the ellipses icon and then Create alert. Examples: Infrastructure pages Here are some examples of how to create alert conditions within the context of the Infrastructure monitoring UI page you are currently viewing. To create an alerts condition from any chart, select the ellipses icon and then Create alert. New Relic will automatically select the appropriate Alert type. Example Problem and solution High CPU usage Problem: Your Ops team monitors a filtered set of host clusters in your eastern region and notices that the CPU usage is constantly high. Solution: Use the CPU chart on Infrastructure monitoring's Hosts page to create an alert condition for system metrics. Virtual memory capacity Problem: Your night shift needs to be alerted when virtual memory for a set of background workers reaches an average of 10G for at least two minutes. Solution: Use the Top memory consumers chart on Infrastructure monitoring's Processes page to create an alert condition for process metrics. Limited bandwidth Problem: You want to monitor performance based on the average number of errors received or transmitted. Solution: Use the Top bandwidth chart on Infrastructure monitoring's Network page to create an alert condition for network metrics. I/O read and write operations Problem: You are testing a new set of hosts in your staging environment, and you want to be notified when their read or write capacity rises above your test threshold level. Solution: Use the Top I/O operations chart on Infrastructure monitoring's Storage page to create an alert condition for storage metrics. Host not reporting Problem: You want to be notified when we have stopped receiving data from an infrastructure agent. Solution: From the Hosts, Processes, Network, or Storage pages, create a host not reporting alert condition. Processes not running as expected Problem: You want to be notified if any of the processes on your hosts stop reporting. OR A process you expected to start on a host (such as a new program) is not actually running. Solution: From the Processes page (or from the Hosts, Network, or Storage pages), create a process running alert condition. Examples: Threshold options Use the thresholds dropdown for the selected Alert type to further define how you want to be alerted. Here are some examples of the options available. Integrations providers With infrastructure integrations, you can create an alert condition from your Integrations page. Depending on the type of provider selected (CloudFront, DynamoDB, EBS, etc.), options will vary from the Define thresholds dropdown; for example, bytes, errors, requests, CPU, connections, memory, records, latency, etc. CPU, disk, load average, memory, swap The System metrics thresholds dropdown allows you to select various criteria for CPU, disk, load average, memory, and swap metrics. Byte size The Network metrics thresholds provide flexibility with your business needs. Depending on the size of your network, you can easily set the threshold in bytes, KB, MB, GB, or TB.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 206.6147,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>alerting</em> examples",
        "sections": "<em>Infrastructure</em> <em>alerting</em> examples",
        "tags": "<em>Infrastructure</em> <em>alert</em> <em>conditions</em>",
        "body": "The <em>Alert</em> type field in <em>infrastructure</em> monitoring&#x27;s Settings &gt; <em>Alerts</em> page shows what options you can select to create <em>infrastructure</em> <em>alert</em> <em>conditions</em>. You can also create <em>alert</em> <em>conditions</em> from any <em>infrastructure</em> chart by selecting the ellipses icon and then Create <em>alert</em>. Examples: <em>Infrastructure</em>"
      },
      "id": "603eb52a28ccbc9027eba7bb"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/events-heatmap-examine-patterns-time-range": [
    {
      "sections": [
        "Infrastructure Inventory page: Search your entire infrastructure",
        "Inventory item naming",
        "Tip",
        "Page functions",
        "Filter the data",
        "Search inventory",
        "View inventory item details",
        "View host's alert threshold violations",
        "Inventory data collection",
        "Linux built-in agent data",
        "Windows built-in agent data",
        "Amazon AWS cloud integrations inventory",
        "Inventory data retention",
        "Chart data attributes"
      ],
      "title": "Infrastructure Inventory page: Search your entire infrastructure",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "64aef10b24b74ac3c0f070358d37f3cab099e5b2",
      "image": "https://docs.newrelic.com/static/2d17c192725956ff09b5e987be5b997b/747d8/inventory-name-source-path.jpg",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-ui-pages/infra-ui-pages/infrastructure-inventory-page-search-your-entire-infrastructure/",
      "published_at": "2021-05-05T03:40:44Z",
      "updated_at": "2021-03-11T12:47:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic can collect detailed information about a system's configuration per host, including system modules, configuration files, metadata, packages, services, user sessions, etc. The Inventory page provides a real-time, filterable, searchable view into each host's configuration. Use the Inventory page to: Ensure a version update was applied successfully across all your hosts. Audit version discrepancies across your hosts. Quickly identify which hosts require an update to fix a security vulnerability. To view and search your inventory data: Go to one.newrelic.com > Infrastructure > Inventory. Inventory item naming The infrastructure inventory is a qualified namespace (structured like a directory) that organizes inventory items into names that resemble a source path. The inventory item name is comprised of three elements: Element Description Category Basic, top level type of data source, typically based on its role in the system. Common examples include config, package, kernel, user session, services, and modules. Source The specific data source for the inventory item. Label The name of the specific inventory item; for example, the filename, package name, or system setting name. Tip For detailed metadata and other information about your hosts, use tagging with New Relic One. Page functions Use Inventory page functions to find information about a particular item on your hosts: Filter the data Use Filter Sets to show only hosts matching certain criteria. Search inventory Search for an inventory item using the search function. For example, if you want to find information related to OpenSSL, search openssl. The search term is matched again the inventory item name. View inventory item details Inventory item details provide host and system information for each host it resides on according to the New Relic inventory item name. If you have different versions of the same item on other hosts, New Relic detects that and flags them on the Inventory page with the variant hosts label and lists each host running each version. Item details are attributes (key/value pairs) that are dictated by their source. Specific attributes are generally stable over time, but new ones may be added and others could be deprecated. Attributes carry the critical metadata that are at the heart of each inventory item. Common inventory item attributes include: Variant hosts (hostname) Architecture Description Essential Priority Status Version View host's alert threshold violations To view one or more host's alert threshold violations, select the host's Critical icon or Warning icon. Inventory data collection Inventory is collected from the infrastructure agent's built-in data collectors, Amazon Elastic Compute Cloud (EC2) integrations, agent integrations provided by New Relic, and customer-built integrations. The data appears on the Inventory page and in other places within the Infrastructure monitoring's user interface. Linux built-in agent data The infrastructure agent collects this data for Linux systems. Category Source Data collected using... applications apm APM Language Agent metadata config selinux sestatus -b, semodule -l selinux-policies sestatus -b, semodule -l selinux-modules sestatus -b, semodule -l sshd /etc/sshd_config (PermitRootLogin, PermitEmptyPasswords, PasswordAuthentication, and ChallengeResponseAuthentication only) kernel modules /sbin/modinfo, /sbin/lsmod, /proc/modules sysctl /proc/sys metadata agent_config Agent's complete config file attributes Agent's custom_attributes host_aliases Agent's display_name, Cloud provider instance-id, os.Hostname(), hostname -f, hostname cloud_security_groups Cloud provider security-groups system uptime -s, /etc/redhat-release, /proc/cpuinfo, /etc/os-release, /proc/sys/kernel/random/boot_id, /proc/sys/kernel/osrelease, /sys/class/dmi/id/product_uuid, /sys/devices/virtual/dmi/id/sys_vendor, /sys/devices/virtual/dmi/id/product_name facter facter -p -j services daemontool ps -e, svstat systemd initctl list upstart systemctl -l, systemctl show, modinfo, lsmod supervisord /var/run/supervisor.sock unix socket connection, supervisor.getAllProcessInfo pidfile var/run, find -L -name, /proc/N/status, /proc/N/stat sessions users who system network_interfaces net.Interfaces() packages dpkg dpkg-query -W -f rpm rpm -qa Windows built-in agent data The infrastructure agent collects this data for Windows systems. Category Source Data collected using... applications apm APM language agent metadata metadata agent_config Agent's complete config file attributes Agent's custom_attributes host_aliases Agent's display_name, Cloud provider instance-id, os.Hostname(), Registry (SYSTEM \\ CurrentControlSet \\ Services \\ Tcpip \\ Parameters (Domain, DhcpDomain, Hostname) system kernel32.dll (GetPhysicallyInstalledSystemMemory), WMI (Win32_OperatingSystem, Win32_Processor), os.Hostname() services windows_services WMI (Win32_Service WHERE State = \"Running\" AND StartMode = \"Auto\") system network_interfaces net.Interfaces() packages windows_programs Registry (SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\, SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\) windows_updates WMI (Win32_QuickFixEngineering) (off by default) Amazon AWS cloud integrations inventory Data collected varies by Amazon Elastic Compute Cloud (EC2) integration. For more information, see New Relic's individual Amazon Integrations documentation. Inventory data retention Inventory data is real-time. If a host stops reporting, its inventory data still displays for up to 24 hours. Chart data attributes For a technical explanation about attributes used to populate the Inventory page, see Default infrastructure attributes and events. This includes a summary of common events by operating system.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.00931,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> Inventory page: Search your entire <em>infrastructure</em>",
        "sections": "<em>Infrastructure</em> Inventory page: Search your entire <em>infrastructure</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": ", Amazon Elastic Compute Cloud (EC2) integrations, agent integrations provided by New Relic, and customer-built integrations. The data appears on the Inventory page and in other places within the <em>Infrastructure</em> <em>monitoring</em>&#x27;s user interface. Linux built-in agent data The <em>infrastructure</em> agent collects"
      },
      "id": "60440a6d64441fdf50378ee7"
    },
    {
      "sections": [
        "Infrastructure Events page: Live feed of config changes",
        "Event types",
        "Events page features",
        "Chart data attributes"
      ],
      "title": "Infrastructure Events page: Live feed of config changes",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "e4a87670c8671072ae7cc6531721f46edc7f925d",
      "image": "https://docs.newrelic.com/static/75373d03d819516d3cbe23f1ea65957b/c1b63/infra-events-ui.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/infrastructure-events-page-live-feed-every-config-change/",
      "published_at": "2021-05-05T03:28:38Z",
      "updated_at": "2021-03-11T11:47:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "The Events page is a live feed of important system and host activity, including inventory change events, configuration changes, and log analytics events. The event feed helps you understand correlations between these events and system performance. Search and filter through your events to decrease the mean time to detect and repair infrastructure issues. You can access the Events page by going to one.newrelic.com > Infrastructure > Events. Event types New Relic collects a variety of change events so you can understand each change in your environment: Events Comments Alert incidents When a violation is opened or closed, New Relic generates an event indicating the host and associated alert condition. Agent connection When an infrastructure agent connects to New Relic, our platform generates an Agent connected event. If New Relic doesn't receive data from an agent for three minutes, the platform generates an Agent disconnected event. Inventory changes These events are generated when inventory data is added, removed, or modified. Select the source icon to to understand which category corresponds to the altered inventory path. For additional details, select an inventory event to see a side-by-side comparison of the old and new state. Inventory events can include: Kernel (includes modules and configuration): Can be added, modified, or deleted. Metadata (includes various additional information about hosts): Can be added, modified, or deleted. Packages: Can be installed, removed, or modified. Services: Can be started, stopped, or restarted. Sessions (includes users): Can be connected or disconnected. Events page features To view the live event feed: Go to one.newrelic.com > Infrastructure > E * *vents. The Events * * page includes a heatmap, which provides a snapshot of the events occurring within the selected time range. one.newrelic.com > Infrastructure > Events: Use the Events to view important, real-time activity in your infrastructure. With the Events page, you can easily search through your event log to quickly find vulnerable packages. If you want to... Do this... Focus on specific events Use the Search events field to look for specific events, config changes or agent installations. To focus on a specific set of events, select or change the filter set. Search within a particular time range Enter a time range to the right of the search bar to investigate events within a specific time range. For example, if you encountered a CPU spike around 11am on the previous day, search Yesterday at 11 am to investigate the possible cause. Compare events with host load, memory, CPU, and more View the events feed on the Hosts page. To compare infrastructure events and performance for a specific time, select a range via the time picker or drag and select a range on a chart. View events specifically related to agents, config, metadata, services, or sessions Group or sort events by selecting the filter icon be the search bar. Drill down into additional details Select an event to view additional details, such as attributes and values. To drill down further, select View in Inventory to see additional details in the Inventory page. View host's alert threshold violation Select the host's Critical icon or Warning icon. Chart data attributes For a technical explanation of the attributes used to populate the Events page, see InfrastructureEvent attributes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.99213,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> Events page: Live feed of config changes",
        "sections": "<em>Infrastructure</em> Events page: Live feed of config changes",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": " the mean time to detect and repair <em>infrastructure</em> issues. You can access the Events page by going to one.newrelic.com &gt; <em>Infrastructure</em> &gt; Events. Event types New Relic collects a variety of change events so you can understand each change in your environment: Events Comments Alert incidents When"
      },
      "id": "6043fa6c28ccbc13742c60a5"
    },
    {
      "sections": [
        "Infrastructure monitoring Hosts page",
        "System tab",
        "System tab functions",
        "APM and infrastructure data",
        "Important",
        "Network tab",
        "Network tab functions",
        "Processes tab",
        "Tip",
        "Processes tab functions",
        "Storage tab",
        "Storage tab functions",
        "Docker containers tab",
        "Docker containers tab functions"
      ],
      "title": "Infrastructure monitoring Hosts page",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "41d8a2ac3ecbbdee164fd0bec6ac94bb3e8def64",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-ui-pages/infrastructure-ui/infrastructure-hosts-page/",
      "published_at": "2021-05-05T00:33:11Z",
      "updated_at": "2021-03-09T04:04:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Hosts page to better understand important performance metrics, like resource usage, network and processes performance, and data about your containers. You can filter your hosts by attributes and compare their performance with inventory change events to help determine root causes. You can also set alert notifications to help prevent problems. To view your hosts information, go to one.newrelic.com > Infrastructure > Hosts, then click any of the following tabs: System: Overview of your hosts' performance Network: Bandwidth and error data about your network interfaces Processes: Data about CPU percentage, I/O bytes, and memory usage for individual or groups of processes Storage: Resources' capacity and efficiency, including your devices' overall utilization, disk usage, or I/O operations Docker containers: CPU percentage, I/O bytes, and memory usage for individual or group of containers System tab Here are the default graphs shown on the Hosts page. For additional chart options, select the dropdown for any chart. CPU %: On the System tab, CPU Percent is a derived metric that is part of the SystemSample event. The CPU percentage is not collected by New Relic, but derived from several other metrics. Specifically, the cpuPercent attribute is an aggregation of cpuUserPercent, cpuSystemPercent, cpuIoWaitPercent and cpuStealPercent. Load average five minute: represents an average of the number of tasks waiting to do work on a system over the last 5 minutes. Memory free %: compares the amount of free memory bytes to the amount of used memory bytes. For explanations of different chart metrics, see Default infrastructure attributes and events. Functions for adjusting chart data include: Select different host groups: Use the host filter. Change time range: Use the time picker above the charts, or drag and select a time range on a chart. Change the number of elements on charts: use the Chart shows dropdown. Change data used to order hosts: Use the table below the charts to order the chart elements by different metrics; for example, from highest to lowest CPU user % to lowest, or from lowest to highest Disk free. System tab functions Here are some of the things you can do from the System tab: If you want to... Do this... Filter and group hosts Use filter sets to show only hosts matching certain criteria, or use group by to aggregate the results. Understand host health Use the Health column of the table. To see more details about specific violations, select the health status icons. Find root causes of issues Use the Events heatmap at the top of the page to compare performance to important events happening in your infrastructure. For more, see Events heatmap. Set an alert condition for a metric Mouse over a chart, select and then Create alert. View host's alert threshold violation If present, select the host's Critical icon or Warning icon. APM and infrastructure data If you have APM data associated with your infrastructure data, there are several ways to access your APM data on the Hosts page: Use the hosts filter to filter on hosts running specific applications. In the host list below the charts, select the Application column to filter on specific applications. From the chart selector dropdown beside a chart's name, select one of the application-related charts. Important APM charts in infrastructure monitoring do not have View query or Create alert options like the other infrastructure charts do. For more about using APM and infrastructure monitoring together, see APM data in infrastructure. Network tab The Network page provides real-time visibility into the health and performance of individual hosts, web servers, or other groups of resources across your network. Default charts show bandwidth metrics by packet, bandwidth by bytes, and errors per second. Details about individual interfaces can help you: Examine saturation levels across your network or for selected filter sets. Compare load balances between specific resources. Identify unexpected differences in capacity patterns between similar hosts. Evaluate the top five network errors that New Relic automatically presents for the selected time period. This real-time network data can then help you determine whether to resolve errors by reconfiguring your network, rebalancing the loads, or taking other preventative maintenance actions before needing to make a purchase decision. From the Network tab you can view bandwidth and error data about your network interfaces. The Network page includes an Events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. For a technical explanation of the attributes used to populate the Network page chart, see NetworkSample attributes. Network tab functions Here are some of the things you can do from the Network tab: If you want to... Do this... Filter and group Use filter sets to show only hosts matching certain criteria, or use Group by to aggregate the results. Select a time range Use the time picker to change the range of time selected. You can also click and drag on the chart to select a custom time range. When you select a time range, it carries over when you go from one infrastructure page to another. Change metrics displayed in chart Use the sorting dropdown to switch what metric the chart is displaying for the chosen process groupings, including: Received Bytes Received Dropped Received Errors Received Packets Transmitted Bytes Transmitted Dropped Transmitted Errors Transmitted Packets Search and filter the list Type in the Search interfaces search bar to filter the list to only those items containing the text you've typed. You can also filter the list by simply selecting the list item or user name you want to filter for. The chosen filters are displayed beside the filter icon above the chart. Add items to chart The chart, by default, displays the top five results. Use the Chart shows dropdown to display more results. To remove an item from a chart, select its name below the chart. Set an alert condition for a metric Mouse over a chart, select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Processes tab Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Use the Processes tab to get information about processes running on your hosts, and to set alerts on process metrics. The Processes tab shows data such as CPU percentage, I/O bytes, and memory usage for individual processes or groupings of processes. The page also includes an events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. Use this information to: Pinpoint processes that are causing performance issues. Create alerts for conditions such as CPU percentage and memory usage. On the Processes page, CPU percent is scoped to individual processes, rather than hosts. Because of this, the CPU percent metric does not take into account the resources of the entire system. Instead, it shows how much of a single CPU core each process is taking. Example Here's an example of how to pinpoint an issue and set an alert: You notice on the Hosts page that a cluster has a CPU percentage much higher than normal. You go to the Processes page and filter down to that cluster's processes. You notice that several instances of the same process have excessive CPU percentage. After fixing the issue, you decide to create an alert condition that triggers when the CPU percentage for that process type exceeds a certain threshold. For a technical explanation of the attributes used to populate the Processes page chart, see ProcessSample attributes. Tip You cannot start, stop, or restart processes from the UI. Instead, follow standard procedures to start, stop, check, or restart the infrastructure agent manually. Processes tab functions Here are some of the things you can do from the Processes tab: If you want to... Do this... Only show hosts matching certain criteria Use filter sets. Aggregate results or group by host attributes Use Group by to change how the processes are grouped in the chart and the list. The dropdown contains host-related attributes such as hostID and entityName. If you are monitoring AWS EC2 instances, EC2-related attributes such as awsRegion will be available. Select a time range Use either of these options: Select any of the time picker options at the top of the page. Click and drag on the chart to select a custom time range. After you select a time range, it carries over from one infrastructure page to another. View Docker metrics To view process usage by Docker containers, see Docker instrumentation. Change charts Select the dropdown beside the chart's name to switch what metric the chart displays. Choices include: Memory: Resident or virtual size CPU percentage Disk I/O: Read or write Search and filter the list Use either of these options: Type in the Search processes search bar. Select the list item or user name you want to filter for. The selected filters appear beside the filter icon above the chart, where you can select and remove them as needed. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Storage tab The Storage page allows you to monitor your resources' capacity and efficiency, including your devices' overall utilization, disk usage, or I/O operations. This can help you to: Examine unexpected patterns; for example, a cluster of mirrored machines that do not process I/O tasks uniformly. Monitor usage levels before disk space completely runs out. Set alert conditions to notify you about problems with one or more hosts; for example, long processing times for read or write operations, disk availability or utilization based on percentage or bytes, etc. Make informed decisions about where to redistribute hosts with higher or lower than normal processing requests. Use data to help plan for additions or changes to your infrastructure budget before an emergency arises. The Storage page includes an Events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. For a technical explanation of the attributes used to populate the Storage page chart, see StorageSample attributes. Storage tab functions Here are some of the things you can do from the Storage tab: If you want to... Do this... Filter and group Use filter sets to show only hosts matching certain criteria, or use Group by to aggregate the results. Select a time range Use the time picker on the upper right to change the range of time selected. You can also click and drag on the chart to select a custom time range. When you select a time range, it carries over when you go from one infrastructure page to another. Change metrics displayed in chart Use the sorting dropdown to switch what metric the chart is displaying for the chosen process groupings. Choices include: Total Utilization % Read Utilization % Write Utilization % Disk Used Bytes Disk Free Bytes I/O Read Operations I/O Write Operations Search and filter the list Type in the Search devices search bar to filter the list. You can also filter the list by simply selecting the list item or user name you want to filter for. The chosen filters are displayed beside the filter icon above the chart. Add/remove chart items The chart, by default, displays the top five results. Use the Chart shows dropdown to display more results. To remove an item, select its name below the chart. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Docker containers tab Use the Docker containers tab to get information about the containers running on your hosts, and to set alerts on container metrics. The Docker containers tab shows data such as CPU percentage, I/O bytes, and memory usage for individual containers or groupings of containers. The page also includes an events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. Use this information to: Identify containers that are experiencing performance issues. Create alerts for conditions such as CPU percentage and memory usage. Docker containers tab functions Here are some of the things you can do from the Docker containers tab: If you want to... Do this... Only show hosts matching certain criteria Use filter sets. Aggregate results or group by host attributes Use Group by to change how the processes are grouped in the chart and the list. The dropdown contains host-related attributes such as image and operatingSystem. Select a time range Use either of these options: Select any of the time picker options at the top of the page. Click and drag on the chart to select a custom time range. After you select a time range, it carries over from one infrastructure page to another. View Docker integration dashboard To open the Docker integration dashboard, click the Dashboard link above the data table. Change charts Select the dropdown beside the chart's name to switch what metric the chart displays. Choices include: CPU: Used cores, kernel percentage Memory: Size limit, cache bytes I/O: Write count per second, total bytes Network: Errors per second, packets Process: Process count, restart count Search and filter the list Use either of these options: Type in the Search processes search bar. Select the list item or user name you want to filter for. The selected filters appear beside the filter icon above the chart, where you can select and remove them as needed. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. View host's alert threshold violation Select the host's Critical icon or Warning icon.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.0693,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>monitoring</em> Hosts page",
        "sections": "<em>Infrastructure</em> <em>monitoring</em> Hosts page",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": "-related charts. Important APM charts in <em>infrastructure</em> <em>monitoring</em> do not have View query or Create alert options like the other <em>infrastructure</em> charts do. For more about using APM and <em>infrastructure</em> <em>monitoring</em> together, see APM data in <em>infrastructure</em>. Network tab The Network page provides real-time"
      },
      "id": "60440a6d196a675f6c960f58"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/infrastructure-events-page-live-feed-every-config-change": [
    {
      "sections": [
        "Events heatmap: Examine patterns in time range"
      ],
      "title": "Events heatmap: Examine patterns in time range",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "bc50e789884c9c4eea404d558d4070519a3eab0c",
      "image": "https://docs.newrelic.com/static/96c3e087c9dfb8b4cb4ad72b79c47e94/c1b63/infra-events-timeline.png",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/infrastructure-ui-pages/events-heatmap-examine-patterns-time-range/",
      "published_at": "2021-05-05T03:31:49Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "page",
      "popularity": 1,
      "body": "he events heatmap provides a snapshot of the infrastructure events occurring within the same time range as the displayed metrics. The darker the color on the heatmap, the more events occurred during that time period. By comparing the heatmap to the charts on the infrastructure page, you can quickly pinpoint issues in your ecosystem. For example, if a massive CPU spike occurs, you can click on the events heatmap for that time range to find the event that caused it. From there you can dive deeper to uncover the real issue. one.newrelic.com > Infrastructure: The heatmap on Infrastructure monitoring UI pages visually shows patterns with events occurring at the same time period for the displayed metrics. The heatmap appears on several Infrastructure UI pages, including: System Network Processes Storage Events",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 186.15826,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": " <em>monitoring</em> <em>UI</em> pages visually shows patterns with events occurring at the same time period for the displayed metrics. The heatmap appears on several <em>Infrastructure</em> <em>UI</em> pages, including: System Network Processes Storage Events"
      },
      "id": "603e8455196a67833da83dc2"
    },
    {
      "sections": [
        "Infrastructure Inventory page: Search your entire infrastructure",
        "Inventory item naming",
        "Tip",
        "Page functions",
        "Filter the data",
        "Search inventory",
        "View inventory item details",
        "View host's alert threshold violations",
        "Inventory data collection",
        "Linux built-in agent data",
        "Windows built-in agent data",
        "Amazon AWS cloud integrations inventory",
        "Inventory data retention",
        "Chart data attributes"
      ],
      "title": "Infrastructure Inventory page: Search your entire infrastructure",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "64aef10b24b74ac3c0f070358d37f3cab099e5b2",
      "image": "https://docs.newrelic.com/static/2d17c192725956ff09b5e987be5b997b/747d8/inventory-name-source-path.jpg",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-ui-pages/infra-ui-pages/infrastructure-inventory-page-search-your-entire-infrastructure/",
      "published_at": "2021-05-05T03:40:44Z",
      "updated_at": "2021-03-11T12:47:47Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic can collect detailed information about a system's configuration per host, including system modules, configuration files, metadata, packages, services, user sessions, etc. The Inventory page provides a real-time, filterable, searchable view into each host's configuration. Use the Inventory page to: Ensure a version update was applied successfully across all your hosts. Audit version discrepancies across your hosts. Quickly identify which hosts require an update to fix a security vulnerability. To view and search your inventory data: Go to one.newrelic.com > Infrastructure > Inventory. Inventory item naming The infrastructure inventory is a qualified namespace (structured like a directory) that organizes inventory items into names that resemble a source path. The inventory item name is comprised of three elements: Element Description Category Basic, top level type of data source, typically based on its role in the system. Common examples include config, package, kernel, user session, services, and modules. Source The specific data source for the inventory item. Label The name of the specific inventory item; for example, the filename, package name, or system setting name. Tip For detailed metadata and other information about your hosts, use tagging with New Relic One. Page functions Use Inventory page functions to find information about a particular item on your hosts: Filter the data Use Filter Sets to show only hosts matching certain criteria. Search inventory Search for an inventory item using the search function. For example, if you want to find information related to OpenSSL, search openssl. The search term is matched again the inventory item name. View inventory item details Inventory item details provide host and system information for each host it resides on according to the New Relic inventory item name. If you have different versions of the same item on other hosts, New Relic detects that and flags them on the Inventory page with the variant hosts label and lists each host running each version. Item details are attributes (key/value pairs) that are dictated by their source. Specific attributes are generally stable over time, but new ones may be added and others could be deprecated. Attributes carry the critical metadata that are at the heart of each inventory item. Common inventory item attributes include: Variant hosts (hostname) Architecture Description Essential Priority Status Version View host's alert threshold violations To view one or more host's alert threshold violations, select the host's Critical icon or Warning icon. Inventory data collection Inventory is collected from the infrastructure agent's built-in data collectors, Amazon Elastic Compute Cloud (EC2) integrations, agent integrations provided by New Relic, and customer-built integrations. The data appears on the Inventory page and in other places within the Infrastructure monitoring's user interface. Linux built-in agent data The infrastructure agent collects this data for Linux systems. Category Source Data collected using... applications apm APM Language Agent metadata config selinux sestatus -b, semodule -l selinux-policies sestatus -b, semodule -l selinux-modules sestatus -b, semodule -l sshd /etc/sshd_config (PermitRootLogin, PermitEmptyPasswords, PasswordAuthentication, and ChallengeResponseAuthentication only) kernel modules /sbin/modinfo, /sbin/lsmod, /proc/modules sysctl /proc/sys metadata agent_config Agent's complete config file attributes Agent's custom_attributes host_aliases Agent's display_name, Cloud provider instance-id, os.Hostname(), hostname -f, hostname cloud_security_groups Cloud provider security-groups system uptime -s, /etc/redhat-release, /proc/cpuinfo, /etc/os-release, /proc/sys/kernel/random/boot_id, /proc/sys/kernel/osrelease, /sys/class/dmi/id/product_uuid, /sys/devices/virtual/dmi/id/sys_vendor, /sys/devices/virtual/dmi/id/product_name facter facter -p -j services daemontool ps -e, svstat systemd initctl list upstart systemctl -l, systemctl show, modinfo, lsmod supervisord /var/run/supervisor.sock unix socket connection, supervisor.getAllProcessInfo pidfile var/run, find -L -name, /proc/N/status, /proc/N/stat sessions users who system network_interfaces net.Interfaces() packages dpkg dpkg-query -W -f rpm rpm -qa Windows built-in agent data The infrastructure agent collects this data for Windows systems. Category Source Data collected using... applications apm APM language agent metadata metadata agent_config Agent's complete config file attributes Agent's custom_attributes host_aliases Agent's display_name, Cloud provider instance-id, os.Hostname(), Registry (SYSTEM \\ CurrentControlSet \\ Services \\ Tcpip \\ Parameters (Domain, DhcpDomain, Hostname) system kernel32.dll (GetPhysicallyInstalledSystemMemory), WMI (Win32_OperatingSystem, Win32_Processor), os.Hostname() services windows_services WMI (Win32_Service WHERE State = \"Running\" AND StartMode = \"Auto\") system network_interfaces net.Interfaces() packages windows_programs Registry (SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\, SOFTWARE\\WOW6432Node\\Microsoft\\Windows\\CurrentVersion\\Uninstall\\) windows_updates WMI (Win32_QuickFixEngineering) (off by default) Amazon AWS cloud integrations inventory Data collected varies by Amazon Elastic Compute Cloud (EC2) integration. For more information, see New Relic's individual Amazon Integrations documentation. Inventory data retention Inventory data is real-time. If a host stops reporting, its inventory data still displays for up to 24 hours. Chart data attributes For a technical explanation about attributes used to populate the Inventory page, see Default infrastructure attributes and events. This includes a summary of common events by operating system.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 184.00931,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> Inventory page: Search your entire <em>infrastructure</em>",
        "sections": "<em>Infrastructure</em> Inventory page: Search your entire <em>infrastructure</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": ", Amazon Elastic Compute Cloud (EC2) integrations, agent integrations provided by New Relic, and customer-built integrations. The data appears on the Inventory page and in other places within the <em>Infrastructure</em> <em>monitoring</em>&#x27;s user interface. Linux built-in agent data The <em>infrastructure</em> agent collects"
      },
      "id": "60440a6d64441fdf50378ee7"
    },
    {
      "sections": [
        "Infrastructure monitoring Hosts page",
        "System tab",
        "System tab functions",
        "APM and infrastructure data",
        "Important",
        "Network tab",
        "Network tab functions",
        "Processes tab",
        "Tip",
        "Processes tab functions",
        "Storage tab",
        "Storage tab functions",
        "Docker containers tab",
        "Docker containers tab functions"
      ],
      "title": "Infrastructure monitoring Hosts page",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring UI",
        "Infrastructure UI"
      ],
      "external_id": "41d8a2ac3ecbbdee164fd0bec6ac94bb3e8def64",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-ui-pages/infrastructure-ui/infrastructure-hosts-page/",
      "published_at": "2021-05-05T00:33:11Z",
      "updated_at": "2021-03-09T04:04:26Z",
      "document_type": "page",
      "popularity": 1,
      "body": "Use the Hosts page to better understand important performance metrics, like resource usage, network and processes performance, and data about your containers. You can filter your hosts by attributes and compare their performance with inventory change events to help determine root causes. You can also set alert notifications to help prevent problems. To view your hosts information, go to one.newrelic.com > Infrastructure > Hosts, then click any of the following tabs: System: Overview of your hosts' performance Network: Bandwidth and error data about your network interfaces Processes: Data about CPU percentage, I/O bytes, and memory usage for individual or groups of processes Storage: Resources' capacity and efficiency, including your devices' overall utilization, disk usage, or I/O operations Docker containers: CPU percentage, I/O bytes, and memory usage for individual or group of containers System tab Here are the default graphs shown on the Hosts page. For additional chart options, select the dropdown for any chart. CPU %: On the System tab, CPU Percent is a derived metric that is part of the SystemSample event. The CPU percentage is not collected by New Relic, but derived from several other metrics. Specifically, the cpuPercent attribute is an aggregation of cpuUserPercent, cpuSystemPercent, cpuIoWaitPercent and cpuStealPercent. Load average five minute: represents an average of the number of tasks waiting to do work on a system over the last 5 minutes. Memory free %: compares the amount of free memory bytes to the amount of used memory bytes. For explanations of different chart metrics, see Default infrastructure attributes and events. Functions for adjusting chart data include: Select different host groups: Use the host filter. Change time range: Use the time picker above the charts, or drag and select a time range on a chart. Change the number of elements on charts: use the Chart shows dropdown. Change data used to order hosts: Use the table below the charts to order the chart elements by different metrics; for example, from highest to lowest CPU user % to lowest, or from lowest to highest Disk free. System tab functions Here are some of the things you can do from the System tab: If you want to... Do this... Filter and group hosts Use filter sets to show only hosts matching certain criteria, or use group by to aggregate the results. Understand host health Use the Health column of the table. To see more details about specific violations, select the health status icons. Find root causes of issues Use the Events heatmap at the top of the page to compare performance to important events happening in your infrastructure. For more, see Events heatmap. Set an alert condition for a metric Mouse over a chart, select and then Create alert. View host's alert threshold violation If present, select the host's Critical icon or Warning icon. APM and infrastructure data If you have APM data associated with your infrastructure data, there are several ways to access your APM data on the Hosts page: Use the hosts filter to filter on hosts running specific applications. In the host list below the charts, select the Application column to filter on specific applications. From the chart selector dropdown beside a chart's name, select one of the application-related charts. Important APM charts in infrastructure monitoring do not have View query or Create alert options like the other infrastructure charts do. For more about using APM and infrastructure monitoring together, see APM data in infrastructure. Network tab The Network page provides real-time visibility into the health and performance of individual hosts, web servers, or other groups of resources across your network. Default charts show bandwidth metrics by packet, bandwidth by bytes, and errors per second. Details about individual interfaces can help you: Examine saturation levels across your network or for selected filter sets. Compare load balances between specific resources. Identify unexpected differences in capacity patterns between similar hosts. Evaluate the top five network errors that New Relic automatically presents for the selected time period. This real-time network data can then help you determine whether to resolve errors by reconfiguring your network, rebalancing the loads, or taking other preventative maintenance actions before needing to make a purchase decision. From the Network tab you can view bandwidth and error data about your network interfaces. The Network page includes an Events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. For a technical explanation of the attributes used to populate the Network page chart, see NetworkSample attributes. Network tab functions Here are some of the things you can do from the Network tab: If you want to... Do this... Filter and group Use filter sets to show only hosts matching certain criteria, or use Group by to aggregate the results. Select a time range Use the time picker to change the range of time selected. You can also click and drag on the chart to select a custom time range. When you select a time range, it carries over when you go from one infrastructure page to another. Change metrics displayed in chart Use the sorting dropdown to switch what metric the chart is displaying for the chosen process groupings, including: Received Bytes Received Dropped Received Errors Received Packets Transmitted Bytes Transmitted Dropped Transmitted Errors Transmitted Packets Search and filter the list Type in the Search interfaces search bar to filter the list to only those items containing the text you've typed. You can also filter the list by simply selecting the list item or user name you want to filter for. The chosen filters are displayed beside the filter icon above the chart. Add items to chart The chart, by default, displays the top five results. Use the Chart shows dropdown to display more results. To remove an item from a chart, select its name below the chart. Set an alert condition for a metric Mouse over a chart, select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Processes tab Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Use the Processes tab to get information about processes running on your hosts, and to set alerts on process metrics. The Processes tab shows data such as CPU percentage, I/O bytes, and memory usage for individual processes or groupings of processes. The page also includes an events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. Use this information to: Pinpoint processes that are causing performance issues. Create alerts for conditions such as CPU percentage and memory usage. On the Processes page, CPU percent is scoped to individual processes, rather than hosts. Because of this, the CPU percent metric does not take into account the resources of the entire system. Instead, it shows how much of a single CPU core each process is taking. Example Here's an example of how to pinpoint an issue and set an alert: You notice on the Hosts page that a cluster has a CPU percentage much higher than normal. You go to the Processes page and filter down to that cluster's processes. You notice that several instances of the same process have excessive CPU percentage. After fixing the issue, you decide to create an alert condition that triggers when the CPU percentage for that process type exceeds a certain threshold. For a technical explanation of the attributes used to populate the Processes page chart, see ProcessSample attributes. Tip You cannot start, stop, or restart processes from the UI. Instead, follow standard procedures to start, stop, check, or restart the infrastructure agent manually. Processes tab functions Here are some of the things you can do from the Processes tab: If you want to... Do this... Only show hosts matching certain criteria Use filter sets. Aggregate results or group by host attributes Use Group by to change how the processes are grouped in the chart and the list. The dropdown contains host-related attributes such as hostID and entityName. If you are monitoring AWS EC2 instances, EC2-related attributes such as awsRegion will be available. Select a time range Use either of these options: Select any of the time picker options at the top of the page. Click and drag on the chart to select a custom time range. After you select a time range, it carries over from one infrastructure page to another. View Docker metrics To view process usage by Docker containers, see Docker instrumentation. Change charts Select the dropdown beside the chart's name to switch what metric the chart displays. Choices include: Memory: Resident or virtual size CPU percentage Disk I/O: Read or write Search and filter the list Use either of these options: Type in the Search processes search bar. Select the list item or user name you want to filter for. The selected filters appear beside the filter icon above the chart, where you can select and remove them as needed. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Storage tab The Storage page allows you to monitor your resources' capacity and efficiency, including your devices' overall utilization, disk usage, or I/O operations. This can help you to: Examine unexpected patterns; for example, a cluster of mirrored machines that do not process I/O tasks uniformly. Monitor usage levels before disk space completely runs out. Set alert conditions to notify you about problems with one or more hosts; for example, long processing times for read or write operations, disk availability or utilization based on percentage or bytes, etc. Make informed decisions about where to redistribute hosts with higher or lower than normal processing requests. Use data to help plan for additions or changes to your infrastructure budget before an emergency arises. The Storage page includes an Events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. For a technical explanation of the attributes used to populate the Storage page chart, see StorageSample attributes. Storage tab functions Here are some of the things you can do from the Storage tab: If you want to... Do this... Filter and group Use filter sets to show only hosts matching certain criteria, or use Group by to aggregate the results. Select a time range Use the time picker on the upper right to change the range of time selected. You can also click and drag on the chart to select a custom time range. When you select a time range, it carries over when you go from one infrastructure page to another. Change metrics displayed in chart Use the sorting dropdown to switch what metric the chart is displaying for the chosen process groupings. Choices include: Total Utilization % Read Utilization % Write Utilization % Disk Used Bytes Disk Free Bytes I/O Read Operations I/O Write Operations Search and filter the list Type in the Search devices search bar to filter the list. You can also filter the list by simply selecting the list item or user name you want to filter for. The chosen filters are displayed beside the filter icon above the chart. Add/remove chart items The chart, by default, displays the top five results. Use the Chart shows dropdown to display more results. To remove an item, select its name below the chart. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. (Not available on APM charts.) View host's alert threshold violation Select the host's Critical icon or Warning icon. Docker containers tab Use the Docker containers tab to get information about the containers running on your hosts, and to set alerts on container metrics. The Docker containers tab shows data such as CPU percentage, I/O bytes, and memory usage for individual containers or groupings of containers. The page also includes an events heatmap, which provides a snapshot of the events occurring within the same time range as the displayed metrics. Use this information to: Identify containers that are experiencing performance issues. Create alerts for conditions such as CPU percentage and memory usage. Docker containers tab functions Here are some of the things you can do from the Docker containers tab: If you want to... Do this... Only show hosts matching certain criteria Use filter sets. Aggregate results or group by host attributes Use Group by to change how the processes are grouped in the chart and the list. The dropdown contains host-related attributes such as image and operatingSystem. Select a time range Use either of these options: Select any of the time picker options at the top of the page. Click and drag on the chart to select a custom time range. After you select a time range, it carries over from one infrastructure page to another. View Docker integration dashboard To open the Docker integration dashboard, click the Dashboard link above the data table. Change charts Select the dropdown beside the chart's name to switch what metric the chart displays. Choices include: CPU: Used cores, kernel percentage Memory: Size limit, cache bytes I/O: Write count per second, total bytes Network: Errors per second, packets Process: Process count, restart count Search and filter the list Use either of these options: Type in the Search processes search bar. Select the list item or user name you want to filter for. The selected filters appear beside the filter icon above the chart, where you can select and remove them as needed. Set an alert condition for a metric Mouse over a chart. Select and then Create alert. View host's alert threshold violation Select the host's Critical icon or Warning icon.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 183.0693,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> <em>monitoring</em> Hosts page",
        "sections": "<em>Infrastructure</em> <em>monitoring</em> Hosts page",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>UI</em>",
        "body": "-related charts. Important APM charts in <em>infrastructure</em> <em>monitoring</em> do not have View query or Create alert options like the other <em>infrastructure</em> charts do. For more about using APM and <em>infrastructure</em> <em>monitoring</em> together, see APM data in <em>infrastructure</em>. Network tab The Network page provides real-time"
      },
      "id": "60440a6d196a675f6c960f58"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/agent-not-starting-there-are-no-logs": [
    {
      "sections": [
        "Infrastructure agent logging behavior",
        "Logging severity levels",
        "Important",
        "Log formatting",
        "Log rotation",
        "Logrotate config file sample",
        "Tip",
        "Smart verbose mode",
        "Logging before Infrastructure agent v1.4.9",
        "Integration log management",
        "Integration STDERR expected format"
      ],
      "title": "Infrastructure agent logging behavior",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot logs"
      ],
      "external_id": "0dc6570e893e47c4d5b5c4232283432926c6476a",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-logs/infrastructure-agent-logging-behavior/",
      "published_at": "2021-05-05T03:30:52Z",
      "updated_at": "2021-03-16T07:31:43Z",
      "document_type": "page",
      "popularity": 1,
      "body": "New Relic's infrastructure agent gathers its own data as well as integrations's logs and consolidates them in a single source. By default, logs appear in standard-output and are added to a log file. To disable logs in standard output, see the agent's config options. Logging severity levels Infrastructure uses a subset of the standard Syslog severity levels: ERROR: Error conditions met WARN: Warning conditions met INFO: Informational messages DEBUG: Contains debug-level messages (useful when troubleshooting) Important DEBUG level is only shown when the verbose mode is enabled. Log formatting For infrastructure agent v1.4.9 or higher, log messages are inlined with context values. This offers better grouping and filtering; for example: containerized agent found in container containerID: VALUE Copy By default, Infrastructure logs are formatted as text: In foreground mode, log output is colored, without a timestamp: DEBUG Sending deltas divided in blocks component=PatchSender mentityKey=ohaimaci mnumberOfBlocks=1 Copy In background mode, logs are timestamped output, used when running as a service or dumping logs to a file: time=\"2019-07-12T09:54:15+02:00\" level=info msg=\"Agent service manager shutdown completed successfully.\" component=AgentService service=newrelic-infra Copy Alternatively, logs can be formatted as a JSON file: {\"context\":{},\"level\":\"info\",\"msg\":\"upstart_interval_sec: 0\",\"timestamp\":\"2019-07-11T18:24:03+02:00\"} {\"context\":{},\"level\":\"info\",\"msg\":\"plugin_dir: \",\"timestamp\":\"2019-07-11T18:24:03+02:00\"} Copy To change the log format, see the agent configuration settings. Log rotation The infrastructure agent does not provide any native log rotation or compression mechanism. Instead, we encourage you to use consolidated log rotation tools, such as the Linux logrotate tool, which is usually installed by default in most Linux distributions. Logrotate can be configured as an entry in /etc/logrotate.conf, or as a file in the /etc/logrotate.d directory. Logrotate config file sample A sample logrotate config file looks like this: /var/log/newrelic-infra/newrelic-infra.log { copytruncate compress daily dateext maxage 7 } Copy Where: /var/log/newrelic-infra/newrelic-infra.log: The Infrastructure agent log file. It must match the log_file configuration parameter in the /etc/newrelic-infra.yml file. copytruncate: Indicates that the log file is truncated but not deleted when it is rotated. This configuration option is mandatory, otherwise the log file will be deleted and won’t be recreated. compress: Compresses (usually in Gzip format) the rotated log files. daily: The agent rotates logs daily. dateext: Appends a date (by default, in the format YYYYMMDD) to the rotated log file (e.g. newrelic-infra.log-20190708.gz) maxage 7: Makes logrotate remove rotated files after 7 days. Tip For a complete description of the logrotate configuration options, see the Linux Logrotate documentation. Since logrotate is usually executed automatically as a cron job, verify that there is a logrotate entry in cron (for example, /etc/cron.daily/logrotate) similar to: #!/bin/sh /usr/sbin/logrotate -s /var/lib/logrotate/logrotate.status /etc/logrotate.conf EXITVALUE=$? if [ $EXITVALUE != 0 ]; then /usr/bin/logger -t logrotate \"ALERT exited abnormally with [$EXITVALUE]\" fi exit 0 Copy Smart verbose mode For infrastructure agent versions 1.9.0 or higher, you can enable smart verbose mode for logs. Smart verbose mode prevents debug messages being logged until an error message is logged. Once an error has been logged, the cached debug messages are logged, but only the most recent number of configured debug messages. For example, if you have a configured limit of 10, after an error is logged, only the 10 most recent debug messages are logged, and older logs are discarded. For more information on how to enable smart verbose mode and the debug message limit, see Infrastructure configuration settings. Logging before Infrastructure agent v1.4.9 Here is a comparison of functionality for Infrastructure agent versions before and after v1.4.9: Agent v1.4.9 and higher Before v1.4.9 Foreground mode logged. The agent couldn't log some entries in foreground mode because the logging service wasn't able to write data until the agent was completely configured. Logs in text and JSON formats. Logs in text only. Logs displayed as inline text. Logs displayed as static literals in a single, decontextualized line. Integration log management Integrations write JSON payloads into STDOUT and plain-text (JSON structured in the future) logs into STDERR. The infrastructure agent handles integration STDERR lines and forward this output into the agent one, usually the service log. Agent handles each STDERR line as follows: when agent runs in verbose mode: it just forwards the full STDERR line as a DEBUG agent log entry placing integration line contexts within the ` msg ` field. otherwise: it parses the line against the expected format (see below) and only logs as agent ERROR level, entries produced by integrations with ` fatal ` or ` error ` severity levels. In this case fields are extracted and forwarded in structured manner (therefore if JSON output is enabled for the agent fields become queryable. Integration STDERR expected format A line is expected to be a list of key-value pairs separated by an equal character. Keys can contain any character, whereas values can have three different formats: string: < quote>any character including escaped quotes \\ \" < quote> map: & { any character} word: any character except spaces Internally agent used this regex to extract the fields: ([^\\s]*?)=(\".*?[^\\\\]\"|&{.*?}|[^\\s]*) Copy For instance, this line: time=\"2015-03-26T01:27:38-04:00\" level=error msg=\"Foo bar baz\" foo=bar Copy Will generate a structured agent log line with these fields: - \"time\": \"2015-03-26T01:27:38-04:00\" - \"level\": \"error\" - \"msg\": \"Foo bar baz\" - \"foo\": \"bar\" Copy",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.29294,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "<em>Infrastructure</em> agent <em>logging</em> behavior",
        "sections": "<em>Infrastructure</em> agent <em>logging</em> behavior",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "New Relic&#x27;s <em>infrastructure</em> agent gathers its own data as well as integrations&#x27;s <em>logs</em> and consolidates them in a single source. By default, <em>logs</em> appear in standard-output and are added to a <em>log</em> file. To disable <em>logs</em> in standard output, see the agent&#x27;s config options. Logging severity levels"
      },
      "id": "603eb3a228ccbc6badeba7a5"
    },
    {
      "sections": [
        "Generate logs for troubleshooting the infrastructure agent",
        "Problem",
        "Important",
        "Solution",
        "Smart verbose mode",
        "Forward the agent logs to New Relic Logs",
        "Notes for specific systems",
        "Containerized agent on CoreOS"
      ],
      "title": "Generate logs for troubleshooting the infrastructure agent",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot logs"
      ],
      "external_id": "a0c2ca22e3fca2b3add8c94d211adffce686661c",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-logs/generate-logs-troubleshooting-infrastructure/",
      "published_at": "2021-05-05T03:31:48Z",
      "updated_at": "2021-03-16T06:35:54Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When troubleshooting your infrastructure agent, generate verbose logs for a few minutes to find and investigate errors. This can be useful for your own troubleshooting or when working with New Relic Support. Important Verbose logging generates a lot of data very quickly. When finished generating logs, be sure to set verbose: 0 to reduce disk space consumption. If you have New Relic infrastructure agent 1.4.0 or higher, you can automate this process by using the newrelic-infra-ctl command. For more information, see the troubleshooting binary documentation. Solution Generating verbose log files requires editing your configuration file. For a sample config file that includes all applicable settings, see the example template. To generate detailed logs: Step Procedures Edit your newrelic-infra.yml file: Enable verbose logging: verbose: 1. (If you use a containerized infrastructure agent on CoreOS, see system-specific notes.) Set log_file to a convenient log file location. Restart the agent so the agent notices the new settings. Let your host run at normal load for about three minutes to generate sufficient logging data. Return your settings to default: Disable verbose logging by setting verbose: 0 in newrelic-infra.yml. Optional: Disable logging to a custom file by removing the log_file line from newrelic-infra.yml. Restart the agent so the agent notices the new settings. Examine the log file for errors. If you need to send your log file to New Relic Support: Include the line in your log file that contains the agent version: New Relic infrastructure agent version X.YY.ZZZ Copy Attach the log file to your support ticket, along with your newrelic-infra.yml. Smart verbose mode Sometimes errors don't occur until after quite some time has passed. This makes debugging difficult, because typically verbose logs are only enabled for a short period time; otherwise there will be many debug logs. For example, if an error occurs one hour after the infrastructure agent has started, getting debug logs around the time of the error can be tricky or impractical. As of infrastructure agent v1.9.0 or higher, you can use smart verbose mode for logs. Smart verbose mode only logs the most recent debug messages after an error has been logged. This allows you to leave smart verbose mode running until an error occurs, without logging lots of irrelevant debug messages, and only logging the most recent debug messages. (The number of messages is determined by your configuration.) For more information on smart verbose mode, see the Infrastructure agent logging behavior docs, and use the Infrastructure configuration settings documentation for details on how to enable smart verbose mode. Forward the agent logs to New Relic Logs The Infrastructure agent can be configured to send its own logs to New Relic Logs. This can be useful for troubleshooting issues with log forwarding, the Infrastructure agent, or when contacting support. For details on how to enable log forwarding for the Infrastructure agent, see Troubleshoot log forwarding. Notes for specific systems These are some additional notes and requirements for specific systems, used to supplement the general logging instructions: Containerized agent on CoreOS If you are using a containerized infrastructure agent on CoreOS: Choose one of these options to change the log level to verbose: Recommended: Set the environment variable NRIA_VERBOSE to 1. Running this on the command line would look like: -e NRIA_VERBOSE=1 Copy OR Edit the config file to set verbose: 1. (Editing the config file in a container is not recommended, because it requires rebuilding the image twice: once to add verbose logging and once to remove it.) Use journalctl to collect the logs: journalctl -u newrelic-infra > newrelic-infra.log Copy Set the verbose logging level back to 0 after collecting logs for a few minutes.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.2742,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Generate <em>logs</em> for <em>troubleshooting</em> the <em>infrastructure</em> agent",
        "sections": "Generate <em>logs</em> for <em>troubleshooting</em> the <em>infrastructure</em> agent",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " verbose mode. Forward the agent <em>logs</em> to New Relic <em>Logs</em> The <em>Infrastructure</em> agent can be configured to send its own <em>logs</em> to New Relic <em>Logs</em>. This can be useful for <em>troubleshooting</em> issues with <em>log</em> forwarding, the <em>Infrastructure</em> agent, or when contacting support. For details on how to enable <em>log</em> forwarding"
      },
      "id": "603e910028ccbc6304eba76d"
    },
    {
      "sections": [
        "Reduce the infrastructure agent's CPU footprint",
        "Problem",
        "Solution",
        "Reduce event sampling",
        "Important",
        "Reduce agent plugin reporting",
        "How to enable and disable plugins",
        "Disable SELinux semodule -l (Linux only)",
        "Reduce or disable Sysctl (Linux only)",
        "Additional plugins to reduce or disable",
        "Review on-host integrations"
      ],
      "title": "Reduce the infrastructure agent's CPU footprint",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "4eea817bfabb6b698ea3ce001b8c5eeca20d475e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint/",
      "published_at": "2021-05-05T03:40:45Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The New Relic infrastructure agent is consuming too much CPU. Solution The New Relic infrastructure agent is designed to report a broad range of system data with minimal CPU and memory consumption. However, if you have a need to reduce your CPU consumption, you can disable or decrease the sampling frequency of various samplers and plugins. This topic highlights some newrelic-infra.yml configurations that may reduce your CPU usage: Reduce event sampling Reduce agent plugin reporting Review on-host integrations Reduce event sampling The infrastructure agent reports several default events at specific frequencies. To lower the overhead, you can reduce the sampling frequency in seconds, or you can completely disable the samplers by setting the corresponding property value to -1. Important We don't recommend a sample rate larger than 60 seconds because you may see gaps in the New Relic user interface charts. The table below lists some samplers to configure: Event Sampling frequency Allow/deny list Network Network sampling rate Not available Process Process sampling rate Allow list (Windows only) Storage Storage sampling rate Deny list System System sampling rate Not available Reduce agent plugin reporting The infrastructure agent has built-in plugins that collect inventory data (specific system configuration and state information). For some systems, the CPU consumption may be relatively high if the plugins are gathering a lot of data. To reduce the footprint, you can disable or decrease the sampling frequency for specific plugins that report data you don’t want. How to enable and disable plugins Disable a single plugin: To disable a plugin, set the corresponding property value to -1. Disable all plugins: disable_all_plugins: true Enable selected plugins: To enable certain plugins, insert an exception in disable_all_plugins. For example, the following configuration disables all plugins, but the Network Interfaces plugin reports every 120 seconds: disable_all_plugins: true network_interface_interval_sec: 120 Copy Disable SELinux semodule -l (Linux only) The SELinux plugin periodically invokes the semodule -l system command to get information about the existing SELinux modules. In most CentOS/RedHat distributions, this command will generate CPU consumption peaks. To disable this functionality, insert the following configuration option in your /etc/newrelic-infra.yml file: selinux_enable_semodule: false Reduce or disable Sysctl (Linux only) The Sysctl plugin walks the whole /sys directory structure and reads values from all the files there. Disabling it or reducing the interval may decrease some CPU System time in the Infrastructure agent. You can disable inventory frequency by setting it to a negative number or reduce the frequeny by setting the sysctl_interval_sec configuration value to the number of seconds between consecutive executions of the plugin. For example, to execute the plugin once every 10 minutes: sysctl_interval_sec: 600 Copy To disable the Sysctl plugin: sysctl_interval_sec: -1 Copy The current default value for the sysctl_interval_sec property is 60. Additional plugins to reduce or disable The following inventory plugins are not especially CPU consuming, but you can still reduce their frequency or disable them by setting the corresponding configuration options. Linux plugins For configuration of these Linux plugins, see Plugin variables: Cloud Security Groups Daemon Tools DPKG Facter Kernel Modules Network interfaces RPM SELinux Supervisord Sysctl Systemd SysV Upstart Users SSHD configuration Windows plugins For configuration of these Windows plugins, see Plugin variables: Network interfaces Windows services Windows updates Review on-host integrations If you use infrastructure on-host integrations, this may have additional impacts on CPU usage. The nature of the impact and the methods to adjust the impact depend on the integration you're using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the monitoring load by adding additional infrastructure agents. For example, the Kafka integration allows a multi-agent deployment.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 133.18256,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "sections": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " on the integration you&#x27;re using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the <em>monitoring</em> load by adding additional <em>infrastructure</em> agents. For example, the Kafka integration allows a multi-agent deployment."
      },
      "id": "603eb9dc64441fbf1f4e8847"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/apm-data-missing-infrastructure": [
    {
      "sections": [
        "Reduce the infrastructure agent's CPU footprint",
        "Problem",
        "Solution",
        "Reduce event sampling",
        "Important",
        "Reduce agent plugin reporting",
        "How to enable and disable plugins",
        "Disable SELinux semodule -l (Linux only)",
        "Reduce or disable Sysctl (Linux only)",
        "Additional plugins to reduce or disable",
        "Review on-host integrations"
      ],
      "title": "Reduce the infrastructure agent's CPU footprint",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "4eea817bfabb6b698ea3ce001b8c5eeca20d475e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint/",
      "published_at": "2021-05-05T03:40:45Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The New Relic infrastructure agent is consuming too much CPU. Solution The New Relic infrastructure agent is designed to report a broad range of system data with minimal CPU and memory consumption. However, if you have a need to reduce your CPU consumption, you can disable or decrease the sampling frequency of various samplers and plugins. This topic highlights some newrelic-infra.yml configurations that may reduce your CPU usage: Reduce event sampling Reduce agent plugin reporting Review on-host integrations Reduce event sampling The infrastructure agent reports several default events at specific frequencies. To lower the overhead, you can reduce the sampling frequency in seconds, or you can completely disable the samplers by setting the corresponding property value to -1. Important We don't recommend a sample rate larger than 60 seconds because you may see gaps in the New Relic user interface charts. The table below lists some samplers to configure: Event Sampling frequency Allow/deny list Network Network sampling rate Not available Process Process sampling rate Allow list (Windows only) Storage Storage sampling rate Deny list System System sampling rate Not available Reduce agent plugin reporting The infrastructure agent has built-in plugins that collect inventory data (specific system configuration and state information). For some systems, the CPU consumption may be relatively high if the plugins are gathering a lot of data. To reduce the footprint, you can disable or decrease the sampling frequency for specific plugins that report data you don’t want. How to enable and disable plugins Disable a single plugin: To disable a plugin, set the corresponding property value to -1. Disable all plugins: disable_all_plugins: true Enable selected plugins: To enable certain plugins, insert an exception in disable_all_plugins. For example, the following configuration disables all plugins, but the Network Interfaces plugin reports every 120 seconds: disable_all_plugins: true network_interface_interval_sec: 120 Copy Disable SELinux semodule -l (Linux only) The SELinux plugin periodically invokes the semodule -l system command to get information about the existing SELinux modules. In most CentOS/RedHat distributions, this command will generate CPU consumption peaks. To disable this functionality, insert the following configuration option in your /etc/newrelic-infra.yml file: selinux_enable_semodule: false Reduce or disable Sysctl (Linux only) The Sysctl plugin walks the whole /sys directory structure and reads values from all the files there. Disabling it or reducing the interval may decrease some CPU System time in the Infrastructure agent. You can disable inventory frequency by setting it to a negative number or reduce the frequeny by setting the sysctl_interval_sec configuration value to the number of seconds between consecutive executions of the plugin. For example, to execute the plugin once every 10 minutes: sysctl_interval_sec: 600 Copy To disable the Sysctl plugin: sysctl_interval_sec: -1 Copy The current default value for the sysctl_interval_sec property is 60. Additional plugins to reduce or disable The following inventory plugins are not especially CPU consuming, but you can still reduce their frequency or disable them by setting the corresponding configuration options. Linux plugins For configuration of these Linux plugins, see Plugin variables: Cloud Security Groups Daemon Tools DPKG Facter Kernel Modules Network interfaces RPM SELinux Supervisord Sysctl Systemd SysV Upstart Users SSHD configuration Windows plugins For configuration of these Windows plugins, see Plugin variables: Network interfaces Windows services Windows updates Review on-host integrations If you use infrastructure on-host integrations, this may have additional impacts on CPU usage. The nature of the impact and the methods to adjust the impact depend on the integration you're using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the monitoring load by adding additional infrastructure agents. For example, the Kafka integration allows a multi-agent deployment.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.3917,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "sections": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " on the integration you&#x27;re using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the <em>monitoring</em> load by adding additional <em>infrastructure</em> agents. For example, the Kafka integration allows a multi-agent deployment."
      },
      "id": "603eb9dc64441fbf1f4e8847"
    },
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2021-05-05T03:27:27Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the New Relic infrastructure agent and waited a few minutes, but no data appears in the Infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Insights, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: New Relic APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.36986,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Infrastructure</em>)",
        "sections": "No data appears (<em>Infrastructure</em>)",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " the <em>infrastructure</em> agent for the first time, the latency for data appearing in the <em>Infrastructure</em> <em>monitoring</em> UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, <em>monitor</em> the <em>Infrastructure</em> UI for a longer period before"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    },
    {
      "sections": [
        "HTTPS proxy configuration not working",
        "Problem",
        "Solution",
        "Version 1.3.1 or higher",
        "Version 1.0.1002 to 1.2.25",
        "Version 1.0.956 to 1.0.989",
        "Version 1.0.944 or lower"
      ],
      "title": "HTTPS proxy configuration not working",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "1eb071405ac556f5d0f577c999dec6bc0871cbfb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/https-proxy-configuration-missing/",
      "published_at": "2021-05-05T03:41:54Z",
      "updated_at": "2021-03-16T06:36:55Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The infrastructure agent's HTTP/HTTPS proxy configuration is not working. For best proxy results, we recommend using the most recent infrastructure agent. Solution If the proxy configuration for your infrastructure agent isn't working, the problem may be due to proxy precedence. Review the section below for your infrastructure agent version: Version 1.3.1 or higher Infrastructure agent versions 1.3.1 or higher uses the following proxy precedence: Configuration Precedence Windows Linux NRIA_PROXY 1 HTTP/HTTPS HTTP/HTTPS proxy (in newrelic-infra.yml) 2 HTTP/HTTPS HTTP/HTTPS HTTPS_PROXY 3 HTTPS HTTPS HTTP_PROXY 4 HTTP HTTP The following options affect TLS certificate configuration: If you use an HTTPS proxy, you probably need to set one of these: The configuration option proxy_validate_certificates: true The environment variable NRIA_PROXY_VALIDATE_CERTIFICATES=true If you use your own self-signed certificates for the HTTPS proxy, keep the default value proxy_validate_certificates: false Here are some additional factors that affect proxy configuration: The NRIA_PROXY and proxy configuration options enable HTTP or HTTPS depending on the URL scheme. The HTTP_PROXY and HTTPS_PROXY environment variables are ignored if either of the these configuration options is set: NRIA_IGNORE_SYSTEM_PROXY=true ignore_system_proxy: true The certificate validation won't work in Centos 5 systems. Version 1.0.1002 to 1.2.25 For infrastructure agent versions 1.0.1002 to 1.2.25, review the section below based on your configuration: Ignore proxy is false Ignore proxy is true Ignore proxy is false If NRIA_IGNORE_SYSTEM_PROXY=FALSE or ignore_system_proxy=FALSE: Configuration Precedence Windows Linux HTTPS_PROXY 1 HTTP 1 HTTP 1 NRIA_PROXY 2 HTTP 1 HTTP 1 proxy (in newrelic-infra.yml) 3 HTTP 1 HTTP 1 HTTP_PROXY 4 HTTP HTTP 1 HTTPS proxy not supported (redirected to HTTP) Ignore proxy is true If NRIA_IGNORE_SYSTEM_PROXY=TRUE or ignore_system_proxy=TRUE: Configuration Precedence Windows Linux NRIA_PROXY 1 HTTP 1 HTTP 1 proxy (in newrelic-infra.yml) 2 HTTP 1 HTTP 1 1 HTTPS proxy not supported (redirected to HTTP) Version 1.0.956 to 1.0.989 Infrastructure agent versions 1.0.956 to 1.0.989 uses the following proxy precedence: Configuration Precedence Windows Linux HTTPS_PROXY 1 HTTPS HTTP 1 NRIA_PROXY 2 HTTP/HTTPS 2 HTTP 1 proxy (in newrelic-infra.yml) 3 HTTP/HTTPS HTTP 1 HTTP_PROXY 4 HTTP/HTTPS TTP 1 HTTPS proxy not supported (redirected to HTTP) 2 When using a HTTPS proxy with a custom TLS/SSL or self-signed certificate, provide either: The certificate file location in ca_bundle_file The certificates directory path in ca_bundle_dir Version 1.0.944 or lower Infrastructure agent versions 1.0.944 or lower uses the following proxy precedence: Configuration Precedence Windows Linux HTTPS_PROXY 1 HTTP 1 HTTP 1 NRIA_PROXY 2 HTTP 1 HTTP 1 proxy (in newrelic-infra.yml) 3 HTTP 1 HTTP 1 HTTP_PROXY 4 HTTP HTTP 1 HTTPS proxy not supported (redirected to HTTP) 2 When using a HTTPS proxy with a custom TLS/SSL or self-signed certificate, provide either: The certificate file location in ca_bundle_file The certificates directory path in ca_bundle_dir",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.352,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The <em>infrastructure</em> agent&#x27;s HTTP&#x2F;HTTPS proxy configuration is not working. For best proxy results, we recommend using the most recent <em>infrastructure</em> agent. Solution If the proxy configuration for your <em>infrastructure</em> agent isn&#x27;t working, the problem may be due to proxy precedence. Review"
      },
      "id": "603ebc24196a67c28ba83d83"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/https-proxy-configuration-missing": [
    {
      "sections": [
        "Reduce the infrastructure agent's CPU footprint",
        "Problem",
        "Solution",
        "Reduce event sampling",
        "Important",
        "Reduce agent plugin reporting",
        "How to enable and disable plugins",
        "Disable SELinux semodule -l (Linux only)",
        "Reduce or disable Sysctl (Linux only)",
        "Additional plugins to reduce or disable",
        "Review on-host integrations"
      ],
      "title": "Reduce the infrastructure agent's CPU footprint",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "4eea817bfabb6b698ea3ce001b8c5eeca20d475e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint/",
      "published_at": "2021-05-05T03:40:45Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The New Relic infrastructure agent is consuming too much CPU. Solution The New Relic infrastructure agent is designed to report a broad range of system data with minimal CPU and memory consumption. However, if you have a need to reduce your CPU consumption, you can disable or decrease the sampling frequency of various samplers and plugins. This topic highlights some newrelic-infra.yml configurations that may reduce your CPU usage: Reduce event sampling Reduce agent plugin reporting Review on-host integrations Reduce event sampling The infrastructure agent reports several default events at specific frequencies. To lower the overhead, you can reduce the sampling frequency in seconds, or you can completely disable the samplers by setting the corresponding property value to -1. Important We don't recommend a sample rate larger than 60 seconds because you may see gaps in the New Relic user interface charts. The table below lists some samplers to configure: Event Sampling frequency Allow/deny list Network Network sampling rate Not available Process Process sampling rate Allow list (Windows only) Storage Storage sampling rate Deny list System System sampling rate Not available Reduce agent plugin reporting The infrastructure agent has built-in plugins that collect inventory data (specific system configuration and state information). For some systems, the CPU consumption may be relatively high if the plugins are gathering a lot of data. To reduce the footprint, you can disable or decrease the sampling frequency for specific plugins that report data you don’t want. How to enable and disable plugins Disable a single plugin: To disable a plugin, set the corresponding property value to -1. Disable all plugins: disable_all_plugins: true Enable selected plugins: To enable certain plugins, insert an exception in disable_all_plugins. For example, the following configuration disables all plugins, but the Network Interfaces plugin reports every 120 seconds: disable_all_plugins: true network_interface_interval_sec: 120 Copy Disable SELinux semodule -l (Linux only) The SELinux plugin periodically invokes the semodule -l system command to get information about the existing SELinux modules. In most CentOS/RedHat distributions, this command will generate CPU consumption peaks. To disable this functionality, insert the following configuration option in your /etc/newrelic-infra.yml file: selinux_enable_semodule: false Reduce or disable Sysctl (Linux only) The Sysctl plugin walks the whole /sys directory structure and reads values from all the files there. Disabling it or reducing the interval may decrease some CPU System time in the Infrastructure agent. You can disable inventory frequency by setting it to a negative number or reduce the frequeny by setting the sysctl_interval_sec configuration value to the number of seconds between consecutive executions of the plugin. For example, to execute the plugin once every 10 minutes: sysctl_interval_sec: 600 Copy To disable the Sysctl plugin: sysctl_interval_sec: -1 Copy The current default value for the sysctl_interval_sec property is 60. Additional plugins to reduce or disable The following inventory plugins are not especially CPU consuming, but you can still reduce their frequency or disable them by setting the corresponding configuration options. Linux plugins For configuration of these Linux plugins, see Plugin variables: Cloud Security Groups Daemon Tools DPKG Facter Kernel Modules Network interfaces RPM SELinux Supervisord Sysctl Systemd SysV Upstart Users SSHD configuration Windows plugins For configuration of these Windows plugins, see Plugin variables: Network interfaces Windows services Windows updates Review on-host integrations If you use infrastructure on-host integrations, this may have additional impacts on CPU usage. The nature of the impact and the methods to adjust the impact depend on the integration you're using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the monitoring load by adding additional infrastructure agents. For example, the Kafka integration allows a multi-agent deployment.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.3917,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "sections": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " on the integration you&#x27;re using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the <em>monitoring</em> load by adding additional <em>infrastructure</em> agents. For example, the Kafka integration allows a multi-agent deployment."
      },
      "id": "603eb9dc64441fbf1f4e8847"
    },
    {
      "sections": [
        "APM data missing from infrastructure monitoring",
        "Problem",
        "Solution",
        "Restart the app server.",
        "Make sure the hostnames are the same in APM and Infrastructure.",
        "Check for replacement host FQDN recognition problems."
      ],
      "title": "APM data missing from infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "68c926e4922c558a2ab2b0f9557f2fe7973ee0af",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/apm-data-missing-infrastructure/",
      "published_at": "2021-05-05T03:34:17Z",
      "updated_at": "2021-03-16T07:35:33Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When APM and infrastructure agents are installed on the same hosts and use the same New Relic license key, APM data should appear in Infrastructure, and vice versa. If you do not see this APM-Infrastructure linkage, follow these troubleshooting tips. Solution If you completed the APM/Infrastructure integration but do not see New Relic APM data in Infrastructure, try these troubleshooting procedures. Restart the app server. If you have not restarted your APM-monitored application in a few weeks or months, the data streams from Infrastructure and APM may not be linked. Restart your app server. Generate some traffic for your app. Wait a few minutes, and then check for APM data in infrastructure monitoring. Make sure the hostnames are the same in APM and Infrastructure. If the hostnames are different in APM and infrastructure monitoring, New Relic cannot integrate the data. One common cause for this issue is that the default hostnames are different. For example, infrastructure monitoring uses a host's FQDN (such as myhost1.example.com), while APM uses the host's name (such as myhost1). Go to one[.newrelic.com](http://one.newrelic.com) > APM > (select an app). From the app's APM Overview page, look at the app's associated host name. Compare that name with the same host's name in infrastructure monitoring. If the names are different, either set the APM agent host's display_name to match its FQDN, or set the host's display_name in Infrastructure to match the one set in APM. Check for replacement host FQDN recognition problems. If the APM-Infrastructure integration previously worked but has stopped, the server may have been replaced by another server that has the same FQDN. If both servers existed simultaneously for a period of time, New Relic cannot automatically recognize the new server. That will break the connection between APM and infrastructure data. To solve this problem, get help at support.newrelic.com. To prevent this problem, make sure there is a time gap between taking down an old server going down and creating a new server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.37169,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "sections": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem When APM and <em>infrastructure</em> agents are installed on the same hosts and use the same New Relic license key, APM data should appear in <em>Infrastructure</em>, and vice versa. If you do not see this APM-<em>Infrastructure</em> linkage, follow these <em>troubleshooting</em> tips. Solution If you completed the APM"
      },
      "id": "603e9100e7b9d2b2962a07e8"
    },
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2021-05-05T03:27:27Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the New Relic infrastructure agent and waited a few minutes, but no data appears in the Infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Insights, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: New Relic APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.36986,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Infrastructure</em>)",
        "sections": "No data appears (<em>Infrastructure</em>)",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " the <em>infrastructure</em> agent for the first time, the latency for data appearing in the <em>Infrastructure</em> <em>monitoring</em> UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, <em>monitor</em> the <em>Infrastructure</em> UI for a longer period before"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/incorrect-host-name-reported": [
    {
      "sections": [
        "Reduce the infrastructure agent's CPU footprint",
        "Problem",
        "Solution",
        "Reduce event sampling",
        "Important",
        "Reduce agent plugin reporting",
        "How to enable and disable plugins",
        "Disable SELinux semodule -l (Linux only)",
        "Reduce or disable Sysctl (Linux only)",
        "Additional plugins to reduce or disable",
        "Review on-host integrations"
      ],
      "title": "Reduce the infrastructure agent's CPU footprint",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "4eea817bfabb6b698ea3ce001b8c5eeca20d475e",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint/",
      "published_at": "2021-05-05T03:40:45Z",
      "updated_at": "2021-03-16T08:35:01Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The New Relic infrastructure agent is consuming too much CPU. Solution The New Relic infrastructure agent is designed to report a broad range of system data with minimal CPU and memory consumption. However, if you have a need to reduce your CPU consumption, you can disable or decrease the sampling frequency of various samplers and plugins. This topic highlights some newrelic-infra.yml configurations that may reduce your CPU usage: Reduce event sampling Reduce agent plugin reporting Review on-host integrations Reduce event sampling The infrastructure agent reports several default events at specific frequencies. To lower the overhead, you can reduce the sampling frequency in seconds, or you can completely disable the samplers by setting the corresponding property value to -1. Important We don't recommend a sample rate larger than 60 seconds because you may see gaps in the New Relic user interface charts. The table below lists some samplers to configure: Event Sampling frequency Allow/deny list Network Network sampling rate Not available Process Process sampling rate Allow list (Windows only) Storage Storage sampling rate Deny list System System sampling rate Not available Reduce agent plugin reporting The infrastructure agent has built-in plugins that collect inventory data (specific system configuration and state information). For some systems, the CPU consumption may be relatively high if the plugins are gathering a lot of data. To reduce the footprint, you can disable or decrease the sampling frequency for specific plugins that report data you don’t want. How to enable and disable plugins Disable a single plugin: To disable a plugin, set the corresponding property value to -1. Disable all plugins: disable_all_plugins: true Enable selected plugins: To enable certain plugins, insert an exception in disable_all_plugins. For example, the following configuration disables all plugins, but the Network Interfaces plugin reports every 120 seconds: disable_all_plugins: true network_interface_interval_sec: 120 Copy Disable SELinux semodule -l (Linux only) The SELinux plugin periodically invokes the semodule -l system command to get information about the existing SELinux modules. In most CentOS/RedHat distributions, this command will generate CPU consumption peaks. To disable this functionality, insert the following configuration option in your /etc/newrelic-infra.yml file: selinux_enable_semodule: false Reduce or disable Sysctl (Linux only) The Sysctl plugin walks the whole /sys directory structure and reads values from all the files there. Disabling it or reducing the interval may decrease some CPU System time in the Infrastructure agent. You can disable inventory frequency by setting it to a negative number or reduce the frequeny by setting the sysctl_interval_sec configuration value to the number of seconds between consecutive executions of the plugin. For example, to execute the plugin once every 10 minutes: sysctl_interval_sec: 600 Copy To disable the Sysctl plugin: sysctl_interval_sec: -1 Copy The current default value for the sysctl_interval_sec property is 60. Additional plugins to reduce or disable The following inventory plugins are not especially CPU consuming, but you can still reduce their frequency or disable them by setting the corresponding configuration options. Linux plugins For configuration of these Linux plugins, see Plugin variables: Cloud Security Groups Daemon Tools DPKG Facter Kernel Modules Network interfaces RPM SELinux Supervisord Sysctl Systemd SysV Upstart Users SSHD configuration Windows plugins For configuration of these Windows plugins, see Plugin variables: Network interfaces Windows services Windows updates Review on-host integrations If you use infrastructure on-host integrations, this may have additional impacts on CPU usage. The nature of the impact and the methods to adjust the impact depend on the integration you're using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the monitoring load by adding additional infrastructure agents. For example, the Kafka integration allows a multi-agent deployment.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.39168,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "sections": "Reduce the <em>infrastructure</em> agent&#x27;s CPU footprint",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " on the integration you&#x27;re using. Here are some ways to adjust on-host integration CPU usage: See if your integration has configuration options you can adjust. If possible, spread out the <em>monitoring</em> load by adding additional <em>infrastructure</em> agents. For example, the Kafka integration allows a multi-agent deployment."
      },
      "id": "603eb9dc64441fbf1f4e8847"
    },
    {
      "sections": [
        "APM data missing from infrastructure monitoring",
        "Problem",
        "Solution",
        "Restart the app server.",
        "Make sure the hostnames are the same in APM and Infrastructure.",
        "Check for replacement host FQDN recognition problems."
      ],
      "title": "APM data missing from infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "68c926e4922c558a2ab2b0f9557f2fe7973ee0af",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/apm-data-missing-infrastructure/",
      "published_at": "2021-05-05T03:34:17Z",
      "updated_at": "2021-03-16T07:35:33Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When APM and infrastructure agents are installed on the same hosts and use the same New Relic license key, APM data should appear in Infrastructure, and vice versa. If you do not see this APM-Infrastructure linkage, follow these troubleshooting tips. Solution If you completed the APM/Infrastructure integration but do not see New Relic APM data in Infrastructure, try these troubleshooting procedures. Restart the app server. If you have not restarted your APM-monitored application in a few weeks or months, the data streams from Infrastructure and APM may not be linked. Restart your app server. Generate some traffic for your app. Wait a few minutes, and then check for APM data in infrastructure monitoring. Make sure the hostnames are the same in APM and Infrastructure. If the hostnames are different in APM and infrastructure monitoring, New Relic cannot integrate the data. One common cause for this issue is that the default hostnames are different. For example, infrastructure monitoring uses a host's FQDN (such as myhost1.example.com), while APM uses the host's name (such as myhost1). Go to one[.newrelic.com](http://one.newrelic.com) > APM > (select an app). From the app's APM Overview page, look at the app's associated host name. Compare that name with the same host's name in infrastructure monitoring. If the names are different, either set the APM agent host's display_name to match its FQDN, or set the host's display_name in Infrastructure to match the one set in APM. Check for replacement host FQDN recognition problems. If the APM-Infrastructure integration previously worked but has stopped, the server may have been replaced by another server that has the same FQDN. If both servers existed simultaneously for a period of time, New Relic cannot automatically recognize the new server. That will break the connection between APM and infrastructure data. To solve this problem, get help at support.newrelic.com. To prevent this problem, make sure there is a time gap between taking down an old server going down and creating a new server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.37169,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "sections": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem When APM and <em>infrastructure</em> agents are installed on the same hosts and use the same New Relic license key, APM data should appear in <em>Infrastructure</em>, and vice versa. If you do not see this APM-<em>Infrastructure</em> linkage, follow these <em>troubleshooting</em> tips. Solution If you completed the APM"
      },
      "id": "603e9100e7b9d2b2962a07e8"
    },
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2021-05-05T03:27:27Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the New Relic infrastructure agent and waited a few minutes, but no data appears in the Infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Insights, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: New Relic APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.36984,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Infrastructure</em>)",
        "sections": "No data appears (<em>Infrastructure</em>)",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " the <em>infrastructure</em> agent for the first time, the latency for data appearing in the <em>Infrastructure</em> <em>monitoring</em> UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, <em>monitor</em> the <em>Infrastructure</em> UI for a longer period before"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    }
  ],
  "/docs/infrastructure/new-relic-infrastructure/troubleshooting/reduce-infrastructure-agents-cpu-footprint": [
    {
      "sections": [
        "APM data missing from infrastructure monitoring",
        "Problem",
        "Solution",
        "Restart the app server.",
        "Make sure the hostnames are the same in APM and Infrastructure.",
        "Check for replacement host FQDN recognition problems."
      ],
      "title": "APM data missing from infrastructure monitoring",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "68c926e4922c558a2ab2b0f9557f2fe7973ee0af",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/apm-data-missing-infrastructure/",
      "published_at": "2021-05-05T03:34:17Z",
      "updated_at": "2021-03-16T07:35:33Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem When APM and infrastructure agents are installed on the same hosts and use the same New Relic license key, APM data should appear in Infrastructure, and vice versa. If you do not see this APM-Infrastructure linkage, follow these troubleshooting tips. Solution If you completed the APM/Infrastructure integration but do not see New Relic APM data in Infrastructure, try these troubleshooting procedures. Restart the app server. If you have not restarted your APM-monitored application in a few weeks or months, the data streams from Infrastructure and APM may not be linked. Restart your app server. Generate some traffic for your app. Wait a few minutes, and then check for APM data in infrastructure monitoring. Make sure the hostnames are the same in APM and Infrastructure. If the hostnames are different in APM and infrastructure monitoring, New Relic cannot integrate the data. One common cause for this issue is that the default hostnames are different. For example, infrastructure monitoring uses a host's FQDN (such as myhost1.example.com), while APM uses the host's name (such as myhost1). Go to one[.newrelic.com](http://one.newrelic.com) > APM > (select an app). From the app's APM Overview page, look at the app's associated host name. Compare that name with the same host's name in infrastructure monitoring. If the names are different, either set the APM agent host's display_name to match its FQDN, or set the host's display_name in Infrastructure to match the one set in APM. Check for replacement host FQDN recognition problems. If the APM-Infrastructure integration previously worked but has stopped, the server may have been replaced by another server that has the same FQDN. If both servers existed simultaneously for a period of time, New Relic cannot automatically recognize the new server. That will break the connection between APM and infrastructure data. To solve this problem, get help at support.newrelic.com. To prevent this problem, make sure there is a time gap between taking down an old server going down and creating a new server.",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.37169,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "sections": "APM data missing from <em>infrastructure</em> <em>monitoring</em>",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem When APM and <em>infrastructure</em> agents are installed on the same hosts and use the same New Relic license key, APM data should appear in <em>Infrastructure</em>, and vice versa. If you do not see this APM-<em>Infrastructure</em> linkage, follow these <em>troubleshooting</em> tips. Solution If you completed the APM"
      },
      "id": "603e9100e7b9d2b2962a07e8"
    },
    {
      "sections": [
        "No data appears (Infrastructure)",
        "Problem",
        "Solution",
        "Important",
        "Missing infrastructure data",
        "Verify install for apt (Debian or Ubuntu)",
        "Verify install for yum (Amazon Linux, CentOS, or RHEL)",
        "Verify install for Windows Server",
        "Verify status with SystemD",
        "Verify status with System V",
        "Verify status with Upstart",
        "Verify status with Windows",
        "Missing integration data"
      ],
      "title": "No data appears (Infrastructure)",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "fd618376814a1ec7b486c00e524b0203bbfa0e09",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/infrastructure-troubleshooting/troubleshoot-infrastructure/no-data-appears-infrastructure/",
      "published_at": "2021-05-05T03:27:27Z",
      "updated_at": "2021-03-16T07:30:05Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem You installed the New Relic infrastructure agent and waited a few minutes, but no data appears in the Infrastructure UI. Solution Data should appear in the Infrastructure monitoring UI within a few minutes for accounts with previously installed agents. Important For accounts installing the infrastructure agent for the first time, the latency for data appearing in the Infrastructure monitoring UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, monitor the Infrastructure UI for a longer period before contacting support.newrelic.com for assistance. Important By default, the infrastructure agent doesn't send data about the operating system's processes. To enable the sending of process data set enable_process_metrics to true. To fine-tune which processes you want to monitor, configure include_matching_metrics. Missing infrastructure data If no data appears in the UI, try the following steps to diagnose the problem: Use your package manager to verify that the infrastructure agent is installed: Verify install for apt (Debian or Ubuntu) Use dpkg to verify that the agent is installed: dpkg -l | grep newrelic-infra Copy If dpkg returns no output, see Install with apt. Verify install for yum (Amazon Linux, CentOS, or RHEL) Use rpm to verify that agent is installed: rpm -qa | grep newrelic-infra Copy If rpm returns no output, see Install with yum. Verify install for Windows Server Use the Windows command prompt or Powershell to verify that the agent directory exists: dir \"C:\\Program Files\\New Relic\\newrelic-infra\" Copy If you receive a File not found error, see Install for Windows Server. Use your init system to verify that the agent is running: Verify status with SystemD Use SystemD commands with CentOS 7, Debian 8, RHEL 7, and Ubuntu 15.04 or higher: Check that the agent is running: sudo systemctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo systemctl start newrelic-infra Copy Verify status with System V Use System V commands with Debian 7: Check that the agent is running: sudo /etc/init.d/newrelic-infra status Copy If the agent isn't running, start the agent manually: sudo /etc/init.d/newrelic-infra start Copy Verify status with Upstart Use Upstart commands with Amazon Linux, CentOS 6, RHEL 6, and Ubuntu 14.10 or lower: Check that the agent is running: sudo initctl status newrelic-infra Copy If the agent isn't running, start the agent manually: sudo initctl start newrelic-infra Copy Verify status with Windows Use the Windows command prompt: Check that the agent is running: sc query \"newrelic-infra\" | find \"RUNNING\" Copy If the agent isn't running, start the agent manually with the Windows command prompt: net start newrelic-infra Copy If running net start newrelic-infra returns The service name is invalid, the Infrastructure agent may not have been installed correctly and the service was not properly created. To test this: From Powershell, run the command get-service newrelic-infra, which will return the status of the service. If it returns an error Cannot find any service with service name newrelic-infra, then follow standard procedures to reinstall the agent. Use New Relic Diagnostics to try to automatically identify the issue. Verify that your newrelic-infra.yml configuration file contains a valid license_key setting. Verify that the host has a unique hostname, and verify that the hostname is not localhost. For more information, see this Explorers Hub post. Verify that no firewalls or proxies are blocking outbound connections from the agent process to the Infrastructure domains and ports. Confirm the host is reporting correctly even though it is not appearing in the Infrastructure monitoring UI by creating a basic query in Insights, like: SELECT * FROM SystemSample SINCE 60 minutes ago LIMIT 100 Copy Use the query results to note the timestamps, which show when the data was reported. To determine when data was first received, look at the earliest timestamp. Generate verbose logs and examine the logs for errors. Missing integration data If you are missing data from an integration, see troubleshooting procedures for: New Relic APM data missing from infrastructure monitoring Amazon/AWS integrations On-host integrations",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.36984,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "title": "No data appears (<em>Infrastructure</em>)",
        "sections": "No data appears (<em>Infrastructure</em>)",
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": " the <em>infrastructure</em> agent for the first time, the latency for data appearing in the <em>Infrastructure</em> <em>monitoring</em> UI can be tens of minutes. If the following steps verify the installation and no obvious error conditions appear in the verbose logs, <em>monitor</em> the <em>Infrastructure</em> UI for a longer period before"
      },
      "id": "603e90b9e7b9d26d8c2a07a9"
    },
    {
      "sections": [
        "HTTPS proxy configuration not working",
        "Problem",
        "Solution",
        "Version 1.3.1 or higher",
        "Version 1.0.1002 to 1.2.25",
        "Version 1.0.956 to 1.0.989",
        "Version 1.0.944 or lower"
      ],
      "title": "HTTPS proxy configuration not working",
      "type": "docs",
      "tags": [
        "Infrastructure",
        "Infrastructure monitoring troubleshooting",
        "Troubleshoot infrastructure"
      ],
      "external_id": "1eb071405ac556f5d0f577c999dec6bc0871cbfb",
      "image": "",
      "url": "https://docs.newrelic.com/docs/infrastructure/new-relic-infrastructure/troubleshooting/https-proxy-configuration-missing/",
      "published_at": "2021-05-05T03:41:54Z",
      "updated_at": "2021-03-16T06:36:55Z",
      "document_type": "troubleshooting_doc",
      "popularity": 1,
      "body": "Problem The infrastructure agent's HTTP/HTTPS proxy configuration is not working. For best proxy results, we recommend using the most recent infrastructure agent. Solution If the proxy configuration for your infrastructure agent isn't working, the problem may be due to proxy precedence. Review the section below for your infrastructure agent version: Version 1.3.1 or higher Infrastructure agent versions 1.3.1 or higher uses the following proxy precedence: Configuration Precedence Windows Linux NRIA_PROXY 1 HTTP/HTTPS HTTP/HTTPS proxy (in newrelic-infra.yml) 2 HTTP/HTTPS HTTP/HTTPS HTTPS_PROXY 3 HTTPS HTTPS HTTP_PROXY 4 HTTP HTTP The following options affect TLS certificate configuration: If you use an HTTPS proxy, you probably need to set one of these: The configuration option proxy_validate_certificates: true The environment variable NRIA_PROXY_VALIDATE_CERTIFICATES=true If you use your own self-signed certificates for the HTTPS proxy, keep the default value proxy_validate_certificates: false Here are some additional factors that affect proxy configuration: The NRIA_PROXY and proxy configuration options enable HTTP or HTTPS depending on the URL scheme. The HTTP_PROXY and HTTPS_PROXY environment variables are ignored if either of the these configuration options is set: NRIA_IGNORE_SYSTEM_PROXY=true ignore_system_proxy: true The certificate validation won't work in Centos 5 systems. Version 1.0.1002 to 1.2.25 For infrastructure agent versions 1.0.1002 to 1.2.25, review the section below based on your configuration: Ignore proxy is false Ignore proxy is true Ignore proxy is false If NRIA_IGNORE_SYSTEM_PROXY=FALSE or ignore_system_proxy=FALSE: Configuration Precedence Windows Linux HTTPS_PROXY 1 HTTP 1 HTTP 1 NRIA_PROXY 2 HTTP 1 HTTP 1 proxy (in newrelic-infra.yml) 3 HTTP 1 HTTP 1 HTTP_PROXY 4 HTTP HTTP 1 HTTPS proxy not supported (redirected to HTTP) Ignore proxy is true If NRIA_IGNORE_SYSTEM_PROXY=TRUE or ignore_system_proxy=TRUE: Configuration Precedence Windows Linux NRIA_PROXY 1 HTTP 1 HTTP 1 proxy (in newrelic-infra.yml) 2 HTTP 1 HTTP 1 1 HTTPS proxy not supported (redirected to HTTP) Version 1.0.956 to 1.0.989 Infrastructure agent versions 1.0.956 to 1.0.989 uses the following proxy precedence: Configuration Precedence Windows Linux HTTPS_PROXY 1 HTTPS HTTP 1 NRIA_PROXY 2 HTTP/HTTPS 2 HTTP 1 proxy (in newrelic-infra.yml) 3 HTTP/HTTPS HTTP 1 HTTP_PROXY 4 HTTP/HTTPS TTP 1 HTTPS proxy not supported (redirected to HTTP) 2 When using a HTTPS proxy with a custom TLS/SSL or self-signed certificate, provide either: The certificate file location in ca_bundle_file The certificates directory path in ca_bundle_dir Version 1.0.944 or lower Infrastructure agent versions 1.0.944 or lower uses the following proxy precedence: Configuration Precedence Windows Linux HTTPS_PROXY 1 HTTP 1 HTTP 1 NRIA_PROXY 2 HTTP 1 HTTP 1 proxy (in newrelic-infra.yml) 3 HTTP 1 HTTP 1 HTTP_PROXY 4 HTTP HTTP 1 HTTPS proxy not supported (redirected to HTTP) 2 When using a HTTPS proxy with a custom TLS/SSL or self-signed certificate, provide either: The certificate file location in ca_bundle_file The certificates directory path in ca_bundle_dir",
      "info": "",
      "_index": "520d1d5d14cc8a32e600034b",
      "_type": "520d1d5d14cc8a32e600034c",
      "_score": 187.352,
      "_version": null,
      "_explanation": null,
      "sort": null,
      "highlight": {
        "tags": "<em>Infrastructure</em> <em>monitoring</em> <em>troubleshooting</em>",
        "body": "Problem The <em>infrastructure</em> agent&#x27;s HTTP&#x2F;HTTPS proxy configuration is not working. For best proxy results, we recommend using the most recent <em>infrastructure</em> agent. Solution If the proxy configuration for your <em>infrastructure</em> agent isn&#x27;t working, the problem may be due to proxy precedence. Review"
      },
      "id": "603ebc24196a67c28ba83d83"
    }
  ]
}